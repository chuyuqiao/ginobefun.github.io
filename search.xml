<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[一个简易的Elasticsearch动态同义词插件]]></title>
      <url>%2Fpost%2Felasticsearch_dynamic_synonym_plugin%2F</url>
      <content type="text"><![CDATA[Elasticsearch自带了一个synonym同义词插件，但是该插件只能使用文件或在分析器中静态地配置同义词，如果需要添加或修改，需要修改配置文件和重启，使用方式不够友好。通过学习Elasticsearch的synonym代码，自研了一个可动态维护同义词的插件，并以运用于生产环境，供大家参考。 Elasticsearch自带的SynonymTokenFilterElasticsearch自带的同义词过滤器支持在分析器配置（使用synonyms参数）和文件中配置（使用synonyms_path参数）同义词，配置方式如下： { &quot;index&quot; : { &quot;analysis&quot; : { &quot;analyzer&quot; : { &quot;synonym_analyzer&quot; : { &quot;tokenizer&quot; : &quot;whitespace&quot;, &quot;filter&quot; : [&quot;my_synonym&quot;] } }, &quot;filter&quot; : { &quot;my_synonym&quot; : { &quot;type&quot; : &quot;synonym&quot;, &quot;expand&quot;: true, &quot;ignore_case&quot;: true, &quot;synonyms_path&quot; : &quot;analysis/synonym.txt&quot; &quot;synonyms&quot; : [&quot;阿迪, 阿迪达斯, adidasi =&gt; Adidas&quot;,&quot;Nike, 耐克, naike&quot;] } } } } } 在配置同义词规则时有Solr synonyms和WordNet synonyms，一般我们使用的都是Solr synonyms。在配置时又存在映射和对等两种方式，区别如下： // 精确映射同义词，【阿迪】、【阿迪达斯】和【adidasi】的token将会转换为【Adidas】存入倒排索引中 阿迪, 阿迪达斯, adidasi =&gt; Adidas // 对等同义词 // 当expand为true时，当出现以下任何一个token，三个token都会存入倒排索引中 // 当expand为false时，当出现以下任何一个token，第一个token也就是【Nike】会存入倒排索引中 Nike, 耐克, naike DynamicSynonymTokenFilter实现方式 DynamicSynonymTokenFilter参考了SynonymTokenFilter的方式，但又予以简化，使用一个HashMap来保存同义词之间的转换关系； DynamicSynonymTokenFilter只支持Solr synonyms，同时也支持expand和ignore_case参数的配置； DynamicSynonymTokenFilter通过数据库来管理同义词的配置，并轮询数据库（通过version字段判断是否存在规则变化）实现同义词的动态管理； 安装1.下载插件源码 git clone git@github.com:ginobefun/elasticsearch-dynamic-synonym.git 2.使用maven编译插件 mvn clean install -DskipTests 3.在ES_HOME/plugin目录新建dynamic-synonym目录，并将target/releases/elasticsearch-dynamic-synonym-VERSION.zip文件解压到该目录 4.在MySQL中创建Elasticsearch同义词数据库并创建用户 create database elasticsearch; DROP TABLE IF EXISTS `dynamic_synonym_rule`; CREATE TABLE `dynamic_synonym_rule` ( `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT, `rule` varchar(255) NOT NULL, `status` tinyint(1) NOT NULL DEFAULT &apos;1&apos; COMMENT &apos;1: available, 0:unavailable&apos;, `version` int(11) NOT NULL, PRIMARY KEY (`id`), KEY `IDX_DYNAMIC_SYNONYM_VERSION` (`version`), KEY `IDX_DYNAMIC_SYNONYM_RULE` (`rule`) ) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=utf8; -- ---------------------------- -- insert sample records -- ---------------------------- INSERT INTO `dynamic_synonym_rule` VALUES (&apos;1&apos;, &apos;阿迪, 阿迪达斯, adidasi =&gt; Adidas&apos;, &apos;1&apos;, &apos;1&apos;); INSERT INTO `dynamic_synonym_rule` VALUES (&apos;2&apos;, &apos;Nike, 耐克, naike&apos;, &apos;1&apos;, &apos;2&apos;); 5.重启Elasticsearch 配置在Elasticsearch的elasticsearch.yml文件或在API创建索引时配置分析器和过滤器： index: analysis: filter: my_synonym: type: dynamic-synonym expand: true ignore_case: true tokenizer: whitespace db_url: jdbc:mysql://localhost:3306/elasticsearch?user=test_user&amp;password=test_pwd&amp;useUnicode=true&amp;characterEncoding=UTF8 analyzer: analyzer_with_dynamic_synonym: type: custom tokenizer: whitespace filter: [&quot;my_synonym&quot;] 使用测试分析器效果【阿迪】 http://localhost:9200/test/_analyze?analyzer=analyzer_with_dynamic_synonym&amp;text=阿迪 { &quot;tokens&quot;: [ { &quot;token&quot;: &quot;adidas&quot;, &quot;start_offset&quot;: 0, &quot;end_offset&quot;: 2, &quot;type&quot;: &quot;SYNONYM&quot;, &quot;position&quot;: 0 } ] } 测试分析器效果【耐克】 http://localhost:9200/test/_analyze?analyzer=analyzer_with_dynamic_synonym&amp;text=耐克 { &quot;tokens&quot;: [ { &quot;token&quot;: &quot;nike&quot;, &quot;start_offset&quot;: 0, &quot;end_offset&quot;: 2, &quot;type&quot;: &quot;SYNONYM&quot;, &quot;position&quot;: 0 }, { &quot;token&quot;: &quot;耐克&quot;, &quot;start_offset&quot;: 0, &quot;end_offset&quot;: 2, &quot;type&quot;: &quot;SYNONYM&quot;, &quot;position&quot;: 1 }, { &quot;token&quot;: &quot;naike&quot;, &quot;start_offset&quot;: 0, &quot;end_offset&quot;: 2, &quot;type&quot;: &quot;SYNONYM&quot;, &quot;position&quot;: 2 } ] } 往数据库中插入一条同义词，测试【范斯】 INSERT INTO `dynamic_synonym_rule` VALUES (&apos;3&apos;, &apos;Vans, 范斯&apos;, &apos;1&apos;, &apos;3&apos;); // wait for 2 minutes to reload [2017-03-15 15:52:28,895][INFO ][node ] [node-local] started [2017-03-15 15:55:29,645][INFO ][dynamic-synonym ] Start to reload synonym rule... [2017-03-15 15:55:29,661][INFO ][dynamic-synonym ] Succeed to reload 3 synonym rule! http://localhost:9200/test/_analyze?analyzer=analyzer_with_dynamic_synonym&amp;text=范斯 { &quot;tokens&quot;: [ { &quot;token&quot;: &quot;vans&quot;, &quot;start_offset&quot;: 0, &quot;end_offset&quot;: 2, &quot;type&quot;: &quot;SYNONYM&quot;, &quot;position&quot;: 0 }, { &quot;token&quot;: &quot;范斯&quot;, &quot;start_offset&quot;: 0, &quot;end_offset&quot;: 2, &quot;type&quot;: &quot;SYNONYM&quot;, &quot;position&quot;: 1 } ] } 总结与后续改进 通过学习Elasticsearch源码自己实现了一个简易版的同义词插件，通过同义词的配置可以实现同义词规则的增删改的动态更新； 需要注意的是，同义词的动态更新存在一个很重要的问题是原本在索引中已存在的数据不受同义词更新动态的影响，因此在使用时需要考虑是否可以容忍该问题，一个通常的做法是在某个时刻集中管理同义词，更新后执行索引重建动作； 另外该插件目前存在一个问题，就是同义词的映射关系在内存中是一个全局数据，因此如果有多个不同的同义词过滤器则会存在问题，代码初始化时以第一个成功初始化的过滤器生成的映射关系为准，这个后续版本考虑改进。 参考资料 Using Synonyms Synonym Token Filter]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[阅读随手记 201703]]></title>
      <url>%2Fpost%2Freading_record_201703%2F</url>
      <content type="text"><![CDATA[关键字：微服务, 架构, 消息中间件, 缓存, RPC, 监控, 高性能, 高并发, 高可用。 Microservices: Decomposing Applications for Deployability and Scalability Chris RichardsonPattern: Microservice Architecture Chris RichardsonIntroduction to Microservices Chris RichardsonBuilding Microservices: Using an API Gateway Chris RichardsonBuilding Microservices: Inter-Process Communication in a Microservices Architecture Chris RichardsonService Discovery in a Microservices Architecture Chris RichardsonEvent-Driven Data Management for Microservices Chris RichardsonChoosing a Microservices Deployment Strategy Chris RichardsonRefactoring a Monolith into Microservices Chris RichardsonFault Tolerance in a High Volume, Distributed System Ben ChristensenJDK1.8 AbstractQueuedSynchronizer的实现分析 刘锟洋从LONGADDER看更高效的无锁实现 jd刘锟洋RxJava2实例解析 Victor Grazi/薛命灯Rays]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[word2vec学习小记]]></title>
      <url>%2Fpost%2Flearning_word2vec%2F</url>
      <content type="text"><![CDATA[word2vec是Google于2013年开源推出的一个用于获取词向量的工具包，它简单、高效，因此引起了很多人的关注。最近项目组使用word2vec来实现个性化搜索，在阅读资料的过程中做了一些笔记，用于后面进一步学习。 前注：word2vec涉及的相关理论和推导是非常严(ku)格(zao)的，本文作为一个初学者的学习笔记，希望能从自己的理解中尽量用简单的描述，如有错误或者歧义的地方，欢迎指正。 word2vec是什么？ This tool provides an efficient implementation of the continuous bag-of-words and skip-gram architectures for computing vector representations of words. These representations can be subsequently used in many natural language processing applications and for further research. 从官方的介绍可以看出word2vec是一个将词表示为一个向量的工具，通过该向量表示，可以用来进行更深入的自然语言处理，比如机器翻译等。 为了理解word2vec的设计思想，我们有必要先学习一下自然语言处理的相关发展历程和基础知识。 基础知识语言模型 语言模型其实就是看一句话是不是正常人说出来的。这玩意很有用，比如机器翻译、语音识别得到若干候选之后，可以利用语言模型挑一个尽量靠谱的结果。在 NLP 的其它任务里也都能用到。 用数学表达的话，就是给定T个词w1,w2,…wT，看它是自然语言的概率P，公式如下所示： 举例来说，比如一段语音识别为“我喜欢吃梨”和“我喜欢吃力”，根据分词和上述公式，可以得到两种表述的概率计算分别为下面的公式，而其中每一个子项的概率我们可以事先通过大量的语料统计得到，这样我们就可以得到更好的识别效果。 P(&apos;我喜欢吃梨&apos;) = P(&apos;我&apos;) * P(&apos;喜欢&apos;|&apos;我&apos;) * P(&apos;吃&apos;|&apos;我&apos;,&apos;喜欢&apos;) * P(&apos;梨&apos;|&apos;我&apos;,&apos;喜欢&apos;,&apos;吃&apos;) P(&apos;我喜欢吃力&apos;) = P(&apos;我&apos;) * P(&apos;喜欢&apos;|&apos;我&apos;) * P(&apos;吃力&apos;|&apos;我&apos;,&apos;喜欢&apos;) N-gram模型 通过上面的语言模型计算的例子，大家可以发现，如果一个句子比较长，那么它的计算量会很大； 牛逼的科学家们想出了一个N-gram模型来简化计算，在计算某一项的概率时Context不是考虑前面所有的词，而是前N-1个词； 当然牛逼的科学家们还在此模型上继续优化，比如N-pos模型从语法的角度出发，先对词进行词性标注分类，在此基础上来计算模型的概率；后面还有一些针对性的语言模型改进，这里就不一一介绍。 通过上面简短的语言模型介绍，我们可以看出核心的计算在于P(wi|Contenti)，对于其的计算主要有两种思路：一种是基于统计的思路，另外一种是通过函数拟合的思路；前者比较容易理解但是实际运用的时候有一些问题（比如如果组合在语料里没出现导致对应的条件概率变为0等），而函数拟合的思路就是通过语料的输入训练出一个函数P(wi|Contexti) = f(wi,Contexti;θ)，这样对于测试数据就直接套用函数计算概率即可，这也是机器学习中惯用的思路之一。 词向量表示 自然语言理解的问题要转化为机器学习的问题，第一步肯定是要找一种方法把这些符号数学化。 最直观的就是把每个词表示为一个很长的向量。这个向量的维度是词表的大小，其中绝大多数元素为0，只有一个维度的值为1，这个维度就代表了当前的词。这种表示方式被称为One-hot Representation。这种方式的优点在于简洁，但是却无法描述词与词之间的关系。 另外一种表示方法是通过一个低维的向量（通常为50维、100维或200维），其基于“具有相似上下文的词，应该具有相似的语义”的假说，这种表示方式被称为Distributed Representation。它是一个稠密、低维的实数向量，它的每一维表示词语的一个潜在特征，该特征捕获了有用的句法和语义特征。其特点是将词语的不同句法和语义特征分布到它的每一个维度上去表示。这种方式的好处是可以通过空间距离或者余弦夹角来描述词与词之间的相似性。 以下我们来举个例子看看两者的区别： // One-hot Representation 向量的维度是词表的大小，比如有10w个词，该向量的维度就是10w v(&apos;足球&apos;) = [0 1 0 0 0 0 0 ......] v(&apos;篮球&apos;) = [0 0 0 0 0 1 0 ......] // Distributed Representation 向量的维度是某个具体的值如50 v(&apos;足球&apos;) = [0.26 0.49 -0.54 -0.08 0.16 0.76 0.33 ......] v(&apos;篮球&apos;) = [0.31 0.54 -0.48 -0.01 0.28 0.94 0.38 ......] 最后需要说明的是一个词的向量表示对于不同的语料和场景结果是不同的。下面就介绍一种最常用的计算词向量的语言模型。 神经网络概率语言模型 神经网络概率语言模型（NNLM）把词向量作为输入（初始的词向量是随机值），训练语言模型的同时也在训练词向量，最终可以同时得到语言模型和词向量。 Bengio等牛逼的科学家们用了一个三层的神经网络来构建语言模型，同样也是N-gram 模型。 网络的第一层是输入层，是是上下文的N-1个向量组成的(n-1)m维向量；第二层是隐藏层，使用tanh作为激活函数；第三层是输出层，每个节点表示一个词的未归一化概率，最后使用softmax激活函数将输出值归一化。 得到这个模型，然后就可以利用梯度下降法把模型优化出来，最终得到语言模型和词向量表示。 word2vec的核心模型 word2vec在NNLM和其他语言模型的基础进行了优化，有CBOW模型和Skip-Gram模型，还有Hierarchical Softmax和Negative Sampling两个降低复杂度的近似方法，两两组合出四种实现。 无论是哪种模型，其基本网络结构都是在下图的基础上，省略掉了隐藏层； CBOW和Skip-gram模型 CBOW（Continuous Bag-of-Words Model）是一种根据上下文的词语预测当前词语的出现概率的模型，其图示如上图左。CBOW是已知上下文，估算当前词语的语言模型； 而Skip-gram只是逆转了CBOW的因果关系而已，即已知当前词语，预测上下文，其图示如上图右； Hierarchical Softmax使用的办法其实是借助了分类的概念。假设我们是把所有的词都作为输出，那么“足球”、“篮球”都是混在一起的。而Hierarchical Softmax则是把这些词按照类别进行区分的，二叉树上的每一个节点可以看作是一个使用哈夫曼编码构造的二分类器。在算法的实现中，模型会赋予这些抽象的中间节点一个合适的向量，真正的词会共用这些向量。这种近似的处理会显著带来性能上的提升同时又不会丢失很大的准确性。 Negative Sampling也是用二分类近似多分类，区别在于它会采样一些负例，调整模型参数使得可以区分正例和负例。换一个角度来看，就是Negative Sampling有点懒，它不想把分母中的所有词都算一次，就稍微选几个算算。 这一部分的模型实现比较复杂，网上也有很多资料可以参考，感兴趣的可以读读这两篇：word2vec原理推导与代码分析和深度学习word2vec笔记之算法篇。 通过实践了解word2vec学习了上面的基础知识之后，我们就通过一个例子来感受一下word2vec的效果。 下载语料从搜狗实验室下载全网新闻数据(SogouCA)，该语料来自若干新闻站点2012年6月—7月期间国内、国际、体育、社会、娱乐等18个频道的新闻数据。 下载该文件解压后大约为1.5G，包含120w条以上的新闻，文件的内容格式如下图所示： 获取新闻内容这里我们先对语料进行初步处理，只获取新闻内容部分。可以执行以下命令获取content部分： cat news_tensite_xml.dat | iconv -f gbk -t utf-8 -c | grep &quot;&lt;content&gt;&quot; &gt; corpus.txt 分词处理由于word2vec处理的数据是单词分隔的语句，对于中文来说，需要先进行分词处理。这里采用的是中国自然语言处理开源组织开源的ansj_seg分词器，核心代码如下所示： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public class WordAnalyzer &#123; private static final String TAG_START_CONTENT = "&lt;content&gt;"; private static final String TAG_END_CONTENT = "&lt;/content&gt;"; private static final String INPUT_FILE = "/home/test/w2v/corpus.txt"; private static final String OUTPUT_FILE = "/home/test/w2v/corpus_out.txt"; public static void main(String[] args) throws Exception &#123; BufferedReader reader = null; PrintWriter pw = null; try &#123; System.out.println("开始处理分词..."); reader = IOUtil.getReader(INPUT_FILE, "UTF-8"); pw = new PrintWriter(OUTPUT_FILE); long start = System.currentTimeMillis(); int totalCharactorLength = 0; int totalTermCount = 0; Set&lt;String&gt; set = new HashSet&lt;String&gt;(); String temp = null; while ((temp = reader.readLine()) != null) &#123; temp = temp.trim(); if (temp.startsWith(TAG_START_CONTENT)) &#123; //System.out.println("处理文本:" + temp); int end = temp.indexOf(TAG_END_CONTENT); String content = temp.substring(TAG_START_CONTENT.length(), end); totalCharactorLength += content.length(); Result result = ToAnalysis.parse(content); for (Term term : result) &#123; String item = term.getName().trim(); totalTermCount++; pw.print(item + " "); set.add(item); &#125; pw.println(); &#125; &#125; long end = System.currentTimeMillis(); System.out.println("共" + totalTermCount + "个Term，共" + set.size() + "个不同的Term，共 " + totalCharactorLength + "个字符，每秒处理字符数:" + (totalCharactorLength * 1000.0 / (end - start))); &#125; finally &#123; // close reader and pw &#125; &#125;&#125; 分词处理之后的文件内容如下所示： 下载word2vec源码并编译这里我没有从官网下载而是从github上的svn2github/word2vec项目下载源码，下载之后执行make命令编译，这个过程很快就可以结束。 开始word2vec处理编译成功后开始处理。我这里用的是CentOS 64位的虚拟机，八核CPU，32G内存，整个处理过程耗时大约4个小时。 ./word2vec -train ../corpus_out.txt -output vectors.bin -cbow 0 -size 200 -window 5 -negative 0 -hs 1 -sample 1e-3 -threads 12 -binary 1 // 参数解释 -train 训练数据 -output 结果输入文件，即每个词的向量 -cbow 是否使用cbow模型，0表示使用skip-gram模型，1表示使用cbow模型，默认情况下是skip-gram模型，cbow模型快一些，skip-gram模型效果好一些 -size 表示输出的词向量维数 -window 为训练的窗口大小，5表示每个词考虑前5个词与后5个词（实际代码中还有一个随机选窗口的过程，窗口大小&lt;=5) -negative 表示是否使用负例采样方法0表示不使用，其它的值目前还不是很清楚 -hs 是否使用Hierarchical Softmax方法，0表示不使用，1表示使用 -sample 表示采样的阈值，如果一个词在训练样本中出现的频率越大，那么就越会被采样 -binary 表示输出的结果文件是否采用二进制存储，0表示不使用（即普通的文本存储，可以打开查看），1表示使用，即vectors.bin的存储类型 测试处理结果处理结束之后，使用distance命令可以测试处理结果，以下是分别测试【足球】和【改革】的效果： 学习小结 word2vec的模型是基于神经网络来训练词向量的工具； word2vec通过一系列的模型和框架对原有的NNLM进行优化，简化了计算但准确度还是保持得很好； word2vec的主要的应用还是自然语言的处理，通过训练出来的词向量，可以进行聚类等处理，或者作为其他深入学习的输入。另外，word2vec还适用于一些时序数据的挖掘，比如用户商品的浏览分析、用户APP的下载等，通过这些数据的分析，可以得到商品或者APP的向量表示，从而用于个性化搜索和推荐。 参考材料 word2vec HomePage Efficient Estimation of Word Representations in Vector Space Deep Learning in NLP （一）词向量和语言模型 word2vec 中的数学原理详解 利用word2vec对关键词进行聚类 word2vec词向量训练及中文文本相似度计算 词表示模型（一）：表示学习；syntagmatic与paradigmatic两类模型；基于矩阵的LSA和GloVe 词表示模型（二）：基于神经网络的模型：NPLM；word2vec（CBOW/Skip-gram） 词表示模型（三）：word2vec（CBOW/Skip-gram）的加速：Hierarchical Softmax与Negative Sampling 深度学习word2vec笔记之基础篇 Google 开源项目 word2vec 的分析？_杨超的回答 Word2Vec-知其然知其所以然 word2vec 原理篇 word2vec原理推导与代码分析]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[阅读随手记 201702]]></title>
      <url>%2Fpost%2Freading_record_201702%2F</url>
      <content type="text"><![CDATA[关键字：微服务, 分布式, 配置中心, Java编程, 推荐系统, 运维, 高并发, 高可用, 机器学习, 深度学习。 Microservices: A definition of this new architectural term Martin Fowler微服务架构风格 一组微型的服务/独立开发/通过轻量级机制通信/自动化的独立部署/各服务之间可采用不同语言和技术方案； 很难给微服务下一个具体的定义，Martin Fowler通过九大特性来阐述微服务； the microservice architectural style is an approach to developing a single application as a suite of small services, each running in its own process and communicating with lightweight mechanisms, often an HTTP resource API. These services are built around business capabilities and independently deployable by fully automated deployment machinery. There is a bare minimum of centralized management of these services, which may be written in different programming languages and use different data storage technologies. 微服务的九大特性1. 组件化与服务 组件（component）是一个可以独立更换和升级的软件单元； 软件库）（libraries是能被链接到程序且通过方法调用的组件； 服务（services）是进程外的组件，通过web service或rpc的机制来通信； 使用服务而非软件库的方式来组件化的主要原因是服务可被独立部署（避免一个组件修改导致需要重新部署整个应用），另外一个好处是能获得更加显式的组件接口（想想单体应用中的API依赖）； 当然也有不足之处，比如远程调用比进程内的调用要更加昂贵； 2. 围绕业务功能组织团队 对一个大型的应用进行分解时，通常是按照技术层面来进行划分，比如分为前端开发、后端开发和数据库开发（当然实际情况比这更细），但是这样会带来一个问题就是一个很小的需求都需要跨团队的项目合作和进度安排；这就是康威定律（任何设计系统的组织，都会产生这样一个设计，即该设计的结构与该组织的沟通结构相一致）起作用的例子； 而微服务是通过业务功能将系统分解为若干服务，这些服务针对该业务领域提供多层次广泛的软件实现；因此团队是跨职能的，它拥有软件开发所需的全方位的技能（如用户体验、数据库和项目管理）； 大型单体应用系统也可以根据业务功能进行模块化设计，但是通常的问题是一个团队会包含太多的功能，而团队之间的边界也不够清晰； 3. 做产品而非做项目 大部分的应用开发都使用这样一个产品模型：一旦某项软件功能已交付，就会将软件移交给维护团队，而开发团队随之被解散； 而微服务主张“一个团队应该拥有该产品的整个生命周期”（原子亚马逊的“you build, you run it”）这样的理念，即一个开发团队对一个生产环境下运行的软件负全责； 这样的产品理念是与业务功能相绑定的，它不会把软件开成是一系列待完成功能的集合，而是一种持续提升客户业务功能的关系； 当然，单体应用也可以采用上述的产品理念，但是更细粒度的服务能使服务的开发者和它的用户更近； 4. 智能终端和傻瓜管道 在不同的进程进行通信时，多数的产品或方法会在其中加入大量的智能特性，有个典型的例子就是ESB（企业服务总线），它通常包括消息路由、编制、转换和业务规则应用； 而微服务主张采用另一种做法：智能终端（smart endpoints）和傻瓜管道（dumb pipes）。这里的理解是应该尽可能地简化进程间的通信，将一些需要智能处理的逻辑（比如路由、重试等）交给服务处理； 微服务最常用的两种协议是：带有资源API的HTTP请求-响应协议和轻量级的消息发送协议（如RabbitMQ、ZeroMQ）； 将一个单体应用拆分为微服务的最大挑战就是改变原有的通信模式，如果直接将原先的进程内方法调用改为RPC会导致微服务直接产生繁琐的通信，因此应该考虑更粗粒度的方式调用； 5. 去中心化的治理 集中治理的一个问题是会趋向于在单一技术平台上制定标准从而带来局限性； 微服务中的分权治理（去中心化的）使得每个服务可以选择不同的技术，比如选择不同的语言和数据库； 相比于选用一组已定义好的标准，微服务的开发者更喜欢自己编写一些有用的工具，这些工具通常源于他们的微服务实施过程并分享给更多的团队；比如Nteflix就是一个很好的例子，这家提供网络视频点播的公司开源了一系列的实施微服务的工具； 对微服务社区来说，像容错读取（Tolerant Reader）和消费者驱动的契约（Consumer-Driven Contracts）的模式已经被用于日常管理； 像Netflix这样的公司已经开始推行分权治理，这样可以令程序员更加注重质量（谁都不想半夜被电话叫去修复问题对吧），而这些与之前传统的集中治理有着天壤之别； 6. 去中心化的数据管理 去中心化的数据管理从最抽象的层面看，意味着各个系统对客观世界所构建的概念模型将彼此不同，比如客户的概念对于销售和支撑团队就有所不同； 思考这类问题的一个有用的方法就是使用领域驱动设计（Domain-Driven Design, DDD）中的上下文边界（Bounded Context）的概念；DDD将一个复杂的领域划分为多个上下文边界，并且将它们的相互关系用图表示出来； 不同于单体应用喜欢共用一个单独的数据库（也许是被商业数据库的license逼出来的），微服务更喜欢让每一个服务管理其自有的数据库，可以采用相同数据库技术的不同实例，也可以采用完全不同的数据库系统； 但是去中心化的数据管理会带来数据一致性的问题，对此微服务强调的是无事务的协调（transactionless coordination between services），这源自微服务社区明确地认识到以下两点：即数据一致性可能只要求数据最终一致性，并且一致性问题能够通过补偿操作来进行处理； 7. 基础设施自动化 云的发展已经很大程度上降低了构建、部署和运维微服务的复杂性了； 许多使用微服务构建的团队都具备持续交付（Continuous Delivery）的经验，这样的团队广泛采用了基础设施自动化的技术，如下图的构建流水线所示： 持续交付需要大量的自动化测试以及自动化部署的能力，通过这两个关键特点使得我们能更有信心、更愉快地部署功能到生成环境；另外，微服务的独立性也使得部署更加容易（这里的容易是指只需要部署修改的服务而不需要部署整个应用），当然也会带来困难（原来只需要部署一个系统，而现在需要部署更多的服务），此时就需要在部署工具上投入精力改进； 8. 容错设计 使用微服务作为组件，需要设计成能容忍这些服务所出现的故障；一旦某个服务出现故障，其他任何对该服务的调用都会出现故障，客户端需要尽可能优雅地处理这种情况；与单体应用相比，这是微服务引入的额外的复杂性； Netflix公司开源的Simian Army能够诱导服务发生故障来测试应用的弹性和监控能力； 断路器（Circuit Breaker）是一种用于隔离故障的模式，Netflix公司的这篇很精彩的博客解释了这些模式是如何应用的； 容错设计要求能够快速地检测出故障，而且在可能的情况下自动恢复服务；此时实时监控就变得非常重要，它可用来检查架构元素指标（比如数据库每秒接收到多少请求）和业务相关指标（如系统每分钟收到多少订单）以便预测故障的发生；这对于微服务来说尤为重要，因为微服务对于服务编排和事件协作的偏好更易导致突发行为； 采用微服务的团队通常希望使用仪表盘或者日志记录装置来监控服务的运行和各项指标； 9. 演进式设计 每当试图将软件系统拆分为各个组件时，都会面对一个棘手的问题，即如何拆分，拆分的原则是什么？ 一个组件的关键属性，是具有独立可替换性和可升级性（independent replacement and upgradeability），这使得我们在重写一个组件时更多的是聚焦于功能而非与不用担心与其他的关联组件； 而事实上，许多做微服务的团队会更进一步，他们明确地预期许多服务将来会报废而不是长期演进；比如英国卫报网站，他们依然使用原先的单体应用作为网站核心，而对于一些新的功能，比如增加报道一个体育赛事的页面，就会采用微服务的方式来添加，一旦赛事结束了，这个服务就可以被废除； 这种强调可更换性的特点是模块化设计一般性原则的一个特例，通过变化模式（the pattern of change）来驱动进行模块化的实现；将那些能在同时发生变化的东西放到相同的模块中，如果发现需要同时反复变更两个服务是，这就是它们两个需要被合并的一个信号； 将一个个组件放入一个个服务中增加了做出更精细化版本发布的机会，对于单体应用，任何变化都需要做一次整个应用系统的全量构建和部署，而对微服务来说，只需要部署修改的服务即可； 微服务是未来的方向吗？ 作者通过该文阐述了微服务的主要思路和原则，在当时已经有一些公司如亚马逊和Netflix提供了正面的经验，收到了不少正面的评价； 架构决策所产生的真正效果通常需要若干年后才能真正显现，当时考虑的限制微服务的因素主要有组件拆分的难度、组件直接的复杂关联关系以及团队技能，但从目前的发展来看，已经有越来越多的企业采用了微服务的架构； Netflix Conductor：一个微服务编制引擎 Abel Avram/杨雷 Netflix开发了一个叫Conductor的编制引擎，已经在内部生产环境中使用了一年了。在这段时间里，Netflix已经运行了大约260万个处理工作流，包括简单的线性工作流，以及运行数天的动态工作流。 主要特性：能够构建复杂工作流；能够通过微服务执行任务；使用JSON DSL描述的工作流蓝图；执行过程可见、可跟踪；能够暂停、恢复、重启、停止任务；任务执行通常是异步的，也可以强制同步执行；处理工作流能够扩展到百万级别； Conductor的架构图如下所示；其中API和存储层都是可插拔的，允许使用不同的队列和存储引擎；工作流中的任务分为两种类型：Worker（运行在远端机器上的用户任务和System（运行在引擎的JVM上的任务），后者是用来对Worker执行任务进行branch、fork、join；Worker任务通过HTTP或者gRPC和Conductor通信； 深度学习并不是在“模拟人脑” 周志华 当特征信息和样本信息不充分时，机器学习可能就帮不上忙； 用机器学习解决问题更多的时候像一个裁缝，一定要量体裁衣，针对某个问题专门设计有效的方法，这样才能得到一个更好的结果；按需设计、度身定制，是在做机器学习应用的时候特别重要的一点； 机器学习有着深厚的理论基础，其中最基本的理论模型叫做概率近似正确模型；机器学习做的事情，是你给我数据之后，希望能够以很高的概率给出一个好模型； 从生物机理来说的话，一个神经元收到很多其它神经元发来的电位信号，信号经过放大到达它这里，如果这个累积信号比它自己的电位高了，那这个神经元就被激活了；其实神经网络本质上，是一个简单函数通过多层嵌套叠加形成的一个数学模型，背后其实是数学和工程在做支撑；而神经生理学起的作用，可以说是给了一点点启发，但是远远不像现在很多人说的神经网络研究受到神经生理学的“指导”，或者是“模拟脑”； 深度学习火起来的3个因素：有了大量的训练数据、有很强的计算设备、使用大量的“窍门”（Trick）； 分布式配置管理平台的设计与实现 架构文摘 分布式配置平台的一些应用场景：对某些配置的更新，不想要重启应用，并且能近似实时生效；希望将配置进行统一管理，而非放入各应用的配置文件中； 分布式配置平台需要满足的一些基本特性：高可用性（服务器集群应该无单点故障）、容错性（主要针对客户端，应保证即便在配置平台不可用时，也不影响客户端的正常运行）、高性能、可靠的存储、近似实时生效、负载均衡、扩展性； 分布式配置平台Diablo的设计与实现：对等的服务器集群（Server被视为是对等的，没有主从关系）、高性能处理（客户端应用获取配置时，仅会从本地缓存中获取，开发人员在控制台更改配置后，会通知客户端刷新缓冲）、使用Redis作存储、重试等待（diablo会通过重试等待等机制保证，在服务端集群不可用时，也不会影响客户端应用的正常运行，而是等待集群恢复）、请求负载（使用一致性哈希分配客户端连接是Server）、配置更新实时生效（diablo使用了特殊Pull模式，即长轮询）； 一篇好TM长的关于配置中心的文章 坤宇 每一个分布式系统都应该有一个动态配置管理系统 配置与环境：某个配置项，其具体的值域的定义往往跟具体的环境相关联，现实中相当一部分配置在不同的环境必须设定不同的值，但是也有相当的另一部分配置在不同的环境要设定为完全一致的值。配置管理系统应该做的是提供方便的交互方式保证这两种不同的一致性诉求同时得到很好的满足，这种诉求分为3个方面，如下示意图: 另外，一个配置中心也应该具备的能力是配置集的导出\导入功能，可以让应用将A环境中的配置集方便的导出和导入到环境B中的能力； 业界最新动态：Apache Commons Configuration（太繁琐）、owner（简单易上手，特性看起来很多，但是在很多关键常用的特性反倒是没有）、cfg4j（简单易上手，cfg4j 支持跟多种后端集成，做配置中心的解决方案，api设计也非常的不错）、Spring Framework、Spring Cloud Config Server； 配置(configuration)与元数据(metadata)：配置的修改基本上都是由人来驱动，并且在ops上实现变更；而元数据的本质是一小段程序元数据，它很多时候是程序产生，程序消费，由程序通过调用Diamond的客户端api来实现变更，中间不会有ops 或者人的介入。Diamond 不光是应用配置存储，其目前存储的数据，很大一部分是metadata，所以Diamond 其实也是一个元数据存储中心。 Reddit是如何使用Memcached来存储3TB缓存数据的？ 薛命灯Reddit的缓存规模和基本策略Reddit目前使用了54个规格为r3.2xlarge的AWS EC2实例，每个实例拥有61GB内存，也就是说总的缓存大小差不多是3.3TB。Reddit的缓存包含了多种类型的数据，包括数据库对象、查询结果集、函数调用，还有一些看起来不太像缓存的东西，比如限定速率、分布式锁等等。 如何管理这么大规模的缓存是一件很挑战性的事情，Reddit采用的是“不要把所有鸡蛋放在同一个篮子里”的基本策略。也就是说，他们并不是把3.3TB的内存看成一个总的大缓存池，而是按照负载类型对缓存进行分类，每种类型占用一定数量的缓存空间。 Reddit的缓存类型 数据库对象缓存（thing-cache）：Reddit最大的缓存池。这些对象是无schema的，开发人员可以很容易地对这些对象添加新属性，而无需对数据库schema进行变更。这些对象包括用户评论、链接和账户等等。该类型缓存是Reddit最繁忙也最有用的缓存，命中率高达99%。 主缓存（cache-main）：主缓存是Reddit第二大缓存池。这个缓存是一般性的缓存，里面存放的所有用来展示/r/all的结果集。 渲染缓存（cache-render）：第三大缓存用来存放渲染过的页面模板或页面片段。这个缓存相对安全，就算发生失效，也不会对系统造成太大影响。它的命中率只有大概50%左右，毕竟页面信息需要不断更新，所以渲染过的页面模板或片段也需要更新。 持久缓存（cache-perma）：它的命中率超过了99%。这个缓存用来存放数据库的查询结果，还有用户评论和链接。为什么管这个缓存叫持久缓存，因为他们使用了读-改-写（read-modify-write）的模式。例如，在用户新增一个评论时，他们会同时更新缓存和后端的数据库（Cassandra），而不是简单地让缓存失效，这样就避免了需要再次从数据库加载数据。 非缓存对象池：除了上述的几种缓存，Reddit还使用了速率限定和分布式锁。 mcrouter mcrouter是由Facebook开源的Memcached连接池。就像访问数据库要使用数据库连接池一样，使用连接池可以对连接进行重用和管理，避免了重复创建和销毁连接的开销。 mcrouter提供了多种路由类型，比如PrefixSelectorRoute，它通过匹配key的前缀来决定应该到哪个缓存上获取数据。这样就可以把特定功能的操作路由到特定的缓存上。 如果要往缓存集群里增加新的缓存实例，那么可以使用WarmUpRoute。WarmUpRoute的工作原理是说，把所有写操作路由到“冷”缓存上，而把未命中的读操作路由到“热”缓存上，然后把在“热”缓存上命中的缓存结果异步地更新到“冷”缓存上，那么下次同样的读操作就也可以在“冷”缓存上命中。 mcrouter还提供了FailoverRoute，顾名思义，这个特性可以避免缓存的单点故障； Reddit还使用了影子缓存，不同于WarmUpRoute，它会把读操作和写操作都拷贝一份到新的实例上，但前提是不改变数据源。 自定义监控 基于Memcached的“stats slabs”命令自己写了一个追踪板块度量指标的工具，他们还开发了一个简陋的可视化仪表盘； Reddit团队还开发了另外一个工具，叫作mcsauna。这个工具被部署在每个缓存服务器上，它可以检测网络流量，并根据配置规则把不同的key保存在不同的bucket里，然后把结果输出到文件上。FilesCollector会收集这些文件，分析里面的key，并以图形化的方式呈现出来。从这些图形上可以看出那些热点的key。 展望缓存为提升网站的响应速度做出了不可磨灭的贡献。而在如何使用缓存方面，Reddit还有很长的路要走。接下来，他们可能要想着如何通过服务发现来对配置进行自动化，从而实现缓存的自动扩展，而不需要人工的介入。而随着Memcached版本的不断改进，他们也要针对现有系统进行调整，从而最大化缓存的性能。 微信高并发资金交易系统设计方案 方乐明 微信红包的两大业务特点：微信红包业务比普通商品“秒杀”有更海量的并发要求、微信红包业务要求更严格的安全级别； 微信红包系统的技术难点：事务级操作量级大、事务性要求严格； 解决高并发问题常用方案：使用内存操作替代实时的DB事务操作（用内存操作替代磁盘操作，提高了并发性能，但是DB持久化可能会丢数据）、使用乐观锁替代悲观锁（可以提高DB的并发处理能力，但是回滚失败带来很差的用户体验）； 微信红包系统的高并发解决方案系统垂直SET化，分而治之红包系统根据微信红包ID，按一定的规则（如按ID尾号取模等），垂直上下切分。切分后，一个垂直链条上的逻辑Server服务器、DB统称为一个SET。各个SET之间相互独立，互相解耦。并且同一个红包ID的所有请求，包括发红包、抢红包、拆红包、查详情详情等，垂直stick到同一个SET内处理，高度内聚。通过这样的方式，系统将所有红包请求这个巨大的洪流分散为多股小流，互不影响，分而治之。 逻辑Server层将请求排队，解决DB并发问题。如果到达DB的事务操作不是并发的，而是串行的，就不会存在“并发抢锁”的问题了。按这个思路，为了使拆红包的事务操作串行地进入DB，只需要将请求在Server层以FIFO的方式排队，就可以达到这个效果。从而问题就集中到Server的FIFO队列设计上。 微信红包系统设计了分布式的、轻巧的、灵活的FIFO队列方案。其具体实现如下： 将同一个红包ID的所有请求stick到同一台Server； 设计单机请求排队方案：将stick到同一台Server上的所有请求在被接收进程接收后，按红包ID进行排队。然后串行地进入worker进程（执行业务逻辑）进行处理，从而达到排队的效果； 增加memcached控制并发：利用memcached的CAS原子累增操作，控制同时进入DB执行拆红包事务的请求数，超过预先设定数值则直接拒绝服务。用于DB负载升高时的降级体验； 双维度库表设计，保障系统性能稳定处理微信红包数据的冷热分离时，系统在以红包ID维度分库表的基础上，增加了以循环天分表的维度，形成了双维度分库表的特色。具体来说，就是分库表规则像db_xx.t_y_dd设计，其中，xx/y是红包ID的hash值后三位，dd的取值范围在01~31，代表一个月天数最多31天。通过这种双维度分库表方式，解决了DB单表数据量膨胀导致性能下降的问题，保障了系统性能的稳定性。同时，在热冷分离的问题上，又使得数据搬迁变得简单而优雅。 分布式系统理论基础 - 一致性、2PC和3PC bangerlee 狭义的分布式系统指由网络连接的计算机系统，每个节点独立地承担计算或存储任务，节点间通过网络协同工作。广义的分布式系统是一个相对的概念，正如Leslie Lamport所说： What is a distributed systeme. Distribution is in the eye of the beholder.To the user sitting at the keyboard, his IBM personal computer is a nondistributed system.To a flea crawling around on the circuit board, or to the engineer who designed it, it’s very much a distributed system. 一致性是分布式理论中的根本性问题；何为一致性问题？简单而言，一致性问题就是相互独立的节点之间如何达成一项决议的问题。分布式系统中，进行数据库事务提交(commit transaction)、Leader选举、序列号生成等都会遇到一致性问题。 假设一个具有N个节点的分布式系统，当其满足以下条件时，我们说这个系统满足一致性：全认同(agreement)、值合法(validity)、可结束(termination)； 分布式系统实现起来并不轻松，因为它面临着这些问题：消息传递异步无序(asynchronous)、节点宕机(fail-stop)、节点宕机恢复(fail-recover)、网络分化(network partition)、拜占庭将军问题(byzantine failure)； 一致性还具备两个属性，一个是强一致(safety)，它要求所有节点状态一致、共进退；一个是可用(liveness)，它要求分布式系统24*7无间断对外服务。FLP定理(FLP impossibility)已经证明在一个收窄的模型中(异步环境并只存在节点宕机)，不能同时满足safety和liveness。FLP定理是分布式系统理论中的基础理论，正如物理学中的能量守恒定律彻底否定了永动机的存在，FLP定理否定了同时满足safety和liveness的一致性协议的存在。 2PC2PC（tow phase commit，两阶段提交）顾名思义它分成两个阶段，先由一方进行提议(propose)并收集其他节点的反馈(vote)，再根据反馈决定提交(commit)或中止(abort)事务。我们将提议的节点称为协调者(coordinator)，其他参与决议节点称为参与者(participants, 或cohorts)。 在阶段一中，coordinator发起一个提议，分别问询各participant是否接受。 在阶段二中，coordinator根据participant的反馈，提交或中止事务，如果participant全部同意则提交，只要有一个participant不同意就中止。 在异步环境(asynchronous)并且没有节点宕机(fail-stop)的模型下，2PC可以满足全认同、值合法、可结束，是解决一致性问题的一种协议。但如果再加上节点宕机(fail-recover)的考虑，就要求 coordinator/participant 记录历史状态，以备coordinator宕机后watchdog对participant查询、coordinator宕机恢复后重新找回状态； 3PC在2PC中一个participant的状态只有它自己和coordinator知晓，假如coordinator提议后自身宕机，在watchdog启用前一个participant又宕机，其他participant就会进入既不能回滚、又不能强制commit的阻塞状态，直到participant宕机恢复。这引出两个疑问： 能不能去掉阻塞，使系统可以在commit/abort前回滚(rollback)到决议发起前的初始状态； 当次决议中，participant间能不能相互知道对方的状态，又或者participant间根本不依赖对方的状态； 相比2PC，3PC增加了一个准备提交(prepare to commit)阶段来解决以上问题。coordinator接收完participant的反馈(vote)之后，进入阶段2，给各个participant发送准备提交(prepare to commit)指令。participant接到准备提交指令后可以锁资源，但要求相关操作必须可回滚。coordinator接收完确认(ACK)后进入阶段3、进行commit/abort，3PC的阶段3与2PC的阶段2无异。协调者备份(coordinator watchdog)、状态记录(logging)同样应用在3PC。 因为有了准备提交(prepare to commit)阶段，3PC的事务处理延时也增加了1个RTT，变为3个RTT(propose+precommit+commit)，但是它防止participant宕机后整个系统进入阻塞态，增强了系统的可用性，对一些现实业务场景是非常值得的。 分布式系统理论基础 - 选举、多数派和租约 bangerlee 选举(election)是分布式系统实践中常见的问题，通过打破节点间的对等关系，选得的leader(或叫master、coordinator)有助于实现事务原子性、提升决议效率。多数派(quorum)的思路帮助我们在网络分化的情况下达成决议一致性，在leader选举的场景下帮助我们选出唯一leader。租约(lease)在一定期限内给予节点特定权利，也可以用于实现leader选举。 选举(electioin)：一致性问题(consistency)是独立的节点间如何达成决议的问题，选出大家都认可的leader本质上也是一致性问题；Bully算法是最常见的选举算法，其要求每个节点对应一个序号，序号最高的节点为leader，leader宕机后次高序号的节点被重选为leader；Bully算法中有2PC的身影，都具有提议(propose)和收集反馈(vote)的过程；在一致性算法Paxos、ZAB、Raft中，为提升决议效率均有节点充当leader的角色； 多数派(quorum)：在网络分化的场景下以上Bully算法会遇到一个问题，被分隔的节点都认为自己具有最大的序号、将产生多个leader；多数派的思路在分布式系统中很常见，其确保网络分化情况下决议唯一；多数派的原理说起来很简单，假如节点总数为2f+1，则一项决议得到多于 f 节点赞成则获得通过。leader选举中，网络分化场景下只有具备多数派节点的部分才可能选出leader，这避免了多leader的产生； 租约(lease)：选举中很重要的一个问题，怎么判断leader不可用、什么时候应该发起重新选举？最先可能想到会通过心跳(heart beat)判别leader状态是否正常，但在网络拥塞或瞬断的情况下，这容易导致出现双主；租约(lease)是解决该问题的常用方法，其最初提出时用于解决分布式缓存一致性问题，后面在分布式锁等很多方面都有应用；租约的原理同样不复杂，中心思想是每次租约时长内只有一个节点获得租约、到期后必须重新颁发租约；租约机制确保了一个时刻最多只有一个leader，避免只使用心跳机制产生双主的问题，在实践应用中，zookeeper、ectd可用于租约颁发。 分布式系统理论基础 - 时间、时钟和事件顺序 bangerlee 现实生活中时间是很重要的概念，时间可以记录事情发生的时刻、比较事情发生的先后顺序。分布式系统的一些场景也需要记录和比较不同节点间事件发生的顺序，但不同于日常生活使用物理时钟记录时间，分布式系统使用逻辑时钟记录事件顺序关系； 在1978年提出逻辑时钟的概念，并描述了一种逻辑时钟的表示方法，这个方法被称为Lamport时间戳(Lamport timestamps)。分布式系统中按是否存在节点交互可分为三类事件，一类发生于节点内部，二是发送事件，三是接收事件。Lamport时间戳原理如下： 每个事件对应一个Lamport时间戳，初始值为0 如果事件在节点内发生，时间戳加1 如果事件属于发送事件，时间戳加1并在消息中带上该时间戳 如果事件属于接收事件，时间戳 = Max(本地时间戳，消息中的时间戳) + 1 Lamport时间戳帮助我们得到事件顺序关系，但还有一种顺序关系不能用Lamport时间戳很好地表示出来，那就是同时发生关系(concurrent)。Vector clock是在Lamport时间戳基础上演进的另一种逻辑时钟方法，它通过vector结构不但记录本节点的Lamport时间戳，同时也记录了其他节点的Lamport时间戳。Vector clock的原理与Lamport时间戳类似，使用图例如下： 基于Vector clock我们可以获得任意两个事件的顺序关系，结果或为先后顺序或为同时发生，识别事件顺序在工程实践中有很重要的引申应用，最常见的应用是发现数据冲突(detect conflict)。分布式系统中数据一般存在多个副本(replication)，多个副本可能被同时更新，这会引起副本间数据不一致，Version vector的实现与Vector clock非常类似，目的用于发现数据冲突。下面通过一个例子说明Version vector的用法： Vector clock只用于发现数据冲突，不能解决数据冲突。如何解决数据冲突因场景而异，具体方法有以最后更新为准(last write win)，或将冲突的数据交给client由client端决定如何处理，或通过quorum决议事先避免数据冲突的情况发生。 由于记录了所有数据在所有节点上的逻辑时钟信息，Vector clock和Version vector在实际应用中可能面临的一个问题是vector过大，用于数据管理的元数据(meta data)甚至大于数据本身。解决该问题的方法是使用server id取代client id创建vector (因为server的数量相对client稳定)，或设定最大的size、如果超过该size值则淘汰最旧的vector信息。 小结：以上介绍了分布式系统里逻辑时钟的表示方法，通过Lamport timestamps可以建立事件的全序关系，通过Vector clock可以比较任意两个事件的顺序关系并且能表示无因果关系的事件，将Vector clock的方法用于发现数据版本冲突，于是有了Version vector。 分布式系统理论基础 - CAP bangerlee CAP由Eric Brewer在2000年PODC会议上提出，是Eric Brewer在Inktomi期间研发搜索引擎、分布式web缓存时得出的关于数据一致性(consistency)、服务可用性(availability)、分区容错性(partition-tolerance)的猜想： It is impossible for a web service to provide the three following guarantees : Consistency, Availability and Partition-tolerance. 该猜想在提出两年后被证明成立[4]，成为我们熟知的CAP定理： 数据一致性(consistency)：如果系统对一个写操作返回成功，那么之后的读请求都必须读到这个新数据；如果返回失败，那么所有读操作都不能读到这个数据，对调用者而言数据具有强一致性(strong consistency) ； 服务可用性(availability)：所有读写请求在一定时间内得到响应，可终止、不会一直等待； 分区容错性(partition-tolerance)：在网络分区的情况下，被分隔的节点仍能正常对外服务； 在某时刻如果满足AP，分隔的节点同时对外服务但不能相互通信，将导致状态不一致，即不能满足C；如果满足CP，网络分区的情况下为达成C，请求只能一直等待，即不满足A；如果要满足CA，在一定时间内要达到节点状态一致，要求不能出现网络分区，则不能满足P。C、A、P三者最多只能满足其中两个，和FLP定理一样，CAP定理也指示了一个不可达的结果(impossibility result)。 要理解P，我们看回CAP证明中P的定义： In order to model partition tolerance, the network will be allowed to lose arbitrarily many messages sent from one node to another. 网络分区的情况符合该定义，网络丢包的情况也符合以上定义，另外节点宕机，其他节点发往宕机节点的包也将丢失，这种情况同样符合定义。现实情况下我们面对的是一个不可靠的网络、有一定概率宕机的设备，这两个因素都会导致Partition，因而分布式系统实现中P是一个必须项，而不是可选项。对于分布式系统工程实践，CAP理论更合适的描述是：在满足分区容错的前提下，没有算法能同时满足数据一致性和服务可用性： In a network subject to communication failures, it is impossible for any web service to implement an atomic read/write shared memory that guarantees a response to every request. CAP定理证明中的一致性指强一致性，强一致性要求多节点组成的被调要能像单节点一样运作、操作具备原子性，数据在时间、时序上都有要求。如果放宽这些要求，还有序列一致性(sequential consistency)和最终一致性(eventual consistency)。工程实践中，较常见的做法是通过异步拷贝副本(asynchronous replication)、quorum/NRW，实现在调用端看来数据强一致、被调端最终一致，在调用端看来服务可用、被调端允许部分节点不可用的效果。 分布式系统理论进阶 - Paxos bangerlee Paxos协议在节点宕机恢复、消息无序或丢失、网络分化的场景下能保证决议的一致性，是被讨论最广泛的一致性协议。 一致性问题是在节点宕机、消息无序等场景可能出现的情况下，相互独立的节点之间如何达成决议的问题，作为解决一致性问题的协议，Paxos的核心是节点间如何确定并只确定一个值(value)。 和2PC类似，Paxos先把节点分成两类，发起提议(proposal)的一方为proposer，参与决议的一方为acceptor。 P1. 一个acceptor接受它收到的第一项提议 //假如只有一个proposer发起提议，并且节点不宕机、消息不丢包 P2. 如果一项值为v的提议被确定，那么后续只确定值为v的提议 P2a. 如果一项值为v的提议被确定，那么acceptor后续只接受值为v的提议 P2b. 如果一项值为v的提议被确定，那么proposer后续只发起值为v的提议 P2c. 对于提议(n,v)，acceptor的多数派S中，如果存在acceptor最近一次(即ID值最大)接受的提议的值为v&apos;，那么要求v = v&apos;；否则v可为任意值 以上提到的各项约束条件可以归纳为3点，如果proposer/acceptor满足下面3点，那么在少数节点宕机、网络分化隔离的情况下，在“确定并只确定一个值”这件事情上可以保证一致性(consistency)： B1(ß): ß中每一轮决议都有唯一的ID标识 B2(ß): 如果决议B被acceptor多数派接受，则确定决议B B3(ß): 对于ß中的任意提议B(n,v)，acceptor的多数派中如果存在acceptor最近一次(即ID值最大)接受的提议的值为v&apos;，那么要求v = v&apos;；否则v可为任意值 至此，proposer/acceptor完成一轮决议可归纳为prepare和accept两个阶段。prepare阶段proposer发起提议问询提议值、acceptor回应问询并进行promise；accept阶段完成决议，图示如下： 分布式系统理论进阶 - Raft、Zab bangerlee Paxos偏向于理论、对如何应用到工程实践提及较少。理解的难度加上现实的骨感，在生产环境中基于Paxos实现一个正确的分布式系统非常难；Raft在2013年提出，提出的时间虽然不长，但已经有很多系统基于Raft实现。相比Paxos，Raft的买点就是更利于理解、更易于实行。 为达到更容易理解和实行的目的，Raft将问题分解和具体化：Leader统一处理变更操作请求，一致性协议的作用具化为保证节点间操作日志副本(log replication)一致，以term作为逻辑时钟(logical clock)保证时序，节点运行相同状态机(state machine)得到一致结果。Raft协议具体过程如下： ①Client发起请求，每一条请求包含操作指令 ②请求交由Leader处理，Leader将操作指令(entry)追加(append)至操作日志，紧接着对Follower发起AppendEntries请求、尝试让操作日志副本在Follower落地 ③如果Follower多数派(quorum)同意AppendEntries请求，Leader进行commit操作、把指令交由状态机处理 ④状态机处理完成后将结果返回给Client Paxos中Leader的存在是为了提升决议效率，Leader的有无和数目并不影响决议一致性，Raft要求具备唯一Leader，并把一致性问题具体化为保持日志副本的一致性，以此实现相较Paxos而言更容易理解、更容易实现的目标。 Zab的全称是Zookeeper atomic broadcast protocol，是Zookeeper内部用到的一致性协议。相比Paxos，Zab最大的特点是保证强一致性(strong consistency)。和Raft一样，Zab要求唯一Leader参与决议，Zab可以分解成discovery、sync、broadcast三个阶段： discovery: 选举产生PL(prospective leader)，PL收集Follower epoch(cepoch)，根据Follower的反馈PL产生newepoch(每次选举产生新Leader的同时产生新epoch，类似Raft的term) sync: PL补齐相比Follower多数派缺失的状态、之后各Follower再补齐相比PL缺失的状态，PL和Follower完成状态同步后PL变为正式Leader(established leader) broadcast: Leader处理Client的写操作，并将状态变更广播至Follower，Follower多数派通过之后Leader发起将状态变更落地(deliver/commit) 了解完Zab的基本原理，我们再来看Zab怎样保证强一致性，Zab通过约束事务先后顺序达到强一致性，先广播的事务先commit、FIFO，Zab称之为primary order(以下简称PO)。实现PO的核心是zxid。 Paxos、Raft、Zab和VR都是解决一致性问题的协议，Paxos协议原文倾向于理论，Raft、Zab、VR倾向于实践，一致性保证程度等的不同也导致这些协议间存在差异。 亿级规模的Elasticsearch优化实战 王卫华 索引优化：SSD是经济压力能承受情况下的不二选择。减少碎片也可以提高索引速度，每天进行优化还是很有必要的。在初次索引的时候，把replica设置为0，也能提高索引速度。 索引优化相关参数：threadpool.index.queue_size、indices.memory.index_buffer_size、index.translog.flush_threshold_ops和refresh_interval。 查询优化：可以使用多个集群，每个集群使用不同的routing，比如用户是一个routing维度。在实践中，这个routing非常重要。 索引越来越大，单个shard也很巨大，查询速度也越来越慢。这时候，是选择分索引还是更多的shards？在实践过程中，更多的shards会带来额外的索引压力，即IO 力。我们选择了分索引，比如按照每个大分类一个索引，或者主要的大城市一个索引。然后将他们进行合并查询。如：http://cluster1:9200/shanghai,beijing/_search?routing=fang； 线程池我们默认使用fixed，使用cached有可能控制不好。主要是比较大的分片relocation时，会导致分片自动下线，集群可能处于危险状态。 128G内存的机器配置一个JVM，然后是巨大的heapsize（如64G）还是配多个JVM instance，较小的 heapsize（如32G）？我的建议是后者。实际使用中，后者也能帮助我们节省不少资源，并提供不错的性能。具体请参阅 “Don’t Cross 32 GB!” （https://www.elastic.co/guide/en/elasticsearch/guide/current/heap-sizing.html#compressed_oops） 怎样读一本书V5.0 ljinkai JVM为什么需要GC？ 周明耀 HotSpot的垃圾回收器总结：如果你想要最小化地使用内存和并行开销，请选Serial GC；如果你想要最大化应用程序的吞吐量，请选Parallel GC；如果你想要最小化GC的中断或停顿时间，请选CMS GC。 G1 GC基本思想：G1 GC是一个压缩收集器，它基于回收最大量的垃圾原理进行设计。G1 GC利用递增、并行、独占暂停这些属性，通过拷贝方式完成压缩目标。此外，它也借助并行、多阶段并行标记这些方式来帮助减少标记、重标记、清除暂停的停顿时间，让停顿时间最小化是它的设计目标之一。 G1 GC的垃圾回收循环组成：年轻代循环、多步骤并行标记循环、混合收集循环、Full GC； G1的区间设计：在G1中，堆被平均分成若干个大小相等的区域（Region）。每个Region都有一个关联的Remembered Set（简称RS），RS的数据结构是Hash表，里面的数据是Card Table （堆中每512byte映射在card table 1byte）。简单的说RS里面存在的是Region中存活对象的指针。当Region中数据发生变化时，首先反映到Card Table中的一个或多个Card上，RS通过扫描内部的Card Table得知Region中内存使用情况和存活对象。在使用Region过程中，如果Region被填满了，分配内存的线程会重新选择一个新的Region，空闲Region被组织到一个基于链表的数据结构（LinkedList）里面，这样可以快速找到新的Region。 让机器读懂用户–大数据中的用户画像 杨杰 用户画像（persona）的概念最早由交互设计之父Alan Cooper提出:“Personas are a concrete representation of target users.” 是指真实用户的虚拟代表，是建立在一系列属性数据之上的目标用户模型。随着互联网的发展，现在我们说的用户画像又包含了新的内涵——通常用户画像是根据用户人口学特征、网络浏览内容、网络社交活动和消费行为等信息而抽象出的一个标签化的用户模型。构建用户画像的核心工作，主要是利用存储在服务器上的海量日志和数据库里的大量数据进行分析和挖掘，给用户贴“标签”，而“标签”是能表示用户某一维度特征的标识。具体的标签形式可以参考下图某网站给其中一个用户打的标签。 用户画像的作用：精准营销、用户研究、个性服务、业务决策； 用户画像的内容：对于大部分互联网公司，用户画像都会包含人口属性和行为特征。人口属性主要指用户的年龄、性别、所在的省份和城市、教育程度、婚姻情况、生育情况、工作所在的行业和职业等。行为特征主要包含活跃度、忠诚度等指标。另外，电商购物网站的用户画像，一般会提取用户的网购兴趣和消费能力等指标。 用户画像的生产，大致可以分为以下几步： 用户建模，指确定提取的用户特征维度，和需要使用到的数据源。 数据收集，通过数据收集工具，如Flume或自己写的脚本程序，把需要使用的数据统一存放到Hadoop集群。 数据清理，数据清理的过程通常位于Hadoop集群，也有可能与数据收集同时进行，这一步的主要工作，是把收集到各种来源、杂乱无章的数据进行字段提取，得到关注的目标特征。 模型训练，有些特征可能无法直接从数据清理得到，比如用户感兴趣的内容或用户的消费水平，那么可以通过收集到的已知特征进行学习和预测。 属性预测，利用训练得到的模型和用户的已知特征，预测用户的未知特征。 数据合并，把用户通过各种数据源提取的特征进行合并，并给出一定的可信度。 数据分发，对于合并后的结果数据，分发到精准营销、个性化推荐、CRM等各个平台，提供数据支持。 应用示例之个性化推荐：很多推荐场景都会用到基于商品的协同过滤，而基于商品协同过滤的核心是一个商品相关性矩阵W，假设有n个商品，那么W就是一个n * n的矩阵，矩阵的元素wij代表商品Ii和Ij之间的相关系数。而根据用户访问和购买商品的行为特征，可以把用户表示成一个n维的特征向量U=[ i1, i2, …, in ]。于是U * W可以看成用户对每个商品的感兴趣程度V=[ v1, v2, …, vn ]，这里v1即是用户对商品I1的感兴趣程度，v1= i1*w11 + i2*w12 + in*w1n。如果把相关系数w11, w12, …, w1n 看成要求的变量，那么就可以用LR模型，代入训练集用户的行为向量U，进行求解。这样一个初步的LR模型就训练出来了，效果和基于商品的协同过滤类似。 推荐系统本质与网易严选实践 沈燕 推荐系统作用本质：有资料称亚马逊的推荐系统带来的GMV占其全站总量的20%-30%；推荐的本质就是提升用户体验，为此它们最主要的方式就是帮助用户快速的找到它需要的产商品，其他的方式还包括给用户新颖感等。 推荐系统工作原理本质：所谓embedding，数学上的意义就是映射。如word2vec通过语料训练把词变成一个数百维的向量，向量的每一维没有明确的物理意义（或者说我们无法理解）。推荐系统如果可以把人很精确地映射成一个向量，把物品也映射成一个同维度同意义的向量，那么推荐就是可以按规则处理的精确的事情了。 电商推荐系统的特点：商品种类数巨大，不同的商品需要不同的embedding；单种商品深度不够，难以有效embedding；人对商品的兴趣大都建立在短期或者瞬时需求之上；大量耐消品的影响；用户理论上对所有商品都会有兴趣。基于以上的原因，在电商领域难以找到完美的embedding方式来实现推荐。其实我们在看各大电商的个性化推荐时，无论宣称背后用怎样复杂的模型融合，从结果看，用户近期行为的权重是非常大的，使得结果非常像itemCF推荐出来的。 网易严选推荐实践网易严选推荐的基础模型采用的是CTR模型，基于LR（逻辑回归）。 在核心的特征工程方面，网易严选推荐团队将用户的具体属性（性别、收入水平、地域等）、用户在网易严选的行为属性（短期，长期）、及时间上下文（季节、上次购买时间间隔等）作为属性空间，从1层迪卡尔积开始往上构造N层迪卡尔积形成复杂属性空间P，挖掘属性空间与商品的相关，对有明显相关（正相关或负相关）的（属性、物品）对构造特征。 用户属性空间 具体属性应用 行为属性作为抽象属性与具体属性置以相同的地位 二阶属性（属性的2重迪卡尔积） 从结果来看，这一套特征工程方法可以挖出比较全的特征集，在鲁棒性与效果上都有不错的效果，自上线以来各项指标均在稳步提升。 深入探索Java 8 Lambda表达式 Richard Warburton/Raoul Urma/Mario Fusco/段建华 为什么匿名内部类不好？编译器会为每一个匿名内部类创建一个类文件，而类在使用之前需要加载类文件并进行验证，这个过程则会影响应用的启动性能，另外类文件的加载很有可能是一个耗时的操作，这其中包含了磁盘IO和解压JAR文件。最重要的，一旦Lambda表达式使用了匿名内部类实现，就会限制了后续Lambda表达式实现的更改，降低了其随着JVM改进而改进的能力。 Lambdas表达式和invokedynamic：将Lambda表达式转化成字节码只需要如下两步：1.生成一个invokedynamic调用点，也叫做Lambda工厂，当调用时返回一个Lambda表达式转化成的函数式接口实例；2.将Lambda表达式的方法体转换成方法供invokedynamic指令调用。需要注意的是编译器对于Lambda表达式的翻译策略并非固定的，因为这样invokedynamic可以使编译器在后期使用不同的翻译实现策略。 性能分析：Lambda工厂的预热准备需要消耗时间，但Lambda工厂方式也会比匿名内部类加载要快，最高可达100倍；如果是不进行捕获变量，这一步会自动进行优化，避免在基于Lambda工厂实现下额外创建对象；对于真实方法的调用，匿名内部类和Lambda表达式执行的操作相同，没有性能上的差别； 对于大多数情况来说，Lambda表达式要比匿名内部类性能更优。然而现状并非完美，基于测量驱动优化，我们仍然有很大的提升空间。 红黑树 Hosee 先来看下算法导论对R-B Tree的介绍： 红黑树，一种二叉查找树，但在每个结点上增加一个存储位表示结点的颜色，可以是Red或Black。通过对任何一条从根到叶子的路径上各个结点着色方式的限制，红黑树确保没有一条路径会比其他路径长出俩倍，因而是接近平衡的。 二叉查找树，也称有序二叉树（ordered binary tree），是指一棵空树或者具有下列性质的二叉树： 1.若任意节点的左子树不空，则左子树上所有结点的值均小于它的根结点的值； 2.若任意节点的右子树不空，则右子树上所有结点的值均大于它的根结点的值； 3.任意节点的左、右子树也分别为二叉查找树。 4.没有键值相等的节点（no duplicate nodes）。 红黑树虽然本质上是一棵二叉查找树，但它在二叉查找树的基础上增加了着色和相关的性质使得红黑树相对平衡，从而保证了红黑树的查找、插入、删除的时间复杂度最坏为O(log n)。但它是如何保证一棵n个结点的红黑树的高度始终保持在logn的呢？这就引出了红黑树的5个性质： 1.每个结点要么是红的要么是黑的。 2.根结点是黑的。 3.每个叶结点（叶结点即指树尾端NIL指针或NULL结点）都是黑的。 4.如果一个结点是红的，那么它的两个儿子都是黑的。 5.对于任意结点而言，其到叶结点树尾端NIL指针的每条路径都包含相同数目的黑结点。 当在对红黑树进行插入和删除等操作时，对树做了修改可能会破坏红黑树的性质。为了继续保持红黑树的性质，可以通过对结点进行重新着色，以及对树进行相关的旋转操作，即通过修改树中某些结点的颜色及指针结构，来达到对红黑树进行插入或删除结点等操作后继续保持它的性质或平衡的目的。 红黑树的插入：首先，将红黑树当作一颗二叉查找树，将节点插入；然后，将节点着色为红色；最后，通过旋转和重新着色等方法来修正该树，使之重新成为一颗红黑树。 红黑树与AVL树的区别：红黑树旋转操作非常局部化，而且次数极少（插入最多两次旋转，删除最多三次旋转），而改变颜色的操作不会影响到用户对树的query操作（即不要lock），另外很多树，如AVL树，2-3树,2-4树都可以转化成红黑树，红黑树能达到O(logn)高度，但是不像AVL树那样严格要求左右子树高度差必需相差不超过1。可以说RB树是目前为止高度要求最灵活的准平衡BST。准平衡是相对完全二叉树来说的，AVL树(比如Fibonacci树)也不是完美平衡的。 业界难题-“跨库分页”的四种方案 58沈剑方法一：全局视野法 将order by time offset X limit Y，改写成order by time offset 0 limit X+Y 服务层对得到的N*(X+Y)条数据进行内存排序，内存排序后再取偏移量X后的Y条记录这种方法随着翻页的进行，性能越来越低。 方法二：业务折衷法-禁止跳页查询 用正常的方法取得第一页数据，并得到第一页记录的time_max 每次翻页，将order by time offset X limit Y，改写成order by time where time&gt;$time_max limit Y 以保证每次只返回一页数据，性能为常量。 方法三：业务折衷法-允许模糊数据将order by time offset X limit Y，改写成order by time offset X/N limit Y/N 方法四：二次查询法 将order by time offset X limit Y，改写成order by time offset X/N limit Y 找到最小值time_min between二次查询，order by time between $time_min and $time_i_max 设置虚拟time_min，找到time_min在各个分库的offset，从而得到time_min在全局的offset 得到了time_min在全局的offset，自然得到了全局的offset X limit Y 从GITLAB误删除数据库想到的 陈皓 事件回顾：Gitlab某员工在做负载均衡工作时需要解决突发情况，误将删除命令敲到生产环境的窗口上导致线上数据库被删除，然后视图通过多种备份机制都无法恢复，最终只能从6小时前的数据库中拷贝回来，导致在这6个小时期间的数据丢失； 人肉运维：一个公司的运维能力的强弱和你上线上环境敲命令是有关的，你越是喜欢上线敲命令你的运维能力就越弱，越是通过自动化来处理问题，你的运维能力就越强。 数据丢失有各种各样的情况，不单单只是人员的误操作，比如，掉电、磁盘损坏、中病毒等等，在这些情况下，你设计的那些想流程、规则、人肉检查、权限系统、checklist等等统统都不管用了，这个时候，你觉得应该怎么做呢？是的，你会发现，你不得不用更好的技术去设计出一个高可用的系统！别无它法。 关于备份：如果你要让你的备份系统随时都可以用，那么你就要让它随时都Live着，而随时都Live着的多结点系统，基本上就是一个分布式的高可用的系统。 非技术方面：故障反思（5 whys分析）、工程师文化（如果你是一个技术公司，你就会更多的相信技术而不是管理）、事件公开（公开所有的细节，会让大众少很多猜测的空间，有利于抵制流言和黑公关，同时，还会赢得大众的理解和支持。）； AWS 的 S3 故障回顾和思考 陈皓 故障原因：AWS某员工在修复账务系统问题，需要移除某些子系统时有一条命令搞错了，移除了大量S3的控制系统，包括对象索引服务和位置服务系统。而这两个系统重启花费了非常长时间（由于该系统非常稳定，以及很长时间没有重启过，而数据量级却一直在增长），最终导致服务挂了4个小时； AWS后续改进措施：改进运维操作工具（让删除服务这个操作变慢一些、任何服务在运行时都应该有一个最小资源数、Review所有和其它的运维工具）；改进恢复过程（分解现有厚重的重要服务成更小的单元、今年内完成对 Index 索引服务的分区计划）； 一个系统的高可用的因素很多，不仅仅只是系统架构，更重要的是——高可用运维。对于高可用的运维，平时的故障演习是很重要的。 关于高可用的系统 陈皓 理解高可用系统：要做到数据不丢，就必需要持久化；要做到服务高可用，就必需要有备用（复本），无论是应用结点还是数据结点；要做到复制，就会有数据一致性的问题；我们不可能做到100%的高可用，也就是说，我们能做到几个9个的SLA； 高可用系统的技术解决方案：下图基本上来说是目前高可用系统中能看得到的所有的解决方案的基础了。M/S、MM实现起来不难，但是会有很多问题，2PC的问题就是性能不行，而Paxos的问题就是太复杂，实现难度太大。 高可用技术方案的示例：MySQL的高可用的方案的SLA（下图下面红色的标识表示了这个方案有几个9）： 1.MySQL Repleaction就是传统的异步数据同步或是半同步Semi-Sync这个方式本质上不到2个9； 2.MySQL Fabric简单来说就是数据分片下的M/S的读写分离模式。这个方案的的可用性可以达到99%； 3.DRBD通过底层的磁盘同步技术来解决数据同步的问题，就是RAID 1——把两台以上的主机的硬盘镜像成一个。这个方案不到3个9； 4.Solaris Clustering/Oracle VM ，这个机制监控了包括硬件、操作系统、网络和数据库。这个方案一般会伴随着节点间的“心跳机制”， 而且还会动用到SAN（Storage Area Network）或是本地的分布式存储系统，还会动用虚拟化技术来做虚拟机的迁移以降低宕机时间的概率。这个解决方案完全就是一个“全栈式的解决方案”。这个方案接近4个9； 5.MySQL Cluster是官方的一个开源方案，其把MySQL的集群分成SQL Node 和Data Node， Data Node是一个自动化sharing和复制的集群NDB，为了更高的可用性，MySQL Cluster采用了“完全同步”的数据复制的机制来冗余数据结点。这个方案接近5个9； 影响高可用的因素：无计划的宕机原因（系统级的故障、数据和中介的故障、自然灾害、人为破坏等）；有计划的宕机原因（日常任务、运维相关、升级相关）； 要干出高可用的系统，其中包括但不限于：软件的设计、编码、测试、上线和软件配置管理的水平；工程师的人员技能水平；运维的管理和技术水平；数据中心的运营管理水平；依赖于第三方服务的管理水平； 深层交的东西则是——对工程这门科学的尊重：对待技术的态度、一个公司的工程文化、领导者对工程的尊重； 专访RocketMQ联合创始人：项目思路、技术细节和未来规划 王小瑞/冯嘉 RocketMQ的由来：第一代，推模式，数据存储采用关系型数据库，典型代表包括Notify、Napoli；第二代，拉模式，自研的专有消息存储，典型代表MetaQ；第三代，以拉模式为主，兼有推模式的高性能、低延迟消息引擎RocketMQ，在二代功能特性的基础上，为电商金融领域添加了可靠重试、基于文件存储的分布式事务等特性，并做了大量优化。 RocketMQ的技术概览：在我们看来，它最大的创新点在于能够通过精巧的横向、纵向扩展，不断满足与日俱增的海量消息在高吞吐、高可靠、低延迟方面的要求。目前RocketMQ主要由NameServer、Broker、Producer以及Consumer四部分构成，所有的集群都具有水平扩展能力，如下图所示： 与其他消息中间件比较：RabbitMQ是AMQP规范的参考实现，AMQP是一个线路层协议，面面俱到，很系统，也稍显复杂；ActiveMQ是JMS规范的参考实现，JMS虽说是一个API级别的协议，但其内部还是定义了一些实现约束，不过缺少多语言支撑；而Kafka最初被设计用来做日志处理，是一个不折不扣的大数据通道，追求高吞吐，存在丢消息的可能；RocketMQ天生为金融互联网领域而生，追求高可靠、高可用、高并发、低延迟，是一个阿里巴巴由内而外成功孕育的典范。 三项技术发力点：消息的顺序（全局保序）、消息的去重（目前的版本是不支持去重的，建议用户通过外置全局存储自己做判重处理，后续版本内置解决方案）、分布式的挑战（基于Zab一致性协议，利用分布式锁和通知机制保障多副本数据的一致性）； 新一代RocketMQ：期望构建一套厂商无关的集线路层、API层于一体的规范，这也是第四代消息引擎最大的亮点。 分布式事务原理与实践 沈询 事务的四大特性分别是：原子型、一致性、隔离性和持久性。 事务单元是通过Begin-Traction，然后Commit（Begin-Traction、Commit和Rollback之间所有针对数据的写入、读取的操作都应该添加同步访问），Begin和Commit之间就是一个同步的事务单元。 Two Phase Lock（2PL）是数据库中非常重要的一个概念。数据库操作Insert、Update、Delete都是先读再写的操作，例如Insert操作是先读取数据，读取之后判读数据是否存在，如果不存在，则写入该数据，如果数据存在，则返回错误。 处理事务的常见方法有排队法、排他锁、读写锁、MVCC等方式。事务处理中最重要也是最简单的方案是排队法，单线程地处理一堆数据；有些场景不适合用单线程操作，可以利用排他锁的方式来快速隔离并发读写事务；读写锁的核心是在多次读的操作中，同时允许多个读者来访问共享资源，提高并发性；MVCC本质是Copy On Write，也就是每次写都是以重新开始一个新的版本的方式写入数据，因此，数据库中也就包含了之前的所有版本，在数据读的过程中，先申请一个版本号，如果该版本号小于正在写入的版本号，则数据一定可以查询到，无需等到新版本完全写完即可返回查询结果。 事务的调优原则：尽可能减少锁的覆盖范围、增加锁上可并行的线程数、选择正确锁类型（比如悲观锁适合并发争抢比较严重的场景，乐观锁适合并发争抢不太严重的场景）； 对比了解Grafana与Kibana的关键差异 Asaf Yigal/冬雨 Kibana是一个分析和可视化平台，它可以让你浏览、可视化存储在Elasticsearch集群上排名靠前的日志数据，并构建仪表盘。你可以执行深入的数据分析并以多种图表、表格和地图方式可视化这些数据。Kibana的仪表盘非常简单易用，任何人都可以使用它，甚至IT技能和知识很少的业务人员也可以使用。 Grafana是一个开源仪表盘工具，它可用于Graphite、InfluxDB与 OpenTSDB一起使用。最新的版本还可以用于其他的数据源，比如Elasticsearch。它包含一个独一无二的Graphite目标解析器，从而可以简化度量和函数的编辑。Grafana快速的客户端渲染默认使用的是 Flot ，即使很长的时间范围也可应对。 日志与度量：Grafana专注于根据CPU和IO利用率之类的特定指标提供时间序列图表，而Kibana能创建一个复杂的日志分析仪表盘； 基于角色的访问：默认情况下Kibana的仪表盘是公开的，Grafana内置的RBA允许你维护用户和团队访问仪表盘的权限； 仪表盘灵活性：虽然Kibana有大量内置的图表类型，但它们之上的控制仍是最初的限制，Grafana包括更多的选择，可以更灵活地浏览和使用图表； 数据源的集成：Grafana支持许多不同的存储后端，它是针对数据源所具备的特性和能力特别定制的，而Kibana原生集成进了ELK栈，这使安装极为简单，对用户非常友好； 开源社区：ELK仍保持着快速的增长，并有潜力在不久的将来保持领先； 共同协作：Kibana和Grafana都是强大的可视化工具。然而，Grafana和InfluxDB组合是用于度量数据的，反之，Kibana是流行的ELK栈的一部分，它可以更为灵活地浏览日志数据。这两个平台都是好的选择，甚至有时还可以互补。首先，用Kibana去分析你的日志。然后，把数据导入到Grafana作为可视化层。这些的前提是需要同一个Elasticsearch库。 百度宣布将在Kubernetes上运行其深度学习平台PaddlePaddle 木环 本月初，Kubernetes在其官网上宣布了百度的PaddlePaddle成为目前唯一官方支持Kubernetes的深度学习框架。PaddlePaddle是百度于2016年9月开源的一款深度学习平台，具有易用，高效，灵活和可伸缩等特点，为百度内部多项产品提供深度学习算法支持。 Kubernetes 把很多分散的物理计算资源抽象成一个巨大的资源池，它利用这些资源来帮助用户执行计算任务。对于用户来说，操作一个分散的集群资源可以像使用一台计算机一样简单。对于这个项目，Kubernetes 主要负责将学习任务分配到集群的物理节点上进行运算；如果遇到任务失败的情况，Kubernetes 会自动重启任务。 能不能将框架的作业和任务模式，同“容器”这个全新的部署概念匹配起来，才是现阶段最重要的。毕竟，如果框架连正常运行起来都很困难，再好的资源利用率提升机制也没有用武之地。在这一点上，Kubernetes应该说是现有的容器管理项目中做的最好的。 容器化实施深度学习的优点：轻量级、更高的资源利用率、基于容器的设计模式、高度的可扩展性和容错能力； 建设DevOps统一运维监控平台，先从日志监控说起 王海龙 DevOps浪潮下带来的监控挑战：监控源的多样化挑战、海量数据的分析处理挑战、软硬件数据资源的管理分析挑战； 一个好的统一监控平台，应当具备：高度抽象模型，扩展监控指标、多种监控视图、强大的数据加工能力、多种数据采集技术、多种报警机制、全路径问题跟踪； 统一监控平台由七大角色构成：监控源、数据采集、数据存储、数据分析、数据展现、预警中心、CMDB(企业软硬件资产管理)。 日志监控的技术栈 ELK-日志监控经典方案 微服务+容器云背景下的日志监控实践：跑在容器中的应用、数据库等软件都会把日志落到容器日志（docker日志），然后在docker系统服务上进行配置，将docker容器日志输出到系统日志服务journald中。这样，容器中的日志就统一到了系统日志中。针对于运行在虚拟机上的系统软件，如kubernetes、etcd等，配置成系统服务service，使用systemd管理，自然也就做到了将其日志输入到journald中。再往上就比较简单了，自实现一个agent，读取journald中的日志，通过tcp协议发送到fluentd中，考虑到现在的日志量并不会太大，所以没有再使用kafka进行缓冲，而是直接经过fluentd的拦截和过滤，将日志发送到Elasticsearch中。 如何选择适合自己的日志监控方案：工具能力是否满足、性能对比、看技术能力是否能cover住、监控平台日志量评估； 函数式编程入门教程 阮一峰 函数式编程的起源，是一门叫做范畴论（Category Theory）的数学分支。理解函数式编程的关键，就是理解范畴论。它是一门很复杂的数学，认为世界上所有的概念体系，都可以抽象成一个个的”范畴”（category）。 范畴就是使用箭头连接的物体。也就是说，彼此之间存在某种关系的概念、事物、对象等等，都构成”范畴”。箭头表示范畴成员之间的关系，正式的名称叫做”态射”（morphism）。范畴论认为，同一个范畴的所有成员，就是不同状态的”变形”（transformation）。通过”态射”，一个成员可以变形成另一个成员。 我们可以把”范畴”想象成是一个容器，里面包含两样东西：值（value）和值的变形关系，也就是函数。 本质上，函数式编程只是范畴论的运算方法，跟数理逻辑、微积分、行列式是同一类东西，都是数学方法，只是碰巧它能用来写程序。 如果一个值要经过多个函数，才能变成另外一个值，就可以把所有中间步骤合并成一个函数，这叫做”函数的合成”（compose）。 f(x)和g(x)合成为f(g(x))，有一个隐藏的前提，就是f和g都只能接受一个参数。如果可以接受多个参数就需要函数柯里化了。所谓”柯里化”，就是把一个多参数的函数，转化为单参数函数。 函子是函数式编程里面最重要的数据类型，也是基本的运算单位和功能单位。它首先是一种范畴，也就是说，是一个容器，包含了值和变形关系。比较特殊的是，它的变形关系可以依次作用于每一个值，将当前容器变形成另一个容器。 学习函数式编程，实际上就是学习函子的各种运算。 随手记系列： 阅读随手记 201701 阅读随手记 201612]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Protocol Buffers简明教程]]></title>
      <url>%2Fpost%2Flearning_protobuf%2F</url>
      <content type="text"><![CDATA[随着微服务架构的流行，RPC框架渐渐地成为服务框架的一个重要部分。在很多RPC的设计中，都采用了高性能的编解码技术，Protocol Buffers就属于其中的佼佼者。Protocol Buffers是Google开源的一个语言无关、平台无关的通信协议，其小巧、高效和友好的兼容性设计，使其被广泛使用。 概述protobuf是什么？ Protocol buffers are Google’s language-neutral, platform-neutral, extensible mechanism for serializing structured data – think XML, but smaller, faster, and simpler. You define how you want your data to be structured once, then you can use special generated source code to easily write and read your structured data to and from a variety of data streams and using a variety of languages. Google良心企业出厂的； 是一种序列化对象框架（或者说是编解码框架），其他功能相似的有Java自带的序列化、Facebook的Thrift和JBoss Marshalling等； 通过proto文件定义结构化数据，其他功能相似的比如XML、JSON等； 自带代码生成器，支持多种语言； 为什么叫“Protocol Buffers”？官方如是说： The name originates from the early days of the format, before we had the protocol buffer compiler to generate classes for us. At the time, there was a class called ProtocolBuffer which actually acted as a buffer for an individual method. Users would add tag/value pairs to this buffer individually by calling methods like AddValue(tag, value). The raw bytes were stored in a buffer which could then be written out once the message had been constructed. Since that time, the “buffers” part of the name has lost its meaning, but it is still the name we use. Today, people usually use the term “protocol message” to refer to a message in an abstract sense, “protocol buffer” to refer to a serialized copy of a message, and “protocol message object” to refer to an in-memory object representing the parsed message. 核心特点 语言无关、平台无关 简洁 高性能 良好的兼容性 “变态的”性能表现有位网友曾经做过各种通用序列化协议技术的对比，我这里直接拿来给大家感受一下： 序列化响应时间对比 序列化bytes对比 具体的数字 快速开始以下示例源码已上传至github：https://github.com/ginobefun/learning_projects/tree/master/learning-protobuf 新建一个maven项目并添加依赖&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.ginobefunny.learning&lt;/groupId&gt; &lt;artifactId&gt;leanring-protobuf&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.google.protobuf&lt;/groupId&gt; &lt;artifactId&gt;protobuf-java&lt;/artifactId&gt; &lt;version&gt;3.2.0&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/project&gt; 新建protobuf的消息定义文件addressbook.protosyntax = &quot;proto3&quot;; // 声明为protobuf 3定义文件 package tutorial; option java_package = &quot;com.ginobefunny.learning.protobuf.message&quot;; // 声明生成消息类的java包路径 option java_outer_classname = &quot;AddressBookProtos&quot;; // 声明生成消息类的类名 message Person { string name = 1; int32 id = 2; string email = 3; enum PhoneType { MOBILE = 0; HOME = 1; WORK = 2; } message PhoneNumber { string number = 1; PhoneType type = 2; } repeated PhoneNumber phones = 4; } message AddressBook { repeated Person people = 1; } 使用protoc工具生成消息对应的Java类 从已发布版本中下载protoc工具，比如protoc-3.2.0-win32； 解压后将bin目录添加到path路径； 执行以下protoc命令生成Java类： protoc -I=. --java_out=src/main/java addressbook.proto 编写测试类写入和读取序列化文件 AddPerson类通过用户每次添加一个联系人，并序列化保存到指定文件中。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273public class AddPerson &#123; // 通过用户输入构建一个Person对象 static AddressBookProtos.Person promptForAddress(BufferedReader stdin, PrintStream stdout) throws IOException &#123; AddressBookProtos.Person.Builder person = AddressBookProtos.Person.newBuilder(); stdout.print("Enter person ID: "); person.setId(Integer.valueOf(stdin.readLine())); stdout.print("Enter name: "); person.setName(stdin.readLine()); stdout.print("Enter email address (blank for none): "); String email = stdin.readLine(); if (email.length() &gt; 0) &#123; person.setEmail(email); &#125; while (true) &#123; stdout.print("Enter a phone number (or leave blank to finish): "); String number = stdin.readLine(); if (number.length() == 0) &#123; break; &#125; AddressBookProtos.Person.PhoneNumber.Builder phoneNumber = AddressBookProtos.Person.PhoneNumber.newBuilder().setNumber(number); stdout.print("Is this a mobile, home, or work phone? "); String type = stdin.readLine(); if (type.equals("mobile")) &#123; phoneNumber.setType(AddressBookProtos.Person.PhoneType.MOBILE); &#125; else if (type.equals("home")) &#123; phoneNumber.setType(AddressBookProtos.Person.PhoneType.HOME); &#125; else if (type.equals("work")) &#123; phoneNumber.setType(AddressBookProtos.Person.PhoneType.WORK); &#125; else &#123; stdout.println("Unknown phone type. Using default."); &#125; person.addPhones(phoneNumber); &#125; return person.build(); &#125; // 加载指定的序列化文件（如不存在则创建一个新的），再通过用户输入增加一个新的联系人到地址簿，最后序列化到文件中 public static void main(String[] args) throws Exception &#123; if (args.length != 1) &#123; System.err.println("Usage: AddPerson ADDRESS_BOOK_FILE"); System.exit(-1); &#125; AddressBookProtos.AddressBook.Builder addressBook = AddressBookProtos.AddressBook.newBuilder(); // Read the existing address book. try &#123; addressBook.mergeFrom(new FileInputStream(args[0])); &#125; catch (FileNotFoundException e) &#123; System.out.println(args[0] + ": File not found. Creating a new file."); &#125; // Add an address. addressBook.addPeople(promptForAddress(new BufferedReader(new InputStreamReader(System.in)), System.out)); // Write the new address book back to disk. FileOutputStream output = new FileOutputStream(args[0]); addressBook.build().writeTo(output); output.close(); &#125;&#125; ListPeople类读取序列化文件并输出所有联系人信息。 123456789101112131415161718192021222324252627282930313233343536373839404142public class ListPeople &#123; // 打印地址簿中所有联系人信息 static void print(AddressBookProtos.AddressBook addressBook) &#123; for (AddressBookProtos.Person person: addressBook.getPeopleList()) &#123; System.out.println("Person ID: " + person.getId()); System.out.println(" Name: " + person.getName()); if (!person.getPhonesList().isEmpty()) &#123; System.out.println(" E-mail address: " + person.getEmail()); &#125; for (AddressBookProtos.Person.PhoneNumber phoneNumber : person.getPhonesList()) &#123; switch (phoneNumber.getType()) &#123; case MOBILE: System.out.print(" Mobile phone #: "); break; case HOME: System.out.print(" Home phone #: "); break; case WORK: System.out.print(" Work phone #: "); break; &#125; System.out.println(phoneNumber.getNumber()); &#125; &#125; &#125; // 加载指定的序列化文件，并输出所有联系人信息 public static void main(String[] args) throws Exception &#123; if (args.length != 1) &#123; System.err.println("Usage: ListPeople ADDRESS_BOOK_FILE"); System.exit(-1); &#125; // Read the existing address book. AddressBookProtos.AddressBook addressBook = AddressBookProtos.AddressBook.parseFrom(new FileInputStream(args[0])); print(addressBook); &#125;&#125; 验证效果先添加一个联系人Gino 再添加一个联系人Slightly 最后显示所有联系人信息 实例小结 通过以上的例子我们能大概感受到开发protobuf序列化的大致步骤：定义proto文件、生成对应的Java类文件、通过消息类的构造器构造对象并通过writeTo序列化、通过parseFrom反序列化对象； 如果查看中间序列化的文件，我们可以发现protobuf序列化的二进制文件非常紧凑，因此文件更小，传输性能更好。 深入学习关于proto文件protobuf版本 protobuf现在主流的有2.X和3.X版本，两者之间相差比较大，对于刚采用的建议使用3.X版本； 如果采用3.X版本，需要再proto文件第一个非注释行声明（就像我们上面的例子那样），因为protobuf默认认为是2.X版本； message结构 在一个proto文件中可以包含多个message定义，message之间可以互相引用，message还可以嵌套message和枚举类； 一个message通常包含一至多个字段； 每个字段包含以下几个部分：字段描述符（可选）、字段类型、字段名称和字段对应的Tag； 字段描述符字段描述符用于描述字段出现的频率，有以下两个可选值： singular：表示出现0次或1次；如果没有声明描述符，默认为singular； repeated：表示出现0次或多次； 字段类型 基本数据类型：包括double、float、bool、string、bytes、int32、int64、uint32、uint64、sint32、sint64、fixed32、fixed64、sfixed32、sfixed64； 引用其他message类型：这个就有点像我们Java里面的对象引用的方式； 枚举类型：对于枚举类型，protobuf有个约束：枚举的第一项对应的值必须为0；下面是一个包含枚举类型的消息定义： message SearchRequest { string query = 1; int32 page_number = 2; int32 result_per_page = 3; enum Corpus { UNIVERSAL = 0; WEB = 1; IMAGES = 2; LOCAL = 3; NEWS = 4; PRODUCTS = 5; VIDEO = 6; } Corpus corpus = 4; } 字段对应的Tag 对应同一个message里面的字段，每个字段的Tag是必须唯一数字； Tag主要用于说明字段在二进制文件的对应关系，一旦指定字段为对应的Tag，不应该在后续进行变更； 对于Tag的分配，1~15只用一个byte进行编码（因此应该留给那些常用的字段），16~2047用两个byte进行编码，最大支持到536870911，但是中间有一段（19000~19999）是protobuf内部使用的； 可以通过reserved关键字来预留Tag和字段名，还有一种场景是如果某个字段已经被废弃了不希望后续被采用，也可以用reserved关键字声明； 字段的默认值protobuf 2.X版本是支持在字段中声明默认值的，但是在3.X版本中去掉了默认值的定义，主要是为了区别用户是否设置了一个和默认值一样的值的情况。对于3.X版本，protobuf采用以下规则处理默认值： 对应string类型，默认值为一个空字符串； 对于bytes类型，默认值为一个空的byte数组； 对于bool类型，默认值为false； 对于数值类型，默认值为0； 对于枚举类型，默认值为第一项，也即值为0的那个枚举值； 对于引用其他message类型：其默认值和对应的语言是相关的； Map字段类型 protobuf也支持定义Map类型的字段，但是对于Map的key的类型只能是整数型（包括各种int32和int64）和string类型； Map类型不能定义为repeated； Map类型的数据是无序的； 以下是一个Map类型的字段定义示例： map&lt;string, Project&gt; projects = 3; 导入其他proto文件 可以通过import关键字导入其他proto文件，从而重用message类型；下面是一个import的示例： import &quot;myproject/other_protos.proto&quot;; 如果proto中的message要扩展怎么办？proto具有很好的扩展性，但是也要遵循以下原则： 不能修改原有字段的Tag； 如果新增一个字段，对于老的二进制序列化文件处理时会给这个字段增加默认值；如果是升级了proto文件而没有升级对应的代码，则新的字段会被忽略； 可以删除字段，但是对应的Tag不应该再被使用，否则对于之前的二进制序列化消息处理时对应关系出现问题； int32、uint32、int64、uint64和bool类型是相互兼容的，这意味着你可以在他们之间修改类型而不会有兼容性问题； Any消息类型 protobuf内置了一些通用的消息类型，Any就是其他的一种，通过查看它的proto文件可以看到它包含了一个URL标识符和一个byte数组； 在使用Any消息类型之前，需要通过import “google/protobuf/any.proto”;导入proto文件定义； Oneof关键字 oneof关键字用于声明一组字段中，必须要有一个字段被赋值；通常比如我们在登陆的时候，可以用手机号、邮箱和用户名登陆，这种时候就可以使用oneof来定义； 当我们对oneof其中一个字段赋值时，其他字段的值将会被清空；所以只有最后一次赋值是有效的； 下面是一个oneof的示例： message LoginMessage { oneof user_identifier { string user_name = 4; string phone_num = 5; string user_email = 6; } string password = 10; } 定义服务 在proto文件中还允许定义RPC服务，以下是一个示例： service SearchService { rpc Search (SearchRequest) returns (SearchResponse); } 小结 随着微服务架构的流行，RPC框架渐渐地成为服务框架的一个重要部分。在很多RPC的设计中，都采用了高性能的编解码技术，protobuf就属于其中的佼佼者； protobuf相对于其他编解码框架，有着非常惊人的性能表现； 通过一个简单的实例，我们了解如果使用protobuf进行序列化和数据交互； 最后，我们列举了一些重要的特性和配置说明，这些在我们使用protobuf中都会给频繁使用； 后续学习：后面我会根据所学的Netty和protobuf知识，开发一个简单的RPC框架。 参考资料 Language Guide (proto3) Protocol Buffer Basics: Java Java Generated Code Protobuf 的 proto3 与 proto2 的区别 几种序列化协议(protobuf,xstream,jackjson,jdk,hessian)相关数据对比]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[代码之外的生存指南]]></title>
      <url>%2Fpost%2Fsoft_skills%2F</url>
      <content type="text"><![CDATA[阅读《软技能》一书的笔记和随想。 笔记整理职业篇 你所能犯的最大错误就是相信自己是在为别人工作。这样一来你对工作的安全感依然尽失。职业发展的驱动力一定是来自个体本身。记住：工作是属于公司的，而职业生涯却是属于你自己的。 大多数软件开发人员从职业生涯一开始就犯了几个严重的错误，最大的错误就是没有把自己的软件开发事业当作一桩生意来看待。你应该把自己当作一个企业去思考，把雇主当作是你的软件开发企业的一个客户。 通常软件开发人员售卖的就是他们把一个想法变成一个数字化的现实产品的能力。因此你需要做到：专注于你正在提供怎么样的服务以及如何营销；想法设法提升你的服务；集中精力称为一位专家； 如何设定目标：起步阶段最简单的就是在心中树立一个大目标，然后再建立能帮你达成这个大目标的小目标。较小的目标可以让你航行在自己的轨道上，激励你保持航向朝着更大的目标前进。 学会与人打交道：每个人都希望感到自己重要；永远不要批评；换位思考；避免争吵； 关于面试：让面试官对你怀有好感（比如阅读过你的博客，比如有员工推荐你）；集中精力证明自己无需督促也能自动自发做好事情以及在技术上你确实胜任工作；坚持阅读技术书籍和博客文章，提升自己的技能；扩展自己的社交网络； 专业化很重要：虽然会把你关在一些机会的大门之外，但与此同时它将打开的机会大门要比你用其他方式打开的多得多。专业化的规则是程序越深，潜在的机会越少，但获得这些机会的可能性越大。 不同规模的公司选择：小公司（承担更多职责但稳定性差）、中等规模公司（工作稳定但变化很慢）、大公司（完备的流程和规范但负责一小部分且可能充斥着官僚作风）； 关于晋升：承担责任（负责不受重视的项目、帮助新人成长、负责文档更新等）；引入注目（记录活动日志、提供演讲、发表意见）；自学（提升技能并分享）；成为问题的解决者； 成为专业人士是一种心态。如果我们总是与恐惧。自毁。拖延和自我怀疑作斗争，那么问题就是我们正在像外行那么思考问题。外行毫不起眼，外行人废话连篇，外行屈从于逆境。专业人士可不这么想。不管怎样，他引人注目，他恪尽职守，他始终如一。 对技术虔诚的一大问题是，我们中的大多数崇拜某项特定的技术，只是因为自己熟悉这种技术，我们很自然地会相信自己选择的是最好的，然而这会让我们经常忽略任何反对意见。 自我营销篇 自我营销的关键在于：如果想让别人喜欢你，想和你一起工作，你必须要为他们提供价值。自我营销无非就是学习如何控制好自己要传达的信息，塑造好自己的形象，扩展信息送达的人群； 尽管有多种媒介可供你使用，但对于软件开发人员，最突出也是我个人推荐的还是博客。我认为博客就是你在互联网上的大本营，这是一个你完全能够控制信息的地方。 自我营销的基本机制是，要想让人们追随你、倾听你，你就要带给他们价值：你能为他们的问题提供答案，甚至是给他们带去欢乐。 打造成功博客的最大秘诀有且仅有一个 – 持之以恒。定好计划，然后坚持不懈，另外还需要重视博客内容品质。 学习篇 通过动手实践和教会他人，我们能学得更好。与其他的学习方式相比，主动学习是效率更高的方式。 十步学习法：要对自己要学的内容有个基本的了解然后利用这些信息勾勒出学习的范围，依靠这些知识找出各种资源来帮助自己学习，最后创建自己的学习计划、列出要学习的相关课程、筛选学习材料，再通过“学习-实践-掌握-教授”的过程假设理解。下面是十步学习法的示意图，第1步到第6步只做一次，集中精力完成足够多的前期调研，确保自己明确知道要学哪些内容，以及如何确认自己已达成目标，另外还需要挑选最好的资源、制定学习计划；第7步到第10步通过LDLT的方式真正领会知识。 寻找导师与做一名导师； 发现自己的知识短板：消除短板的关键就是定位短板，然后通过十步学习法用心掌握它。 生产力篇 如何专注：挑选短时间片专注于单一任务（番茄工作法）、克服集中于单一任务的痛感（学会享受任务和奖励）、屏蔽打扰； 生产力提升计划：季度计划 -&gt; 月计划 -&gt; 周计划 -&gt; 日计划及执行，使用看板实时关注进展； 番茄工作法：每25分钟一个番茄，专注于当前优先级最高的任务，拥抱变化并诚实地记录中断； 定额工作法：给自己在确定的期限确立一个明确的目标；挑选一些需要重复去做的事情，设定一个定额，如每周写一篇博客； 批量处理生产效率更高，比如处理电子邮件、开短会，避免多任务同时处理； 职业倦怠：穿多那堵墙（很多倦怠是自然而然产生，但是如果咬牙坚持或许就是不一样的风景）； 追踪你的时间：了解自己每天时间的使用情况（比如RescueTime工具），避免浪费； 习惯主要由三个要素构成：暗示、惯例和奖励。找出坏习惯，改掉！养成好习惯。 任何行动往往都比没有行动好，特别是当你一直停滞在不愉快的情势下很长时间的时候。如果这是一个错误，至少你学到了一些东西。这样一来，它就不再是一个错误。如果你仍然选择停滞不前，那么你就学不到任何东西。 理财篇 金钱只是一种工具，它会带你去往任何你想去的地方，但不会取代你成为司机； 是成为百万富翁还是一生都靠薪水过活，选择权在你自己，而且在很大程度上取决于你在财务管理方面的知识，以及世界金额系统运行方面的知识。 怎样支配你的薪水：拒绝短期思维（更长远地看待薪水的分配而不仅仅是当前）；资产与负债（通过成本和价值来考虑的理财思维）； 怎样进行薪酬谈判：薪酬水平受声望的影响（自我营销）；先出价者输（先出价的人处于明显的劣势）；被要求先出价怎么办（先要求了解预算范围、不透露当前薪酬、了解自己值什么价钱）； 期权：赋予你再未来某个日期之前以固定价格购买一定数量股票的选择权。 规划退休计划的关键就是利用逆向思维，计算退休目标。 健身篇 人的身体就是人的灵魂的最好写照。 健身不仅是保持健康体魄的关键要素之一，也是灵活的、具有创造性的脑力活动的基础。健身可以增强自信心、提高创造力、减少对疾病的恐惧。 设置你的健身标准：挑选一个具体的目标（比如增长肌肉）、创建里程碑、对进展进行可视化。最后保持健康的生活方式。 精神篇 信念决定思想，思想决定言语，言语决定行动，行动决定习惯，习惯决定价值，价值决定命运。（by 甘地） 拥有正确的心态：重新启动。积极思考问题的根源是这样一种信念 – 你比你所处的环境更伟大。这种信念让你总能先看到事物好的一面，因为无论身处何处，你都有能力改变自己的未来。这是人类成就的最高信念，是世界上最强大的力量。 随想 转变心态，从被卖身契束缚的工人转变为一个自主管理的商人； 设定一个大目标，比如称为一个卓越的高效的工程师；但是这样不够清晰，那么我希望在十年后能成为一个软件开发的自由工作者，让自己依然能高效而简洁地解决编程问题，但是不受企业低效的管理束缚且有能满足高质量生活的收入水平； 对于2017年，希望自己能巩固好编程基础、Java语言核心特性，同时学习微服务的关键设计和实现，搭建自己熟悉的快速开发框架。每个月定时的跟踪和调整这个小目标，激励自己前行； 关于专业化，我想目前给自己比较好的定位还是一个企业级基础平台或中间件的工程师，因为比较喜欢深研技术而不大喜欢具体的业务实现。那么我就应该用很多时间投入到这一块的学习中，掌握常见的中间件技术，并深入理解其中的设计、原理和实现； 对于公司的选择，我最关注的几点：1.团队或导师是否优秀（是否能得到成长）；2.开放的技术氛围（是否高效）；3.开放的时间安排（是否能自主选择和安排）；4.项目有挑战（有挑战才有成长）； 关于博客，我想第一个目的还是写给自己看的（总结和记录，并经常回看和更新）；当然如果自己用心写，肯定能吸引到志同道合者一起讨论学习，从而扩展自己的社交圈和影响力。关于博客的建立，尝试过很多平台之后，我选择了Github Pages + Hexo的方式，这种方式即最大可能地减少对服务器的依赖和搭建过程的繁琐，又不缺少灵活性，使得我们能将更多精力放在写作上而不是博客的维护上；关于博客写作的频率，我希望自己一年下来最少能有60篇博文，每个月至少6篇，当然因为我经常写一些读书笔记，因此这个数量不会成为很大的挑战。另外，对于原创类的博文也要更加注重质量、结构和行文。 关于学习，我现在比较喜欢的就是完整的阅读书籍，相比于作者提供的十步学习法，存在几个地方存在不足：一个是关于目标的设定，在学习的过程中没有给自己明确的学习目标，这样在学习之后无法验证，所以在后续的月度计划时我会增加这一部分的内容；二是缺少全局的掌握，比如学习Java网络，我应该先了解下关于Java网络的知识以及我自己所欠缺的，然后有针对性地去学习；三是关于LDLT，现在主要的就是通过笔记来记录和分享学习的过程，后面会要求自己每次阅读结束都需要写书评或读书笔记。 对于生产力的提升，我现在是通过“年度计划 -&gt; 月计划 -&gt; 周计划 -&gt; 日计划”来规划的，对于年度计划和月计划，通过“小目标”定期维护，而对于周计划和日计划，通过任务清单和番茄钟来管理和展示，这样可以确保自己沿着自己的目标方向前进。 关于习惯，现在我上班第一件事就是打开任务清单和番茄计时器，找到今天优先级最高的任务录入番茄计时器，这相当于给自己一个暗示，我今天有这么多的任务需要完成。慢慢地这就成为了一个惯例，我每次休息后第一件事就是看看番茄计时器的完全情况，如果今天完成的不错，会给自己一些奖励，比如下楼喝个奶茶、允许查看网页或者看半个小时手机。 关于理财，我现在比较关注的就是ETF投资，一个是因为本身积累的财富有限，而ETF的定投对于投资的额度限制比较小，另外自己对这一块也比较感兴趣。 关于健身，目前我对自己熬夜方面控制得还算满意。在阅读本书的过程中，我忽然有个想法，我应该增强一些力量、增长一些肌肉，好吧，加一个番茄钟，这个周末研究研究。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Elasticsearch Reference阅读笔记]]></title>
      <url>%2Fpost%2Felasticsearch_reference_notes%2F</url>
      <content type="text"><![CDATA[花了几天把Elasticsearch的官方文档读了一遍，随手记一些关键的笔记。 1. Getting Started1.1 ElasticsearchElasticsearch is a highly scalable open-source full-text search and analytics engine. It allows you to store, search, and analyze big volumes of data quickly and in near real time. It is generally used as the underlying engine/technology that powers applications that have complex search features and requirements. 1.2 Sharding is important for two primary reasons:It allows you to horizontally split/scale your content volumeIt allows you to distribute and parallelize operations across shards (potentially on multiple nodes) thus increasing performance/throughput 1.3 Replication is important for two primary reasons:It provides high availability in case a shard/node fails. For this reason, it is important to note that a replica shard is never allocated on the same node as the original/primary shard that it was copied from.It allows you to scale out your search volume/throughput since searches can be executed on all replicas in parallel. 1.4 Cluster Healthcurl &apos;localhost:9200/_cat/health?v&apos; curl &apos;localhost:9200/_cat/nodes?v&apos; 1.5 List All Indicescurl &apos;localhost:9200/_cat/indices?v&apos; 1.6 Create an Indexcurl -XPUT &apos;localhost:9200/customer?pretty&apos; 1.7 Index and Query a Documentcurl -XPUT &apos;localhost:9200/customer/external/1?pretty&apos; -d &apos; { &quot;name&quot;: &quot;John Doe&quot; }&apos; curl -XGET &apos;localhost:9200/customer/external/1?pretty&apos; 1.8 Delete an Indexcurl -XDELETE &apos;localhost:9200/customer?pretty&apos; 1.9 Updating DocumentsNote though that Elasticsearch does not actually do in-place updates under the hood. Whenever we do an update, Elasticsearch deletes the old document and then indexes a new document with the update applied to it in one shot. 1.10 Deleting Documentscurl -XDELETE &apos;localhost:9200/customer/external/2?pretty&apos; 1.11 Batch ProcessingIn addition to being able to index, update, and delete individual documents, Elasticsearch also provides the ability to perform any of the above operations in batches using the _bulk API. This functionality is important in that it provides a very efficient mechanism to do multiple operations as fast as possible with as little network roundtrips as possible. The bulk API executes all the actions sequentially and in order. If a single action fails for whatever reason, it will continue to process the remainder of the actions after it. When the bulk API returns, it will provide a status for each action (in the same order it was sent in) so that you can check if a specific action failed or not. 1.12 The Search APIIt is important to understand that once you get your search results back, Elasticsearch is completely done with the request and does not maintain any kind of server-side resources or open cursors into your results. This is in stark contrast to many other platforms such as SQL wherein you may initially get a partial subset of your query results up-front and then you have to continuously go back to the server if you want to fetch (or page through) the rest of the results using some kind of stateful server-side cursor. 1.13 Executing FiltersIn the previous section, we skipped over a little detail called the document score (_score field in the search results). The score is a numeric value that is a relative measure of how well the document matches the search query that we specified. The higher the score, the more relevant the document is, the lower the score, the less relevant the document is. But queries do not always need to produce scores, in particular when they are only used for “filtering” the document set. Elasticsearch detects these situations and automatically optimizes query execution in order not to compute useless scores. 1.14 Executing AggregationsAggregations provide the ability to group and extract statistics from your data. The easiest way to think about aggregations is by roughly equating it to the SQL GROUP BY and the SQL aggregate functions. In Elasticsearch, you have the ability to execute searches returning hits and at the same time return aggregated results separate from the hits all in one response. This is very powerful and efficient in the sense that you can run queries and multiple aggregations and get the results back of both (or either) operations in one shot avoiding network roundtrips using a concise and simplified API. 2. Setup2.1 Environment VariablesMost times it is better to leave the default JAVA_OPTS as they are, and use the ES_JAVA_OPTS environment variable in order to set / change JVM settings or arguments. The ES_HEAP_SIZE environment variable allows to set the heap memory that will be allocated to elasticsearch java process. It will allocate the same value to both min and max values, though those can be set explicitly (not recommended) by setting ES_MIN_MEM (defaults to 256m), and ES_MAX_MEM (defaults to 1g). It is recommended to set the min and max memory to the same value, and enable mlockall. 2.2 File DescriptorsMake sure to increase the number of open files descriptors on the machine (or for the user running elasticsearch). Setting it to 32k or even 64k is recommended. In order to test how many open files the process can open, start it with -Des.max-open-files set to true. This will print the number of open files the process can open on startup. Alternatively, you can retrieve the max_file_descriptors for each node using the Nodes Info API, with: curl localhost:9200/_nodes/stats/process?pretty 2.3 Virtual memoryElasticsearch uses a hybrid mmapfs / niofs directory by default to store its indices. The default operating system limits on mmap counts is likely to be too low, which may result in out of memory exceptions. On Linux, you can increase the limits by running the following command as root: sysctl -w vm.max_map_count=262144 To set this value permanently, update the vm.max_map_count setting in /etc/sysctl.conf. 2.4 Memory SettingsMost operating systems try to use as much memory as possible for file system caches and eagerly swap out unused application memory, possibly resulting in the elasticsearch process being swapped. Swapping is very bad for performance and for node stability, so it should be avoided at all costs. There are three options: Disable swap、Configure swappiness、mlockall 2.5 Elasticsearch Settingselasticsearch configuration files can be found under ES_HOME/config folder. The folder comes with two files, the elasticsearch.yml for configuring Elasticsearch different modules, and logging.yml for configuring the Elasticsearch logging. The configuration format is YAML. 2.6 Directory Layoutzip and tar.gz|Type | Description| Location ||:— |:———–|:———|home |Home of elasticsearch installation|{extract.path}bin |Binary scripts including elasticsearch to start a node|{extract.path}/binconf |Configuration files elasticsearch.yml and logging.yml|{extract.path}/configdata |The location of the data files of each index / shard allocated on the node|{extract.path}/datalogs |Log files location|{extract.path}/logsplugins|Plugin files location. Each plugin will be contained in a subdirectory|{extract.path}/pluginsrepo |Shared file system repository locations.|Not configuredscript|Location of script files.|{extract.path}/config/scripts 3 Breaking changes (skipped)4 API Conventions (skipped)5 Document APIs5.1 Index APIThe index API adds or updates a typed JSON document in a specific index, making it searchable. The following example inserts the JSON document into the “twitter” index, under a type called “tweet” with an id of 1: curl -XPUT &apos;http://localhost:9200/twitter/tweet/1&apos; -d &apos;{ &quot;user&quot; : &quot;kimchy&quot;, &quot;post_date&quot; : &quot;2009-11-15T14:12:12&quot;, &quot;message&quot; : &quot;trying out Elasticsearch&quot; }&apos; 5.2 Automatic Index CreationThe index operation automatically creates an index if it has not been created before (check out the create index API for manually creating an index), and also automatically creates a dynamic type mapping for the specific type if one has not yet been created (check out the put mapping API for manually creating a type mapping). 5.3 VersioningEach indexed document is given a version number. The associated version number is returned as part of the response to the index API request. The index API optionally allows for optimistic concurrency control when the version parameter is specified. This will control the version of the document the operation is intended to be executed against. A good example of a use case for versioning is performing a transactional read-then-update. Specifying a version from the document initially read ensures no changes have happened in the meantime (when reading in order to update, it is recommended to set preference to _primary). For example: curl -XPUT &apos;localhost:9200/twitter/tweet/1?version=2&apos; -d &apos;{ &quot;message&quot; : &quot;elasticsearch now has versioning support, double cool!&quot; }&apos; 5.4 Operation TypeThe index operation also accepts an op_type that can be used to force a create operation, allowing for “put-if-absent” behavior. When create is used, the index operation will fail if a document by that id already exists in the index. Here is an example of using the op_type parameter: curl -XPUT &apos;http://localhost:9200/twitter/tweet/1?op_type=create&apos; -d &apos;{ &quot;user&quot; : &quot;kimchy&quot;, &quot;post_date&quot; : &quot;2009-11-15T14:12:12&quot;, &quot;message&quot; : &quot;trying out Elasticsearch&quot; }&apos; 5.5 RoutingBy default, shard placement — or routing — is controlled by using a hash of the document’s id value. For more explicit control, the value fed into the hash function used by the router can be directly specified on a per-operation basis using the routing parameter. For example: curl -XPOST &apos;http://localhost:9200/twitter/tweet?routing=kimchy&apos; -d &apos;{ &quot;user&quot; : &quot;kimchy&quot;, &quot;post_date&quot; : &quot;2009-11-15T14:12:12&quot;, &quot;message&quot; : &quot;trying out Elasticsearch&quot; }&apos; 5.6 Parents &amp; Children （**适合什么场景?）A child document can be indexed by specifying its parent when indexing. For example: curl -XPUT localhost:9200/blogs/blog_tag/1122?parent=1111 -d &apos;{ &quot;tag&quot; : &quot;something&quot; }&apos; When indexing a child document, the routing value is automatically set to be the same as its parent, unless the routing value is explicitly specified using the routing parameter. 5.7 DistributedThe index operation is directed to the primary shard based on its route (see the Routing section above) and performed on the actual node containing this shard. After the primary shard completes the operation, if needed, the update is distributed to applicable replicas. 5.8 Write ConsistencyTo prevent writes from taking place on the “wrong” side of a network partition, by default, index operations only succeed if a quorum (&gt;replicas/2+1) of active shards are available. 5.9 Write Consistency (**如果index设置为1个主分片，两个复制分片。当两个复制分片都不可用的时候index在主分片是否成功？复制分片可用了是否能复制？)To prevent writes from taking place on the “wrong” side of a network partition, by default, index operations only succeed if a quorum (&gt;replicas/2+1) of active shards are available. The index operation only returns after all active shards within the replication group have indexed the document (sync replication). 5.10 RefreshTo refresh the shard (not the whole index) immediately after the operation occurs, so that the document appears in search results immediately, the refresh parameter can be set to true. Setting this option to true should ONLY be done after careful thought and verification that it does not lead to poor performance, both from an indexing and a search standpoint. Note, getting a document using the get API is completely realtime and doesn’t require a refresh. 5.11 Get APIThe get API allows to get a typed JSON document from the index based on its id. The following example gets a JSON document from an index called twitter, under a type called tweet, with id valued 1: curl -XGET &apos;http://localhost:9200/twitter/tweet/1&apos; 5.12 PreferenceControls a preference of which shard replicas to execute the get request on. By default, the operation is randomized between the shard replicas. The preference can be set to: _primary: The operation will go and be executed only on the primary shards. _local: The operation will prefer to be executed on a local allocated shard if possible. Custom (string) value: A custom value will be used to guarantee that the same shards will be used for the same custom value. This can help with “jumping values” when hitting different shards in different refresh states. A sample value can be something like the web session id, or the user name. 5.13 Delete APIThe delete API allows to delete a typed JSON document from a specific index based on its id. The following example deletes the JSON document from an index called twitter, under a type called tweet, with id valued 1: curl -XDELETE &apos;http://localhost:9200/twitter/tweet/1&apos; The delete operation gets hashed into a specific shard id. It then gets redirected into the primary shard within that id group, and replicated (if needed) to shard replicas within that id group. 5.14 Update APIThe update API allows to update a document based on a script provided. The operation gets the document (collocated with the shard) from the index, runs the script (with optional script language and parameters), and index back the result (also allows to delete, or ignore the operation). It uses versioning to make sure no updates have happened during the “get” and “reindex”. Note, this operation still means full reindex of the document, it just removes some network roundtrips and reduces chances of version conflicts between the get and the index. The _source field needs to be enabled for this feature to work. 5.15 Update By Query API (new and should still be considered experimental)The simplest usage of _update_by_query just performs an update on every document in the index without changing the source. This is useful to pick up a new property or some other online mapping change. Here is the API: curl -XPOST &apos;localhost:9200/twitter/_update_by_query?conflicts=proceed&apos; All update and query failures cause the _update_by_query to abort and are returned in the failures of the response. The updates that have been performed still stick. In other words, the process is not rolled back, only aborted. 5.16 Multi Get APIMulti GET API allows to get multiple documents based on an index, type (optional) and id (and possibly routing). The response includes a docs array with all the fetched documents, each element similar in structure to a document provided by the get API. Here is an example: curl &apos;localhost:9200/_mget&apos; -d &apos;{ &quot;docs&quot; : [ { &quot;_index&quot; : &quot;test&quot;, &quot;_type&quot; : &quot;type&quot;, &quot;_id&quot; : &quot;1&quot; }, { &quot;_index&quot; : &quot;test&quot;, &quot;_type&quot; : &quot;type&quot;, &quot;_id&quot; : &quot;2&quot; } ] }&apos; 5.17 Bulk APIThe bulk API makes it possible to perform many index/delete operations in a single API call. This can greatly increase the indexing speed. The REST API endpoint is /_bulk, and it expects the following JSON structure: action_and_meta_data\n optional_source\n action_and_meta_data\n optional_source\n .... action_and_meta_data\n optional_source\n NOTE: the final line of data must end with a newline character \n. The possible actions are index, create, delete and update. index and create expect a source on the next line, and have the same semantics as the op_type parameter to the standard index API (i.e. create will fail if a document with the same index and type exists already, whereas index will add or replace a document as necessary). delete does not expect a source on the following line, and has the same semantics as the standard delete API. update expects that the partial doc, upsert and script and its options are specified on the next line. Here is an example of a correct sequence of bulk commands: { &quot;index&quot; : { &quot;_index&quot; : &quot;test&quot;, &quot;_type&quot; : &quot;type1&quot;, &quot;_id&quot; : &quot;1&quot; } } { &quot;field1&quot; : &quot;value1&quot; } { &quot;delete&quot; : { &quot;_index&quot; : &quot;test&quot;, &quot;_type&quot; : &quot;type1&quot;, &quot;_id&quot; : &quot;2&quot; } } { &quot;create&quot; : { &quot;_index&quot; : &quot;test&quot;, &quot;_type&quot; : &quot;type1&quot;, &quot;_id&quot; : &quot;3&quot; } } { &quot;field1&quot; : &quot;value3&quot; } { &quot;update&quot; : {&quot;_id&quot; : &quot;1&quot;, &quot;_type&quot; : &quot;type1&quot;, &quot;_index&quot; : &quot;index1&quot;} } { &quot;doc&quot; : {&quot;field2&quot; : &quot;value2&quot;} } The endpoints are /_bulk, /{index}/_bulk, and {index}/{type}/_bulk. When the index or the index/type are provided, they will be used by default on bulk items that don’t provide them explicitly. A note on the format. The idea here is to make processing of this as fast as possible. As some of the actions will be redirected to other shards on other nodes, only action_meta_data is parsed on the receiving node side. 5.18 Reindex API (new and should still be considered experimental)The most basic form of _reindex just copies documents from one index to another. This will copy documents from the twitter index into the new_twitter index: POST /_reindex { &quot;source&quot;: { &quot;index&quot;: &quot;twitter&quot; }, &quot;dest&quot;: { &quot;index&quot;: &quot;new_twitter&quot; } } You can limit the documents by adding a type to the source or by adding a query. This will only copy tweet’s made by kimchy into new_twitter: POST /_reindex { &quot;source&quot;: { &quot;index&quot;: &quot;twitter&quot;, &quot;type&quot;: &quot;tweet&quot;, &quot;query&quot;: { &quot;term&quot;: { &quot;user&quot;: &quot;kimchy&quot; } } }, &quot;dest&quot;: { &quot;index&quot;: &quot;new_twitter&quot; } } 5.19 Term VectorsReturns information and statistics on terms in the fields of a particular document. The document could be stored in the index or artificially provided by the user. Term vectors are realtime by default, not near realtime. This can be changed by setting realtime parameter to false. curl -XGET &apos;http://localhost:9200/twitter/tweet/1/_termvectors?pretty=true&apos; Three types of values can be requested: term information, term statistics and field statistics. By default, all term information and field statistics are returned for all fields but no term statistics. Term information term frequency in the field (always returned) term positions (positions : true) start and end offsets (offsets : true) term payloads (payloads : true), as base64 encoded bytes Term statistics total term frequency (how often a term occurs in all documents) document frequency (the number of documents containing the current term) Setting term_statistics to true (default is false) will return term statistics. By default these values are not returned since term statistics can have a serious performance impact. Field statistics document count (how many documents contain this field) sum of document frequencies (the sum of document frequencies for all terms in this field) sum of total term frequencies (the sum of total term frequencies of each term in this field) The term and field statistics are not accurate. Deleted documents are not taken into account. The information is only retrieved for the shard the requested document resides in, unless dfs is set to true. The term and field statistics are therefore only useful as relative measures whereas the absolute numbers have no meaning in this context. By default, when requesting term vectors of artificial documents, a shard to get the statistics from is randomly selected. See more examples: https://www.elastic.co/guide/en/elasticsearch/reference/2.3/docs-termvectors.html#_behaviour 6 Search APIs6.1 SearchThe search API allows you to execute a search query and get back search hits that match the query. The query can either be provided using a simple query string as a parameter, or using a request body. All search APIs can be applied across multiple types within an index, and across multiple indices with support for the multi index syntax. 6.2 URI SearchA search request can be executed purely using a URI by providing request parameters. Not all search options are exposed when executing a search using this mode, but it can be handy for quick “curl tests”. Here is an example: curl -XGET &apos;http://localhost:9200/twitter/tweet/_search?q=user:kimchy&apos; 6.3 Request Body SearchThe search request can be executed with a search DSL, which includes the Query DSL, within its body. Here is an example: curl -XGET &apos;http://localhost:9200/twitter/tweet/_search&apos; -d &apos;{ &quot;query&quot; : { &quot;term&quot; : { &quot;user&quot; : &quot;kimchy&quot; } } }&apos; 6.4 QueryThe query element within the search request body allows to define a query using the Query DSL. { &quot;query&quot; : { &quot;term&quot; : { &quot;user&quot; : &quot;kimchy&quot; } } } 6.5 From / SizePagination of results can be done by using the from and size parameters. { &quot;from&quot; : 0, &quot;size&quot; : 10, &quot;query&quot; : { &quot;term&quot; : { &quot;user&quot; : &quot;kimchy&quot; } } } Note that from + size can not be more than the index.max_result_window index setting which defaults to 10,000. See the Scroll API for more efficient ways to do deep scrolling. 6.6 Sort (**多个字段的排序规则是怎么样的？)Allows to add one or more sort on specific fields. Each sort can be reversed as well. The sort is defined on a per field level, with special field name for _score to sort by score, and _doc to sort by index order. { &quot;sort&quot; : [ { &quot;post_date&quot; : {&quot;order&quot; : &quot;asc&quot;}}, &quot;user&quot;, { &quot;name&quot; : &quot;desc&quot; }, { &quot;age&quot; : &quot;desc&quot; }, &quot;_score&quot; ], &quot;query&quot; : { &quot;term&quot; : { &quot;user&quot; : &quot;kimchy&quot; } } } The sort values for each document returned are also returned as part of the response.The order option can have the following values: asc: Sort in ascending order desc: Sort in descending order The order defaults to desc when sorting on the _score, and defaults to asc when sorting on anything else. 6.7 Sort mode optionElasticsearch supports sorting by array or multi-valued fields. The mode option controls what array value is picked for sorting the document it belongs to. The mode option can have the following values: min: Pick the lowest value. max: Pick the highest value. sum: Use the sum of all values as sort value. Only applicable for number based array fields. avg: Use the average of all values as sort value. Only applicable for number based array fields. median: Use the median of all values as sort value. Only applicable for number based array fields. 6.8 Missing ValuesThe missing parameter specifies how docs which are missing the field should be treated: The missing value can be set to _last, _first, or a custom value (that will be used for missing docs as the sort value). For example: { &quot;sort&quot; : [ { &quot;price&quot; : {&quot;missing&quot; : &quot;_last&quot;} }, ], &quot;query&quot; : { &quot;term&quot; : { &quot;user&quot; : &quot;kimchy&quot; } } } 6.9 Script Based SortingAllow to sort based on custom scripts, here is an example: { &quot;query&quot; : { .... }, &quot;sort&quot; : { &quot;_script&quot; : { &quot;type&quot; : &quot;number&quot;, &quot;script&quot; : { &quot;inline&quot;: &quot;doc[&apos;field_name&apos;].value * factor&quot;, &quot;params&quot; : { &quot;factor&quot; : 1.1 } }, &quot;order&quot; : &quot;asc&quot; } } } 6.10 Memory ConsiderationsWhen sorting, the relevant sorted field values are loaded into memory. This means that per shard, there should be enough memory to contain them. For string based types, the field sorted on should not be analyzed / tokenized. For numeric types, if possible, it is recommended to explicitly set the type to narrower types (like short, integer and float). 6.11 Source filteringAllows to control how the _source field is returned with every hit. By default operations return the contents of the _source field unless you have used the fields parameter or if the _source field is disabled. To disable _source retrieval set to false. The _source also accepts one or more wildcard patterns to control what parts of the _source should be returned. Finally, for complete control, you can specify both include and exclude patterns. 6.12 Fields The fields parameter is about fields that are explicitly marked as stored in the mapping, which is off by default and generally not recommended. Use source filtering instead to select subsets of the original source document to be returned. 6.13 Script FieldsAllows to return a script evaluation (based on different fields) for each hit, for example: { &quot;query&quot; : { ... }, &quot;script_fields&quot; : { &quot;test1&quot; : { &quot;script&quot; : &quot;_source.obj1.obj2&quot; }, &quot;test2&quot; : { &quot;script&quot; : { &quot;inline&quot;: &quot;doc[&apos;my_field_name&apos;].value * factor&quot;, &quot;params&quot; : { &quot;factor&quot; : 2.0 } } } } } Note the _source keyword here to navigate the json-like model. It’s important to understand the difference between doc[‘my_field’].value and _source.my_field. The first, using the doc keyword, will cause the terms for that field to be loaded to memory (cached), which will result in faster execution, but more memory consumption. Also, the doc[…] notation only allows for simple valued fields (can’t return a json object from it) and make sense only on non-analyzed or single term based fields. The _source on the other hand causes the source to be loaded, parsed, and then only the relevant part of the json is returned. 6.14 Field Data Fields (**需要了解stored和fielddata的概念)Allows to return the field data representation of a field for each hit, for example: { &quot;query&quot; : { ... }, &quot;fielddata_fields&quot; : [&quot;test1&quot;, &quot;test2&quot;] } Field data fields can work on fields that are not stored. It’s important to understand that using the fielddata_fields parameter will cause the terms for that field to be loaded to memory (cached), which will result in more memory consumption. 6.15 Post filterThe post_filter is applied to the search hits at the very end of a search request, after aggregations have already been calculated. { &quot;query&quot;: { &quot;bool&quot;: { &quot;filter&quot;: { &quot;term&quot;: { &quot;brandName&quot;: &quot;vans&quot; } } } }, &quot;aggs&quot;: { &quot;colors&quot;: { &quot;terms&quot;: { &quot;field&quot;: &quot;colorNames&quot; } }, &quot;color_red&quot;: { &quot;filter&quot;: { &quot;term&quot;: { &quot;colorNames&quot;: &quot;红色&quot; } }, &quot;aggs&quot;: { &quot;smallSorts&quot;: { &quot;terms&quot;: { &quot;field&quot;: &quot;smallSort&quot; } } } } }, &quot;post_filter&quot;: { &quot;term&quot;: { &quot;colorNames&quot;: &quot;红色&quot; } } } The main query now finds all products by vans, regardless of color. The colors agg returns popular colors by vans. The color_red agg limits the small sort sub-aggregation to red vans products. Finally, the post_filter removes colors other than red from the search hits. 6.16 HighlightingAllows to highlight search results on one or more fields. The implementation uses either the lucene highlighter, fast-vector-highlighter or postings-highlighter. The following is an example of the search request body: { &quot;query&quot; : {...}, &quot;highlight&quot; : { &quot;fields&quot; : { &quot;content&quot; : {} } } } 6.16.1 Plain highlighterThe default choice of highlighter is of type plain and uses the Lucene highlighter. It tries hard to reflect the query matching logic in terms of understanding word importance and any word positioning criteria in phrase queries. 6.16.2 Postings highlighterIf index_options is set to offsets in the mapping the postings highlighter will be used instead of the plain highlighter. The postings highlighter: Is faster since it doesn’t require to reanalyze the text to be highlighted: the larger the documents the better the performance gain should be Requires less disk space than term_vectors, needed for the fast vector highlighter Breaks the text into sentences and highlights them. Plays really well with natural languages, not as well with - fields containing for instance html markup Treats the document as the whole corpus, and scores individual sentences as if they were documents in this corpus, using the BM25 algorithm 6.16.3 Fast vector highlighterIf term_vector information is provided by setting term_vector to with_positions_offsets in the mapping then the fast vector highlighter will be used instead of the plain highlighter. The fast vector highlighter: Is faster especially for large fields (&gt; 1MB) Can be customized with boundary_chars, boundary_max_scan, and fragment_offset (see below) Requires setting term_vector to with_positions_offsets which increases the size of the index Can combine matches from multiple fields into one result. See matched_fields Can assign different weights to matches at different positions allowing for things like phrase matches being - sorted above term matches when highlighting a Boosting Query that boosts phrase matches over term matches 6.17 RescoringRescoring can help to improve precision by reordering just the top (eg 100 - 500) documents returned by the query and post_filter phases, using a secondary (usually more costly) algorithm, instead of applying the costly algorithm to all documents in the index. A rescore request is executed on each shard before it returns its results to be sorted by the node handling the overall search request. Currently the rescore API has only one implementation: the query rescorer, which uses a query to tweak the scoring. In the future, alternative rescorers may be made available, for example, a pair-wise rescorer. By default the scores from the original query and the rescore query are combined linearly to produce the final _score for each document. The relative importance of the original query and of the rescore query can be controlled with the query_weight and rescore_query_weight respectively. Both default to 1. { &quot;query&quot;: { &quot;match&quot;: { &quot;productName.productName_ansj&quot;: { &quot;operator&quot;: &quot;or&quot;, &quot;query&quot;: &quot;连帽 套装&quot;, &quot;type&quot;: &quot;boolean&quot; } } }, &quot;_source&quot;: [ &quot;productName&quot; ], &quot;rescore&quot;: { &quot;window_size&quot;: 50, &quot;query&quot;: { &quot;rescore_query&quot;: { &quot;match&quot;: { &quot;productName.productName_ansj&quot;: { &quot;query&quot;: &quot;连帽 套装&quot;, &quot;type&quot;: &quot;phrase&quot;, &quot;slop&quot;: 2 } } }, &quot;query_weight&quot;: 0.7, &quot;rescore_query_weight&quot;: 1.2 } } } Score Mode total:Add the original score and the rescore query score. The default. multiply: Multiply the original score by the rescore query score. Useful for function query rescores. avg: Average the original score and the rescore query score. max: Take the max of original score and the rescore query score. min: Take the min of the original score and the rescore query score. 6.18 Search TypeThere are different execution paths that can be done when executing a distributed search. The distributed search operation needs to be scattered to all the relevant shards and then all the results are gathered back. When doing scatter/gather type execution, there are several ways to do that, specifically with search engines. One of the questions when executing a distributed search is how many results to retrieve from each shard. For example, if we have 10 shards, the 1st shard might hold the most relevant results from 0 till 10, with other shards results ranking below it. For this reason, when executing a request, we will need to get results from 0 till 10 from all shards, sort them, and then return the results if we want to ensure correct results. Another question, which relates to the search engine, is the fact that each shard stands on its own. When a query is executed on a specific shard, it does not take into account term frequencies and other search engine information from the other shards. If we want to support accurate ranking, we would need to first gather the term frequencies from all shards to calculate global term frequencies, then execute the query on each shard using these global frequencies. Also, because of the need to sort the results, getting back a large document set, or even scrolling it, while maintaining the correct sorting behavior can be a very expensive operation. For large result set scrolling, it is best to sort by _doc if the order in which documents are returned is not important. Elasticsearch is very flexible and allows to control the type of search to execute on a per search request basis. The type can be configured by setting the search_type parameter in the query string. The types are: 6.18.1 Query Then Fetch(query_then_fetch)The request is processed in two phases. In the first phase, the query is forwarded to all involved shards. Each shard executes the search and generates a sorted list of results, local to that shard. Each shard returns just enough information to the coordinating node to allow it merge and re-sort the shard level results into a globally sorted set of results, of maximum length size. During the second phase, the coordinating node requests the document content (and highlighted snippets, if any) from only the relevant shards. Note: This is the default setting, if you do not specify a search_type in your request. 6.18.2 Dfs, Query Then Fetch(dfs_query_then_fetch)Same as “Query Then Fetch”, except for an initial scatter phase which goes and computes the distributed term frequencies for more accurate scoring. 6.18.3 Count (Deprecated in 2.0.0-beta1)6.18.4 Scan (Deprecated in 2.1.0)6.19 ScrollWhile a search request returns a single “page” of results, the scroll API can be used to retrieve large numbers of results (or even all results) from a single search request, in much the same way as you would use a cursor on a traditional database. Scrolling is not intended for real time user requests, but rather for processing large amounts of data, e.g. in order to reindex the contents of one index into a new index with a different configuration. 6.20 PreferenceControls a preference of which shard replicas to execute the search request on. By default, the operation is randomized between the shard replicas. The preference is a query string parameter which can be set to: _primary: The operation will go and be executed only on the primary shards. _primary_first: The operation will go and be executed on the primary shard, and if not available (failover), will execute on other shards. _replica: The operation will go and be executed only on a replica shard. _replica_first: The operation will go and be executed only on a replica shard, and if not available (failover), will execute on other shards. _local: The operation will prefer to be executed on a local allocated shard if possible. _only_node:xyz: Restricts the search to execute only on a node with the provided node id (xyz in this case). _prefer_node:xyz: Prefers execution on the node with the provided node id (xyz in this case) if applicable. _shards:2,3: Restricts the operation to the specified shards. (2 and 3 in this case). This preference can be combined with other preferences but it has to appear first: _shards:2,3;_primary _only_nodes: Restricts the operation to nodes specified in node specification https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster.html Custom (string) value: A custom value will be used to guarantee that the same shards will be used for the same custom value. This can help with “jumping values” when hitting different shards in different refresh states. A sample value can be something like the web session id, or the user name. 6.21 ExplainEnables explanation for each hit on how its score was computed. { &quot;explain&quot;: true, &quot;query&quot; : { &quot;term&quot; : { &quot;user&quot; : &quot;kimchy&quot; } } } 6.22 VersionReturns a version for each search hit. { &quot;version&quot;: true, &quot;query&quot; : { &quot;term&quot; : { &quot;user&quot; : &quot;kimchy&quot; } } } 6.23 Index BoostAllows to configure different boost level per index when searching across more than one indices. This is very handy when hits coming from one index matter more than hits coming from another index (think social graph where each user has an index). { &quot;indices_boost&quot; : { &quot;index1&quot; : 1.4, &quot;index2&quot; : 1.3 } } 6.24 Inner hits (**Skipped: 和parent/child有关系，后续一起学习)6.25 Search TemplateThe /_search/template endpoint allows to use the mustache language to pre render search requests, before they are executed and fill existing templates with template parameters. GET /_search/template { &quot;inline&quot; : { &quot;query&quot;: { &quot;match&quot; : { &quot;{{my_field}}&quot; : &quot;{{my_value}}&quot; } }, &quot;size&quot; : &quot;{{my_size}}&quot; }, &quot;params&quot; : { &quot;my_field&quot; : &quot;foo&quot;, &quot;my_value&quot; : &quot;bar&quot;, &quot;my_size&quot; : 5 } } 6.26 Search Shards APIThe search shards api returns the indices and shards that a search request would be executed against. This can give useful feedback for working out issues or planning optimizations with routing and shard preferences. curl -XGET &apos;localhost:9200/twitter/_search_shards&apos; 6.27 Suggesters (Skipped)The suggest feature suggests similar looking terms based on a provided text by using a suggester. Parts of the suggest feature are still under development. 6.28 Count APIThe count API allows to easily execute a query and get the number of matches for that query. It can be executed across one or more indices and across one or more types. The query can either be provided using a simple query string as a parameter, or using the Query DSL defined within the request body. Here is an example: curl -XGET &apos;http://localhost:9200/twitter/tweet/_count?q=user:kimchy&apos; curl -XGET &apos;http://localhost:9200/twitter/tweet/_count&apos; -d &apos; { &quot;query&quot; : { &quot;term&quot; : { &quot;user&quot; : &quot;kimchy&quot; } } }&apos; 6.29 Validate APIThe validate API allows a user to validate a potentially expensive query without executing it. The following example shows how it can be used: curl -XPUT &apos;http://localhost:9200/twitter/tweet/1&apos; -d &apos;{ &quot;user&quot; : &quot;kimchy&quot;, &quot;post_date&quot; : &quot;2009-11-15T14:12:12&quot;, &quot;message&quot; : &quot;trying out Elasticsearch&quot; }&apos; When the query is valid, the response contains valid:true: curl -XGET &apos;http://localhost:9200/twitter/_validate/query?q=user:foo&apos; {&quot;valid&quot;:true,&quot;_shards&quot;:{&quot;total&quot;:1,&quot;successful&quot;:1,&quot;failed&quot;:0}} 6.30 Explain APIThe explain api computes a score explanation for a query and a specific document. This can give useful feedback whether a document matches or didn’t match a specific query. curl -XGET &apos;localhost:9200/twitter/tweet/1/_explain&apos; -d &apos;{ &quot;query&quot; : { &quot;term&quot; : { &quot;message&quot; : &quot;search&quot; } } }&apos; 6.30 Profile API (experimental and may be changed or removed)The Profile API provides detailed timing information about the execution of individual components in a query. It gives the user insight into how queries are executed at a low level so that the user can understand why certain queries are slow, and take steps to improve their slow queries. The output from the Profile API is very verbose, especially for complicated queries executed across many shards. Pretty-printing the response is recommended to help understand the output. curl -XGET &apos;localhost:9200/_search&apos; -d &apos;{ &quot;profile&quot;: true, &quot;query&quot; : { &quot;match&quot; : { &quot;message&quot; : &quot;search test&quot; } } } 6.31 Field stats API (experimental and may be changed or removed)The field stats api allows one to find statistical properties of a field without executing a search, but looking up measurements that are natively available in the Lucene index. This can be useful to explore a dataset which you don’t know much about. For example, this allows creating a histogram aggregation with meaningful intervals based on the min/max range of values. The field stats api by defaults executes on all indices, but can execute on specific indices too. All indices: curl -XGET &quot;http://localhost:9200/_field_stats?fields=rating&quot; Specific indices: curl -XGET &quot;http://localhost:9200/index1,index2/_field_stats?fields=rating&quot; 7 Aggregations7.1 AggregationsThe aggregations framework helps provide aggregated data based on a search query. It is based on simple building blocks called aggregations, that can be composed in order to build complex summaries of the data. An aggregation can be seen as a unit-of-work that builds analytic information over a set of documents. The context of the execution defines what this document set is (e.g. a top-level aggregation executes within the context of the executed query/filters of the search request). There are many different types of aggregations, each with its own purpose and output. To better understand these types, it is often easier to break them into three main families: Bucketing: A family of aggregations that build buckets, where each bucket is associated with a key and a document criterion. When the aggregation is executed, all the buckets criteria are evaluated on every document in the context and when a criterion matches, the document is considered to “fall in” the relevant bucket. By the end of the aggregation process, we’ll end up with a list of buckets - each one with a set of documents that “belong” to it. Metric: Aggregations that keep track and compute metrics over a set of documents. Pipeline: Aggregations that aggregate the output of other aggregations and their associated metrics The interesting part comes next. Since each bucket effectively defines a document set (all documents belonging to the bucket), one can potentially associate aggregations on the bucket level, and those will execute within the context of that bucket. This is where the real power of aggregations kicks in: aggregations can be nested! 7.2 Structuring AggregationsThe following snippet captures the basic structure of aggregations: &quot;aggregations&quot; : { &quot;&lt;aggregation_name&gt;&quot; : { &quot;&lt;aggregation_type&gt;&quot; : { &lt;aggregation_body&gt; } [,&quot;meta&quot; : { [&lt;meta_data_body&gt;] } ]? [,&quot;aggregations&quot; : { [&lt;sub_aggregation&gt;]+ } ]? } [,&quot;&lt;aggregation_name_2&gt;&quot; : { ... } ]* } 7.3 Metrics AggregationsThe aggregations in this family compute metrics based on values extracted in one way or another from the documents that are being aggregated. The values are typically extracted from the fields of the document (using the field data), but can also be generated using scripts. Numeric metrics aggregations are a special type of metrics aggregation which output numeric values. Some aggregations output a single numeric metric (e.g. avg) and are called single-value numeric metrics aggregation, others generate multiple metrics (e.g. stats) and are called multi-value numeric metrics aggregation. The distinction between single-value and multi-value numeric metrics aggregations plays a role when these aggregations serve as direct sub-aggregations of some bucket aggregations (some bucket aggregations enable you to sort the returned buckets based on the numeric metrics in each bucket). 7.3.1 Avg AggregationA single-value metrics aggregation that computes the average of numeric values that are extracted from the aggregated documents. These values can be extracted either from specific numeric fields in the documents, or be generated by a provided script. { &quot;size&quot;: 0, &quot;query&quot;: { &quot;match_all&quot;: {} }, &quot;aggs&quot;: { &quot;avg_price&quot;: { &quot;avg&quot;: { &quot;field&quot;: &quot;salesPrice&quot; } } } } &quot;aggregations&quot;: { &quot;avg_price&quot;: { &quot;value&quot;: 428.51063644785825 } } 7.3.2 Cardinality AggregationA single-value metrics aggregation that calculates an approximate count of distinct values. Values can be extracted either from specific fields in the document or generated by a script. { &quot;size&quot;: 0, &quot;query&quot;: { &quot;match_all&quot;: {} }, &quot;aggs&quot;: { &quot;brand_count&quot;: { &quot;cardinality&quot;: { &quot;field&quot;: &quot;brandId&quot; } } } } &quot;aggregations&quot;: { &quot;brand_count&quot;: { &quot;value&quot;: 1186 } } 7.3.3 Stats AggregationA multi-value metrics aggregation that computes stats over numeric values extracted from the aggregated documents. These values can be extracted either from specific numeric fields in the documents, or be generated by a provided script. The stats that are returned consist of: min, max, sum, count and avg. { &quot;size&quot;: 0, &quot;query&quot;: { &quot;match_all&quot;: {} }, &quot;aggs&quot;: { &quot;price_stat&quot;: { &quot;stats&quot;: { &quot;field&quot;: &quot;salesPrice&quot; } } } } &quot;aggregations&quot;: { &quot;price_stat&quot;: { &quot;count&quot;: 221275, &quot;min&quot;: 0, &quot;max&quot;: 131231, &quot;avg&quot;: 428.51063644785825, &quot;sum&quot;: 94818691.07999983 } } 7.3.4 Extended Stats AggregationA multi-value metrics aggregation that computes stats over numeric values extracted from the aggregated documents. These values can be extracted either from specific numeric fields in the documents, or be generated by a provided script. The extended_stats aggregations is an extended version of the stats aggregation, where additional metrics are added such as sum_of_squares, variance, std_deviation and std_deviation_bounds. { &quot;size&quot;: 0, &quot;query&quot;: { &quot;match_all&quot;: {} }, &quot;aggs&quot;: { &quot;price_stat&quot;: { &quot;extended_stats&quot;: { &quot;field&quot;: &quot;salesPrice&quot; } } } } &quot;aggregations&quot;: { &quot;price_stat&quot;: { &quot;count&quot;: 221275, &quot;min&quot;: 0, &quot;max&quot;: 131231, &quot;avg&quot;: 428.51063644785825, &quot;sum&quot;: 94818691.07999983, &quot;sum_of_squares&quot;: 118950750156.63016, &quot;variance&quot;: 353948.4012870255, &quot;std_deviation&quot;: 594.9356278514723, &quot;std_deviation_bounds&quot;: { &quot;upper&quot;: 1618.3818921508027, &quot;lower&quot;: -761.3606192550864 } } } 7.3.5 Geo Bounds Aggregation (Skipped)7.3.6 Geo Centroid Aggregation (Skipped)7.3.7 Max AggregationA single-value metrics aggregation that keeps track and returns the maximum value among the numeric values extracted from the aggregated documents. &quot;aggs&quot;: { &quot;max_price&quot;: { &quot;max&quot;: { &quot;field&quot;: &quot;salesPrice&quot; } } } 7.3.8 Min AggregationA single-value metrics aggregation that keeps track and returns the minimum value among numeric values extracted from the aggregated documents. &quot;aggs&quot;: { &quot;min_price&quot;: { &quot;min&quot;: { &quot;field&quot;: &quot;salesPrice&quot; } } } 7.3.9 Percentiles AggregationA multi-value metrics aggregation that calculates one or more percentiles over numeric values extracted from the aggregated documents. Percentiles show the point at which a certain percentage of observed values occur. For example, the 95th percentile is the value which is greater than 95% of the observed values. When a range of percentiles are retrieved, they can be used to estimate the data distribution and determine if the data is skewed, bimodal, etc. &quot;aggs&quot;: { &quot;price_outlier&quot;: { &quot;percentiles&quot;: { &quot;field&quot;: &quot;salesPrice&quot; } } } &quot;aggregations&quot;: { &quot;price_outlier&quot;: { &quot;values&quot;: { &quot;1.0&quot;: 19, &quot;5.0&quot;: 49.049088235742005, &quot;25.0&quot;: 148.8903318997934, &quot;50.0&quot;: 288.33201291736634, &quot;75.0&quot;: 521.2972145384141, &quot;95.0&quot;: 1286.9096656603726, &quot;99.0&quot;: 2497.931283641535 } } } 7.3.10 Percentile Ranks AggregationA multi-value metrics aggregation that calculates one or more percentile ranks over numeric values extracted from the aggregated documents. Percentile rank show the percentage of observed values which are below certain value. For example, if a value is greater than or equal to 95% of the observed values it is said to be at the 95th percentile rank. &quot;aggs&quot;: { &quot;price_outlier&quot;: { &quot;percentile_ranks&quot;: { &quot;field&quot;: &quot;salesPrice&quot;, &quot;values&quot;: [ 200, 500 ] } } } &quot;aggregations&quot;: { &quot;price_outlier&quot;: { &quot;values&quot;: { &quot;200.0&quot;: 37.906112721751086, &quot;500.0&quot;: 74.407593883831 } } } 7.3.11 Scripted Metric Aggregation (experimental and may be changed or removed)A metric aggregation that executes using scripts to provide a metric output. See a detailed example: https://www.elastic.co/guide/en/elasticsearch/reference/2.3/search-aggregations-metrics-scripted-metric-aggregation.html 7.3.12 Sum AggregationA single-value metrics aggregation that sums up numeric values that are extracted from the aggregated documents. { &quot;query&quot;: { &quot;term&quot;: { &quot;brandName&quot;: &quot;vans&quot; } }, &quot;aggs&quot;: { &quot;salesNum_total&quot;: { &quot;sum&quot;: { &quot;field&quot;: &quot;salesNum&quot; } } } } &quot;aggregations&quot;: { &quot;salesNum_total&quot;: { &quot;value&quot;: 253365 } } 7.3.13 Top hits AggregationA top_hits metric aggregator keeps track of the most relevant document being aggregated. This aggregator is intended to be used as a sub aggregator, so that the top matching documents can be aggregated per bucket. The top_hits aggregator can effectively be used to group result sets by certain fields via a bucket aggregator. One or more bucket aggregators determines by which properties a result set get sliced into. Options: from: The offset from the first result you want to fetch. size: The maximum number of top matching hits to return per bucket. By default the top three matching hits are returned. sort: How the top matching hits should be sorted. By default the hits are sorted by the score of the main query. 7.3.14 Value Count AggregationA single-value metrics aggregation that counts the number of values that are extracted from the aggregated documents. Typically, this aggregator will be used in conjunction with other single-value aggregations. For example, when computing the avg one might be interested in the number of values the average is computed over. 7.4 Bucket AggregationsBucket aggregations don’t calculate metrics over fields like the metrics aggregations do, but instead, they create buckets of documents. Each bucket is associated with a criterion (depending on the aggregation type) which determines whether or not a document in the current context “falls” into it. In other words, the buckets effectively define document sets. In addition to the buckets themselves, the bucket aggregations also compute and return the number of documents that “fell into” each bucket. Bucket aggregations, as opposed to metrics aggregations, can hold sub-aggregations. These sub-aggregations will be aggregated for the buckets created by their “parent” bucket aggregation. There are different bucket aggregators, each with a different “bucketing” strategy. Some define a single bucket, some define fixed number of multiple buckets, and others dynamically create the buckets during the aggregation process. 7.4.1 Children AggregationA special single bucket aggregation that enables aggregating from buckets on parent document types to buckets on child documents. 7.4.2 Histogram AggregationA multi-bucket values source based aggregation that can be applied on numeric values extracted from the documents. It dynamically builds fixed size (a.k.a. interval) buckets over the values. For example, if the documents have a field that holds a price (numeric), we can configure this aggregation to dynamically build buckets with interval 5 (in case of price it may represent $5). When the aggregation executes, the price field of every document will be evaluated and will be rounded down to its closest bucket - for example, if the price is 32 and the bucket size is 5 then the rounding will yield 30 and thus the document will “fall” into the bucket that is associated with the key 30. From the rounding function above it can be seen that the intervals themselves must be integers. { &quot;size&quot;: 0, &quot;query&quot;: { &quot;term&quot;: { &quot;brandName&quot;: &quot;vans&quot; } }, &quot;aggs&quot;: { &quot;prices&quot;: { &quot;histogram&quot;: { &quot;field&quot;: &quot;salesPrice&quot;, &quot;interval&quot;: 200 } } } } &quot;aggregations&quot;: { &quot;prices&quot;: { &quot;buckets&quot;: [ { &quot;key&quot;: 0, &quot;doc_count&quot;: 838 } , { &quot;key&quot;: 200, &quot;doc_count&quot;: 1123 } , { &quot;key&quot;: 400, &quot;doc_count&quot;: 804 } , { &quot;key&quot;: 600, &quot;doc_count&quot;: 283 } , { &quot;key&quot;: 800, &quot;doc_count&quot;: 64 } , { &quot;key&quot;: 1000, &quot;doc_count&quot;: 16 } , { &quot;key&quot;: 1200, &quot;doc_count&quot;: 18 } , { &quot;key&quot;: 1400, &quot;doc_count&quot;: 8 } , { &quot;key&quot;: 1600, &quot;doc_count&quot;: 7 } ] } } 7.4.3 Date Histogram AggregationA multi-bucket aggregation similar to the histogram except it can only be applied on date values. Since dates are represented in elasticsearch internally as long values, it is possible to use the normal histogram on dates as well, though accuracy will be compromised. The reason for this is in the fact that time based intervals are not fixed (think of leap years and on the number of days in a month). For this reason, we need special support for time based data. From a functionality perspective, this histogram supports the same features as the normal histogram. The main difference is that the interval can be specified by date/time expressions. Requesting bucket intervals of a month. { &quot;aggs&quot; : { &quot;articles_over_time&quot; : { &quot;date_histogram&quot; : { &quot;field&quot; : &quot;date&quot;, &quot;interval&quot; : &quot;month&quot; } } } } 7.4.4 Range AggregationA multi-bucket value source based aggregation that enables the user to define a set of ranges - each representing a bucket. During the aggregation process, the values extracted from each document will be checked against each bucket range and “bucket” the relevant/matching document. Note that this aggregation includes the from value and excludes the to value for each range. { &quot;size&quot;: 0, &quot;query&quot;: { &quot;term&quot;: { &quot;brandName&quot;: &quot;vans&quot; } }, &quot;aggs&quot;: { &quot;price_ranges&quot;: { &quot;range&quot;: { &quot;field&quot;: &quot;salesPrice&quot;, &quot;ranges&quot;: [ { &quot;to&quot;: 200 }, { &quot;from&quot;: 200, &quot;to&quot;: 500 }, { &quot;from&quot;: 500 } ] } } } } &quot;aggregations&quot;: { &quot;price_ranges&quot;: { &quot;buckets&quot;: [ { &quot;key&quot;: &quot;*-200.0&quot;, &quot;to&quot;: 200, &quot;to_as_string&quot;: &quot;200.0&quot;, &quot;doc_count&quot;: 838 } , { &quot;key&quot;: &quot;200.0-500.0&quot;, &quot;from&quot;: 200, &quot;from_as_string&quot;: &quot;200.0&quot;, &quot;to&quot;: 500, &quot;to_as_string&quot;: &quot;500.0&quot;, &quot;doc_count&quot;: 1594 } , { &quot;key&quot;: &quot;500.0-*&quot;, &quot;from&quot;: 500, &quot;from_as_string&quot;: &quot;500.0&quot;, &quot;doc_count&quot;: 729 } ] } } 7.4.5 Date Range AggregationA range aggregation that is dedicated for date values. The main difference between this aggregation and the normal range aggregation is that the from and to values can be expressed in Date Math expressions, and it is also possible to specify a date format by which the from and to response fields will be returned. Note that this aggregation includes the from value and excludes the to value for each range. { &quot;aggs&quot;: { &quot;range&quot;: { &quot;date_range&quot;: { &quot;field&quot;: &quot;date&quot;, &quot;format&quot;: &quot;MM-yyy&quot;, &quot;ranges&quot;: [ { &quot;to&quot;: &quot;now-10M/M&quot; }, { &quot;from&quot;: &quot;now-10M/M&quot; } ] } } } } 7.4.6 Filter AggregationDefines a single bucket of all the documents in the current document set context that match a specified filter. Often this will be used to narrow down the current aggregation context to a specific set of documents. { &quot;size&quot;: 0, &quot;query&quot;: { &quot;term&quot;: { &quot;brandName&quot;: &quot;vans&quot; } }, &quot;aggs&quot;: { &quot;red_products&quot;: { &quot;filter&quot;: { &quot;term&quot;: { &quot;colorNames&quot;: &quot;红色&quot; } }, &quot;aggs&quot;: { &quot;avg_price&quot;: { &quot;avg&quot;: { &quot;field&quot;: &quot;salesPrice&quot; } } } } } } 7.4.7 Filters AggregationDefines a multi bucket aggregation where each bucket is associated with a filter. Each bucket will collect all documents that match its associated filter. { &quot;aggs&quot; : { &quot;messages&quot; : { &quot;filters&quot; : { &quot;filters&quot; : { &quot;errors&quot; : { &quot;term&quot; : { &quot;body&quot; : &quot;error&quot; }}, &quot;warnings&quot; : { &quot;term&quot; : { &quot;body&quot; : &quot;warning&quot; }} } }, &quot;aggs&quot; : { &quot;monthly&quot; : { &quot;histogram&quot; : { &quot;field&quot; : &quot;timestamp&quot;, &quot;interval&quot; : &quot;1M&quot; } } } } } } In the above example, we analyze log messages. The aggregation will build two collection (buckets) of log messages - one for all those containing an error, and another for all those containing a warning. And for each of these buckets it will break them down by month.Response: &quot;aggs&quot; : { &quot;messages&quot; : { &quot;buckets&quot; : { &quot;errors&quot; : { &quot;doc_count&quot; : 34, &quot;monthly&quot; : { &quot;buckets&quot; : [ ... // the histogram monthly breakdown ] } }, &quot;warnings&quot; : { &quot;doc_count&quot; : 439, &quot;monthly&quot; : { &quot;buckets&quot; : [ ... // the histogram monthly breakdown ] } } } } } 7.4.8 Geo Distance Aggregation (Skipped)7.4.9 GeoHash grid Aggregation (Skipped)7.4.10 Global AggregationDefines a single bucket of all the documents within the search execution context. This context is defined by the indices and the document types you’re searching on, but is not influenced by the search query itself. 7.4.11 IPv4 Range AggregationJust like the dedicated date range aggregation, there is also a dedicated range aggregation for IPv4 typed fields: 7.4.12 Missing AggregationA field data based single bucket aggregation, that creates a bucket of all documents in the current document set context that are missing a field value (effectively, missing a field or having the configured NULL value set). This aggregator will often be used in conjunction with other field data bucket aggregators (such as ranges) to return information for all the documents that could not be placed in any of the other buckets due to missing field data values. 7.4.13 Nested AggregationA special single bucket aggregation that enables aggregating nested documents. 7.4.14 Reverse nested AggregationA special single bucket aggregation that enables aggregating on parent docs from nested documents. Effectively this aggregation can break out of the nested block structure and link to other nested structures or the root document, which allows nesting other aggregations that aren’t part of the nested object in a nested aggregation. 7.4.15 Significant Terms AggregationAn aggregation that returns interesting or unusual occurrences of terms in a set. Warning: The significant_terms aggregation can be very heavy when run on large indices. Work is in progress to provide more lightweight sampling techniques. As a result, the API for this feature may change in backwards incompatible ways. Example use cases: Suggesting “H5N1” when users search for “bird flu” in text Identifying the merchant that is the “common point of compromise” from the transaction history of credit card owners reporting loss Suggesting keywords relating to stock symbol $ATI for an automated news classifier Spotting the fraudulent doctor who is diagnosing more than his fair share of whiplash injuries Spotting the tire manufacturer who has a disproportionate number of blow-outs { &quot;query&quot;: { &quot;terms&quot;: { &quot;smallSort&quot;: [ &quot;牛仔裤&quot; ] } }, &quot;aggregations&quot;: { &quot;significantColors&quot;: { &quot;significant_terms&quot;: { &quot;field&quot;: &quot;colorNames&quot; } } } } “aggregations”: { &quot;significantColors&quot;: { &quot;doc_count&quot;: 7365, &quot;buckets&quot;: [ { &quot;key&quot;: &quot;蓝色&quot;, &quot;doc_count&quot;: 4750, &quot;score&quot;: 1.9775784118031037, &quot;bg_count&quot;: 35656 } , { &quot;key&quot;: &quot;蓝&quot;, &quot;doc_count&quot;: 1287, &quot;score&quot;: 0.35538801144606, &quot;bg_count&quot;: 12949 } , { &quot;key&quot;: &quot;原色&quot;, &quot;doc_count&quot;: 39, &quot;score&quot;: 0.09168423890725523, &quot;bg_count&quot;: 65 } , { &quot;key&quot;: &quot;浅蓝色&quot;, &quot;doc_count&quot;: 79, &quot;score&quot;: 0.031059308568117887, &quot;bg_count&quot;: 619 } , { &quot;key&quot;: &quot;水洗&quot;, &quot;doc_count&quot;: 10, &quot;score&quot;: 0.030522422208204166, &quot;bg_count&quot;: 13 } , { &quot;key&quot;: &quot;深蓝色&quot;, &quot;doc_count&quot;: 131, &quot;score&quot;: 0.024955048079471253, &quot;bg_count&quot;: 1664 } ] } } GINO: 为什么深蓝色排在最后？在所有的商品(总数为224778)中，共有1664件商品为深蓝色；但是对于牛仔裤(总数为7365)，只有131件牛仔裤为深蓝色，因此认为他们之间的关联度很低，就不是很推荐深蓝色的牛仔裤。再试试看品牌推荐的效果： { &quot;query&quot;: { &quot;terms&quot;: { &quot;smallSort&quot;: [ &quot;牛仔裤&quot; ] } }, &quot;aggregations&quot;: { &quot;significantBrands&quot;: { &quot;significant_terms&quot;: { &quot;field&quot;: &quot;brandNameEn&quot; } } } } &quot;aggregations&quot;: { &quot;significantBrands&quot;: { &quot;doc_count&quot;: 7365, &quot;buckets&quot;: [ { &quot;key&quot;: &quot;xinfeiyang&quot;, &quot;doc_count&quot;: 1061, &quot;score&quot;: 4.179762739096525, &quot;bg_count&quot;: 1079 } , { &quot;key&quot;: &quot;lee&quot;, &quot;doc_count&quot;: 432, &quot;score&quot;: 0.5581216486055066, &quot;bg_count&quot;: 1254 } , { &quot;key&quot;: &quot;levi&apos;s&quot;, &quot;doc_count&quot;: 473, &quot;score&quot;: 0.5004641576199044, &quot;bg_count&quot;: 1642 } , { &quot;key&quot;: &quot;jasonwood&quot;, &quot;doc_count&quot;: 495, &quot;score&quot;: 0.3789564299098067, &quot;bg_count&quot;: 2276 } , { &quot;key&quot;: &quot;able&quot;, &quot;doc_count&quot;: 304, &quot;score&quot;: 0.36273857444325336, &quot;bg_count&quot;: 948 } , { &quot;key&quot;: &quot;jeans&quot;, &quot;doc_count&quot;: 307, &quot;score&quot;: 0.3581144552023549, &quot;bg_count&quot;: 977 } , { &quot;key&quot;: &quot;agamemnon&quot;, &quot;doc_count&quot;: 119, &quot;score&quot;: 0.3311112854845905, &quot;bg_count&quot;: 169 } , { &quot;key&quot;: &quot;krbl/korakublue&quot;, &quot;doc_count&quot;: 183, &quot;score&quot;: 0.31616211163218183, &quot;bg_count&quot;: 407 } , { &quot;key&quot;: &quot;wrangler&quot;, &quot;doc_count&quot;: 198, &quot;score&quot;: 0.264814268306479, &quot;bg_count&quot;: 557 } , { &quot;key&quot;: &quot;evisu&quot;, &quot;doc_count&quot;: 143, &quot;score&quot;: 0.19733121908058618, &quot;bg_count&quot;: 391 } ] } } 7.4.16 Sampler Aggregation (experimental and may be changed or removed)A filtering aggregation used to limit any sub aggregations’ processing to a sample of the top-scoring documents. Optionally, diversity settings can be used to limit the number of matches that share a common value such as an “author”. Example use cases: Tightening the focus of analytics to high-relevance matches rather than the potentially very long tail of low-quality matches Removing bias from analytics by ensuring fair representation of content from different sources Reducing the running cost of aggregations that can produce useful results using only samples e.g. significant_terms 7.4.17 Terms AggregationA multi-bucket value source based aggregation where buckets are dynamically built - one per unique value. { &quot;query&quot;: { &quot;terms&quot;: { &quot;smallSort&quot;: [ &quot;牛仔裤&quot; ] } }, &quot;aggs&quot;: { &quot;genders&quot;: { &quot;terms&quot;: { &quot;field&quot;: &quot;genderS&quot; } } } } &quot;aggregations&quot;: { &quot;genders&quot;: { &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ { &quot;key&quot;: &quot;男&quot;, &quot;doc_count&quot;: 5271 } , { &quot;key&quot;: &quot;女&quot;, &quot;doc_count&quot;: 2255 } ] } } The size parameter can be set to define how many term buckets should be returned out of the overall terms list. By default, the node coordinating the search process will request each shard to provide its own top size term buckets and once all shards respond, it will reduce the results to the final list that will then be returned to the client. This means that if the number of unique terms is greater than size, the returned list is slightly off and not accurate (it could be that the term counts are slightly off and it could even be that a term that should have been in the top size buckets was not returned). If set to 0, the size will be set to Integer.MAX_VALUE. 7.5 Pipeline Aggregations (experimental and may be changed or removed, Skipped）7.6 Caching heavy aggregationsFrequently used aggregations (e.g. for display on the home page of a website) can be cached for faster responses. These cached results are the same results that would be returned by an uncached aggregation – you will never get stale results. See Shard request cache for more details. 7.7 Returning only aggregation resultsThere are many occasions when aggregations are required but search hits are not. For these cases the hits can be ignored by setting size=0. For example: curl -XGET &apos;http://localhost:9200/twitter/tweet/_search&apos; -d &apos;{ &quot;size&quot;: 0, &quot;aggregations&quot;: { &quot;my_agg&quot;: { &quot;terms&quot;: { &quot;field&quot;: &quot;text&quot; } } } } &apos; Setting size to 0 avoids executing the fetch phase of the search making the request more efficient. 8 Indices APIsThe indices APIs are used to manage individual indices, index settings, aliases, mappings, index templates and warmers. 8.1 Create IndexThe create index API allows to instantiate an index. Elasticsearch provides support for multiple indices, including executing operations across several indices. curl -XPUT &apos;http://localhost:9200/twitter/&apos; -d &apos;{ &quot;settings&quot; : { &quot;index&quot; : { &quot;number_of_shards&quot; : 3, &quot;number_of_replicas&quot; : 2 } } }&apos; The create index API allows to provide a set of one or more mappings: curl -XPOST localhost:9200/test -d &apos;{ &quot;settings&quot; : { &quot;number_of_shards&quot; : 1 }, &quot;mappings&quot; : { &quot;type1&quot; : { &quot;properties&quot; : { &quot;field1&quot; : { &quot;type&quot; : &quot;string&quot;, &quot;index&quot; : &quot;not_analyzed&quot; } } } } }&apos; 8.2 Delete IndexThe delete index API allows to delete an existing index. curl -XDELETE &apos;http://localhost:9200/twitter/&apos; 8.3 Get IndexThe get index API allows to retrieve information about one or more indexes. curl -XGET &apos;http://localhost:9200/twitter/&apos; 8.4 Indices ExistsUsed to check if the index (indices) exists or not. For example: curl -XHEAD -i &apos;http://localhost:9200/twitter&apos; The HTTP status code indicates if the index exists or not. A 404 means it does not exist, and 200 means it does. 8.5 Open / Close Index APIThe open and close index APIs allow to close an index, and later on opening it. A closed index has almost no overhead on the cluster (except for maintaining its metadata), and is blocked for read/write operations. A closed index can be opened which will then go through the normal recovery process. The REST endpoint is /{index}/_close and /{index}/_open. For example: curl -XPOST &apos;localhost:9200/my_index/_close&apos; curl -XPOST &apos;localhost:9200/my_index/_open&apos; 8.6 Put MappingThe PUT mapping API allows you to provide type mappings while creating a new index, add a new type to an existing index, or add new fields to an existing type: PUT twitter { &quot;mappings&quot;: { &quot;tweet&quot;: { &quot;properties&quot;: { &quot;message&quot;: { &quot;type&quot;: &quot;string&quot; } } } } } PUT twitter/_mapping/user { &quot;properties&quot;: { &quot;name&quot;: { &quot;type&quot;: &quot;string&quot; } } } PUT twitter/_mapping/tweet { &quot;properties&quot;: { &quot;user_name&quot;: { &quot;type&quot;: &quot;string&quot; } } } Creates an index called twitter with the message field in the tweet mapping type. Uses the PUT mapping API to add a new mapping type called user. Uses the PUT mapping API to add a new field called user_name to the tweet mapping type. 8.7 Get MappingThe get mapping API allows to retrieve mapping definitions for an index or index/type. curl -XGET &apos;http://localhost:9200/twitter/_mapping/tweet&apos; 8.8 Get Field MappingThe get field mapping API allows you to retrieve mapping definitions for one or more fields. This is useful when you do not need the complete type mapping returned by the Get Mapping API. The following returns the mapping of the field text only: curl -XGET &apos;http://localhost:9200/twitter/_mapping/tweet/field/text&apos; 8.9 Types ExistsUsed to check if a type/types exists in an index/indices. curl -XHEAD -i &apos;http://localhost:9200/twitter/tweet&apos; 8.10 Index AliasesAPIs in elasticsearch accept an index name when working against a specific index, and several indices when applicable. The index aliases API allow to alias an index with a name, with all APIs automatically converting the alias name to the actual index name. An alias can also be mapped to more than one index, and when specifying it, the alias will automatically expand to the aliases indices. An alias can also be associated with a filter that will automatically be applied when searching, and routing values. An alias cannot have the same name as an index. Here is a sample of associating the alias alias1 with index test1: curl -XPOST &apos;http://localhost:9200/_aliases&apos; -d &apos; { &quot;actions&quot; : [ { &quot;add&quot; : { &quot;index&quot; : &quot;test1&quot;, &quot;alias&quot; : &quot;alias1&quot; } } ] }&apos; Filtered AliasesAliases with filters provide an easy way to create different “views” of the same index. The filter can be defined using Query DSL and is applied to all Search, Count, Delete By Query and More Like This operations with this alias. To create a filtered alias, first we need to ensure that the fields already exist in the mapping: curl -XPUT &apos;http://localhost:9200/test1&apos; -d &apos;{ &quot;mappings&quot;: { &quot;type1&quot;: { &quot;properties&quot;: { &quot;user&quot; : { &quot;type&quot;: &quot;string&quot;, &quot;index&quot;: &quot;not_analyzed&quot; } } } } }&apos; Now we can create an alias that uses a filter on field user: curl -XPOST &apos;http://localhost:9200/_aliases&apos; -d &apos;{ &quot;actions&quot; : [ { &quot;add&quot; : { &quot;index&quot; : &quot;test1&quot;, &quot;alias&quot; : &quot;alias2&quot;, &quot;filter&quot; : { &quot;term&quot; : { &quot;user&quot; : &quot;kimchy&quot; } } } } ] }&apos; An alias can also be added or deleted with the endpoint curl -XPUT &apos;localhost:9200/logs_201305/_alias/2013&apos; curl -XDELETE &apos;localhost:9200/logs_201305/_alias/2013&apos; curl -XGET &apos;localhost:9200/_alias/2013&apos; curl -XHEAD -i &apos;localhost:9200/_alias/2013&apos; 8.11 Update Indices SettingsChange specific index level settings in real time. The REST endpoint is /_settings (to update all indices) or {index}/_settings to update one (or more) indices settings. curl -XPUT &apos;localhost:9200/my_index/_settings&apos; -d &apos; { &quot;index&quot; : { &quot;number_of_replicas&quot; : 4 } }&apos; Bulk Indexing UsageFor example, the update settings API can be used to dynamically change the index from being more performant for bulk indexing, and then move it to more real time indexing state. Before the bulk indexing is started, use: curl -XPUT localhost:9200/test/_settings -d &apos;{ &quot;index&quot; : { &quot;refresh_interval&quot; : &quot;-1&quot; } }&apos; (Another optimization option is to start the index without any replicas, and only later adding them, but that really depends on the use case). Then, once bulk indexing is done, the settings can be updated (back to the defaults for example): curl -XPUT localhost:9200/test/_settings -d &apos;{ &quot;index&quot; : { &quot;refresh_interval&quot; : &quot;1s&quot; } }&apos; And, a force merge should be called: curl -XPOST &apos;http://localhost:9200/test/_forcemerge?max_num_segments=5&apos; 8.12 Get SettingsThe get settings API allows to retrieve settings of index/indices: curl -XGET &apos;http://localhost:9200/twitter/_settings&apos; 8.13 AnalyzePerforms the analysis process on a text and return the tokens breakdown of the text. Can be used without specifying an index against one of the many built in analyzers: curl -XGET &apos;localhost:9200/twitter/_analyze&apos; -d &apos; { &quot;analyzer&quot; : &quot;standard&quot;, &quot;text&quot; : &quot;this is a test&quot; }&apos; All parameters can also supplied as request parameters. For example: curl -XGET &apos;localhost:9200/_analyze?tokenizer=keyword&amp;filter=lowercase&amp;text=this+is+a+test&apos; Explain AnalyzeIf you want to get more advanced details, set explain to true (defaults to false). It will output all token attributes for each token. You can filter token attributes you want to output by setting attributes option. POST productindex/_analyze { &quot;field&quot;: &quot;productName.productName_ansj&quot;, &quot;text&quot;: &quot;S.T.A.M.P.S./诗坦表 时尚PU皮表带&quot;, &quot;explain&quot;: true } 8.14 Index TemplatesIndex templates allow you to define templates that will automatically be applied when new indices are created. The templates include both settings and mappings, and a simple pattern template that controls whether the template should be applied to the new index. 8.15 Shadow replica indices(experimental and may be changed or removed)If you would like to use a shared filesystem, you can use the shadow replicas settings to choose where on disk the data for an index should be kept, as well as how Elasticsearch should replay operations on all the replica shards of an index. 8.16 Indices StatsIndices level stats provide statistics on different operations happening on an index. The API provides statistics on the index level scope (though most stats can also be retrieved using node level scope). The following returns high level aggregation and index level stats for all indices: curl localhost:9200/_stats Specific index stats can be retrieved using: curl localhost:9200/index1,index2/_stats By default, all stats are returned, returning only specific stats can be specified as well in the URI. Those stats can be any of: docs:The number of docs / deleted docs (docs not yet merged out). Note, affected by refreshing the index. store: The size of the index. indexing: Indexing statistics, can be combined with a comma separated list of types to provide document type level stats. get: Get statistics, including missing stats. search: Search statistics. You can include statistics for custom groups by adding an extra groups parameter (search operations can be associated with one or more groups). The groups parameter accepts a comma separated list of group names. Use _all to return statistics for all groups. completion: Completion suggest statistics. fielddata: Fielddata statistics. flush:Flush statistics. merge:Merge statistics. request_cache: Shard request cache statistics. refresh: Refresh statistics. suggest: Suggest statistics. warmer: Warmer statistics. translog: Translog statistics. 8.17 Indices SegmentsProvide low level segments information that a Lucene index (shard level) is built with. Allows to be used to provide more information on the state of a shard and an index, possibly optimization information, data “wasted” on deletes, and so on. Endpoints include segments for a specific index, several indices, or all: curl -XGET &apos;http://localhost:9200/test/_segments&apos; curl -XGET &apos;http://localhost:9200/test1,test2/_segments&apos; curl -XGET &apos;http://localhost:9200/_segments&apos; _0: The key of the JSON document is the name of the segment. This name is used to generate file names: all files starting with this segment name in the directory of the shard belong to this segment. generation: A generation number that is basically incremented when needing to write a new segment. The segment name is derived from this generation number. num_docs: The number of non-deleted documents that are stored in this segment. deleted_docs: The number of deleted documents that are stored in this segment. It is perfectly fine if this number is greater than 0, space is going to be reclaimed when this segment gets merged. size_in_bytes: The amount of disk space that this segment uses, in bytes. memory_in_bytes: Segments need to store some data into memory in order to be searchable efficiently. This number returns the number of bytes that are used for that purpose. A value of -1 indicates that Elasticsearch was not able to compute this number. committed: Whether the segment has been sync’ed on disk. Segments that are committed would survive a hard reboot. No need to worry in case of false, the data from uncommitted segments is also stored in the transaction log so that Elasticsearch is able to replay changes on the next start. search: Whether the segment is searchable. A value of false would most likely mean that the segment has been written to disk but no refresh occurred since then to make it searchable. version: The version of Lucene that has been used to write this segment. compound: Whether the segment is stored in a compound file. When true, this means that Lucene merged all files from the segment in a single one in order to save file descriptors. 8.18 Indices Recovery(Advanced Topic)The indices recovery API provides insight into on-going index shard recoveries. Recovery status may be reported for specific indices, or cluster-wide. 8.19 Indices Shard StoresProvides store information for shard copies of indices. Store information reports on which nodes shard copies exist, the shard copy version, indicating how recent they are, and any exceptions encountered while opening the shard index or from earlier engine failure. By default, only lists store information for shards that have at least one unallocated copy. When the cluster health status is yellow, this will list store information for shards that have at least one unassigned replica. When the cluster health status is red, this will list store information for shards, which has unassigned primaries. Endpoints include shard stores information for a specific index, several indices, or all: curl -XGET &apos;http://localhost:9200/test/_shard_stores&apos; curl -XGET &apos;http://localhost:9200/test1,test2/_shard_stores&apos; curl -XGET &apos;http://localhost:9200/_shard_stores&apos; 8.20 Clear CacheThe clear cache API allows to clear either all caches or specific cached associated with one or more indices. curl -XPOST &apos;http://localhost:9200/twitter/_cache/clear&apos; The API, by default, will clear all caches. Specific caches can be cleaned explicitly by setting query, fielddata or request. All caches relating to a specific field(s) can also be cleared by specifying fields parameter with a comma delimited list of the relevant fields. 8.21 FlushThe flush API allows to flush one or more indices through an API. The flush process of an index basically frees memory from the index by flushing data to the index storage and clearing the internal transaction log. By default, Elasticsearch uses memory heuristics in order to automatically trigger flush operations as required in order to clear memory. POST /twitter/_flush The flush API accepts the following request parameters: wait_if_ongoing: If set to true the flush operation will block until the flush can be executed if another flush operation is already executing. The default is false and will cause an exception to be thrown on the shard level if another flush operation is already running. force: Whether a flush should be forced even if it is not necessarily needed ie. if no changes will be committed to the index. This is useful if transaction log IDs should be incremented even if no uncommitted changes are present. (This setting can be considered as internal) 8.22 Synced Flush(Advanced Topic)Elasticsearch tracks the indexing activity of each shard. Shards that have not received any indexing operations for 5 minutes are automatically marked as inactive. This presents an opportunity for Elasticsearch to reduce shard resources and also perform a special kind of flush, called synced flush. A synced flush performs a normal flush, then adds a generated unique marker (sync_id) to all shards. 8.23 RefreshThe refresh API allows to explicitly refresh one or more index, making all operations performed since the last refresh available for search. The (near) real-time capabilities depend on the index engine used. For example, the internal one requires refresh to be called, but by default a refresh is scheduled periodically. curl -XPOST &apos;http://localhost:9200/twitter/_refresh&apos; 8.24 Force MergeThe force merge API allows to force merging of one or more indices through an API. The merge relates to the number of segments a Lucene index holds within each shard. The force merge operation allows to reduce the number of segments by merging them. This call will block until the merge is complete. If the http connection is lost, the request will continue in the background, and any new requests will block until the previous force merge is complete. curl -XPOST &apos;http://localhost:9200/twitter/_forcemerge&apos; The force merge API accepts the following request parameters: max_num_segments: The number of segments to merge to. To fully merge the index, set it to 1. Defaults to simply checking if a merge needs to execute, and if so, executes it. only_expunge_deletes: Should the merge process only expunge segments with deletes in it. In Lucene, a document is not deleted from a segment, just marked as deleted. During a merge process of segments, a new segment is created that does not have those deletes. This flag allows to only merge segments that have deletes. Defaults to false. Note that this won’t override the index.merge.policy.expunge_deletes_allowed threshold. flush: Should a flush be performed after the forced merge. Defaults to true. 9 cat APIsJSON is great… for computers. Even if it’s pretty-printed, trying to find relationships in the data is tedious. Human eyes, especially when looking at an ssh terminal, need compact and aligned text. The cat API aims to meet this need. All the cat commands accept a query string parameter help to see all the headers and info they provide, and the /_cat command alone lists all the available commands. 9.1 cat aliasesaliases shows information about currently configured aliases to indices including filter and routing infos. curl &apos;localhost:9200/_cat/aliases?v&apos; 9.2 cat allocationallocation provides a snapshot of how many shards are allocated to each data node and how much disk space they are using. curl &apos;localhost:9200/_cat/allocation?v&apos; 9.3 cat countcount provides quick access to the document count of the entire cluster, or individual indices. curl &apos;localhost:9200/_cat/indices?v&apos; green wiki1 3 0 10000 331 168.5mb 168.5mb green wiki2 3 0 428 0 8mb 8mb curl &apos;localhost:9200/_cat/count?v&apos; 1384314124582 19:42:04 10428 curl &apos;localhost:9200/_cat/count/wiki2?v&apos; 1384314139815 19:42:19 428 9.4 cat fielddatafielddata shows how much heap memory is currently being used by fielddata on every data node in the cluster. curl &apos;localhost:9200/_cat/fielddata?v&apos; id host ip node total body text c223lARiSGeezlbrcugAYQ myhost1 10.20.100.200 Jessica Jones 385.6kb 159.8kb 225.7kb waPCbitNQaCL6xC8VxjAwg myhost2 10.20.100.201 Adversary 435.2kb 159.8kb 275.3kb yaDkp-G3R0q1AJ-HUEvkSQ myhost3 10.20.100.202 Microchip 284.6kb 109.2kb 175.3kb Fields can be specified either as a query parameter, or in the URL path. 9.5 cat healthhealth is a terse, one-line representation of the same information from /_cluster/health. It has one option ts to disable the timestamping. curl &apos;localhost:9200/_cat/health?v&amp;ts=0&apos; cluster status nodeTotal nodeData shards pri relo init unassign tasks foo green 3 3 3 3 0 0 0 0 9.6 cat indicesThe indices command provides a cross-section of each index. This information spans nodes. curl &apos;localhost:9200/_cat/indices?v&apos; 9.7 cat mastermaster doesn’t have any extra options. It simply displays the master’s node ID, bound IP address, and node name. curl &apos;localhost:9200/_cat/master?v&apos; id ip node Ntgn2DcuTjGuXlhKDUD4vA 192.168.56.30 Solarr 9.8 cat nodeattrsThe nodeattrs command shows custom node attributes. curl &apos;localhost:9200/_cat/nodeattrs?v&apos; node host ip attr value Black Bolt epsilon 192.168.1.8 rack rack314 Black Bolt epsilon 192.168.1.8 azone us-east-1 9.9 cat nodesThe nodes command shows the cluster topology. curl &apos;localhost:9200/_cat/nodes?v&apos; SP4H 4727 192.168.56.30 9300 2.3.4 1.8.0_73 72.1gb 35.4 93.9mb 79 239.1mb 0.45 3.4h d m Boneyard _uhJ 5134 192.168.56.10 9300 2.3.4 1.8.0_73 72.1gb 33.3 93.9mb 85 239.1mb 0.06 3.4h d * Athena HfDp 4562 192.168.56.20 9300 2.3.4 1.8.0_73 72.2gb 74.5 93.9mb 83 239.1mb 0.12 3.4h d m Zarek 9.10 cat pending taskspending_tasks provides the same information as the /_cluster/pending_tasks API in a convenient tabular format. curl &apos;localhost:9200/_cat/pending_tasks?v&apos; 9.11 cat pluginsThe plugins command provides a view per node of running plugins. This information spans nodes. curl &apos;localhost:9200/_cat/plugins?v&apos; 9.12 cat recoveryThe recovery command is a view of index shard recoveries, both on-going and previously completed. It is a more compact view of the JSON recovery API. curl &apos;localhost:9200/_cat/recovery?v&apos; 9.13 cat repositoriesThe repositories command shows the snapshot repositories registered in the cluster. curl &apos;localhost:9200/_cat/repositories?v&apos; 9.14 cat thread poolThe thread_pool command shows cluster wide thread pool statistics per node. By default the active, queue and rejected statistics are returned for the bulk, index and search thread pools. curl &apos;localhost:9200/_cat/thread_pool?v&apos; 9.15 cat shardsThe shards command is the detailed view of what nodes contain which shards. It will tell you if it’s a primary or replica, the number of docs, the bytes it takes on disk, and the node where it’s located. Here we see a single index, with three primary shards and no replicas: curl &apos;localhost:9200/_cat/shards?v&apos; 9.16 cat segmentsThe segments command provides low level information about the segments in the shards of an index. It provides information similar to the _segments endpoint. curl &apos;http://localhost:9200/_cat/segments?v&apos; 9.17 cat snapshotsThe snapshots command shows all snapshots that belong to a specific repository. To find a list of available repositories to query, the command /_cat/repositories can be used. Querying the snapshots of a repository named repo1 then looks as follows. curl &apos;localhost:9200/_cat/snapshots/repo1?v&apos; 10. Cluster APIs10.1 Cluster HealthThe cluster health API allows to get a very simple status on the health of the cluster. curl &apos;http://localhost:9200/_cluster/health?pretty=true&apos; { &quot;cluster_name&quot; : &quot;testcluster&quot;, &quot;status&quot; : &quot;green&quot;, &quot;timed_out&quot; : false, &quot;number_of_nodes&quot; : 2, &quot;number_of_data_nodes&quot; : 2, &quot;active_primary_shards&quot; : 5, &quot;active_shards&quot; : 10, &quot;relocating_shards&quot; : 0, &quot;initializing_shards&quot; : 0, &quot;unassigned_shards&quot; : 0, &quot;delayed_unassigned_shards&quot;: 0, &quot;number_of_pending_tasks&quot; : 0, &quot;number_of_in_flight_fetch&quot;: 0, &quot;task_max_waiting_in_queue_millis&quot;: 0, &quot;active_shards_percent_as_number&quot;: 100 } 10.2 Cluster StateThe cluster state API allows to get a comprehensive state information of the whole cluster. curl &apos;http://localhost:9200/_cluster/state&apos; 10.3 Cluster StatsThe Cluster Stats API allows to retrieve statistics from a cluster wide perspective. The API returns basic index metrics (shard numbers, store size, memory usage) and information about the current nodes that form the cluster (number, roles, os, jvm versions, memory usage, cpu and installed plugins). curl -XGET &apos;http://localhost:9200/_cluster/stats?human&amp;pretty&apos; 10.4 Pending cluster tasksThe pending cluster tasks API returns a list of any cluster-level changes (e.g. create index, update mapping, allocate or fail shard) which have not yet been executed. curl -XGET &apos;http://localhost:9200/_cluster/pending_tasks&apos; 10.5 Cluster Reroute(Advanced Topic)The reroute command allows to explicitly execute a cluster reroute allocation command including specific commands. For example, a shard can be moved from one node to another explicitly, an allocation can be canceled, or an unassigned shard can be explicitly allocated on a specific node. Here is a short example of how a simple reroute API call: curl -XPOST &apos;localhost:9200/_cluster/reroute&apos; -d &apos;{ &quot;commands&quot; : [ { &quot;move&quot; : { &quot;index&quot; : &quot;test&quot;, &quot;shard&quot; : 0, &quot;from_node&quot; : &quot;node1&quot;, &quot;to_node&quot; : &quot;node2&quot; } }, { &quot;allocate&quot; : { &quot;index&quot; : &quot;test&quot;, &quot;shard&quot; : 1, &quot;node&quot; : &quot;node3&quot; } } ] }&apos; 10.6 Cluster Update SettingsAllows to update cluster wide specific settings. Settings updated can either be persistent (applied cross restarts) or transient (will not survive a full cluster restart). Here is an example: curl -XPUT localhost:9200/_cluster/settings -d &apos;{ &quot;persistent&quot; : { &quot;discovery.zen.minimum_master_nodes&quot; : 2 } }&apos; Or: curl -XPUT localhost:9200/_cluster/settings -d &apos;{ &quot;transient&quot; : { &quot;discovery.zen.minimum_master_nodes&quot; : 2 } }&apos; The cluster responds with the settings updated. So the response for the last example will be: { &quot;persistent&quot; : {}, &quot;transient&quot; : { &quot;discovery.zen.minimum_master_nodes&quot; : &quot;2&quot; } } Cluster wide settings can be returned using: curl -XGET localhost:9200/_cluster/settings 10.7 Nodes statisticsThe cluster nodes stats API allows to retrieve one or more (or all) of the cluster nodes statistics. curl -XGET &apos;http://localhost:9200/_nodes/stats&apos; curl -XGET &apos;http://localhost:9200/_nodes/nodeId1,nodeId2/stats&apos; By default, all stats are returned. You can limit this by combining any of indices, os, process, jvm, transport, http, fs, breaker and thread_pool. For example: indices: Indices stats about size, document count, indexing and deletion times, search times, field cache size, merges and flushes fs: File system information, data path, free disk space, read/write stats (see FS information) http: HTTP connection information jvm: JVM stats, memory pool information, garbage collection, buffer pools, number of loaded/unloaded classes os: Operating system stats, load average, mem, swap (see OS statistics) process: Process statistics, memory consumption, cpu usage, open file descriptors (see Process statistics) thread_pool: Statistics about each thread pool, including current size, queue and rejected tasks transport: Transport statistics about sent and received bytes in cluster communication breaker: Statistics about the field data circuit breaker 10.8 Nodes InfoThe cluster nodes info API allows to retrieve one or more (or all) of the cluster nodes information. curl -XGET &apos;http://localhost:9200/_nodes&apos; curl -XGET &apos;http://localhost:9200/_nodes/nodeId1,nodeId2&apos; By default, it just returns all attributes and core settings for a node. It also allows to get only information on settings, os, process, jvm, thread_pool, transport, http and plugins. 10.9 Nodes hot_threadsAn API allowing to get the current hot threads on each node in the cluster. Endpoints are /_nodes/hot_threads, and /_nodes/{nodesIds}/hot_threads. curl -XGET &apos;http://localhost:9200/_nodes/hot_threads&apos; The output is plain text with a breakdown of each node’s top hot threads. Parameters allowed are: threads: number of hot threads to provide, defaults to 3. interval: the interval to do the second sampling of threads. Defaults to 500ms. type: The type to sample, defaults to cpu, but supports wait and block to see hot threads that are in wait or block state. ignore_idle_threads: If true, known idle threads (e.g. waiting in a socket select, or to get a task from an empty queue) are filtered out. Defaults to true. 11. Query DSLElasticsearch provides a full Query DSL based on JSON to define queries. Think of the Query DSL as an AST of queries, consisting of two types of clauses: Leaf query clauses look for a particular value in a particular field, such as the match, term or range queries. These queries can be used by themselves. Compound query clauses wrap other leaf or compound queries and are used to combine multiple queries in a logical fashion (such as the bool or dis_max query), or to alter their behaviour (such as the not or constant_score query). Query clauses behave differently depending on whether they are used in query context or filter context. Query contextA query clause used in query context answers the question “How well does this document match this query clause?” Besides deciding whether or not the document matches, the query clause also calculates a _score representing how well the document matches, relative to other documents. Query context is in effect whenever a query clause is passed to a query parameter, such as the query parameter in the search API. Filter contextIn filter context, a query clause answers the question “Does this document match this query clause?” The answer is a simple Yes or No – no scores are calculated. Filter context is mostly used for filtering structured data, e.g. Does this timestamp fall into the range 2015 to 2016? Is the status field set to &quot;published&quot;? Frequently used filters will be cached automatically by Elasticsearch, to speed up performance. Filter context is in effect whenever a query clause is passed to a filter parameter, such as the filter or must_not parameters in the bool query, the filter parameter in the constant_score query, or the filter aggregation. 11.1 Match All QueryThe most simple query, which matches all documents, giving them all a _score of 1.0. { &quot;match_all&quot;: {} } 11.2 Full text queriesThe high-level full text queries are usually used for running full text queries on full text fields like the body of an email. They understand how the field being queried is analyzed and will apply each field’s analyzer (or search_analyzer) to the query string before executing. The queries in this group are: match query:The standard query for performing full text queries, including fuzzy matching and phrase or proximity queries. multi_match query: The multi-field version of the match query. common_terms query: A more specialized query which gives more preference to uncommon words. query_string query: Supports the compact Lucene query string syntax, allowing you to specify AND|OR|NOT conditions and multi-field search within a single query string. For expert users only. simple_query_string: A simpler, more robust version of the query_string syntax suitable for exposing directly to users. 11.2.1 Match QueryA family of match queries that accepts text/numerics/dates, analyzes them, and constructs a query. For example: POST /productindex/_search { &quot;query&quot;: { &quot;match&quot;: { &quot;productName.productName_ansj&quot;: &quot;vans鞋&quot; } } } There are three types of match query: boolean, phrase, and phrase_prefix: booleanThe default match query is of type boolean. It means that the text provided is analyzed and the analysis process constructs a boolean query from the provided text. The operator flag can be set to or or and to control the boolean clauses (defaults to or). The minimum number of optional should clauses to match can be set using the minimum_should_match parameter. The analyzer can be set to control which analyzer will perform the analysis process on the text. It defaults to the field explicit mapping definition, or the default search analyzer. The lenient parameter can be set to true to ignore exceptions caused by data-type mismatches, such as trying to query a numeric field with a text query string. Defaults to false. POST /productindex/_search { &quot;query&quot;: { &quot;match&quot;: { &quot;productName.productName_ansj&quot;: { &quot;query&quot;: &quot;vans鞋&quot;, &quot;operator&quot;: &quot;and&quot; } } } } phraseThe match_phrase query analyzes the text and creates a phrase query out of the analyzed text. For example: { &quot;match_phrase&quot; : { &quot;message&quot; : &quot;this is a test&quot; } } Since match_phrase is only a type of a match query, it can also be used in the following manner: { &quot;match&quot; : { &quot;message&quot; : { &quot;query&quot; : &quot;this is a test&quot;, &quot;type&quot; : &quot;phrase&quot; } } } match_phrase_prefixThe match_phrase_prefix is the same as match_phrase, except that it allows for prefix matches on the last term in the text. For example: { &quot;match_phrase_prefix&quot; : { &quot;message&quot; : &quot;quick brown f&quot; } } 11.2.2 Multi Match QueryThe multi_match query builds on the match query to allow multi-field queries: { &quot;multi_match&quot; : { &quot;query&quot;: &quot;this is a test&quot;, &quot;fields&quot;: [ &quot;subject^3&quot;, &quot;message&quot; ] } } Individual fields can be boosted with the caret (^) notation. Types of multi_match query: best_fields: (default) Finds documents which match any field, but uses the _score from the best field. most_fields: Finds documents which match any field and combines the _score from each field. cross_fields: Treats fields with the same analyzer as though they were one big field. Looks for each word in any field. phrase: Runs a match_phrase query on each field and combines the _score from each field. phrase_prefix: Runs a match_phrase_prefix query on each field and combines the _score from each field. best_fields { &quot;multi_match&quot; : { &quot;query&quot;: &quot;brown fox&quot;, &quot;type&quot;: &quot;best_fields&quot;, &quot;fields&quot;: [ &quot;subject^3&quot;, &quot;message&quot; ], &quot;tie_breaker&quot;: 0.3 } } Normally the best_fields type uses the score of the single best matching field, but if tie_breaker is specified, then it calculates the score as follows: the score from the best matching field plus tie_breaker * _score for all other matching fields Also, accepts analyzer, boost, operator, minimum_should_match, fuzziness, prefix_length, max_expansions, rewrite, zero_terms_query and cutoff_frequency, as explained in match query. The best_fields and most_fields types are field-centric – they generate a match query per field. This means that the operator and minimum_should_match parameters are applied to each field individually, which is probably not what you want.In other words, all terms must be present in a single field for a document to match. cross_fieldsThe cross_fields type is particularly useful with structured documents where multiple fields should match. For instance, when querying the first_name and last_name fields for “Will Smith”, the best match is likely to have “Will” in one field and “Smith” in the other. This sounds like a job for most_fields but there are two problems with that approach： The first problem is that operator and minimum_should_match are applied per-field, instead of per-term (see explanation above). The second problem is to do with relevance: the different term frequencies in the first_name and last_name fields can produce unexpected results. In other words, all terms must be present in at least one field for a document to match. (Compare this to the logic used for best_fields and most_fields.) One way of dealing with these types of queries is simply to index the first_name and last_name fields into a single full_name field. Of course, this can only be done at index time. most_fieldsThe most_fields type is most useful when querying multiple fields that contain the same text analyzed in different ways. For instance, the main field may contain synonyms, stemming and terms without diacritics. A second field may contain the original terms, and a third field might contain shingles. By combining scores from all three fields we can match as many documents as possible with the main field, but use the second and third fields to push the most similar results to the top of the list. phrase and phrase_prefixThe phrase and phrase_prefix types behave just like best_fields, but they use a match_phrase or match_phrase_prefix query instead of a match query. 11.2.3 Common Terms Query (Advanced Topic)The common terms query is a modern alternative to stopwords which improves the precision and recall of search results (by taking stopwords into account), without sacrificing performance. The common terms query divides the query terms into two groups: more important (ie low frequency terms) and less important (ie high frequency terms which would previously have been stopwords). First it searches for documents which match the more important terms. These are the terms which appear in fewer documents and have a greater impact on relevance. Then, it executes a second query for the less important terms – terms which appear frequently and have a low impact on relevance. But instead of calculating the relevance score for all matching documents, it only calculates the _score for documents already matched by the first query. In this way the high frequency terms can improve the relevance calculation without paying the cost of poor performance. 11.2.4 Query String Query (Advanced Topic)A query that uses a query parser in order to parse its content. Here is an example: { &quot;query_string&quot; : { &quot;default_field&quot; : &quot;content&quot;, &quot;query&quot; : &quot;this AND that OR thus&quot; } } 11.2.5 Simple Query String Query (Advanced Topic)A query that uses the SimpleQueryParser to parse its context. Unlike the regular query_string query, the simple_query_string query will never throw an exception, and discards invalid parts of the query. Here is an example: { &quot;simple_query_string&quot; : { &quot;query&quot;: &quot;\&quot;fried eggs\&quot; +(eggplant | potato) -frittata&quot;, &quot;analyzer&quot;: &quot;snowball&quot;, &quot;fields&quot;: [&quot;body^5&quot;,&quot;_all&quot;], &quot;default_operator&quot;: &quot;and&quot; } } 11.3 Term level queriesWhile the full text queries will analyze the query string before executing, the term-level queries operate on the exact terms that are stored in the inverted index. These queries are usually used for structured data like numbers, dates, and enums, rather than full text fields. Alternatively, they allow you to craft low-level queries, foregoing the analysis process. The queries in this group are: term query: Find documents which contain the exact term specified in the field specified. terms query: Find documents which contain any of the exact terms specified in the field specified. range query: Find documents where the field specified contains values (dates, numbers, or strings) in the range specified. exists query: Find documents where the field specified contains any non-null value. missing query: Find documents where the field specified does is missing or contains only null values. prefix query: Find documents where the field specified contains terms which begin with the exact prefix specified. wildcard query: Find documents where the field specified contains terms which match the pattern specified, where the pattern supports single character wildcards (?) and multi-character wildcards (*) regexp query: Find documents where the field specified contains terms which match the regular expression specified. fuzzy query: Find documents where the field specified contains terms which are fuzzily similar to the specified term. Fuzziness is measured as a Levenshtein edit distance of 1 or 2. type query: Find documents of the specified type. ids query: Find documents with the specified type and IDs. 11.3.1 Term Query{ &quot;query&quot;: { &quot;term&quot;: { &quot;productSkn&quot;: &quot;51022624&quot; } } } A boost parameter can be specified to give this term query a higher relevance score than another query, for instance: GET /_search { &quot;query&quot;: { &quot;bool&quot;: { &quot;should&quot;: [ { &quot;term&quot;: { &quot;status&quot;: { &quot;value&quot;: &quot;urgent&quot;, &quot;boost&quot;: 2.0 } } }, { &quot;term&quot;: { &quot;status&quot;: &quot;normal&quot; } } ] } } } 11.3.2 Terms QueryFilters documents that have fields that match any of the provided terms (not analyzed). For example: { &quot;query&quot;: { &quot;terms&quot;: { &quot;brandId&quot;: [ &quot;144&quot;, &quot;248&quot; ] } } } 更高效的方法是使用constant_score.filter { &quot;constant_score.&quot; : { &quot;filter&quot; : { &quot;terms&quot; : { &quot;brandId&quot; : [&quot;144&quot;, &quot;248&quot;]} } } } search on all the tweets that match the followers of user 2 curl -XGET localhost:9200/tweets/_search -d &apos;{ &quot;query&quot; : { &quot;terms&quot; : { &quot;user&quot; : { &quot;index&quot; : &quot;users&quot;, &quot;type&quot; : &quot;user&quot;, &quot;id&quot; : &quot;2&quot;, &quot;path&quot; : &quot;followers&quot; } } } }&apos; 11.3.3 Range QueryMatches documents with fields that have terms within a certain range. The type of the Lucene query depends on the field type, for string fields, the TermRangeQuery, while for number/date fields, the query is a NumericRangeQuery. { &quot;query&quot;: { &quot;range&quot;: { &quot;salesNum&quot;: { &quot;gte&quot;: &quot;5&quot;, &quot;lte&quot;: &quot;100&quot; } } } } The range query accepts the following parameters: gte： Greater-than or equal to gt： Greater-than lte： Less-than or equal to lt： Less-than boost： Sets the boost value of the query, defaults to 1.0 11.3.4 Exists QueryReturns documents that have at least one non-null value in the original field: { &quot;exists&quot; : { &quot;field&quot; : &quot;user&quot; } } The exists query can advantageously replace the missing query (Missing Query Deprecated in 2.2.0) when used inside a must_not clause as follows: &quot;bool&quot;: { &quot;must_not&quot;: { &quot;exists&quot;: { &quot;field&quot;: &quot;user&quot; } } } This query returns documents that have no value in the user field. 11.3.5 Prefix QueryMatches documents that have fields containing terms with a specified prefix (not analyzed). The prefix query maps to Lucene PrefixQuery. The following matches documents where the user field contains a term that starts with ki: { &quot;query&quot;: { &quot;prefix&quot;: { &quot;productName&quot;: &quot;VA&quot; } } } 11.3.6 Wildcard QueryMatches documents that have fields matching a wildcard expression (not analyzed). Supported wildcards are , which matches any character sequence (including the empty one), and ?, which matches any single character. Note that this query can be slow, as it needs to iterate over many terms. In order to prevent extremely slow wildcard queries, a wildcard term should not start with one of the wildcards or ?. The wildcard query maps to Lucene WildcardQuery. { &quot;query&quot;: { &quot;wildcard&quot;: { &quot;productName&quot;: &quot;VA*鞋*&quot; } } } 11.3.7 Regexp QueryThe regexp query allows you to use regular expression term queries. See Regular expression syntax for details of the supported regular expression language. The “term queries” in that first sentence means that Elasticsearch will apply the regexp to the terms produced by the tokenizer for that field, and not to the original text of the field. Note: The performance of a regexp query heavily depends on the regular expression chosen. Matching everything like . is very slow as well as using lookaround regular expressions. If possible, you should try to use a long prefix before your regular expression starts. Wildcard matchers like .?+ will mostly lower performance. { &quot;regexp&quot;:{ &quot;name.first&quot;: &quot;s.*y&quot; } } 11.3.8 Fuzzy QueryThe fuzzy query uses similarity based on Levenshtein edit distance for string fields, and a +/- margin on numeric and date fields. The fuzzy query generates all possible matching terms that are within the maximum edit distance specified in fuzziness and then checks the term dictionary to find out which of those generated terms actually exist in the index. Here is a simple example: { &quot;fuzzy&quot; : { &quot;user&quot; : &quot;ki&quot; } } 11.3.9 Type QueryFilters documents matching the provided document / mapping type. { &quot;type&quot; : { &quot;value&quot; : &quot;my_type&quot; } } 11.3.10 Ids QueryFilters documents that only have the provided ids. Note, this query uses the _uid field. { &quot;ids&quot; : { &quot;type&quot; : &quot;my_type&quot;, &quot;values&quot; : [&quot;1&quot;, &quot;4&quot;, &quot;100&quot;] } } The type is optional and can be omitted, and can also accept an array of values. If no type is specified, all types defined in the index mapping are tried. 11.4 Compound queriesCompound queries wrap other compound or leaf queries, either to combine their results and scores, to change their behaviour, or to switch from query to filter context. The queries in this group are: constant_score query: A query which wraps another query, but executes it in filter context. All matching documents are given the same “constant” _score. bool query: The default query for combining multiple leaf or compound query clauses, as must, should, must_not, or filter clauses. The must and should clauses have their scores combined – the more matching clauses, the better – while the must_not and filter clauses are executed in filter context. dis_max query: A query which accepts multiple queries, and returns any documents which match any of the query clauses. While the bool query combines the scores from all matching queries, the dis_max query uses the score of the single best- matching query clause. function_score query: Modify the scores returned by the main query with functions to take into account factors like popularity, recency, distance, or custom algorithms implemented with scripting. boosting query: Return documents which match a positive query, but reduce the score of documents which also match a negative query. indices query: Execute one query for the specified indices, and another for other indices. 11.4.1 Constant Score QueryA query that wraps another query and simply returns a constant score equal to the query boost for every document in the filter. Maps to Lucene ConstantScoreQuery. { &quot;constant_score&quot; : { &quot;filter&quot; : { &quot;term&quot; : { &quot;user&quot; : &quot;kimchy&quot;} }, &quot;boost&quot; : 1.2 } } 11.4.2 Bool QueryA query that matches documents matching boolean combinations of other queries. The bool query maps to Lucene BooleanQuery. It is built using one or more boolean clauses, each clause with a typed occurrence. The occurrence types are: Occur Description must The clause (query) must appear in matching documents and will contribute to the score. filter The clause (query) must appear in matching documents. However unlike must the score of the query will be ignored. should The clause (query) should appear in the matching document. In a boolean query with no must or filter clauses, one or more should clauses must match a document. The minimum number of should clauses to match can be set using the minimum_should_match parameter. must_not The clause (query) must not appear in the matching documents. If this query is used in a filter context and it has should clauses then at least one should clause is required to match. The bool query takes a more-matches-is-better approach, so the score from each matching must or should clause will be added together to provide the final _score for each document. { &quot;bool&quot; : { &quot;must&quot; : { &quot;term&quot; : { &quot;user&quot; : &quot;kimchy&quot; } }, &quot;filter&quot;: { &quot;term&quot; : { &quot;tag&quot; : &quot;tech&quot; } }, &quot;must_not&quot; : { &quot;range&quot; : { &quot;age&quot; : { &quot;from&quot; : 10, &quot;to&quot; : 20 } } }, &quot;should&quot; : [ { &quot;term&quot; : { &quot;tag&quot; : &quot;wow&quot; } }, { &quot;term&quot; : { &quot;tag&quot; : &quot;elasticsearch&quot; } } ], &quot;minimum_should_match&quot; : 1, &quot;boost&quot; : 1.0 } } 11.4.3 Dis Max QueryA query that generates the union of documents produced by its subqueries, and that scores each document with the maximum score for that document as produced by any subquery, plus a tie breaking increment for any additional matching subqueries. This is useful when searching for a word in multiple fields with different boost factors (so that the fields cannot be combined equivalently into a single search field). We want the primary score to be the one associated with the highest boost, not the sum of the field scores (as Boolean Query would give). If the query is “albino elephant” this ensures that “albino” matching one field and “elephant” matching another gets a higher score than “albino” matching both fields. To get this result, use both Boolean Query and DisjunctionMax Query: for each term a DisjunctionMaxQuery searches for it in each field, while the set of these DisjunctionMaxQuery’s is combined into a BooleanQuery. The tie breaker capability allows results that include the same term in multiple fields to be judged better than results that include this term in only the best of those multiple fields, without confusing this with the better case of two different terms in the multiple fields.The default tie_breaker is 0.0. This query maps to Lucene DisjunctionMaxQuery. { &quot;dis_max&quot; : { &quot;tie_breaker&quot; : 0.7, &quot;boost&quot; : 1.2, &quot;queries&quot; : [ { &quot;term&quot; : { &quot;age&quot; : 34 } }, { &quot;term&quot; : { &quot;age&quot; : 35 } } ] } } 11.4.4 Function Score QueryThe function_score allows you to modify the score of documents that are retrieved by a query. This can be useful if, for example, a score function is computationally expensive and it is sufficient to compute the score on a filtered set of documents. To use function_score, the user has to define a query and one or more functions, that compute a new score for each document returned by the query. &quot;function_score&quot;: { &quot;query&quot;: {}, &quot;boost&quot;: &quot;boost for the whole query&quot;, &quot;functions&quot;: [ { &quot;filter&quot;: {}, &quot;FUNCTION&quot;: {}, &quot;weight&quot;: number }, { &quot;FUNCTION&quot;: {} }, { &quot;filter&quot;: {}, &quot;weight&quot;: number } ], &quot;max_boost&quot;: number, &quot;score_mode&quot;: &quot;(multiply|max|...)&quot;, &quot;boost_mode&quot;: &quot;(multiply|replace|...)&quot;, &quot;min_score&quot; : number } ++The scores produced by the filtering query of each function do not matter.++ First, each document is scored by the defined functions. The parameter score_mode specifies how the computed scores are combined: multiply: scores are multiplied (default) sum: scores are summed avg: scores are averaged first: the first function that has a matching filter is applied max: maximum score is used min: minimum score is used The newly computed score is combined with the score of the query. The parameter boost_mode defines how: multiply: query score and function score is multiplied (default) replace: only function score is used, the query score is ignored sum: query score and function score are added avg: average max: max of query score and function score min: min of query score and function score The function_score query provides several types of score functions: script_score weight random_score field_value_factor decay functions: gauss, linear, exp Field Value factorThe field_value_factor function allows you to use a field from a document to influence the score. It’s similar to using the script_score function, however, it avoids the overhead of scripting. If used on a multi-valued field, only the first value of the field is used in calculations. As an example, imagine you have a document indexed with a numeric popularity field and wish to influence the score of a document with this field, an example doing so would look like: &quot;field_value_factor&quot;: { &quot;field&quot;: &quot;popularity&quot;, &quot;factor&quot;: 1.2, &quot;modifier&quot;: &quot;sqrt&quot;, &quot;missing&quot;: 1 } Which will translate into the following formula for scoring: sqrt(1.2 * doc[&apos;popularity&apos;].value) There are a number of options for the field_value_factor function: field: Field to be extracted from the document. factor: Optional factor to multiply the field value with, defaults to 1. modifier: Modifier to apply to the field value, can be one of: none, log, log1p, log2p, ln, ln1p, ln2p, square, sqrt, or reciprocal. Defaults to none. Modifier Meaning none Do not apply any multiplier to the field value log Take the logarithm of the field value log1p Add 1 to the field value and take the logarithm log2p Add 2 to the field value and take the logarithm ln Take the natural logarithm of the field value ln1p Add 1 to the field value and take the natural logarithm ln2p Add 2 to the field value and take the natural logarithm square Square the field value (multiply it by itself) sqrt Take the square root of the field value reciprocal Reciprocate the field value, same as 1/x where x is the field’s value 11.4.5 Boosting QueryThe boosting query can be used to effectively demote results that match a given query. Unlike the “NOT” clause in bool query, this still selects documents that contain undesirable terms, but reduces their overall score. { &quot;boosting&quot; : { &quot;positive&quot; : { &quot;term&quot; : { &quot;field1&quot; : &quot;value1&quot; } }, &quot;negative&quot; : { &quot;term&quot; : { &quot;field2&quot; : &quot;value2&quot; } }, &quot;negative_boost&quot; : 0.2 } } 11.4.6 Indices QueryThe indices query is useful in cases where a search is executed across multiple indices. It allows to specify a list of index names and an inner query that is only executed for indices matching names on that list. For other indices that are searched, but that don’t match entries on the list, the alternative no_match_query is executed. { &quot;indices&quot; : { &quot;indices&quot; : [&quot;index1&quot;, &quot;index2&quot;], &quot;query&quot; : { &quot;term&quot; : { &quot;tag&quot; : &quot;wow&quot; } }, &quot;no_match_query&quot; : { &quot;term&quot; : { &quot;tag&quot; : &quot;kow&quot; } } } } 11.5 Joining queriesPerforming full SQL-style joins in a distributed system like Elasticsearch is prohibitively expensive. Instead, Elasticsearch offers two forms of join which are designed to scale horizontally. nested query: Documents may contains fields of type nested. These fields are used to index arrays of objects, where each object can be queried (with the nested query) as an independent document. has_child and has_parent queries: A parent-child relationship can exist between two document types within a single index. The has_child query returns parent documents whose child documents match the specified query, while the has_parent query returns child documents whose parent document matches the specified query. Also see the terms-lookup mechanism in the terms query, which allows you to build a terms query from values contained in another document. 11.6 Geo queries (Advanced Topic)11.7 Specialized queries (Advanced Topic)This group contains queries which do not fit into the other groups: more_like_this query: This query finds documents which are similar to the specified text, document, or collection of documents. template query: The template query accepts a Mustache template (either inline, indexed, or from a file), and a map of parameters, and combines the two to generate the final query to execute. script query: This query allows a script to act as a filter. Also see the function_score query. 11.8 Span queries (Advanced Topic)Span queries are low-level positional queries which provide expert control over the order and proximity of the specified terms. These are typically used to implement very specific queries on legal documents or patents. Span queries cannot be mixed with non-span queries (with the exception of the span_multi query). The queries in this group are: span_term query: The equivalent of the term query but for use with other span queries. span_multi query: Wraps a term, range, prefix, wildcard, regexp, or fuzzy query. span_first query: Accepts another span query whose matches must appear within the first N positions of the field. span_near query: Accepts multiple span queries whose matches must be within the specified distance of each other, and possibly in the same order. span_or query: Combines multiple span queries – returns documents which match any of the specified queries. span_not query: Wraps another span query, and excludes any documents which match that query. span_containing query: Accepts a list of span queries, but only returns those spans which also match a second span query. span_within query: The result from a single span query is returned as long is its span falls within the spans returned by a list of other span queries. 12 MappingMapping is the process of defining how a document, and the fields it contains, are stored and indexed. For instance, use mappings to define: which string fields should be treated as full text fields. which fields contain numbers, dates, or geolocations. whether the values of all fields in the document should be indexed into the catch-all _all field. the format of date values. custom rules to control the mapping for dynamically added fields. Mapping TypesEach index has one or more mapping types, which are used to divide the documents in an index into logical groups. User documents might be stored in a user type, and blog posts in a blogpost type. Each mapping type has: Meta-fields: Meta-fields are used to customize how a document’s metadata associated is treated. Examples of meta-fields include the document’s _index, _type, _id, and _source fields. Fields or properties: Each mapping type contains a list of fields or properties pertinent to that type. A user type might contain title, name, and age fields, while a blogpost type might contain title, body, user_id and created fields. Fields with the same name in different mapping types in the same index must have the same mapping. Field datatypesEach field has a data type which can be: a simple type like string, date, long, double, boolean or ip. a type which supports the hierarchical nature of JSON such as object or nested. or a specialised type like geo_point, geo_shape, or completion. It is often useful to index the same field in different ways for different purposes. For instance, a string field could be indexed as an analyzed field for full-text search, and as a not_analyzed field for sorting or aggregations. Alternatively, you could index a string field with the standard analyzer, the english analyzer, and the french analyzer. This is the purpose of multi-fields. Most datatypes support multi-fields via the fields parameter. Dynamic mappingFields and mapping types do not need to be defined before being used. Thanks to dynamic mapping, new mapping types and new field names will be added automatically, just by indexing a document. New fields can be added both to the top-level mapping type, and to inner object and nested fields. The dynamic mapping rules can be configured to customise the mapping that is used for new types and new fields. Explicit mappingsYou know more about your data than Elasticsearch can guess, so while dynamic mapping can be useful to get started, at some point you will want to specify your own explicit mappings. You can create mapping types and field mappings when you create an index, and you can add mapping types and fields to an existing index with the PUT mapping API. Updating existing mappingsOther than where documented, existing type and field mappings cannot be updated. Changing the mapping would mean invalidating already indexed documents. Instead, you should create a new index with the correct mappings and reindex your data into that index. Fields are shared across mapping typesMapping types are used to group fields, but the fields in each mapping type are not independent of each other. Fields with: the same name in the same index in different mapping types map to the same field internally, and must have the same mapping. If a title field exists in both the user and blogpost mapping types, the title fields must have exactly the same mapping in each type. The only exceptions to this rule are the copy_to, dynamic, enabled, ignore_above, include_in_all, and properties parameters, which may have different settings per field. Usually, fields with the same name also contain the same type of data, so having the same mapping is not a problem. When conflicts do arise, these can be solved by choosing more descriptive names, such as user_title and blog_title. Example mappingA mapping for the example described above could be specified when creating the index, as follows: PUT my_index { &quot;mappings&quot;: { &quot;user&quot;: { &quot;_all&quot;: { &quot;enabled&quot;: false }, &quot;properties&quot;: { &quot;title&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;name&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;age&quot;: { &quot;type&quot;: &quot;integer&quot; } } }, &quot;blogpost&quot;: { &quot;properties&quot;: { &quot;title&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;body&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;user_id&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;index&quot;: &quot;not_analyzed&quot; }, &quot;created&quot;: { &quot;type&quot;: &quot;date&quot;, &quot;format&quot;: &quot;strict_date_optional_time||epoch_millis&quot; } } } } } 12.1 Field datatypesElasticsearch supports a number of different datatypes for the fields in a document: Core datatypesString datatype: stringThe following parameters are accepted by string fields: Parameter Description analyzer The analyzer which should be used for analyzed string fields, both at index-time and at search-time (unless overridden by the search_analyzer). Defaults to the default index analyzer, or the standard analyzer. boost Field-level index time boosting. Accepts a floating point number, defaults to 1.0. doc_values Should the field be stored on disk in a column-stride fashion, so that it can later be used for sorting, aggregations, or scripting? Accepts true or false. Defaults to true for not_analyzed fields. Analyzed fields do not support doc values. fielddata Can the field use in-memory fielddata for sorting, aggregations, or scripting? Accepts disabled or paged_bytes (default). Not analyzed fields will use doc values in preference to fielddata. fields Multi-fields allow the same string value to be indexed in multiple ways for different purposes, such as one field for search and a multi-field for sorting and aggregations, or the same string value analyzed by different analyzers. ignore_above Do not index or analyze any string longer than this value. Defaults to 0 (disabled). include_in_all Whether or not the field value should be included in the _all field? Accepts true or false. Defaults to false if index is set to no, or if a parent object field sets include_in_all to false. Otherwise defaults to true. index Should the field be searchable? Accepts analyzed (default, treat as full-text field), not_analyzed (treat as keyword field) and no. index_options What information should be stored in the index, for search and highlighting purposes. Defaults to positions for analyzed fields, and to docs for not_analyzed fields. norms Whether field-length should be taken into account when scoring queries. Defaults depend on the index setting: analyzed fields default to { “enabled”: true, “loading”: “lazy” }; not_analyzed fields default to { “enabled”: false }. null_value Accepts a string value which is substituted for any explicit null values. Defaults to null, which means the field is treated as missing. If the field is analyzed, the null_value will also be analyzed. position_increment_gap The number of fake term position which should be inserted between each element of an array of strings. Defaults to the position_increment_gap configured on the analyzer which defaults to 100. 100 was chosen because it prevents phrase queries with reasonably large slops (less than 100) from matching terms across field values. store Whether the field value should be stored and retrievable separately from the _source field. Accepts true or false (default). search_analyzer The analyzer that should be used at search time on analyzed fields. Defaults to the analyzer setting. search_quote_analyzer The analyzer that should be used at search time when a phrase is encountered. Defaults to the search_analyzer setting. similarity Which scoring algorithm or similarity should be used. Defaults to default, which uses TF/IDF. term_vector Whether term vectors should be stored for an analyzed field. Defaults to no. Numeric datatypes: long, integer, short, byte, double, float Date datatype: date JSON doesn’t have a date datatype, so dates in Elasticsearch can either be: strings containing formatted dates, e.g. “2015-01-01” or “2015/01/01 12:10:30” a long number representing milliseconds-since-the-epoch. an integer representing seconds-since-the-epoch.) Boolean datatype: booleanBinary datatype: binary The binary type accepts a binary value as a Base64 encoded string. The field is not stored by default and is not searchable. Complex datatypesArray datatypeArray support does not require a dedicated type (In Elasticsearch, there is no dedicated array type. Any field can contain zero or more values by default, however, all values in the array must be of the same datatype.) Object datatype: object for single JSON objectsNested datatype: nested for arrays of JSON objects Geo datatypes (Advanced Topic) Geo-point datatype: geo_point for lat/lon points Geo-Shape datatype: geo_shape for complex shapes like polygons Specialised datatypes IPv4 datatype： ip for IPv4 addresses Completion datatype： completion to provide auto-complete suggestions Token count datatype： token_count to count the number of tokens in a string mapper-murmur3： murmur3 to compute hashes of values at index-time and store them in the index Attachment datatype： See the mapper-attachments plugin which supports indexing attachments like Microsoft Office formats, Open Document formats, ePub, HTML, etc. into an attachment datatype. Multi-fieldsIt is often useful to index the same field in different ways for different purposes. For instance, a string field could be indexed as an analyzed field for full-text search, and as a not_analyzed field for sorting or aggregations. Alternatively, you could index a string field with the standard analyzer, the english analyzer, and the french analyzer. This is the purpose of multi-fields. Most datatypes support multi-fields via the fields parameter. 12.2 Meta-FieldsEach document has metadata associated with it, such as the _index, mapping _type, and _id meta-fields. The behaviour of some of these meta-fields can be customised when a mapping type is created. Identity meta-fields _index: The index to which the document belongs. _uid: A composite field consisting of the _type and the _id. _type: The document’s mapping type. _id: The document’s ID. Document source meta-fields _source: The original JSON representing the body of the document. _size: The size of the _source field in bytes, provided by the mapper-size plugin. Indexing meta-fields _all: The _all field is a special catch-all field which concatenates the values of all of the other fields into one big string, using space as a delimiter, which is then analyzed and indexed, but not stored. This means that it can be searched, but not retrieved; The _all field can be completely disabled per-type by setting enabled to false; While there is only a single _all field per index, the copy_to parameter allows the creation of multiple custom _all fields. For instance, first_name and last_name fields can be combined together into the full_name field. _field_names: The _field_names field indexes the names of every field in a document that contains any value other than null. This field is used by the exists and missing queries to find documents that either have or don’t have any non-null value for a particular field; The value of the _field_name field is accessible in queries, aggregations, and scripts. Routing meta-fields _parent: Used to create a parent-child relationship between two mapping types. _routing: A custom routing value which routes a document to a particular shard. Other meta-field _meta: Application specific metadata. 12.3 Mapping parametersThe following mapping parameters are common to some or all field datatypes: analyzer boost coerce copy_to: The copy_to parameter allows you to create custom _all fields. In other words, the values of multiple fields can be copied into a group field, which can then be queried as a single field. For instance, the first_name and last_name fields can be copied to the full_name field. doc_values: Doc values are the on-disk data structure, built at document index time, which makes this data access pattern possible. They store the same values as the _source but in a column-oriented fashion that is way more efficient for sorting and aggregations. Doc values are supported on almost all field types, with the notable exception of analyzed string fields. dynamic enabled fielddata: Most fields can use index-time, on-disk doc_values to support this type of data access pattern, but analyzed string fields do not support doc_values; Instead, analyzed strings use a query-time data structure called fielddata. This data structure is built on demand the first time that a field is used for aggregations, sorting, or is accessed in a script. It is built by reading the entire inverted index for each segment from disk, inverting the term ↔︎ document relationship, and storing the result in memory, in the JVM heap; Loading fielddata is an expensive process so, once it has been loaded, it remains in memory for the lifetime of the segment. geohash geohash_precision geohash_prefix format ignore_above ignore_malformed include_in_all index_options lat_lon index fields: It is often useful to index the same field in different ways for different purposes. This is the purpose of multi-fields. For instance, a string field could be indexed as an analyzed field for full-text search, and as a not_analyzed field for sorting or aggregations. norms: Norms store various normalization factors – a number to represent the relative field length and the index time boost setting – that are later used at query time in order to compute the score of a document relatively to a query; Although useful for scoring, norms also require quite a lot of memory (typically in the order of one byte per document per field in your index, even for documents that don’t have this specific field). As a consequence, if you don’t need scoring on a specific field, you should disable norms on that field. In particular, this is the case for fields that are used solely for filtering or aggregations. null_value position_increment_gap properties: Type mappings, object fields and nested fields contain sub-fields, called properties. These properties may be of any datatype, including object and nested. search_analyzer similarity store: By default, field values are indexed to make them searchable, but they are not stored. This means that the field can be queried, but the original field value cannot be retrieved; Usually this doesn’t matter. The field value is already part of the _source field, which is stored by default. If you only want to retrieve the value of a single field or of a few fields, instead of the whole _source, then this can be achieved with source filtering; In certain situations it can make sense to store a field. For instance, if you have a document with a title, a date, and a very large content field, you may want to retrieve just the title and the date without having to extract those fields from a large _source field. term_vector: Term vectors contain information about the terms produced by the analysis process, including: a list of terms/the position (or order) of each term/the start and end character offsets mapping the term to its origin in the original string; These term vectors can be stored so that they can be retrieved for a particular document. The term_vector setting accepts: no: No term vectors are stored. (default) yes: Just the terms in the field are stored. with_positions: Terms and positions are stored. with_offsets: Terms and character offsets are stored. with_positions_offsets: Terms, positions, and character offsets are stored. 12.4 Dynamic MappingOne of the most important features of Elasticsearch is that it tries to get out of your way and let you start exploring your data as quickly as possible. To index a document, you don’t have to first create an index, define a mapping type, and define your fields – you can just index a document and the index, type, and fields will spring to life automatically: PUT data/counters/1 { &quot;count&quot;: 5 } Creates the data index, the counters mapping type, and a field called count with datatype long. The automatic detection and addition of new types and fields is called dynamic mapping. The dynamic mapping rules can be customised to suit your purposes with: default mapping: Configure the base mapping to be used for new mapping types. Dynamic field mappings: The rules governing dynamic field detection. Dynamic templates: Custom rules to configure the mapping for dynamically added fields. 13 Analysis (See another documents)14 Modules (Skipped)15 Index Modules (Skipped)16 Testing (Skipped)17 Glossary of terms (Skipped)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[《深入理解Java虚拟机》读书笔记7：高效并发]]></title>
      <url>%2Fpost%2Fdeep_in_jvm_notes_part7%2F</url>
      <content type="text"><![CDATA[国内JVM相关书籍NO.1，Java程序员必读。读书笔记第七部分对应原书的第十二章和第十三章，主要介绍Java内存模型、先行发生原则、线程安全和虚拟机的锁优化细节。 第五部分 高效并发第十二章 Java内存模型与线程并发处理的广泛应用是使得Amdahl定律代替摩尔定律成为计算机性能发展源动力的根本原因，也是人类“压榨”计算机运算能力的最有力武器。 12.1 概述 多任务处理在现代计算机操作系统中几乎已是一项必备的功能了； 除了充分利用计算机处理器的能力外，一个服务端同时对多个客户端提供服务则是另一个更具体的并发应用场景； 服务端是Java语言最擅长的领域之一，不过如何写好并发应用程序却又是服务端程序开发的难点之一，处理好并发方面的问题通常需要更多的编码经验来支持，幸好Java语言和虚拟机提供了许多工具，把并发编码的门槛降低了不少； 12.2 硬件的效率与一致性 绝大多数的运算任务不可能只靠处理器计算就能完成，处理器至少要与内存交互，所以现代计算机系统都不得不加入一层读写速度尽可能接近处理器运算速度的高速缓存来作为内存与处理器之间的缓冲：将运算需要使用到的数据复制到缓存中，让运算能快速运行，当运算结束后再从缓存同步回内存之中，这样处理器就无须等待缓慢的内存读写了； 基于高速缓存的存储交互很好地解决了处理器与内存的速度矛盾，但是也为计算机系统带来更高的复杂度，因为它引入了一个新的问题：缓存一致性；为了解决一致性的问题，需要各个处理器访问缓存时都遵循一些协议，在读写时要根据协议来进行操作，这类协议有MSI、MESI、MOSI、Synapse、Firefly及Dragon Protocol等； 本章将会多次提到内存模型一词，可以理解在特定的操作协议下，对特定的内存或高速缓存进行读写访问的过程抽象；不同架构的物理机器可以拥有不一样的内存模型，而Java虚拟机也有自己的内存模型，并且这里介绍的内存访问操作与硬件的缓存访问具有很高的可比性； 除了增加高速缓存之外，为了使得处理器内部的运算单元能尽量被充分利用，处理器可能会对输入代码进行乱序执行优化，处理器会在计算之后将乱序执行的结果重组，保证该结果与顺序执行的结果是一致的； 12.3 Java内存模型Java虚拟机规范中视图定义一种Java内存模型（JMM）来屏蔽掉各种硬件和操作系统的内存访问差异，以实现让Java程序在各种平台下都能达到一致的内存访问效果。 12.3.1 主内存与工作内存 Java内存模型的主要目标是定义程序中各个变量的访问规则，即在虚拟机中将变量存储到内存和从内存中取出变量这样的底层细节；此处的变量与Java编程中所说的变量有所区别，它包括了实例字段、静态字段和构成数组对象的元素，但不包括局部变量与方法参数，因为后者是线程私有的，不会被共享； Java内存模型规定了所有的变量都存储在主内存中，每个线程还有自己的工作内存，线程的工作内存中保存了被该线程使用到的变量的主内存副本拷贝，线程对变量的所有操作都必须在工作内存中进行，而不能直接读写主内存中的变量； 这里所讲的主内存、工作内存与第二章所讲的Java内存区域中的Java堆、栈、方法区等并不是同一个层次的内存划分，这两者基本上是没有关系的；线程、主内存和工作内存的关系如下所示： 12.3.2 内存间交互操作关于主内存与工作内存之间具体的交互协议，即一个变量如何从主内存拷贝到工作内存、如何从工作内存同步回主内存之类的实现细节，Java内存模型中定义了以下八种操作来完成，虚拟机实现时必须保证下面提及的每一种操作都是原子的、不可再分的（对于double和long类型的变量的某些操作在某些平台允许有例外）： lock unlock read load use assign store write 基于理解难度和严谨性考虑，最新的JSR-133文档中，已经放弃采用这八种操作去定义Java内存模型的访问协议了，后面将会介绍一个等效判断原则 – 先行发生原则，用来确定一个访问在并发环境下是否安全； 12.3.3 对于volatile型变量的特殊规则 关键字volatile可以说是Java虚拟机提供的最轻量级的同步机制； 当一个变量定义为volatile之后，它将具备两种特性：第一是保证此变量对所有线程的可见性，这里的可见性是指当一个线程修改了这个变量的值，新的值对于其他线程来说是可以立即得知的，而普通的变量的值在线程间传递均需要通过主内存来完成；另外一个是禁止指令重排序优化，普通的变量仅仅会保证在该方法的执行过程中所有依赖赋值结果的地方都能获取到正确的结果，而不能保证变量赋值操作的顺序与程序代码中的执行顺序一致； volatile变量在各个线程的工作内存中不存在一致性问题，但是Java里面的运算并非原子操作，导致volatile变量的运算在并发下一样是不安全的； 在不符合以下两条规则的运算场景中，我们仍然要通过加锁来保证原子性：运算结果并不依赖变量的当前值或者能够确保只有单一的线程修改变量的值、变量不需要与其他的状态变量共同参与不变约束； volatile变量读操作的性能消耗与普通变量几乎没有任何差别，但是写操作则可能会慢一些；不过大多数场景下volatile的总开销仍然要比锁低，我们在volatile与锁之中选择的唯一依据仅仅是volatile的语义能否满足使用场景的需求； 12.3.4 对于long和double型变量的特殊规则 允许虚拟机将没有被volatile修饰的64位数据的读写操作划分为两次32位的操作来进行，即允许虚拟机实现选择可以不保证64位数据类型的load、store、read和write这4个操作的原子性，这点就是所谓的long和double的非原子性协定； 但允许虚拟机选择把这些操作实现为具有原子性的操作，目前各种平台下的商用虚拟机几乎都选择把64位数据的读写操作作为原子操作来对待； 12.3.5 原子性、可见性与有序性 原子性（Atomicity）：由Java内存模型来直接保证的原子性变量操作包括read、load、assign、use、store和write；在synchronized块之间的操作也具备原子性； 可见性（Visibility）：是指当一个线程修改了共享变量的值，其他线程能够立即得知这个修改；除了volatile之外，Java还有synchronized和final关键字能实现可见性； 有序性（Ordering）：如果在本线程内观察，所有的操作都是有序的；如果在一个线程中观察另一个线程，所有的操作都是无序的；Java语言提供了volatile和synchronized两个关键字来保证线程之间操作的有序性； 12.3.6 先行发生原则 先行发生是Java内存模型中定义的两项操作之间的偏序关系，如果说操作A先行发生于操作B，其实就是说在发生操作B之前，操作A产生的影响能被操作B观察到，影响包括了修改了内存中共享变量的值、发送了消息、调用了方法等； 下面是Java内存模型下一些天然的先行发生关系：程序次序规则、管程锁定规则、volatile变量规则、线程启动规则、线程终止规则、线程中断规则、对象终结规则、传递性； 时间先后顺序与先行发生原则之间基本没有太大的关系，所以我们衡量并发安全问题的时候不要受到时间顺序的干扰，一切必须以先行发生原则为准； 12.4 Java与线程12.4.1 线程的实现 线程是比进程更轻量级的调度执行单位，线程的引入可以把一个进程的资源分配和执行调度分开，各个线程既可以共享进程资源又可以独立调度； Thread类与大部分的Java API有显著的差别，它的所有关键方法都是声明为Native的； 实现线程主要有三种方式：使用内核线程实现（系统调用代价相对较高、一个系统支持轻量级进程的数量是有限的）、使用用户线程实现（优势在于不需要系统内核支援，劣势在于所有线程操作都需要用户程序自己处理）和使用用户线程加轻量级进程混合实现（用户线程是完全建立在用户空间中，因此用户线程的创建、切换等操作依然廉价，并且可以支持大规模的用户线程并发；而操作系统提供支持的轻量级进程则作为用户线程和内核线程之间的桥梁，这样可以使用内核提供的线程调度功能及处理器映射，并且用户线程的系统调用要通过轻量级线程来完成，大大降低了整个进程被完全阻塞的风险）； 对于Sun JDK来说，它的Windows版与Linux版都是使用一对一的线程模型实现的，一条Java线程就映射到一条轻量级进程之中，因为Windows和Linux系统提供的线程模式就是一对一的； 12.4.2 Java线程调度 线程调度是指系统为线程分配处理器使用权的过程，主要调度方式有两种，分别是协同式线程调度（线程的执行时间由线程本身来控制）和抢占式线程调度（线程由系统来分配执行时间，线程的切换不由线程本身来决定）； Java语言一共设置了10个级别的线程优先级，不过线程优先级并不是太靠谱，原因就是操作系统的线程优先级不见得总是与Java线程的优先级一一对应，另外优先级还可能被系统自行改变； 12.4.3 状态转换 Java语言定义了五种线程状态，在任意一个时间点，一个线程只能有且只有其中一种状态，分别是新建（New）、运行（Runnable）、无限期等待（Waiting）、限期等待（Timed Waiting）、阻塞（Blocled）、结束（Terminated）。它们之间相互的转换关系如下所示： 12.5 本章小结本章我们首先了解了虚拟机Java内存模型的结构及操作，然后讲解了原子性、可见性、有序性在Java内存模型中的体现，最后介绍了先行发生原则的规则及使用。另外，我们还了解了线程在Java语言之中是如何实现的。 在本章主要介绍了虚拟机如何实现并发，而在下一章我们主要关注点将是虚拟机如何实现高效，以及虚拟机对我们编写的并发代码提供了什么样的优化手段。 第十三章 线程安全与锁优化13.1 概述 首先需要保证并发的正确性，然后在此基础上实现高效； 13.2 线程安全Brian Goetz对线程安全有一个比较恰当的定义：当多个线程访问一个对象时，如果不用考虑这些线程在运行时环境下的调度和交替执行，也不需要进行额外的同步，或者在调用方进行任何其他的协调操作，调用这个对象的行为都可以获得正确的结果，那这个对象是线程安全的。 13.2.1 Java语言中的线程安全 我们可以将Java语言中各个操作共享的数据分为以下五类：不可变、绝对线程安全、相对线程安全、线程兼容和线程对立； 不可变：不可变带来的安全性是最简单和最纯粹的，如final的基本数据类型；如果共享的数据是一个对象，那就需要保证对象的行为不会对其状态产生任何影响才行，比如String类的substring、replace方法；Number类型的大部分子类都符合不可变要求的类型，但是AtomicInteger和AtomicLong则并非不可变的； 线程绝对安全：Java API中标注自己是线程安全的类，大多数都不是绝对的线程安全；比如java.util.Vector，不意味着调用它的是时候永远都不再需要同步手段了； 线程相对安全：是我们通常意义上所讲的线程安全，在Java语言中，大部分的线程安全类都属于这种类型； 线程兼容：指对象本身并不是线程安全的，但是可以通过在调用端正确地使用同步手段来保证对象在并发环境中可以安全地使用；我们说一个类不是线程安全的，绝大多数时候指的是这一种情况； 线程对立：无论调用端是否采取了同步措施，都无法在多线程环境中并发使用的代码，Java语言中很少出现； 13.2.2 线程安全的实现方法 互斥同步：同步是指在多个线程并发访问共享数据时，保证共享数据在同一个时刻只被一个线程使用，而互斥是实现同步的一种手段，临界区、互斥量和信号量都是主要的互斥实现方式；Java中最基本的互斥同步手段就是synchronized关键字，它对同一个线程来说是可重入的且会阻塞后面其他线程的进入；另外还可以使用java.util.concurrent包中的重入锁（ReentrantLock）来实现同步，相比synchronized关键字ReentrantLock增加了一些高级功能：等待可中断、可实现公平锁以及锁可以绑定多个条件； 非阻塞同步：互斥同步最主要的问题就是进行线程阻塞和唤醒带来的性能问题，其属于一种悲观的并发策略；随着硬件指令集的发展，我们有了另外一个选择即基于冲突检测的乐观并发策略，就是先进行操作，如果没有其他线程争用共享数据那就操作成功了，如果有争用产生了冲突，那就再采取其他的补偿措施（最常见的就是不断重试直至成功），这种同步操作称为非阻塞同步；Java并发包的整数原子类，其中的compareAndSet和getAndIncrement等方法都使用了Unsafe类的CAS操作； 无同步方案：要保证线程安全，并不是一定就要进行同步；有一些代码天生就是线程安全的，比如可重入代码和线程本地存储的代码； 13.3 锁优化13.3.1 自旋锁与自适应自旋 互斥同步对性能最大的影响是阻塞的实现，挂起线程和恢复线程的操作都需要转入内核态中完成，这些操作给系统的并发性能带来了很大的压力；另外在共享数据的锁定状态只会持续很短的一段时间，为了这段时间去挂起和恢复线程并不值得，如果让两个或以上的线程同时并行执行，让后面请求锁的那个线程稍等一下，但不放弃处理器的执行时间，看看持有锁的线程是否很快就会释放锁；为了让线程等待，我们只需让线程执行一个忙循环，这些技术就是所谓的自旋锁； 在JDK 1.6已经默认开启自旋锁；如果锁被占用的时间很短自旋等待的效果就会非常好，反之则会白白消耗处理器资源； 在JDK 1.6中引入了自适应的自旋锁，这意味着自旋的时间不再固定，而是由前一次在同一个锁上的自旋时间及锁的拥有者的状态来决定； 13.3.2 锁消除 锁消除是指虚拟机即时编译器在运行时，对一些代码上要求同步，但是被检测到不可能存在共享数据竞争的锁进行消除； 锁消除的主要判断依据来源于逃逸分析的数据支持； 13.3.3 锁粗化 原则上总是推荐将同步块的作用范围限制得尽量小 – 只有在共享数据的实际作用域中才进行同步，这样是为了使得需要同步的操作数量尽可能变小，如果存在锁竞争，那等待锁的线程也能尽快拿到锁； 但是如果一系列的连续操作都对同一个对象反复加锁和解锁，甚至加锁操作是出现在循环体中的，那即使没有线程竞争，频繁地进行互斥同步操作也会导致不必要的性能损耗； 13.3.4 轻量级锁 轻量级锁是JDK 1.6之中加入的新型锁机制，它是相对于使用操作系统互斥量来实现的传统锁而言的；它并不是用来代替重量级锁的，它的本意是在没有多线程竞争的前提下，减少传统的重量级锁使用操作系统互斥量产生的性能消耗； 要理解轻量级锁，以及后面会讲到的偏向锁的原理和运作过程，必须从HotSpot虚拟机的对象的内存布局开始介绍；HotSpot虚拟机的对象头分为两部分信息：第一部分用于存储对象自身的运行时数据，如哈希码、GC分代年龄等，这部分官方称之为Mark Word，是实现轻量级锁和偏向锁的关键，另外一部分用于存储指向方法区对象类型数据的指针； Mark Word被设计成一个非固定的数据结构以便在极小的空间存储尽量多的信息，在32位的HotSpot虚拟机中对象未被锁定的状态下，25bit用于存储对象哈希码，4bit用于存储对象分代年龄，2bit用于存储锁标志位，1bit固定为0；在其他状态（轻量级锁定、重量级锁定、GC标志、可偏向）下对象的存储内容如下： 在代码进入同步块的时候，如果此同步对象没有被锁定，虚拟机首先将在当前线程的栈帧中建立一个名为锁记录（Lock Record）的空间，用于存储对象目前的Mark Word的拷贝（官方称之为Displaced Mark Word）；然后虚拟机将使用CAS操作尝试将对象的Mark Word更新为指向Lock Record的指针，如果更新成功了那么这个线程就拥有了该对象的锁，并且对象Mark Word的锁标志位将转变为“00”，即表示此对象处于轻量级锁定状态；如果这个更新操作失败了，虚拟机首先会检查对象的Mark Word是否指向当前线程的栈帧，如果是就可以直接进入同步块继续执行，否则说明这个锁对象已经被其他线程抢占了；如果有两条以上的线程争用同一个锁，那轻量级锁就不再有效，要膨胀为重量级锁，锁标志的状态值变为“10”，Mark Word中存储的就是指向重量级锁的指针，后面等待锁的线程也要进行阻塞状态； 轻量级锁能提升程序同步性能的依据是“对于绝大部分的锁，在整个同步周期内都是不存在竞争的”，这是一个经验数据； 13.3.5 偏向锁 偏向锁也是JDK 1.6中引入的一项锁优化，它的目的是消除数据在无竞争情况下的同步原语，进一步提高程序的运行性能；如果说轻量级锁是在无竞争的情况下使用CAS操作去消除同步使用的互斥量，那偏向锁就是在无竞争的情况下把整个同步都消除掉，连CAS操作都不做了； 偏向锁会偏向于第一个获得它的线程，如果在接下来的执行过程中，该锁没有被其他的线程获取，则持有偏向锁的线程将永远不需要再进行同步； 假设当前虚拟机启动了偏向锁，那么当锁对象第一次被线程获取的时候，虚拟机将会把对象头中的标志位设为“01”，即偏向模式；同时使用CAS操作把获取到这个锁的线程ID记录在对象的Mark Word之中；如果CAS操作成功，持有偏向锁的线程以后每次进入这个锁相关的同步块时，虚拟机都可以不再进行任何同步操作；当有另外一个线程去尝试获取这个锁时，偏向模式就宣告结束，根据锁对象目前是否被锁定的状态，撤销偏向后恢复到未锁定或轻量级锁定的状态，后续的同步操作就如上面介绍的轻量级锁那样执行；偏向锁、轻量级锁的状态转化以及对象Mark Work的关系如下图所示： 偏向锁可以提高带有同步但无竞争的程序性能，它同样是一个带有效益权衡性质的优化； 本章小结本章介绍了线程安全所涉及的概念和分类、同步实现的方式及虚拟机的底层运行原理，并且介绍了虚拟机为了实现高效并发所采取的一系列锁优化措施。 系列读书笔记 《深入理解Java虚拟机》读书笔记1：Java技术体系、Java内存区域和内存溢出异常 《深入理解Java虚拟机》读书笔记2：垃圾收集器与内存分配策略 《深入理解Java虚拟机》读书笔记3：虚拟机性能监控与调优实战 《深入理解Java虚拟机》读书笔记4：类文件结构 《深入理解Java虚拟机》读书笔记5：类加载机制与字节码执行引擎 《深入理解Java虚拟机》读书笔记6：程序编译与代码优化 《深入理解Java虚拟机》读书笔记7：高效并发]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[《深入理解Java虚拟机》读书笔记6：程序编译与代码优化]]></title>
      <url>%2Fpost%2Fdeep_in_jvm_notes_part6%2F</url>
      <content type="text"><![CDATA[国内JVM相关书籍NO.1，Java程序员必读。读书笔记第六部分对应原书的第十章和第十一章，主要介绍javac编译过程、HotSpot的即时编译器以及常见的编译优化技术，通过了解这部分的内容有利于我们更好的编码。 第四部分 程序编译与代码优化第十章 早期（编译器）优化10.1 概述 前端编译器（或叫编译器前端）：把.java文件转变为.class文件的过程，比如Sun的javac、Eclipse JDT中的ECJ； 后端运行编译器（JIT编译器）：把字节码转变为机器码的过程，比如HotSpot VM的C1、C2编译器； 静态提前编译器（AOT编译器）：直接把*.java文件编译成本地机器代码的过程，比如GNU Compiler for the Java； 本章主要针对第一类，把第二类的编译过程留到下一章讨论； javac这类编译器对代码运行效率几乎没有任何优化措施，虚拟机设计团队把对性能的优化集中到了后端的即时编译器中，这样那些不是由javac产生的Class文件也同样能享受到编译器优化所带来的好处； javac做了许多针对Java语言编码过程的优化措施来改善程序员的编码风格和提高编码效率；可以说，Java中即时编译器在运行期的优化过程对于程序运行来说更重要，而前端编译器在编译器的优化过程对于程序编码来说关系更加密切； 10.2 javac编译器javac编译器本身就是一个由Java语言编写的程序，这为纯Java的程序员了解它的编译过程带来了很大的便利。 10.2.1 javac的源码与调试 javac的源码存放在JDK_SRC_HOME/langtools/src/share/classes/com/sun/tools/javac，除了JDK自身的API外，就只引用了JDK_SRC_HOME/langtools/src/share/classes/com/sun/*里面的代码； 导入javac的源码后就可以运行com.sun.tools.javac.Main的main方法来执行编译了； javac编译过程大概可以分为3个过程：解析与填充符号表过程、插入式注解处理器的注解处理过程、分析与字节码生成过程； 10.2.2 解析与填充符号表 解析步骤由parseFiles方法完成； 词法分析将源代码的字符流转变为标记（Token）集合，由com.sun.tools.javac.parser.Scanner类完成； 语法分析是根据Token序列构造抽象语法树（AST，一种用来描述程序代码语法结构的树形表示方式）的过程，由com.sun.tools.javac.parser.Parser类实现，AST由com.sun.tools.javac.tree.JCTree类表示； 填充符号表：由enterTrees方法完成；符号表是由一组符号地址和符号信息构成的表格，所登记的信息在编译的不同阶段都要用到，在语义分析中用于语义检查，在目标代码生成时用于地址分配；由com.sun.tools.javac.comp.Enter类实现； 10.2.3 注解处理器 在JDK 1.6中实现了JSR-269规范，提供了一组插入式注解处理器的标准API在编译期间对注解进行处理，可以读取、修改、添加抽象语法树中的任意元素； 通过插入式注解处理器实现的插件在功能上有很大的发挥空间，程序员可以使用插入式注解处理器来实现许多原本只能在编码中完成的事情； javac中，在initProcessAnnotations初始化，在processAnnotations执行，如果有新的注解处理器，通过com.sun.tools.javac.processing.JavacProcessingEnviroment类的doProcessing方法生成一个新的JavaCompiler对象对编译的后续步骤进行处理； 10.2.4 语义分析与字节码生成 语义分析的主要任务是对结构上正确的源程序进行上下文有关性质的审查，主要包括标注检查、数据及控制流分析两个步骤； 解语法糖（Syntactic Sugar，添加的某种对语言功能没有影响但方便程序员使用的语法）：Java中最常用的语法糖主要是泛型、变长参数、自动装箱等，他们在编译阶段还原回简单的基础语法结构；在com.sun.tools.javac.comp.TransTypes类和com.sun.tools.javac.comp.Lower类中完成； 字节码生成：javac编译的最后一个阶段，不仅仅是把前面各个步骤所生成的信息转化为字节码写入到磁盘中，编译器还进行了少量的代码添加和转换工作（如实例构造器方法和类构造器方法）；由com.sun.tools.javac.jvm.ClassWriter类的writeClass方法输出字节码，生成最终的Class文件； 10.3 Java语法糖的味道10.3.1 泛型与类型擦除 Java语言的泛型只在程序源码中存在，在编译后的字节码文件中，就已经替换为原来的原生类型了，并且在相应的地方插入了强制转换，这种基于类型擦除的泛型实现是一种伪泛型； JCP组织引入了Signature属性，它的作用就是存储一个方法在字节码层面的特征签名，这个属性中保存的参数类型并不是原生类型，而是包括了参数化类型的信息，这样我们就可以通过反射手段获取参数化类型； 10.3.2 自动装箱、拆箱与遍历循环 它们的实现比较简单，但却是Java语言里使用最多的语法糖； 10.3.3 条件编译 Java语言之中并没有使用预处理器，因为Java编译器并非一个个地编译Java文件，而是将所有编译单元的语法树顶级节点输入到待处理列表后再进行编译； Java语言可以使用条件为常量的if语句进行条件编译；编译器将会把分支中不成立的代码块消除掉； 10.4 实战：插入式注解处理器 实战目标：使用注解处理器API来编写一款拥有自己编码风格的校验工具； 代码实现：继承javax.annotation.processing.AbstractProcessor，实现process方法，从第一个参数annotations获取此注解处理器所要处理的注解集合，从第二个参数roundEnv中访问到当前这个Round中的语法树节点；另外还有一个很常用的实例变量processingEnv，它代表了注解处理器框架提供的一个上下文环境；可以配合使用的@SupportedAnnotationTypes和@SupportedSourceVersion注解； 123456789101112131415161718192021222324@SupportedAnnotationTypes("*")@SupportedSourceVersion(SourceVersion.RELEASE_6)public class NameCheckProcessor extends AbstractProcessor&#123; private NameChecker nameChecker; @Override public void init(ProcessingEnviroment processingEnv)&#123; super.init(processingEnv); nameChecker = new NameChecker(processingEnv); &#125; @Override public boolean process(Set&lt;? extends TypeElement&gt; annotations, RoundEnviroment roundEnv)&#123; if(!roundEnv.processingOver)&#123; for(Element element : roundEnv.getRootElements())&#123; nameChecker.checkNames(element); &#125; &#125; return false; &#125; &#125; 10.5 本章小结本章我们从编译器源码实现的层次上了解了javac源代码编译为字节码的过程，分析了Java语言中多种语法糖的前因后果，并实战实习了如何使用插入式注解处理器来完成一个检查程序命名规范的编译器插件。下一章我们将会介绍即时编译器的运作和优化过程。 第十一章 晚期（运行期）优化11.1 概述 为了提高热点代码的执行效率，在运行时虚拟机将会把这些代码编译成与本地平台相关的机器码，并进行各种层次的优化，完成这个任务的编译器称为即时编译器（JIT）； JIT不是虚拟机必需的，但是其编译性能的好坏、代码优化程度的高低却是衡量一款商用虚拟机优秀与否的最关键的指标之一，它也是虚拟机中最核心且最能体现虚拟机技术水平的部分； 11.2 HotSpot虚拟机内的即时编译器11.2.1 解释器与编译器 当程序需要迅速启动和执行的时候，解释器可以先发挥作用，省去编译的时间立即执行；在程序运行后，随着时间的推移，编译器把越来越多的代码编译成本地代码提升执行效率； HotSpot虚拟机中内置了两个即时编译器，分别为Client Compiler和Server Compiler，或简称为C1编译器和C2编译器；虚拟机会根据自身版本与宿主机器的硬件性能自动选择运行模式，也可以使用“-client”或“-server”参数去强制指定运行模式； 想要编译出优化程度更高的代码，解释器可能还要替编译器收集性能监控信息，为了在程序启动响应速度与运行效率之间达到最佳平衡，HotSpot虚拟机还会逐渐启动分层编译的策略：第0层，程序解释运行；第1层，C1编译；第2层，C2编译； 实施分层编译后，Client Compiler和Server Compiler将会同时工作，许多代码都可能会被多次编译，用Client Compiler获取更高的编译速度，用Server Compiler来获取更好的编译质量，在解释执行的时候也无须再承担性能收集监控信息的任务； 11.2.2 编译对象与触发条件 被JIT编译的热点代码有两类：被多次调用的方法、被多次执行的循环体；对于前者编译器会以整个方法作为编译对象，属于标准的JIT编译方式；对于后者尽管编译动作是由循环体所触发的，但编译器依然会以整个方法作为编译对象，这种编译方式称之为栈上替换（OSR编译）； 热点探测：基于采样的热点探测和基于计数器的热点探测，在HotSpot虚拟机中使用的是第二种，通过方法计数器和回边计数器进行热点探测。方法调用计数器触发的即时编译交互过程如下图所示： 11.2.3 编译过程 对于Client Compiler来说，它是一个简单快速的三段式编译器，主要的关注点在于局部性的优化，而放弃了很多耗时较长的全局优化手段；第一阶段一个平台独立的前端将字节码构造成一个高级中间代码表示（HIR），第二阶段一个平台相关的后端从HIR中产生低级中间代码表示（LIR），最后阶段是在平台相关的后端使用线性扫描算法在LIR上分配寄存器，并在LIR上做窥孔优化，然后产生机器代码。其大致过程如下所示： Server Compiler是专门面向服务端的典型应用并为服务端的性能配置特别调整过的编译器，也是一个充分优化过的高级编译器，几乎能达到GNU C++编译器使用-02参数时的优化强大，它会执行所有经典的优化动作，如无用代码消除、循环展开、循环表达式外提、消除公共子表达式、常量传播、基本块重排序等，还会实现如范围检查消除、空值检查消除等Java语言特性密切相关的优化技术； 11.2.4 查看及分析即时编译结果 本节的运行参数有一部分需要Debug或FastDebug版虚拟机的支持； 要知道某个方法是否被编译过，可以使用参数-XX:+PrintCompilation要求虚拟机在即时编译时将被编译成本地代码的方法名称打印出来； 还可以加上参数-XX:+PrintInlining要求虚拟机输出方法内联信息，输出内容如下： 除了查看那些方法被编译之外，还可以进一步查看即时编译器生成的机器码内容，这个需要结合虚拟机提供的反汇编接口来阅读； 11.3 编译优化技术11.3.1 优化技术概览 11.3.2 公共子表达式消除 如果一个表达式E已经计算过了，并且从先前的计算到现在E中所有变量的值都没有发生变化，那么E的这次出现就成为了公共子表达式，只需要直接用前面计算过的表达式结果代替E就可以了； 11.3.3 数组边界检查消除 对于虚拟机的执行子系统来说，每次数组元素的读写都带有一次隐含的条件判断，对于拥有大量数组访问的程序代码无疑是一种性能负担； 11.3.4 方法内联 除了消除方法调用的成本外更重要的意义是为其他优化手段建立良好的基础； 为了解决虚方法的内联问题，引入了类型继承关系分析（CHA）技术和内联缓存（Inline Cache）来完成方法内联； 11.3.5 逃逸分析 逃逸分析的基本行为就是分析对象动态作用域，当一个对象在方法中被定义后，它可能被外部方法所引用（方法逃逸），甚至还可能被外部线程所访问到（线程逃逸）；如果能证明一个对象不会逃逸到方法或线程之外，则可能为这个变量进行一些高效的优化，比如栈上分配（减轻垃圾收集的压力）、同步消除（读写不会有竞争）、标量替换； 11.4 Java与C/C++的编译器对比 Java虚拟机的即时编译器与C/C++的静态优化编译器相比，可能会由于下列这些原因而导致输出的本地代码有一些劣势：即时编译器运行占用用户程序运行时间、动态类型安全语言导致的频繁检查、运行时对方法接收者进行多态选择的频率大、可以动态扩展导致很多全局的优化难以运行、大部分对象在堆上分配导致垃圾收集机制的效率低； Java语言的特性换取了开发效率的提升、还有许多优化是静态优化编译器不好做的，比如别名分析、还有一些以运行期性能监控为基础的优化措施如调用频率预测等； 11.5 本章小结本章我们着重了解了虚拟机的热点探测方法、HotSpot的即时编译器、编译触发条件以及如何从虚拟机外部观察和分析JIT编译的数据和结果，还选择了集中场景的编译期优化技术进行讲解。对Java编译器的深入了解，有助于在工作中分辨哪些代码是编译器可以帮我们处理的，哪些代码需要自己调节以便更适合编译器的优化。 系列读书笔记 《深入理解Java虚拟机》读书笔记1：Java技术体系、Java内存区域和内存溢出异常 《深入理解Java虚拟机》读书笔记2：垃圾收集器与内存分配策略 《深入理解Java虚拟机》读书笔记3：虚拟机性能监控与调优实战 《深入理解Java虚拟机》读书笔记4：类文件结构 《深入理解Java虚拟机》读书笔记5：类加载机制与字节码执行引擎 《深入理解Java虚拟机》读书笔记6：程序编译与代码优化 《深入理解Java虚拟机》读书笔记7：高效并发]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[基于Elasticsearch实现搜索建议]]></title>
      <url>%2Fpost%2Fsearch_suggestion_implemention_based_elasticsearch%2F</url>
      <content type="text"><![CDATA[搜索建议是搜索的一个重要组成部分，一个搜索建议的实现通常需要考虑建议词的来源、匹配、排序、聚合、关联的文档数和拼写纠错等，本文介绍一个基于Elasticsearch实现的搜索建议。 问题描述电商网站的搜索是最基础最重要的功能之一，搜索框上面的良好体验能为电商带来更高的收益，我们先来看看淘宝、京东、亚马逊网站的搜索建议。 在淘宝的搜索框输入【卫衣】时，下方的搜索建议包括建议词以及相关的标签： 在京东的搜索框输入【卫衣】时，下方搜索建议右方显示建议词关联的商品数量： 在亚马逊的搜索框输入【卫衣】时，搜索建议上部分能支持在特定的分类下进行搜索： 通过上述对比可以看出，不同的电商对于搜索建议的侧重点略有不同，但核心的问题包括： 匹配：能够通过用户的输入进行前缀匹配； 排序：根据建议词的优先级进行排序； 聚合：能够根据建议词关联的商品进行聚合，比如聚合分类、聚合标签等； 纠错：能够对用户的输入进行拼写纠错； 搜索建议实现在我们的搜索建议实现里，主要考虑了建议词的来源、匹配、排序、关联的商品数量和拼写纠错。 SuggestionDiscovery SuggestionDiscovery的职责是发现建议词； 建议词的来源可以是商品的分类名称、品牌名称、商品标签、商品名称的高频词、热搜词，也可以是一些组合词，比如“分类 + 性别”和“分类 + 标签”，还可以是一些自定义添加的词； 建议词维护的时候需要考虑去重，比如“卫衣男”和“卫衣 男”应该是相同的，“Nike”和“nike”也应该是相同的； 由于建议词的来源通常比较稳定，所以执行的周期可以比较长一点，比如每周一次； SuggestionCounter SuggestionCounter的职责是获取建议词关联的商品数量，如果需要可以进行一些聚合操作，比如聚合分类和标签； SuggestionCounter的实现的时候由于要真正地调用搜索接口，应该尽量避免对用户搜索的影响，比如在凌晨执行并且使用单线程调用； 为了提升效率，应该使用Elasticsearch的Multi Search接口批量进行count，同时批量更新数据库里建议词的count值； 由于SuggestionCounter是比较耗资源的，可以考虑延长执行的周期，但是这可能会带来count值与实际搜索时误差较大的问题，这个需要根据实际情况考虑； SuggestionIndexRebuiler SuggestionIndexRebuiler的职责是负责重建索引； 考虑到用户的搜索习惯，可以使用Multi-fields来给建议词增加多个分析器。比如对于【卫衣 套头】的建议词使用Multi-fields增加不分词字段、拼音分词字段、拼音首字母分词字段、IK分词字段，这样输入【weiyi】和【套头】都可以匹配到该建议词； 重建索引时通过是通过bulk批量添加到临时索引中，然后通过别名来更新； 重建索引的数据依赖于SuggestionCounter，因此其执行的周期应该与SuggestionCounter保持一致； SuggestionService SuggestionService是真正处于用户搜索建议的服务类； 通常的实现是先到缓存中查询是否能匹配到缓存记录，如果能匹配到则直接返回；否则的话调用Elasticsearch的Prefix Query进行搜索，由于我们在重建索引的时候定义了Multi-fields，在搜索的时候应该用boolQuery来处理；如果此时Elasticsearch返回不为空的结果数据，那么加入缓存并返回即可； POST /suggestion/_search { &quot;from&quot; : 0, &quot;size&quot; : 10, &quot;query&quot; : { &quot;bool&quot; : { &quot;must&quot; : { &quot;bool&quot; : { &quot;should&quot; : [ { &quot;prefix&quot; : { &quot;keyword&quot; : &quot;卫衣&quot; } }, { &quot;prefix&quot; : { &quot;keyword.keyword_ik&quot; : &quot;卫衣&quot; } }, { &quot;prefix&quot; : { &quot;keyword.keyword_pinyin&quot; : &quot;卫衣&quot; } }, { &quot;prefix&quot; : { &quot;keyword.keyword_first_py&quot; : &quot;卫衣&quot; } } ] } }, &quot;filter&quot; : { &quot;range&quot; : { &quot;count&quot; : { &quot;from&quot; : 5, &quot;to&quot; : null, &quot;include_lower&quot; : true, &quot;include_upper&quot; : true } } } } }, &quot;sort&quot; : [ { &quot;weight&quot; : { &quot;order&quot; : &quot;desc&quot; } }, { &quot;count&quot; : { &quot;order&quot; : &quot;desc&quot; } } ] } 如果Elasticsearch返回的是空结果，此时应该需要增加拼写纠错的处理（拼写纠错也可以在调用Elasticsearch搜索的时候带上，但是通常情况下用户并没有拼写错误，所以建议还是在后面单独调用suggester）；如果返回的suggest不为空，则根据新的词调用建议词服务；比如用户输入了【adidss】，调用Elasticsearch的suggester获取到的结果是【adidas】，则再根据adidas进行搜索建议词处理。 POST /suggestion/_search { &quot;size&quot; : 0, &quot;suggest&quot; : { &quot;keyword_suggestion&quot; : { &quot;text&quot; : &quot;adidss&quot;, &quot;term&quot; : { &quot;field&quot; : &quot;keyword&quot;, &quot;size&quot; : 1 } } } } 关于排序：在我们的实现里面是通过weight和count进行排序的，weight目前只考虑了建议词的类型（比如分类 &gt; 品牌 &gt; 标签）； 实现效果和后续改进 通过上面的实现，我们已经能实现一个比较强大的搜索建议词了，实际的效果如下所示： 后续可以考虑的改进：参考亚马逊增加分类的聚合展示、增加用户个性化的处理支持更好的建议词排序、基于用户的搜索历史支持更好的建议词推荐； 参考资料 Elasticsearch Prefix Query Elasticsearch Suggester]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[《深入理解Java虚拟机》读书笔记5：类加载机制与字节码执行引擎]]></title>
      <url>%2Fpost%2Fdeep_in_jvm_notes_part5%2F</url>
      <content type="text"><![CDATA[国内JVM相关书籍NO.1，Java程序员必读。读书笔记第五部分对应原书的第七章至第九章，主要介绍虚拟机的类加载机制、字节码执行引擎，并通过实例和实战加深对虚拟机执行子系统这一部分的理解。 第七章 虚拟机类加载机制7.1 概述 虚拟机把描述类的数据从Class文件加载到内存，并对数据进行校验、转换解析和初始化，最终形成可以被虚拟机直接使用的Java类型，这就是虚拟机的类加载机制。 在Java语言里面，类型的加载、连接和初始化过程都是在程序运行期间完成，这虽然增量一些性能开销，但是会为Java应用程序提供高度的灵活性。 7.2 类加载的时机 类的整个生命周期：加载、验证、准备、解析、初始化、使用和卸载；其中验证、准备和解析统称为连接； 虚拟机规范没有强制约束类加载的时机，但严格规定了有且只有5种情况必须立即对类进行初始化：遇到new、getstatic、putstatic和invokestatic指令；对类进行反射调用时如果类没有进行过初始化；初始化时发现父类还没有进行初始化；虚拟机启动指定的主类；动态语言中MethodHandle实例最后解析结果REF_getStatic等的方法句柄对应的类没有初始化时； 7.3 类加载的过程7.3.1 加载 通过一个类的全限定名来获取定义此类的二进制字节流； 将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构； 在内存中生成一个代表这个类的java.lang.Class对象，作为方法区这个类的各种数据的访问入口； 7.3.2 验证 验证是连接阶段的第一步，其目的是确保Class文件的字节流中包含的信息符合当前虚拟机的要求，并且不会危害虚拟机自身的安全； 验证阶段是非常重要的，这个阶段是否严谨决定了Java虚拟机是否能承受恶意代码的攻击； 校验动作：文件格式验证（基于二进制字节流）、元数据验证（对类的元数据语义分析）、字节码验证（对方法体语义分析）、符号引用验证（对类自身以外的信息进行匹配性校验）； 7.3.3 准备 正式为变量分配内存并设置类变量初始值的阶段，这些变量所使用的内存都将在这个方法区中进行分配； 需要强调两点：这时候内存分配的仅包括类变量，而不包括类实例变量；这里所说的初始化通常情况下是数据类型的零值，真正的赋值是在初始化阶段，如果是static final的则是直接赋值； 7.3.4 解析 解析阶段是虚拟机将常量池内的符号引用（如CONSTANT_Class_info、CONSTANT_Fieldref_info、CONSTANT_Methodref_info等7种）替换为直接引用的过程； 符号引用可以是任何形式的字面量，与虚拟机实现的内存布局无关，引用的目标并不一定已经加载到内存中；而直接引用是直接指向目标的指针、相对偏移量或是一个能间接定位到目标的句柄，它和虚拟机实现的内存布局相关，引用的目标必定以及在内存中存在； 对同一个符号引用进行多次解析请求是很常见的事情，虚拟机实现可以对第一次解析的结果进行缓存； 7.3.5 初始化 是类加载过程的最后一步，真正开始执行类中定义的Java程序代码（或者说是字节码）； 初始化阶段是执行类构造器方法的过程，该方法是由编译器自动收集类中的所有类变量的赋值动作和静态语句块中的语句合并产生的； 方法与类的构造函数（或者说是实例构造器方法）不同，它不需要显式地调用父类构造器，虚拟机会保证在子类的方法执行之前，父类的方法已执行完毕； 执行接口的方法不需要先执行父接口的方法，只有当父接口中定义的变量使用时父接口才会初始化，接口的实现类在初始化时也一样不会执行接口的方法； 方法初始化是加锁阻塞等待的，应当避免在方法中有耗时很长的操作； 7.4 类加载器 虚拟机设计团队把类加载阶段的“通过一个类的全限定名来获取描述此类的二进制字节流”这个动作放到虚拟机外部去实现，实现这个动作的代码模块称为类加载器； 这时Java语言的一项创新，也是Java语言流行的重要原因，在类层次划分、OSGI、热部署、代码加密等领域大放异彩； 7.4.1 类与类加载器 对于任意一个类，都需要由加载它的类加载器和这个类本身一同确立其在Java虚拟机的唯一性，每一个类加载器都拥有一个独立的类名称空间； 比较两个类是否相等（如Class对象的equals方法、isAssignableFrom方法、isInstance方法），只有在这两个类是由同一个类加载器加载的前提下才有意义； 7.4.2 双亲委派模型 三种系统提供的类加载器：启动类加载器（Bootstrap ClassLoader）、扩展类加载器（Extension ClassLoader）、应用程序类加载器（Application ClassLoader）； 双亲委派模型要求除了顶层的启动类加载器外，其他的类加载器都应当有自己的父类加载器，这里一般不会以继承的关系来实现，而是使用组合的关系来复用父加载器的代码； 其工作过程是：如果一个类加载器收到了类加载的请求，它首先不会自己去尝试加载这个类，而是把这个请求委派给父类加载器去完成，只有父类加载器反馈自己无法完成这个加载请求时（它的搜索范围中没有找到所需的类），子加载器才会尝试自己去加载； 这样的好处是Java类随着它的类加载器具备了一种带有优先级的层次关系，对保证Java程序的稳定运作很重要； 实现双亲委派的代码都集中在java.lang.ClassLoader的loadClass方法中，逻辑清晰易懂； 7.4.3 破坏双亲委派模型 上一小节的双亲委派模型是Java设计者推荐给开发者的类加载器实现方法，但不是一个强制性的约束模型； 典型的两种情况：为了解决JNI接口提供者（SPI）引入的线程上下文类加载器；为了程序动态性加强的OSGI的Bundle类加载器； 7.5 本章小结本章介绍了类加载过程的加载、验证、准备、解析和初始化五个阶段中虚拟机进行了哪些动作，还介绍了类加载器的工作原理及其对虚拟机的意义。下一章将一起看看虚拟机如果执行定义在Class文件里的字节码。 第八章 虚拟机字节码执行引擎8.1 概述 执行引擎是Java虚拟机最核心的组成部分之一，区别于物理机的执行引擎是直接建立在处理器、硬件、指令集和操作系统层面上的，虚拟机的执行引擎是自己实现的，可以自行制定指令集与执行引擎的结构体系，并且能够执行哪些不被硬件直接支持的指令集格式； 在虚拟机规范中制定了虚拟机字节码执行引擎的概念模型，该模型成为各种虚拟机执行引擎的统一外观； 在不同的虚拟机实现里面，执行引擎在执行Java代码时可能会有解释执行和编译执行两种选择，也可能两者兼备，甚至还可能会包含几个不同级别的编译器执行引擎，但从外观来说是一致的：输入的都是字节码文件，处理过程是字节码解析的等效过程，输出的是执行结果。 8.2 运行时栈帧结构 栈帧是用于支持虚拟机进行方法调用和方法执行的数据结构，它是虚拟机运行时数据区中的虚拟机栈的栈元素； 栈帧存储了方法的局部变量表、操作数栈、动态连接和方法返回地址等信息，每一个方法从调用开始至执行完成的过程，都对应着一个栈帧在虚拟机里面从入栈到出栈的过程； 栈帧需要分配多少内存在编译时就完全确定并写入到方法表的Code属性之中了，不会受到程序运行期变量数据的影响； 对于执行引擎来说，在活动线程中只有位于栈顶的栈帧才算有效的，称为当前栈帧，与这个栈帧相关联的方法称为当前方法，执行引擎运行的所有字节码指令都只针对当前栈帧进行操作。 8.2.1 局部变量表 是一组变量值存储空间，用于存放方法参数和方法内部定义的局部变量，Code属性的max_locals确定了该方法所需要分配的局部变量表的最大容量； 其容量以变量槽（Variable Slot）为最小单位，虚拟机规范允许Slot的长度随处理器、操作系统或虚拟机的不同而发生变化； 一个Slot可以存放一个32位以内的数据类型，包括boolean、byte、char。short、int、float、reference和returnAddress这八种类型；对于64位的数据类型（long和double），虚拟机会以高位对齐的方式为其分配两个连续的Slot空间； 8.2.2 操作数栈 也常称为操作栈，它是一个后入先出栈；Code属性的max_stacks确定了其最大深度； 比如整数加法的字节码指令iadd在运行的时候操作数栈中最接近栈顶的两个元素已经存入了两个int型的数值，当执行这个指令时，会将这两个int值出栈并相加，然后将相加的结果入栈； 操作数栈中元素的类型必须与字节码指令的序列严格匹配； Java虚拟机的解释执行引擎称为“基于栈的执行引擎”，其中所指的栈就是操作数栈； 8.2.3 动态连接 每个栈帧都包含一个执行运行时常量池中该栈帧所属方法引用，持有这个引用是为了支持方法调用过程中的动态连接（Dynamic Linking）； Class文件的常量池的符号引用，有一部分在类加载阶段或者第一次使用时就转换为直接引用，这种称为静态解析，而另外一部分在每一次运行期间转换为直接引用，这部分称为动态连接； 8.2.4 方法返回地址 退出方法的方式：正常完成出口和异常完成出口； 方法退出的过程实际上就等同于把当前栈帧出栈，因此退出时可能只需的操作有：恢复上层方法的局部变量表和操作数栈，把返回值压入调用者栈帧的操作数中，调整PC计数器的值以只需方法调用指令后面的一套指令等； 8.2.5 附加信息 虚拟机规范允许具体的虚拟机实现增加一些规范里没有描述的信息到栈帧中，例如与调试相关的信息，这部分完成取决于具体的虚拟机实现； 方法调用 方法调用并不等同于方法执行，方法调用阶段唯一的任务就是确定被调用方法的版本即调用哪一个方法，暂时还不涉及方法内部的具体运行过程； Class文件的编译过程中不报警传统编译的连接步骤，一切方法调用在Class文件里面存储的都只是符号引用，而不是方法在实际运行时内存布局的入口地址。这个特性给Java带来了更强大的动态扩展能力，但也使得Java方法调用过程变得相对复杂； 8.3.1 解析 方法在程序真正运行之前就有一个可确定的调用版本，并且这个方法的调用版本在运行期是不可改变的，这类方法的调用称为解析； 在Java语言中符合编译器可知、运行期不可变这个要求的方法，主要包括静态方法和私有方法两大类； 五条方法调用字节码指令：invokestatic、invokespecial、invokevirtual、invokeinterface、invokedynamic； 解析调用是一个静态的过程，在编译期间就完全确定，在类加载的解析阶段就会把涉及的符号引用全部转变为可确定的直接引用；而分派调用则可能是静态的也可能是动态的； 8.3.2 分派 静态分派：“Human man = new Man();”语句中Human称为变量的静态类型，后面的Man称为变量的实际类型；静态类型和实际类型在程序中都可以发生一些变化，区别是静态类型的变化仅仅在使用时发生，变量本身的静态类型不会被改变，并且最终的静态类型是在编译器可知的；而实际类型的变化在运行期才确定，编译器在编译程序的时候并不知道一个对象的实际类型是什么；编译器在重载时是通过参数的静态类型而不是实际类型作为判定依据的；所有根据静态类型来定位方法执行版本的分派动作称为静态分派，其典型应用是方法重载； 动态分派：invokevirtual指令执行的第一步就是在运行期间确定接收者的实际类型，所以两次调用中invokevirtual指令把常量池中的类方法符号引用解析到了不同的直接引用上，这个过程就是Java语言中方法重写的本质；我们把这种在运行期根据实际类型确定方法执行版本的分派过程称为动态分派； 单分派与多分派：方法的接收者与方法的参数统称为方法的宗量，根据分派基于多少种宗量，可以将分派分为单分派（根据一个宗量对目标方法进行选择）与多分派（根据多于一个宗量对目标方法进行选择）两种；今天的Java语言是一门静态多分派、动态单分派的语言； 虚拟机动态分派的实现：在方法区中建立一个虚方法表（Virtual Method Table），使用虚方法表索引来代替元数据查找以提高性能；方法表一般在类加载的连接阶段进行初始化，准备了类的变量初始化值后，虚拟机会把该类的方法表也初始化完毕； 8.3.3 动态类型语言支持 JDK 1.7发布增加的invokedynamic指令实现了“动态类型语言”支持，也是为JDK 1.8顺利实现Lambda表达式做技术准备； 动态类型语言的关键特征是它的类型检查的主体过程是在运行期而不是编译器，比如JavaScript、Python等； Java语言在编译期间就将方法完整的符号引用生成出来，作为方法调用指令的参数存储到Class文件中；这个符号引用包含了此方法定义在哪个具体类型之中、方法的名字以及参数顺序、参数类型和方法返回值等信息；而在ECMAScript等动态语言中，变量本身是没有类型的，变量的值才具有类型，编译时最多只能确定方法名称、参数、返回值这些信息，而不会去确定方法所在的具体类型；变量无类型而变量值才有类型，这个特点也是动态类型语言的一个重要特征； JDK 1.7实现了JSR-292，新加入的java.lang.invoke包的主要目的是在之前单纯依靠符号引用来确定调用的目标方法外，提供一种新的动态确定目标方法的机制，称为MethodHandle； 从本质上讲，Reflection（反射）和MethodHandle机制都是在模拟方法调用，但Reflection是在模拟Java代码层次的方法调用，而MethodHandle是在模拟字节码层次的方法调用，前者是重量级，而后者是轻量级；另外前者只为Java语言服务，后者可服务于所有Java虚拟机之上的语言； 每一处含有invokedynamic指令的位置都称为“动态调用点(Dynamic Call Site)”，这条指令的第一个参数不再是代表符号引用的CONSTANT_Methodref_info常量，而是CONSTANT_InvokeDynamic_info常量（可以得到引导方法、方法类型和名称）； invokedynamic指令与其他invoke指令的最大差别就是它的分派逻辑不是由虚拟机决定的，而是由程序员决定的； 8.4 基于栈的字节码解释执行引擎上节主要讲虚拟机是如何调用方法的，这节探讨虚拟机是如何执行方法中的字节码指令的。 8.4.1 解释执行 只有确定了谈论对象是某种具体的Java实现版本和执行引擎运行模式时，谈解释执行还是编译执行才比较确切； Java语言中，javac编译器完成了程序代码经过词法分析、语法分析到抽象语法树，再遍历语法树生成线性的字节码指令流的过程；因为这一部分动作是在Java虚拟机之外进行的，而解释器在虚拟机的内部，所以Java程序的编译就是半独立的实现； 8.4.2 基于栈的指令集与基于寄存器的指令集 Java编译器输出的指令集，基本上是一种基于栈的指令集架构，指令流中的指令大部分是零地址指令，它们依赖操作数栈进行工作； 基于栈的指令集主要的优点是可移植性，寄存器由硬件直接提供，程序直接依赖这些硬件寄存器则不可避免地要受到硬件的约束；主要缺点是执行速度相对来说会稍慢一点； 8.4.3 基于栈的解释器执行过程一段简单的算法代码 123456public int calc()&#123; int a = 100; int b = 200; int c = 300; return (a + b) * c;&#125; 上述代码的字节码表示 public int calc(); Code: Stack=2, Locals=4, Args_size=1 0:bipush 100 2:istore_1 3:sipush 200 6:istore_2 7:sipush 300 10:istore_3 11:iload_1 12:iload_2 13:iadd 14:iload_3 15:imul 16:ireturn javap提示这段代码需要深度为2的操作数栈和4个Slot的局部变量空间，作者根据这些信息画了示意图来说明执行过程中的变化情况： 执行偏移地址为0的指令 执行偏移地址为2的指令 执行偏移地址为11的指令 执行偏移地址为12的指令 执行偏移地址为13的指令 执行偏移地址为14的指令 执行偏移地址为16的指令 注：上面的执行过程仅仅是一种概念模型，虚拟机中解析器和即时编译器会对输入的字节码进行优化。 8.5 本章小结本章分析了虚拟机在执行代码时，如何找到正确的方法、如何执行方法内的字节码以及执行代码时涉及的内存结构。这第六、七、八三章中，我们针对Java程序是如何存储的、如何载入的以及如何执行的问题进行了讲解，下一章一起看看这些理论知识在具体开发中的经典应用。 第九章 类加载及执行子系统的案例与实战9.1 概述 在Class文件格式与执行引擎这部分中，用户的程序能直接影响的内容并不多； 能通过程序进行操作的，主要是字节码生成与类加载器这两部分的功能，但仅仅在如何处理这两点上，就已经出现了许多值得欣赏和借鉴的思路； 9.2 案例分析9.2.1 Tomcat：正统的类加载器架构 Java Web服务器：部署在同一个服务器上的两个Web应用程序所使用的Java类库可以实现相互隔离又要可以互相共享；尽可能保证自身的安全不受部署的Web应用程序影响；要支持JSP生成类的热替换； 上图中，灰色背景的三个类加载器是JDK默认提供的类加载器，而CommonClassLoader、CatalinaClassLoader、SharedClassLoader和WebappClassLoader是Tomcat自己定义的类加载器，分别加载/common/（可被Tomcat和Web应用共用）、/server/（可被Tomcat使用）、/shared/（可被Web应用使用）和/WebApp/WEB-INF/（可被当前Web应用使用）中的Java类库，Tomcat 6.x把前面三个目录默认合并到一起变成一个/lib目录（作用同原先的common目录）； 9.2.2 OSGI：灵活的类加载架构 OSGI的每个模块称为Bundle，可以声明它所依赖的Java Package（通过Import-Package描述），也可以声明它允许导出发布的Java Package（通过Export-Package描述）； 除了更精确的模块划分和可见性控制外，引入OSGI的另外一个重要理由是基于OSGI的程序很可能可以实现模块级的热插拔功能； OSGI的类加载器之间只有规则，没有固定的委派关系；加载器之间的关系更为复杂、运行时才能确定的网状结构，提供灵活性的同时，可能会产生许多的隐患； 9.2.3 字节码生成技术与动态代理的实现 在Java里面除了javac和字节码类库外，使用字节码生成的例子还有Web服务器中的JSP编译器、编译时植入的AOP框架和很常用的动态代理技术等，这里选择其中相对简单的动态代理来看看字节码生成技术是如何影响程序运作的； 动态代理的优势在于实现了在原始类和接口还未知的时候就确定类的代理行为，可以很灵活地重用于不同的应用场景之中； 以下的例子中生成的代理类“$Proxy0.class”文件可以看到代理为传入接口的每一个方法统一调用了InvocationHandler对象的invoke方法；其生成代理类的字节码大致过程其实就是根据Class文件的格式规范去拼接字节码； 123456789101112131415161718192021222324252627282930313233343536public class DynamicProxyTest &#123; interface IHello &#123; void sayHello(); &#125; static class Hello implements IHello &#123; @Override public void sayHello() &#123; System.out.println("Hello world"); &#125; &#125; static class DynamicProxy implements InvocationHandler &#123; Object originalObj; Object bind(Object originalObj) &#123; this.originalObj = originalObj; return Proxy.newProxyInstance(originalObj.getClass().getClassLoader(), originalObj.getClass().getInterfaces(), this); &#125; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; System.out.println("Welcome"); return method.invoke(originalObj, args); &#125; &#125; public static void main(String[] args) &#123; // add this property to generate proxy class file System.getProperties().put("sun.misc.ProxyGenerator.saveGeneratedFiles", "true"); IHello hello = (IHello) new DynamicProxy().bind(new Hello()); hello.sayHello(); &#125;&#125; 9.2.4 Retrotranslator：跨越JDK版本 Retrotranslator的作用是将JDK 1.5编译出来的Class文件转变为可以在JDK 1.4或JDK 1.3部署的版本，它可以很好地支持自动装箱、泛型、动态注解、枚举、变长参数、遍历循环、静态导入这些语法特性，甚至还可以支持JDK 1.5中新增的集合改进、并发包以及对泛型、注解等的反射操作； JDK升级通常包括四种类型：编译器层面的做的改进、Java API的代码增强、需要再字节码中进行支持的活动以及虚拟机内部的改进，Retrotranslator只能模拟前两类，第二类通过独立类库实现，第一类则通过ASM框架直接对字节码进行处理； 9.3 实战：自己动手实现远程执行功能 目标：不依赖JDK版本、不改变原有服务端程序的部署，不依赖任何第三方类库、不侵入原有程序、临时代码的执行结果能返回到客户端； 思路：如何编译提交到服务器的Java代码（客户端编译好上传Class文件而不是Java代码）、如何执行编译之后的Java代码（要能访问其他类库，要能卸载）、如何收集Java代码的执行结果（在执行的类中把System.out的符号引用替换为我们准备的PrintStream的符号引用）； 具体实现：HotSwapClassLoader用于实现同一个类的代码可以被多次加载，通过公开父类ClassLoader的defineClass实现；HackSystem是为了替换java.lang.System，它直接修改Class文件格式的byte[]数组中的常量池部分，将常量池中指定内容的CONSTANT_Utf8_info常量替换为新的字符串；ClassModifier涉及对byte[]数组操作的部分，主要是将byte[]与int和String互相转换，以及把对byte[]数据的替换操作封装在ByteUtils类中；经过ClassModifier处理过的byte[]数组才会传给HotSwapClassLoader.loadByte方法进行类加载；而JavaClassExecutor是提供给外部调用的入口； 123456789101112131415161718public class JavaClassExecutor &#123; public static String execute(byte[] classByte) &#123; HackSystem.clearBuffer(); ClassModifier cm = new ClassModifier(classByte); byte[] modifiedBytes = cm.modifyUTF8Constant("java/lang/System", "org/fenixsoft/classloading/execute/HackSystem"); HotSwapClassLoader hotSwapClassLoader = new HotSwapClassLoader(); Class clazz = hotSwapClassLoader.loadByte(modifiedBytes); try &#123; Method method = clazz.getMethod("main", new Class[]&#123;String[].class&#125;); method.invoke(null, new String[]&#123;null&#125;); &#125; catch (Throwable t) &#123; t.printStackTrace(HackSystem.out); &#125; return HackSystem.getBufferString(); &#125;&#125; 用于测试的JSP 12345678910111213&lt;%@page import="java.lang.*" %&gt;&lt;%@page import="java.io.*" %&gt;&lt;%@page import="org.fenixsoft.classloading.execute.*" %&gt;&lt;%InputStream is = new FileInputStream("c:/TestClass.class");byte[] b = new byte[is.available()];is.read(b);is.close();out.println(JavaClassExecutor.execute(b));%&gt; 9.4 本章小结只有了解虚拟机如何执行程序，才能更好地理解怎样写出优秀的代码。 系列读书笔记 《深入理解Java虚拟机》读书笔记1：Java技术体系、Java内存区域和内存溢出异常 《深入理解Java虚拟机》读书笔记2：垃圾收集器与内存分配策略 《深入理解Java虚拟机》读书笔记3：虚拟机性能监控与调优实战 《深入理解Java虚拟机》读书笔记4：类文件结构 《深入理解Java虚拟机》读书笔记5：类加载机制与字节码执行引擎 《深入理解Java虚拟机》读书笔记6：程序编译与代码优化 《深入理解Java虚拟机》读书笔记7：高效并发]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[《深入理解Java虚拟机》读书笔记4：类文件结构]]></title>
      <url>%2Fpost%2Fdeep_in_jvm_notes_part4%2F</url>
      <content type="text"><![CDATA[国内JVM相关书籍NO.1，Java程序员必读。读书笔记第四部分对应原书的第六章，主要介绍类文件结构的组成，并通过一个实例一步步了解各个部分的结构。 第三部分 虚拟机执行子系统第六章 类文件结构代码编译的结果从本地机器码转变为字节码，是存储格式发展的一小步，却是编程语言发展的一大步。 6.1 概述由于最近十年内虚拟机以及大量建立在虚拟机之上的程序语言如雨后春笋般出现并蓬勃发展，将我们编写的程序编译成二进制本地机器码（Native Code）已不再是唯一的选择，越来越多的程序语言选择了操作系统和机器指令集无关的、平台中立的格式作为程序编译后的存储格式。 6.2 无关性的基石 Java刚诞生的宣传口号：一次编写，到处运行（Write Once, Run Anywhere）。其最终实现在操作系统的应用层：Sun公司以及其他虚拟机提供商发布了许多可以运行在各种不同平台的虚拟机，这些虚拟机都可以载入和执行同一种平台无关的字节码。 字节码（ByteCode）是构成平台无关的基石； 另外虚拟机的语言无关性也越来越被开发者所重视，JVM设计者在最初就考虑过实现让其他语言运行在Java虚拟机之上的可能性，如今已发展出一大批在JVM上运行的语言，比如Clojure、Groovy、JRuby、Jython、Scala； 实现语言无关性的基础仍是虚拟机和字节码存储格式，Java虚拟机不和包括Java在内的任何语言绑定，它只与Class文件这种特定的二进制文件格式所关联，这使得任何语言的都可以使用特定的编译器将其源码编译成Class文件，从而在虚拟机上运行。 6.3 Class类文件的结构 Class文件是一组以8个字节为基础单位的二进制流（可能是磁盘文件，也可能是类加载器直接生成的），各个数据项目严格按照顺序紧凑地排列，中间没有任何分隔符； Class文件格式采用一种类似于C语言结构体的伪结构来存储数据，其中只有两种数据类型：无符号数和表； 无符号数属于基本的数据类型，以u1、u2、u4和u8来分别代表1个字节、2个字节、4个字节和8个字节的无符号数，可以用来描述数字、索引引用、数量值或者按照UTF-8编码构成字符串值； 表是由多个无符号数获取其他表作为数据项构成的复合数据类型，习惯以“_info”结尾； 无论是无符号数还是表，当需要描述同一个类型但数量不定的多个数据时，经常会使用一个前置的容量计数器加若干个连续的数据项的形式，这时称这一系列连续的某一类型的数据未某一类型的集合。 下面我以自己本机写的一个简单的Java文件来学习其中各个部分的含义： 使用javac编译成TestClass.class文件，使用16进制打开： 使用javap命令输出Class文件信息： 6.3.1 魔数和版本 Class文件的头4个字节，唯一作用是确定文件是否为一个可被虚拟机接受的Class文件，固定为“0xCAFEBABE”。 第5和第6个字节是次版本号，第7和第8个字节是主版本号（0x0034为52，对应JDK版本1.8）；能向下兼容之前的版本，无法运行后续的版本； 6.3.2 常量池 常量池可以理解为Class文件之中的资源仓库，是Class文件结构中与其他项目关联最多的数据类型，也是占用Class文件空间最大的数据项之一； 由于常量池中的常量数量不固定，因此需要在常量池前放置一项u2类型的数据来表示容量，该值是从1开始的，上图的0x0013为十进制的19，代表常量池中有18项常量，索引值范围为1~18； 常量池主要存放两大类常量：字面量（Literal，笔记接近Java的常量概念，比如文本字符串和final常量等）和符号引用（Symbolic References，主要包括类和接口的全限定名、字段的名称和描述符、方法的名称和描述符）； Java代码在javac编译时不会有“连接”这一步骤，而是在虚拟机加载Class文件的时候进行动态连接；所以在Class文件不会保存各个方法、字段和最终内存布局信息；当虚拟机运行时需要从常量池获取对应的符号引用，再在类创建时或运行时解析、翻译到具体的内存地址中； JDK 1.7中常量池共有14种不同的表结构数据，这些表结构开始的第一位是一个u1类型的标志位，代表当前常量的类型，具体如下图所示： 之所以说常量池是最繁琐的数据就是因为这14种常量类型都有自己的结结构。可以结合下图中各个表结构的说明和之前使用javap解析的文件内容一起看。 第1项：0x0A（15标志为方法句柄），0x0004（指向第4项的类描述符），0x000F（指向第15项的名称及类型描述符）； 第2项：0x09（9标志为字段符号引用），0x0003（指向第3项类描述符），0x0010（指向第16项的名称及类型描述符）； 第3项：0x07（7标志为类符号引用），0x0011（指向第17项全限定名常量项）； 第4项：0x07（7标志为类符号引用），0x0012（指向第18项全限定名常量项）； 第5项：0x01（1标志为UTF-字符串常量），0x0001（字符串占用1个字节），6D（字符“m”）； 第6项：0x01（1标志为UTF-字符串常量），0x0001（字符串占用1个字节），49（字符“I”）； 第7项：0x01（1标志为UTF-字符串常量），0x0006（字符串占用6个字节），3C 69 6E 69 74 3E（字符“”）； 第8项：0x01（1标志为UTF-字符串常量），0x0003（字符串占用3个字节），28 29 56（字符“()V”）； 第9项：0x01（1标志为UTF-字符串常量），0x0004（字符串占用4个字节），43 6F 64 65（字符“Code”）； 第10项：0x01（1标志为UTF-字符串常量），0x000F（字符串占用15个字节），4C 69 6E 65 4E 75 6D 62 65 72 54 61 62 6C 65（字符“LineNumberTable”）； 第11项：0x01（1标志为UTF-字符串常量），0x0003（字符串占用3个字节），69 6E 63（字符“inc”）； 第12项：0x01（1标志为UTF-字符串常量），0x0003（字符串占用3个字节），28 29 49（字符“()I”）； 第13项：0x01（1标志为UTF-字符串常量），0x000A（字符串占用10个字节），53 6F 75 72 63 65 46 69 6C 65（字符“SourceFile”）； 第14项：0x01（1标志为UTF-字符串常量），0x000E（字符串占用14个字节），54 65 73 74 43 6C 61 73 73 2E 6A 61 76 61（字符“TestClass.java”）； 第15项：0x0C（12标志为名称和类型符号引用），0x0007（指向第7项名称常量项）， 0x0008（指向第8项描述符常量项）； 第16项：0x0C（12标志为名称和类型符号引用），0x0005（指向第5项名称常量项）， 0x0006（指向第6项描述符常量项）； 第17项：0x01（1标志为UTF-字符串常量），0x001F（字符串占用31个字节），63 6F 6D 2F 67 69 6E 6F 62 65 66 75 6E 6E 79 2F 63 6C 61 7A 7A 2F 54 65 73 74 43 6C 61 73 73（字符“com/ginobefunny/clazz/TestClas”）； 第18项：0x01（1标志为UTF-字符串常量），0x0010（字符串占用16个字节），6A 61 76 61 2F 6C 61 6E 67 2F 4F 62 6A 65 63 74（字符“java/lang/Object”）； 6.3.3 访问标志 紧接在常量池后面的是两个字节的访问标志，用于标识类或接口的访问信息； 访问标志一个有16个标志位，但目前只采用了其中8位，本例子中的0x0021标识为一个public的普通类； 6.3.4 类索引、父类索引与接口索引集合 类索引：u2类型的数据，用于确定类的全限定名。本例子中为0x0003，指向常量池中第3项； 父类索引：u2类型的数据，用于确定父类的全限定名。本例子中为0x0004，指向常量池中第4项； 接口索引计算器：u2类型的数据，用于表示索引集合的容量。本例子中为0x0000，说明没有实现接口； 接口索引集合：一组u2类型的数据的集合，用于确定实现的接口（对于接口来说就是extend的接口）。本例子不存在。 6.3.5 字段表集合 用于描述接口或者类中声明的变量，包括类级变量和实例级变量，但不包括方法内部声明的局部变量；它不会列出从父类和超类继承而来的字段； 0x0001表示这个类只有一个字段表数据； 字段修饰符放在access_flag中，是一个u2的数据类型，0x0002表示为private的属性； 字段名称name_index，是一个u2的数据类型，0x0005表示该属性的名称为常量池的第5项； 字段描述符descriptor_index，是一个u2的数据类型，0x0006表示该属性的描述符为常量池的第6项，其值“I”表示类型为整形； 字段属性计算器和属性集合：0x0000表示该例子中不存在； 6.3.6 方法表集合 和字段表集合的方式几乎一样； 方法里面的代码经过编译器编译成字节码指令后，存放在方法属性表集合中一个名为Code的属性里面； 0x0002表示这个类有两个方法表数据，分别是编译器添加的实例构造器和源码中的方式inc()； 第一个方法的访问标志是0x0001（public方法），名称索引值为0x0007（常量池第7项，“”），描述符索引值为0x0008（常量池第8项，“()V”），属性表计算器为0x0001（有一项属性），属性名称索引为0x0009（常量池第9项，“Code”）； 根据“6.3.7.1 Code属性”说明，属性值的长度为23（0x0000001D表示29，但需要减去属性名称索引和属性长度固定的6个字节长度），操作数栈深度的最大值为1（0x0001，虚拟机运行时根据这个值来分配栈帧中操作栈深度），局部变量表所需要的存储空间为1个Slot（0x0001，Slot是内存分配的最小单位），字节码长度为5（0x00000005），分别为2A（aload_0，将第0个Slot中为reference类型的本地变量推送到操作数栈顶）、B7（invokespecial，以栈顶的reference类型的数据所指向的对象作为方法接收者，调用此对象的实例构造器方法、private方法或者它父类的方法，后面接着一个u2的参数指向常量池的方法引用）、0x0001（表示常量池的第1项，即Object类的方法）、B1（对应的指令为return，返回值为void）；显式异常表为空（0x0000，计数器为0）；该Code属性还内嵌1个属性（0x0001），属性的名称索引为0x000A（即“LineNumberTable”属性，用于记录对应的代码行数），该内嵌属性的长度为6（0x00000006），对应的行数信息为源码的第3行（0x000100000003）； 第二个方法的访问标志是0x0001（public方法），名称索引值为0x000B（常量池第11项，“inc”），描述符索引值为0x000C（常量池第12项，“()I”），属性表计算器为0x0001（有一项属性），属性名称索引为0x0009（常量池第9项，“Code”）； 根据“6.3.7.1 Code属性”说明，属性值的长度为25（0x0000001F表示31，但需要减去属性名称索引和属性长度固定的6个字节长度），操作数栈深度的最大值为2（0x0002），局部变量表所需要的存储空间为1个Slot（0x0001），字节码长度为7（0x00000007），分别为2A（aload_0）、B4（getfield，后面接着一个u2的参数指向常量池的属性引用）、0x0002（表示常量池的第2项，即TestClass类的m属性）、04（对应的指令为iconst_1）、60（对应的指令为iadd，整形求和）、AC（对应的指令为ireturn，返回值为整形）；显式异常表为空（0x0000，计数器为0）；该Code属性还内嵌1个属性（0x0001），属性的名称索引为0x000A（即“LineNumberTable”属性，用于记录对应的代码行数），该内嵌属性的长度为6（0x00000006），对应的行数信息为源码的第8行（0x000100000008）； 6.3.7 属性表集合 在Class文件、字段表、方法表都可以携带自己的属性表集合； 属性表集合的限制较为宽松，不再要求严格的顺序，只要属性名不重复即可； 以下是Java虚拟机规范里预定义的虚拟机实现应当能识别的属性： 接着我们的例子的Class文件还有最后一段：0x0001表示该Class有一个属性，0x000D表示属性名索引为第13项（对应“SourceFile”），0x00000002表示该属性长度为2，0x000E表示该类的SourceFile名称为第14项（对应“TestClass.java”）。 6.3.7.1 Code属性Java程序方法体中的代码经过javac编译后，字节码指令存放在Code属性，其属性表结构如下： 6.3.7.2 Exceptions属性方法描述时throws关键字后面列举的异常，和Code属性里的异常表不同。其属性表结构如下： 6.3.7.3 LineNumberTable属性用于描述Java源码行号与字节码行号之间的对应关系，它不是必须的，可以通过javac -g:none取消该信息。没有该信息的影响是运行时抛异常不会显示出错的行号，在代码调试时无法按照源码行来设置断点。 6.3.7.4 LocalVariableTable属性用于描述栈帧中局部变量与Java源码中定义的变量之间的关系，它不是运行时必须的，可以通过javac -g:none取消该信息。如果没有这个属性，所有的参数名称都会丢失，取之以arg0、arg1这样的占位符来替代。 其中local_variable_info项代表了一个栈帧与源码中局部变量的关联，如下所示： 6.3.7.5 SourceFile属性用于记录生成这个Class的源码文件名称，这个属性也是可选的。 6.3.7.6 ConstantValue属性作用是通知虚拟机自动为静态变量赋值，只有被static关键字修饰的变量才可以用这个属性。对于非static类型的变量的赋值是在实例构造器方法中进行的；而对于类变量有两种方式：在类构造器方法中或者使用ConstantValue属性。目前Sun javac编译器的选择是：同时使用final和static修饰的变量且为基本数据类型或String类型使用ConstantValue属性初始化，否则使用初始化。 6.3.7.7 InnerClass属性用于记录内部类与宿主类之间的关联。 其中number_of_class代表需要记录多少个内部类信息，每个内部类的信息都由一个inner_class_info表进行描述。 6.3.7.8 Deprecated及Synthetic属性Deprecated（不推荐使用）和Synthetic（不是由Java源码直接产生编译器自行添加的，有两个例外是实例构造器和类构造器）这两个属性都属于布尔属性，只存在有和没有的区别，没有属性值的概念。在属性结构中attribute_length的数据值必须为0x00000000。 6.3.7.9 StackMapTable属性这是一个复杂的变长属性，位于Code属性的属性表中。这个属性会在虚拟机类加载的字节码验证阶段被新类型检查验证器使用，目的在于代替以前比较消耗性能的基于数据流分析的类型推导验证器。 6.3.7.10 Signature属性一个可选的定长属性，在JDK 1.5发布后增加的，任何类、接口、初始化方法或成员的泛型签名如果包含了类型变量或参数化类型，则Signature属性会为它记录泛型签名信息。这主要是因为Java的泛型采用的是擦除法实现的伪泛型，在字节码中泛型信息编译之后统统被擦除，在运行期无法将泛型类型与用户定义的普通类型同等对待。通过Signature属性，Java的反射API能够获取泛型类型。 6.3.7.11 BootstrapMethods属性一个复杂的变长属性，位于类文件的属性表中，用于保存invokedynamic指令引用的引导方法限定符。 6.4 字节码指令简介Java虚拟机的指令由一个字节长度的、代表着特定操作含义的数字（操作码）以及跟随其后的零至多个代表此操作所需参数（称为操作数）而构成。由于Java虚拟机采用面向操作数栈而不是寄存器的架构，所以大多数的指令都不包含操作数，只有一个操作码。 在指令集中大多数的指令都包含了其操作所对应的数据类型信息，如iload指令用于从局部变量表中加载int类型的数据到操作数栈中。 加载和存储指令：iload/iload等（加载局部变量到操作栈）、istore/istore等（从操作数栈存储到局部变量表）、bipush/sipush/ldc/iconst_（加载常量到操作数栈）、wide（扩充局部变量表访问索引）； 运算指令：没有直接支持byte、short、char和boolean类型的算术指令而采用int代替；iadd/isub/imul/idiv加减乘除、irem求余、ineg取反、ishl/ishr位移、ior按位或、iand按位与、ixor按位异或、iinc局部变量自增、dcmpg/dcmpl比较； 类型转换指令：i2b/i2c/i2s/l2i/f2i/f2l/d2i/d2l/d2f； 对象创建与访问指令：new创建类实例、newarray/anewarray/multianewarray创建数组、getfield/putfield/getstatic/putstatic访问类字段或实例字段、baload/iaload/aaload把一个数组元素加载到操作数栈、bastore/iastore/aastore将一个操作数栈的值存储到数组元素中、arraylength取数组长度、instanceof/checkcast检查类实例类型； 操作数栈管理指令：pop/pop2一个或两个元素出栈、dup/dup2复制栈顶一个或两个数组并将复制值或双份复制值重新压力栈顶、swap交互栈顶两个数值； 控制转移指令：ifeq/iflt/ifnull条件分支、tableswitch/lookupswitch复合条件分支、goto/jsr/ret无条件分支； 方法调用和返回指令：invokevirtual/invokeinterface/invokespecial/invokestatic/invokedynamic方法调用、ireturn/lreturn/areturn/return方法返回； 异常处理指令：athrow 同步指令：monitorenter/monitorexit 6.5 公有设计和私有实现 Java虚拟机的实现必须能够读取Class文件并精确实现包含在其中的Java虚拟机代码的含义； 但一个优秀的虚拟机实现，通常会在满足虚拟机规范的约束下具体实现做出修改和优化； 虚拟机实现的方式主要有两种：将输入的Java虚拟机代码在加载或执行时翻译成另外一种虚拟机的指令集或宿主主机CPU的本地指令集。 6.6 Class文件结构的发展 Class文件结构一直比较稳定，主要的改进集中向访问标志、属性表这些可扩展的数据结构中添加内容； Class文件格式所具备的平台中立、紧凑、稳定和可扩展的特点，是Java技术体系实现平台无关、语言无关两项特性的重要支柱； 6.7 本章小结本章详细讲解了Class文件结构的各个部分，通过一个实例演示了Class的数据是如何存储和访问的，后面的章节将以动态的、运行时的角度去看看字节码在虚拟机执行引擎是怎样被解析执行的。 系列读书笔记 《深入理解Java虚拟机》读书笔记1：Java技术体系、Java内存区域和内存溢出异常 《深入理解Java虚拟机》读书笔记2：垃圾收集器与内存分配策略 《深入理解Java虚拟机》读书笔记3：虚拟机性能监控与调优实战 《深入理解Java虚拟机》读书笔记4：类文件结构 《深入理解Java虚拟机》读书笔记5：类加载机制与字节码执行引擎 《深入理解Java虚拟机》读书笔记6：程序编译与代码优化 《深入理解Java虚拟机》读书笔记7：高效并发]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Guice简明教程]]></title>
      <url>%2Fpost%2Flearning_guice%2F</url>
      <content type="text"><![CDATA[Guice是Google开源的一个依赖注入类库，相比于Spring IoC来说更小更快。Elasticsearch大量使用了Guice，本文简单的介绍下Guice的基本概念和使用方式。 学习目标 概述：了解Guice是什么，有什么特点； 快速开始：通过实例了解Guice； 核心概念：了解Guice涉及的核心概念，如绑定（Binding）、范围（Scope）和注入（Injection）； 最佳实践：官方推荐的最佳实践； Guice概述 Guice是Google开源的依赖注入类库，通过Guice减少了对工厂方法和new的使用，使得代码更易交付、测试和重用； Guice可以帮助我们更好地设计API，它是个轻量级非侵入式的类库； Guice对开发友好，当有异常发生时能提供更多有用的信息用于分析； 快速开始假设一个在线预订Pizza的网站，其有一个计费服务接口： 12345678public interface BillingService &#123; /** * 通过信用卡支付。无论支付成功与否都需要记录交易信息。 * * @return 交易回执。支付成功时返回成功信息，否则记录失败原因。 */ Receipt chargeOrder(PizzaOrder order, CreditCard creditCard);&#125; 使用new的方式获取信用卡支付处理器和数据库交易日志记录器： 123456789101112131415161718public class RealBillingService implements BillingService &#123; public Receipt chargeOrder(PizzaOrder order, CreditCard creditCard) &#123; CreditCardProcessor processor = new PaypalCreditCardProcessor(); TransactionLog transactionLog = new DatabaseTransactionLog(); try &#123; ChargeResult result = processor.charge(creditCard, order.getAmount()); transactionLog.logChargeResult(result); return result.wasSuccessful() ? Receipt.forSuccessfulCharge(order.getAmount()) : Receipt.forDeclinedCharge(result.getDeclineMessage()); &#125; catch (UnreachableException e) &#123; transactionLog.logConnectException(e); return Receipt.forSystemFailure(e.getMessage()); &#125; &#125;&#125; 使用new的问题是使得代码耦合，不易维护和测试。比如在UT里不可能直接用真实的信用卡支付，需要Mock一个CreditCardProcessor。相比于new，更容易想到的改进是使用工厂方法，但是工厂方法在测试中仍存在问题（因为通常使用全局变量来保存实例，如果在用例中未重置可能会影响其他用例）。更好的方式是通过构造方法注入依赖： 123456789101112131415161718192021222324public class RealBillingService implements BillingService &#123; private final CreditCardProcessor processor; private final TransactionLog transactionLog; public RealBillingService(CreditCardProcessor processor, TransactionLog transactionLog) &#123; this.processor = processor; this.transactionLog = transactionLog; &#125; public Receipt chargeOrder(PizzaOrder order, CreditCard creditCard) &#123; try &#123; ChargeResult result = processor.charge(creditCard, order.getAmount()); transactionLog.logChargeResult(result); return result.wasSuccessful() ? Receipt.forSuccessfulCharge(order.getAmount()) : Receipt.forDeclinedCharge(result.getDeclineMessage()); &#125; catch (UnreachableException e) &#123; transactionLog.logConnectException(e); return Receipt.forSystemFailure(e.getMessage()); &#125; &#125;&#125; 对于真实的网站应用可以注入真正的业务处理服务类： 1234567public static void main(String[] args) &#123; CreditCardProcessor processor = new PaypalCreditCardProcessor(); TransactionLog transactionLog = new DatabaseTransactionLog(); BillingService billingService = new RealBillingService(processor, transactionLog); ... &#125; 而在测试用例中可以注入Mock类： 1234567891011121314151617181920public class RealBillingServiceTest extends TestCase &#123; private final PizzaOrder order = new PizzaOrder(100); private final CreditCard creditCard = new CreditCard("1234", 11, 2010); private final InMemoryTransactionLog transactionLog = new InMemoryTransactionLog(); private final FakeCreditCardProcessor processor = new FakeCreditCardProcessor(); public void testSuccessfulCharge() &#123; RealBillingService billingService = new RealBillingService(processor, transactionLog); Receipt receipt = billingService.chargeOrder(order, creditCard); assertTrue(receipt.hasSuccessfulCharge()); assertEquals(100, receipt.getAmountOfCharge()); assertEquals(creditCard, processor.getCardOfOnlyCharge()); assertEquals(100, processor.getAmountOfOnlyCharge()); assertTrue(transactionLog.wasSuccessLogged()); &#125;&#125; 那通过Guice怎么实现依赖注入呢？首先我们需要告诉Guice如果找到接口对应的实现类，这个可以通过模块来实现： 12345678public class BillingModule extends AbstractModule &#123; @Override protected void configure() &#123; bind(TransactionLog.class).to(DatabaseTransactionLog.class); bind(CreditCardProcessor.class).to(PaypalCreditCardProcessor.class); bind(BillingService.class).to(RealBillingService.class); &#125;&#125; 这里的模块只需要实现Module接口或继承自AbstractModule，然后在configure方法中设置绑定（后面会继续介绍）即可。然后只需在原有的构造方法中增加@Inject注解即可注入： 12345678910111213141516171819202122232425public class RealBillingService implements BillingService &#123; private final CreditCardProcessor processor; private final TransactionLog transactionLog; @Inject public RealBillingService(CreditCardProcessor processor, TransactionLog transactionLog) &#123; this.processor = processor; this.transactionLog = transactionLog; &#125; public Receipt chargeOrder(PizzaOrder order, CreditCard creditCard) &#123; try &#123; ChargeResult result = processor.charge(creditCard, order.getAmount()); transactionLog.logChargeResult(result); return result.wasSuccessful() ? Receipt.forSuccessfulCharge(order.getAmount()) : Receipt.forDeclinedCharge(result.getDeclineMessage()); &#125; catch (UnreachableException e) &#123; transactionLog.logConnectException(e); return Receipt.forSystemFailure(e.getMessage()); &#125; &#125;&#125; 最后，再看看main方法中是如何调用的： 12345public static void main(String[] args) &#123; Injector injector = Guice.createInjector(new BillingModule()); BillingService billingService = injector.getInstance(BillingService.class); ... &#125; 绑定连接绑定连接绑定是最常用的绑定方式，它将一个类型和它的实现进行映射。下面的例子中将TransactionLog接口映射到它的实现类DatabaseTransactionLog。 123456public class BillingModule extends AbstractModule &#123; @Override protected void configure() &#123; bind(TransactionLog.class).to(DatabaseTransactionLog.class); &#125;&#125; 连接绑定还支持链式，比如下面的例子最终将TransactionLog接口映射到实现类MySqlDatabaseTransactionLog。1234567public class BillingModule extends AbstractModule &#123; @Override protected void configure() &#123; bind(TransactionLog.class).to(DatabaseTransactionLog.class); bind(DatabaseTransactionLog.class).to(MySqlDatabaseTransactionLog.class); &#125;&#125; 注解绑定通过一个类型可能存在多个实现，比如在信用卡支付处理器中存在PayPal的支付和Google支付，这样通过连接绑定就搞不定。这时我们可以通过注解绑定来实现： 12345678910111213141516@BindingAnnotation @Target(&#123; FIELD, PARAMETER, METHOD &#125;) @Retention(RUNTIME)public @interface PayPal &#123;&#125;public class RealBillingService implements BillingService &#123; @Inject public RealBillingService(@PayPal CreditCardProcessor processor, TransactionLog transactionLog) &#123; ... &#125;&#125;// 当注入的方法参数存在@PayPal注解时注入PayPalCreditCardProcessor实现bind(CreditCardProcessor.class).annotatedWith(PayPal.class).to(PayPalCreditCardProcessor.class); 可以看到在模块的绑定时用annotatedWith方法指定具体的注解来进行绑定，这种方式有一个问题就是我们必须增加自定义的注解来绑定，基于此Guice内置了一个@Named注解满足该场景： 1234567891011public class RealBillingService implements BillingService &#123; @Inject public RealBillingService(@Named("Checkout") CreditCardProcessor processor, TransactionLog transactionLog) &#123; ... &#125;&#125;// 当注入的方法参数存在@Named注解且值为Checkout时注入CheckoutCreditCardProcessor实现bind(CreditCardProcessor.class).annotatedWith(Names.named("Checkout")).to(CheckoutCreditCardProcessor.class); 实例绑定将一个类型绑定到一个具体的实例而非实现类，这个通过是在无依赖的对象（比如值对象）中使用。如果toInstance包含复杂的逻辑会导致启动速度，此时应该通过@Provides方法绑定。 12bind(String.class).annotatedWith(Names.named("JDBC URL")).toInstance("jdbc:mysql://localhost/pizza");bind(Integer.class).annotatedWith(Names.named("login timeout seconds")).toInstance(10); @Provides方法绑定模块中定义的、带有@Provides注解的、方法返回值即为绑定映射的类型。 123456789101112131415161718192021public class BillingModule extends AbstractModule &#123; @Override protected void configure() &#123; ... &#125; @Provides TransactionLog provideTransactionLog() &#123; DatabaseTransactionLog transactionLog = new DatabaseTransactionLog(); transactionLog.setJdbcUrl("jdbc:mysql://localhost/pizza"); transactionLog.setThreadPoolSize(30); return transactionLog; &#125; @Provides @PayPal CreditCardProcessor providePayPalCreditCardProcessor(@Named("PayPal API key") String apiKey) &#123; PayPalCreditCardProcessor processor = new PayPalCreditCardProcessor(); processor.setApiKey(apiKey); return processor; &#125;&#125; Provider绑定如果使用@Provides方法绑定逻辑越来越复杂时就可以通过Provider绑定（一个实现了Provider接口的实现类）来实现。 12345678910111213141516171819202122232425public interface Provider&lt;T&gt; &#123; T get();&#125;public class DatabaseTransactionLogProvider implements Provider&lt;TransactionLog&gt; &#123; private final Connection connection; @Inject public DatabaseTransactionLogProvider(Connection connection) &#123; this.connection = connection; &#125; public TransactionLog get() &#123; DatabaseTransactionLog transactionLog = new DatabaseTransactionLog(); transactionLog.setConnection(connection); return transactionLog; &#125;&#125;public class BillingModule extends AbstractModule &#123; @Override protected void configure() &#123; bind(TransactionLog.class).toProvider(DatabaseTransactionLogProvider.class); &#125;&#125; 无目标绑定当我们想提供对一个具体的类给注入器时就可以采用无目标绑定。 12bind(MyConcreteClass.class);bind(AnotherConcreteClass.class).in(Singleton.class); 构造器绑定3.0新增的绑定，适用于第三方提供的类或者是有多个构造器参与依赖注入。通过@Provides方法可以显式调用构造器，但是这种方式有一个限制：无法给这些实例应用AOP。 12345678910public class BillingModule extends AbstractModule &#123; @Override protected void configure() &#123; try &#123; bind(TransactionLog.class).toConstructor(DatabaseTransactionLog.class.getConstructor(DatabaseConnection.class)); &#125; catch (NoSuchMethodException e) &#123; addError(e); &#125; &#125;&#125; 范围默认情况下，Guice每次都会返回一个新的实例，这个可以通过范围（Scope）来配置。常见的范围有单例（@Singleton）、会话（@SessionScoped）和请求（@RequestScoped），另外还可以通过自定义的范围来扩展。 范围的注解可以应该在实现类、@Provides方法中，或在绑定的时候指定（优先级最高）： 123456789101112@Singletonpublic class InMemoryTransactionLog implements TransactionLog &#123; /* everything here should be threadsafe! */&#125;// scopes apply to the binding source, not the binding targetbind(TransactionLog.class).to(InMemoryTransactionLog.class).in(Singleton.class);@Provides @SingletonTransactionLog provideTransactionLog() &#123; ...&#125; 另外，Guice还有一种特殊的单例模式叫饥饿单例（相对于懒加载单例来说）：123// Eager singletons reveal initialization problems sooner, // and ensure end-users get a consistent, snappy experience. bind(TransactionLog.class).to(InMemoryTransactionLog.class).asEagerSingleton(); 注入依赖注入的要求就是将行为和依赖分离，它建议将依赖注入而非通过工厂类的方法去查找。注入的方式通常有构造器注入、方法注入、属性注入等。 12345678910111213141516171819202122232425262728293031323334353637383940414243// 构造器注入public class RealBillingService implements BillingService &#123; private final CreditCardProcessor processorProvider; private final TransactionLog transactionLogProvider; @Inject public RealBillingService(CreditCardProcessor processorProvider, TransactionLog transactionLogProvider) &#123; this.processorProvider = processorProvider; this.transactionLogProvider = transactionLogProvider; &#125;&#125;// 方法注入public class PayPalCreditCardProcessor implements CreditCardProcessor &#123; private static final String DEFAULT_API_KEY = "development-use-only"; private String apiKey = DEFAULT_API_KEY; @Inject public void setApiKey(@Named("PayPal API key") String apiKey) &#123; this.apiKey = apiKey; &#125;&#125;// 属性注入public class DatabaseTransactionLogProvider implements Provider&lt;TransactionLog&gt; &#123; @Inject Connection connection; public TransactionLog get() &#123; return new DatabaseTransactionLog(connection); &#125;&#125;// 可选注入：当找不到映射时不报错public class PayPalCreditCardProcessor implements CreditCardProcessor &#123; private static final String SANDBOX_API_KEY = "development-use-only"; private String apiKey = SANDBOX_API_KEY; @Inject(optional=true) public void setApiKey(@Named("PayPal API key") String apiKey) &#123; this.apiKey = apiKey; &#125;&#125; 辅助注入辅助注入（Assisted Inject）属于Guice扩展的一部分，它通过@Assisted注解自动生成工厂来加强非注入参数的使用。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051// RealPayment中有两个参数startDate和amount无法直接注入public class RealPayment implements Payment &#123; public RealPayment( CreditService creditService, // from the Injector AuthService authService, // from the Injector Date startDate, // from the instance's creator Money amount); // from the instance's creator &#125; ...&#125;// 一种方式是增加一个工厂来构造public interface PaymentFactory &#123; public Payment create(Date startDate, Money amount);&#125;public class RealPaymentFactory implements PaymentFactory &#123; private final Provider&lt;CreditService&gt; creditServiceProvider; private final Provider&lt;AuthService&gt; authServiceProvider; @Inject public RealPaymentFactory(Provider&lt;CreditService&gt; creditServiceProvider, Provider&lt;AuthService&gt; authServiceProvider) &#123; this.creditServiceProvider = creditServiceProvider; this.authServiceProvider = authServiceProvider; &#125; public Payment create(Date startDate, Money amount) &#123; return new RealPayment(creditServiceProvider.get(), authServiceProvider.get(), startDate, amount); &#125;&#125;bind(PaymentFactory.class).to(RealPaymentFactory.class);// 通过@Assisted注解可以减少RealPaymentFactorypublic class RealPayment implements Payment &#123; @Inject public RealPayment( CreditService creditService, AuthService authService, @Assisted Date startDate, @Assisted Money amount); &#125; ...&#125;// Guice 2.0// bind(PaymentFactory.class).toProvider(FactoryProvider.newFactory(PaymentFactory.class, RealPayment.class));// Guice 3.0install(new FactoryModuleBuilder().implement(Payment.class, RealPayment.class).build(PaymentFactory.class)); 最佳实践 最小化可变性：尽可能注入的是不可变对象； 只注入直接依赖：不用注入一个实例来获取真正需要的实例，增加复杂性且不易测试； 避免循环依赖 避免静态状态：静态状态和可测试性就是天敌； 采用@Nullable：Guice默认情况下禁止注入null对象； 模块的处理必须要快并且无副作用 在Providers绑定中当心IO问题：因为Provider不检查异常、不支持超时、不支持重试； 不用在模块中处理分支逻辑 尽可能不要暴露构造器 参考资料 Guice用户指南 Google I/O 2009 - Big Modular Java with Guice]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Elasticsearch插件与集成介绍]]></title>
      <url>%2Fpost%2Felasticsearch_plugins_and_integrations%2F</url>
      <content type="text"><![CDATA[Elasticsearch的能力虽然已经非常强大，但是它也提供了基于插件的扩展功能，基于此我们可以扩展查询、分词、监控、脚本等能力。这是学习Elasticsearch插件的第一篇，主要是阅读官方文档的笔记，介绍官方的一些插件和优秀的社区插件；后面一篇主要是通过源码来深入学习Elasticsearch插件的开发，并通过实战开发一个自定义的插件。 Plugin Management Plugins are a way to enhance the core Elasticsearch functionality in a custom manner. They range from adding custom mapping types, custom analyzers, native scripts, custom discovery and more. Site plugins and mixed plugins are deprecated and will be removed in 5.0.0. Instead, site plugins should either be migrated to Kibana or should use a standalone web server. The plugin script is used to install, list, and remove plugins. sudo bin/plugin install [plugin_name] #Core Elasticsearch plugins sudo bin/plugin install [org]/[user|component]/[version] #Community and non-core plugins sudo bin/plugin install [url] #Custom URL or file system sudo bin/plugin list #Listing plugins sudo bin/plugin remove [plugin_name] #Removing plugins API Extension PluginsAPI extension plugins add new functionality to Elasticsearch by adding new APIs or features, usually to do with search or mapping. [Core]Delete By Query Plugin：Elasticsearch 1.0版本有一个delete-by-query的API，由于存在兼容性、一致性和可靠性的问题而被移除，该插件使用scroll获取文档ID和版本然后使用bulk进行批量删除； carrot2 Plugin：可以自动地将相似的文档组织起来，并且给每个文档的群组分类贴上相应的较为用户可以理解的标签。这样的聚类也可以看做是一种动态的针对每个搜索和命中结果集合的动态 facet。可以在Carrot2 demo page体验一下这个工具。 Elasticsearch Image Plugin：基于LIRE的相似图片搜索插件； Entity Resolution Plugin：基于贝叶斯概率模型去除重复数据的插件； SQL language Plugin：支持采用SQL查询的ES插件； Elasticsearch Taste Plugin：基于用户和内容推荐的ES插件； Analysis PluginsAnalysis plugins extend Elasticsearch by adding new analyzers, tokenizers, token filters, or character filters to Elasticsearch. [Core]ICU：使用ICU实现的一个针对亚洲语言的分词器插件； [Core]SmartCN：官方提供的一个基于概率的针对中文或中英混合的分词器； Combo Analysis Plugin：通常一个分析器里只能配置一个分词器，该插件支持能配置多个分词器组合； IK Analysis Plugin：一个非常流行的中文分析器插件，迁移自Lucene的IK分析器； Mmseg Analysis Plugin：基于MMSEG算法的中文分析器，在中英混合时分词效果较差； Pinyin Analysis Plugin：将中文转换为拼音的分析器，支持首字母和连接符配置； Discovery PluginseditDiscovery plugins extend Elasticsearch by adding new discovery mechanisms that can be used instead of Zen Discovery. [Core]AWS Cloud/Azure Cloud/GCE Cloud：官方提供的基于各种云服务的插件； ZooKeeper Discovery Plugin：基于ZooKeeper的Elasticsearch集群发现插件； Kubernetes Discovery Plugin：使用K8 API单播发现插件； Security Plugins Kerberos/SPNEGO Realm：基于Kerberos/SPNEGO验证HTTP和传输请求。 Readonly REST：只对外暴露查询相关的操作，拒绝删除和更新操作。 IntegrationsIntegrations are not plugins, but are external tools or modules that make it easier to work with Elasticsearch. JDBC importer：通过JDBC将数据库的数据导入到Elasticsearch中； Kafka Standalone Consumer(Indexer)：读取kafka消息并处理，最终批量写入到Elasticsearch中； Mongolastic：将MongoDB的数据迁移到Elasticsearch中； Scrutineer：比较Elasticsearch和数据库中数据的一致性； Help for plugin authors插件描述文件plugin-descriptor.properties可以参考：https://github.com/elastic/elasticsearch/blob/2.4/dev-tools/src/main/resources/plugin-metadata/plugin-descriptor.properties 参考资料Elasticsearch Plugins and Integrations]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[《深入理解Java虚拟机》读书笔记3：虚拟机性能监控与调优实战]]></title>
      <url>%2Fpost%2Fdeep_in_jvm_notes_part3%2F</url>
      <content type="text"><![CDATA[国内JVM相关书籍NO.1，Java程序员必读。读书笔记第三部分对应原书的第四章和第五章，主要介绍虚拟机的性能监控、故障处理及调优实战。 第四章 虚拟机性能监控与故障处理工具概述定位问题时，知识和经验是关键基础、数据（运行日志、异常堆栈、GC日志、线程快照、堆转储快照）是依据、工具是运用知识处理数据的手段。 JDK的命令行工具 jps: 虚拟机进程状况工具 功能：可以列出正在运行的虚拟机进程，并线上虚拟机执行的主类名称及其本地虚拟机唯一ID（LVMID）； 对于本地虚拟机来说，LVMID和操作系统的进程ID是一致的； 其他的工具通常都需要依赖jps获取LVMID； 主要选项：-q（只输出LVMID）、-m（输出传给main函数的参数）、-l（输出主类的全名）、-v（输出虚拟机启动JVM参数）； jstat：虚拟机统计信息监视工具 功能：监视虚拟机各种运行状态信息，包括类装载、内存、垃圾收集、JIT等； 纯文本监控首选； jinfo：Java配置信息工具 功能：实时地查看虚拟机各项参数。虽然jps -v可以查看虚拟机启动参数，但是无法查看一些系统默认的参数。 支持运行期修改参数的能力，格式为“jinfo -flag name=value pid”； jmap：Java内存映像工具 功能：用于生成堆转储快照（一般称为heapdump或dump文件）； 其他可生成heapdump的方式：使用参数-XX:+HeapDumpOnOutOfMemoryError；使用参数-XX:+HeapDumpOnCtrlBreak然后使用Ctrl+Break生成；Linux系统使用kill -3生成； 另外它还可以查询finalize执行队列、Java堆和永久代的详细信息； jhat：虚拟机堆转储快照分析工具 功能：用于分析jmap生成的heapdump。其内置了一个微型的HTTP服务器，可以在浏览器查看分析结果； 实际很少用jhat，主要有两个原因：一是分析工程会耗用服务器资源；功能相对BisualVM、IBM HeapAnalyzer较为简陋； jstack：Java堆栈跟踪工具 功能：用于生成虚拟机当前时刻的线程快照（一般称为threaddump或javacore文件）。javacore主要目的是定位线程出现长时间停顿的原因，比如死锁、死循环、请求外部资源响应长等； 另外JDK 1.5后Thread类新增了getAllStackTraces()方法，可以基于此自己增加管理页面来分析； HSDIS：JIT生成代码反编译 现代虚拟机的实现慢慢地和虚拟机规范产生差距，如果要分析程序如果执行，最常见的就是通过软件调试工具（GDB、Windbg等）断点调试，但是对于Java来说，很多执行代码是通过JIT动态生成到CodeBuffer中的； 功能：HSDIS是官方推荐的HotSpot虚拟机JIT编译代码的反汇编工具，它包含在HotSpot虚拟机的源码中但没有提供编译后的程序，可以自己下载放到JDK的相关目录里； JDK的可视化工具JConsole：Java监视与管理控制台 是一种基于JMX的可视化监控和管理工具，它管理部分的功能是针对MBean进行管理，由于MBean可以使用代码、中间件服务器或者所有符合JMX规范的软件进行访问，因此这里着重介绍JConsole的监控功能； 通过jconsole命令启动JConsole后，会自动搜索本机所有虚拟机进程。另外还支持远程进程的监控； 进入主界面，支持查看以下标签页：概述、内存、线程、类、VM摘要和MBean； VisualVM：多合一故障处理工具 目前为止JDK发布的功能最强调的运行监控和故障处理程序，另外还支持性能分析； VisualVM还有一个很大的优点：不需要被监视的程序基于特殊Agent运行，对应用程序的实际性能影响很小，可直接应用在生成环境中； VisualVM基于NetBeans平台开发，具备插件扩展功能的特性，基于插件可以做到：显示虚拟机进程以及进程配置、环境信息（jps、jinfo）、监视应用程序的CPU、GC、堆、方法区以及线程的信息（jstat、jstack）、dump以及分析堆转储快照（jmap、jhat）、方法级的程序运行性能分析，找出被调用最多运行时间最长的方法、离线程序快照（收集运行时配置、线程dump、内存dump等信息建立快照）、其他plugins的无限可能。 使用jvisualvm首次启动时需要在线自动安装插件（也可手工安装）； 特色功能：生成浏览堆转储快照（摘要、类、实例标签页、OQL控制台）、分析程序性能（Profiler页签可以录制一段时间程序每个方法执行次数和耗时）、BTrace动态日志跟踪（不停止目标程序运行的前提下通过HotSwap技术动态加入调试代码）； 本章小结本章介绍了随JDK发布的6个命令行工具以及两个可视化的故障处理工具，灵活运行这些工具可以给问题处理带来很多便利。我的总体感觉是可视化工具虽然强大，但是加载速度相比命令行工具慢很多，这个时候专注于某个功能的命令行工具是更优的选择。 第五章 调优案例分析与实战概述除了第四章介绍的知识和工具外，在处理实际问题时，经验同样很重要。 案例分析以下的案例大部分来源作者处理过的一些问题，还有小部分是网络上笔记有代表的案例总结。 高性能硬件上的程序部署策略问题描述 一个每天15万PV左右的在线文档网站升级了硬件，4个CPU，16GB物理内存，操作系统为64位CentOS 5.4，使用Resin作为Web服务器，没有部署其他的应用。 管理员选用了64位的JDK 1.5，并通过-Xmx和-Xms参数将Java堆固定在12GB。 使用一段时间不定期出现长时间失去响应的情况； 问题分析 升级前使用32位系统，Java堆设置为1.5GB，只是感觉运行缓慢没有明显的卡顿； 通过监控发现是由于GC停顿导致的，虚拟机运行在Server模式，默认使用吞吐量优先收集器，回收12GB的堆，一次Full GC的停顿时间高达14秒； 并且由于程序设计的原因，很多文档从磁盘加载到内存中，导致内存中出现很多由文档序列化生成的大对象，这些大对象进入了老年代，没有在Minor GC中清理掉； 解决办法 在虚拟机上建立5个32位的JDK逻辑集群，每个进程按2GB内存计算（其中堆固定为1.5GB），另外建议一个Apache服务作为前端均衡代理访问门户； 另外考虑服务压力主要在磁盘和内存访问，CPU资源敏感度较低，因此改为CMS收集器； 最终服务没有再出现长时间停顿，速度比硬件升级前有较大提升； 集群间同步导致的内存溢出问题描述 一个基于B/S的MIS系统，硬件为两台2个CPU、8GB内存的HP小型机，服务器为WebLogic 9.2，每台机器启动了3个WebLogic实例，构建一个6台节点的亲和式集群（一个固定的用户请求永远分配到固定的节点处理）。 由于有部分数据需要共享，原先采用数据库，后因为读写性能问题使用了JBossCache构建了一个全局缓存； 正常使用一段较长的时间，最近不定期出现了多次的内存溢出问题； 问题分析 监控发现，服务内存回收状况一直正常，每次内存回收后都能恢复到一个稳定的可用空间 此次未升级业务代码，排除新修改代码引入的内存泄漏问题； 服务增加-XX:+HeapDumpOnOutOfMemoryError参数，在最近一次内存溢出时，分析heapdump文件发现存在大量的org.jgroups.protocols.pbcast,NAKACK对象； 最终分析发现是由于JBossCache的NAKACK栈在页面产生大量请求时，有个负责安全校验的全局Filter导致集群各个节点之间网络交互非常频繁，当网络情况不能满足传输要求时，大量的需要失败重发的数据在内存中不断堆积导致内存溢出。 解决办法 JBossCache版本改进； 程序设计优化，JBossCahce集群缓存同步，不大适合有频繁写操作的情况； 堆外内存导致的溢出错误问题描述 一个学校的小型项目，基于B/S的电子考试系统，服务器是Jetty 7.1.4，硬件是一台普通PC机，Core i5 CPU，4GB内存，运行32位Windows操作系统； 为了实现客户端能实时地从服务器端接收考试数据，使用了逆向AJAX技术（也称为Comet或Server Side Push），选用CometD 1.1.1作为服务端推送框架； 测试期间发现服务端不定期抛出内存溢出；加入-XX:+HeapDumpOnOutOfMemoryError后抛出内存溢出时什么问题都没有，采用jstat观察GC并不频繁且GC回收正常；最后在内存溢出后从系统日志发现如下异常堆栈： 问题分析 在第二章里曾经说过直接内存溢出的场景，垃圾收集时，虚拟机虽然会对直接内存进行回收，但它只能等老年代满了触发Full GC时顺便清理，否则只能等内存溢出时catch住然后调用System.gc()，如果虚拟机还是不听（比如打开了-XX:+DisableExplictGC）则只能看着堆中还有许多空闲内存而溢出； 本案例中的CometD框架正好有大量的NIO操作需要使用直接内存； 外部命令导致系统缓慢问题描述 一个数字校园应用系统，运行在一个4个CPU的Solaris 10操作系统上，中间件为GlassFish服务器； 系统在做大并发压力测试时，发现请求响应时间比较慢，通过监控工具发现CPU使用率很高，并且系统占用绝大多数的CPU资源的程序并不是应用系统本身； 通过Dtrace脚本发现最消耗CPU的竟然是fork系统调用（Linux用来产生新进程的）； 问题分析 最终发现是每个用户请求需要执行一个外部的shell脚本来获取一些系统信息，是通过Runtime.getRuntime().exec()方法调用的； Java虚拟机在执行这个命令时先克隆一个和当前虚拟机拥有一样环境变量的进程，再用这个新进程去执行外部命令，如果频繁地执行这个操作，系统消耗会很大； 最终修改时改用Java的API去获取这些信息，系统恢复了正常； 服务器JVM进程奔溃问题描述 一个基于B/S的MIS系统，硬件为两台2个CPU、8GB内存的HP系统，服务器是WebLogic 9.2（和案例”集群间同步导致的内存溢出”相同的系统）； 正常运行一段时间后发现运行期间频繁出现集群节点的虚拟机进程自动关闭的现象，留下一个hs_err_pid###.log，奔溃前不久都发生大量相同的异常，日志如下所示： 问题分析 这是一个远端断开连接的异常，得知在MIS系统工作流的待办事项变化时需要通过Web服务通知OA门户系统； 通过SoapUI测试发现调用后竟然需要长达3分钟才能返回，并且返回结果都是连接中断； 由于MIS使用异步方式调用，两边处理速度不对等，导致在等待的线程和Socket连接越来越多，最终在超过虚拟机承受能力后进场奔溃； 解决方法：将异步调用修改为生产者/消费者模型的消息队列处理，系统恢复正常； 不恰当数据结构导致内存占用过大问题描述 有一个后台RPC服务器，使用64位虚拟机，内存配置为-Xms4g -Xmx8g -Xmn1g，使用ParNew + CMS的收集器组合； 平时Minor GC时间约在20毫秒内，但业务需要每10分钟加载一个约80MB的数据文件到内存进行数据分析，这些数据会在内存中形成超过100万个HashMap Entry，在这段时间里Minor GC会超过500毫秒，这个时间过长，GC日志如下： 问题分析 在分析数据文件期间，800M的Eden空间在Minor GC后对象还是存活的，而ParNew垃圾收集器使用的是复制算法，把这些对象复制到Survivor并维持这些对象引用成为沉重的负担，导致GC时间变长； 从GC可以将Survivor空间去掉（加入参数-XX:SurvivorRatio=65536、-XX:MaxTenuringThreshold=0或者-XX:AlwaysTenure），让新生代存活的对象第一次Minor GC后立即进入老年代，等到Major GC再清理。这种方式可以治标，但也有很大的副作用。 另外一种是从程序设计的角度看，HashMap结构中，只有key和value所存放的两个长整形数据是有效数据，共16B（2 * 8B），而实际耗费的内存位88B（长整形包装为Long对象需要多8B的MarkWord、8B的Klass指针，Map.Entry多了16B的对象头、8B的next字段和4B的int型hash字段、为对齐添加的4B空白填充，另外还有8B的Entry引用），内存空间效率（18%）太低。 由Windows虚拟内存导致的长时间停顿问题描述 有一个带心跳检测功能的GUI桌面程序，每15秒发送一次心跳检查信号，如果对方30秒内都没有信信号返回，则认为和对方已断开连接； 程序上线后发现有误报，查询日志发现误报是因为程序会偶尔出现间隔约1分钟左右的时间完全无日志输出，处于停顿状态； 另外观察到GUI程序最小化时，资源管理中显示的占用内存大幅减小，但虚拟内存没变化； 因为是桌面程序，所需内存不大（-Xmx256m），加入参数-XX:+PrintGCApplicationStoppedTime -XX：PrintGCDateStamps -Xloggc:gclog.log后，从日志文件确认是GC导致的，大部分的GC时间在100ms以内，但偶尔会出现一次接近1min的GC； 加入参数-XX：PrintReferenceGC参数查看GC的具体日志信息，发现执行GC动作的时间并不长，但从准备开始GC到真正GC直接却消耗了大部分时间，如下所示： 问题分析 初步怀疑是最小化时工作内存被自动交换到磁盘的页面文件中，这样发生GC时就有可能因为恢复页面文件的操作而导致不正常的GC停顿； 在MSDN查证确认了这种猜想，加入参数-Dsun.awt.keepWorkingSetOnMinimize=true来解决；这个参数在很多AWT程序如VisualVM都有应用。 实战：Eclipse运行速度调优 升级JDK； 设置-XX:MaxPermSize=256M解决Eclipse判断虚拟机版本的bug； 加入参数-Xverfify:none禁止字节码验证； 虚拟机运行在client模式，采用C1轻量级编译器； 把-Xms和-XX：PermSize参数设置为-Xmx和-XX:MaxPermSize一样，这样强制虚拟机启动时把老年代和永久代的容量固定下来，避免运行时自动扩展； 增加参数-XX：DisableExplicitGC屏蔽掉显式GC触发； 采用ParNew+CMS的垃圾收集器组合； 最终从Eclipse启动耗时15秒到7秒左右， eclipse.ini配置如下： 本章小结Java虚拟机的内存管理和垃圾收集是虚拟机结构体系最重要的组成部分，对程序的性能和稳定性有非常大的影响。通过案例和实战部分，加深了对前面理论知识和工具的理解。 系列读书笔记 《深入理解Java虚拟机》读书笔记1：Java技术体系、Java内存区域和内存溢出异常 《深入理解Java虚拟机》读书笔记2：垃圾收集器与内存分配策略 《深入理解Java虚拟机》读书笔记3：虚拟机性能监控与调优实战 《深入理解Java虚拟机》读书笔记4：类文件结构 《深入理解Java虚拟机》读书笔记5：类加载机制与字节码执行引擎 《深入理解Java虚拟机》读书笔记6：程序编译与代码优化 《深入理解Java虚拟机》读书笔记7：高效并发]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[《深入理解Java虚拟机》读书笔记2：垃圾收集器与内存分配策略]]></title>
      <url>%2Fpost%2Fdeep_in_jvm_notes_part2%2F</url>
      <content type="text"><![CDATA[国内JVM相关书籍NO.1，Java程序员必读。读书笔记第二部分对应原书的第三章，主要介绍JVM的垃圾回收算法、实现。 第三章 垃圾收集器与内存分配策略概述思考GC需要完成的3件事情： 哪些内存需要回收？ 什么时候回收？ 如何回收？ 再回头看看第二章介绍的Java内存运行时区域的各个部分： 程序计时器、虚拟机栈、本地方法栈：随线程而灭，栈帧随方法而进行出栈和入栈，每一个栈帧分配的内存在类结构确定就已知，因此这几个区域不需要考虑回收； 对于Java堆和方法区，只有程序运行期间才知道会创建哪些对象，内存的分配和回收都是动态的，垃圾收集器所关注的是这部分内存； 对象已死吗？在垃圾收集器进行回收前，第一件事就是确定这些对象哪些还存活，哪些已经死去。 引用计数算法给对象添加引用计数器，当有地方引用它时就加1，引用失效就减1，为0时就认为对象不再被使用可回收。该算法失效简单，判断高效，但并不被主流虚拟机采用，主要原因是它很难解决对象之间相互循环引用的问题。 可达性分析算法通过一系列的称为“GC Roots”的对象作为起点，从这些节点开始向下搜索，搜索所走过的路径称为引用链（Reference Chain），如果一个对象到GC Roots没有引用链相连，则该对象是不可用的。 在Java语言中，可作为GC Roots的对象包括： 虚拟机栈（栈帧中的本地变量表）中引用的对象； 方法区中类静态属性引用的对象； 方法区中常量引用的对象； 本地方法栈中JNI（即一般说的Native方法）引用的对象； 再谈引用在JDK 1.2之后，Java对引用的概念进行了扩充，将引用分为强引用、软引用、弱引用和虚引用，这4种引用强度依次减弱。 生存还是死亡要真正宣告一个对象死亡，至少要经历两次标记过程：如果对象在进行可达性分析后发现没有与GC Roots相连接的引用链，那它将会被第一次标记并且进行一次筛选，筛选的条件是此对象是否有必要执行finalize方法（如没有重写finalize方法或者已经被调用过则认为没有必要执行）；如果有必要执行则将该对象放置在F-Queue队列中，并在稍后由一个由虚拟机自己建立的、低优先级的Finalizer线程去执行它；稍后GC将对F-Queue中的对象进行第二次标记，如果对象还是没有被引用，则会被回收。 但是作者不建议通过finalize方法“拯救”对象，因为它运行代价高、不确定性大、无法保证各个对象的调用顺序。 回收方法区永久代的垃圾收集主要回收两部分内容：废弃常量和无用的类。 一个无用的类需要满足以下三个条件： 该类的所有实例都已经被回收； 加载该类的ClassLoader已经被回收； 该类对象的java.lang.Class对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法； 在大量使用反射、动态代理、CGLib等ByteCode框架、动态生成JSP以及OSGI这类频繁自定义ClassLoader的场景都需要虚拟机具备类卸载的功能（HotSpot提供-Xnoclassgc参数控制），以保证永久代不会溢出。 垃圾收集算法 标记-清除算法：首先标记出所有需要回收的对象，然后统一回收所有被标记的对象；缺点是效率不高且容易产生大量不连续的内存碎片； 复制算法：将可用内存分为大小相等的两块，每次只使用其中一块；当这一块用完了，就将还活着的对象复制到另一块上，然后把已使用过的内存清理掉。在HotSpot里，考虑到大部分对象存活时间很短将内存分为Eden和两块Survivor，默认比例为8:1:1。代价是存在部分内存空间浪费，适合在新生代使用； 标记-整理算法：首先标记出所有需要回收的对象，然后让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存。适用于老年代。 分代收集算法：一般把Java堆分新生代和老年代，在新生代用复制算法，在老年代用标记-清理或标记-整理算法，是现代虚拟机通常采用的算法。 HotSpot的算法实现枚举根节点 由于要确保在一致性的快照中进行可达性分析，从而导致GC进行时必须要停顿所有Java执行线程； 在HotSpot里通过一组OopMap数据结构来知道哪些地方存放着对象引用； 安全点 HotSpot只在特定的位置记录了OopMap，这些位置称为安全点（SafePoint）； 即程序执行时并非在所有地方都能停顿下来开始GC，只有到达安全点时才能暂停； 对于安全点基本上是以程序“是否具有让程序长时间执行的特征”（比如方法调用、循环跳转、异常跳转等）为标准进行选定的； 另外还需要考虑如果在GC时让所有线程都跑到最近的安全点上，有两种方案：抢先式中断和主动式中断（主流选择）； 安全区域 如果程序没有分配CPU时间（如线程处于Sleep或Blocked），此时就需要安全区域（Safe Region），其是指在一段代码片段之中，引用关系不会发生变化； 线程执行到安全区域时，首先标识自己已经进入了安全区域，这样JVM在GC时就不管这些线程了； 垃圾收集器 垃圾收集算法是内存回收的方法论，垃圾收集器就是内存回收的具体实现。 这里讨论JDK 1.7 Update 14之后的HotSpot虚拟机（此时G1仍处于实验状态），包含的虚拟机如下图所示（存在连线的表示可以搭配使用）： Serial收集器 最基本、发展历史最悠久，在JDK 1.3之前是新生代收集的唯一选择； 是一个单线程（并非指一个收集线程，而是会暂停索引工作线程）的收集器； 现在依然是虚拟机运行在Client模式下的默认新生代收集器，主要就是因为它简单而高效（没有线程交互的开销）； ParNew收集器 其实就是Serial收集器的多线程版本； ParNew收集器在单CPU环境中绝对不会有比Serial收集器更好的效果； 是许多运行在Server模式下虚拟机首选的新生代收集器，重要原因就是除了Serial收集器外，只有它能与CMS收集器配合工作； 并行（Parallel）：指多条垃圾收集线程并行工作，但此时用户线程仍处于等待状态； 并发（Concurrent）：指用户线程与垃圾收集线程同时执行，用户线程在继续执行而垃圾收集程序运行在另外一个CPU上； Parallel Scavenge收集器 新生代收集器，使用复制算法，并行的多线程收集器； 与其他收集器关注于尽可能缩短垃圾收集时用户线程停顿时间不同，它的目标是达到一个可控制的吞吐量； 高吞吐量可以高效率利用CPU时间，适合在后台运算而不需要太多交互的任务； -XX:MaxGCPauseMillis参数可以设置最大停顿时间，而停顿时间缩短是以牺牲吞吐量和新生代空间来换取的； 另外它还支持GC自适应的调节策略； Serial Old收集器 是Serial收集器的老年代版本，同样是单线程，使用标记-整理算法； 主要是给Client模式下的虚拟机使用的； 在Server模式下主要是给JDK 1.5及之前配合Parallel Scavenge使用或作为CMS收集器的后备预案； Parallel Old收集器 是Parallel Scavenge的老年代版本，使用多线程和标记-整理算法； 是JDK 1.6中才开始提供的； CMS收集器 是一种以获取最短回收停顿时间为目标的收集器，特别适合互联网站或者B/S的服务端； 主要包括4个步骤：初始标记、并发标记、重新标记和并发清除； 还有3个明显的缺点：CMS收集器对CPU非常敏感、无法处理浮动垃圾、大量内存碎片产生； G1收集器 一款面向服务端应用的垃圾收集器，后续会替换掉CMS垃圾收集器； 特点：并行与并发（充分利用多核多CPU缩短Stop-The-World时间）、分代收集（独立管理整个Java堆，但针对不同年龄的对象采取不同的策略）、空间整合（基于标记-整理）、可预测的停顿（将堆分为大小相等的独立区域，避免全区域的垃圾收集）； 关于Region：新生代和老年代不再物理隔离，只是部分Region的集合；G1跟踪各个Region垃圾堆积的价值大小，在后台维护一个优先列表，根据允许的收集时间优先回收价值最大的Region；Region之间的对象引用以及其他收集器中的新生代与老年代之间的对象引用，采用Remembered Set来避免全堆扫描； 分为几个步骤：初始标记（标记一下GC Roots能直接关联的对象并修改TAMS值，需要STW但耗时很短）、并发标记（从GC Root从堆中对象进行可达性分析找存活的对象，耗时较长但可以与用户线程并发执行）、最终标记（为了修正并发标记期间产生变动的那一部分标记记录，这一期间的变化记录在Remembered Set Log里，然后合并到Remembered Set里，该阶段需要STW但是可并行执行）、筛选回收（对各个Region回收价值排序，根据用户期望的GC停顿时间制定回收计划来回收）； 理解GC日志 最前面的数字代表GC发生的时间（虚拟机启动以后的秒杀）； “[GC”和“[Full GC”说明停顿类型，有Full代表的是Stop-The-World的； “[DefNew”、“[Tenured”和“[Perm”表示GC发生的区域； 方括号内部的“3324K -&gt; 152K(3712K)” 含义是 “GC前该内存已使用容量 -&gt; GC后该内存区域已使用容量(该区域总容量)”; 方括号之外的“3324K -&gt; 152K(11904)” 含义是 “GC前Java堆已使用容量 -&gt; GC后Java堆已使用容量(Java堆总容量)”; 再往后“0.0025925 secs”表示该内存区域GC所占用的时间； 垃圾收集器参数总结 内存分配与回收策略 对象优先在新生代分配 大对象直接进入老年代 长期存活的对象将进入老年代 动态对象年龄判断：如果在Survivor空间中相同年龄所有对象大小总和大于Survivor空间的一半，大于或等于该年龄的对象直接进入老年代； 空间分配担保：发生Minor GC前，虚拟机会先检查老年代最大可用连续空间是否大于新生代所有对象总空间，如果不成立，虚拟机会查看HandlePromotionFailure设置值是否允许担保失败，如果允许继续检查老年代最大可用的连续空间是否大于历次晋升到老年代的平均大小，如果大于会尝试进行一次Minor GC；如果小于或者不允许冒险，会进行一次Full GC； 本章小结本章介绍了垃圾回收算法、几款JDK 1.7中提供的垃圾收集器特点以及运作原理。内存回收与垃圾收集器在很多时候都是影响系统性能、并发能力的主要因素之一，然而没有固定收集器和参数组合，也没有最优的调优方法，需要根据实践了解各自的行为、优势和劣势。 系列读书笔记 《深入理解Java虚拟机》读书笔记1：Java技术体系、Java内存区域和内存溢出异常 《深入理解Java虚拟机》读书笔记2：垃圾收集器与内存分配策略 《深入理解Java虚拟机》读书笔记3：虚拟机性能监控与调优实战 《深入理解Java虚拟机》读书笔记4：类文件结构 《深入理解Java虚拟机》读书笔记5：类加载机制与字节码执行引擎 《深入理解Java虚拟机》读书笔记6：程序编译与代码优化 《深入理解Java虚拟机》读书笔记7：高效并发]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[《深入理解Java虚拟机》读书笔记1：Java技术体系、Java内存区域和内存溢出异常]]></title>
      <url>%2Fpost%2Fdeep_in_jvm_notes_part1%2F</url>
      <content type="text"><![CDATA[国内JVM相关书籍NO.1，Java程序员必读。读书笔记第一部分对应原书的前两章，主要介绍了Java的技术体系、Java虚拟机的发展历史、Java运行时区域的划分、对象的创建和访问以及内存溢出的实战。 Part 1: 走进Java第一章 走进Java概述Java的优点 结构严谨、面向对象 摆脱平台的束缚，一次编写到处运行 提供了相对安全的内存管理和访问机制 实现了热点代码检测和运行时编译及优化 一套完善的应用程序接口以及无数的第三方类库 Java技术体系Sun官方所定义的Java技术体系包括： Java程序设计语言 各种硬件平台上的Java虚拟机 Class文件格式 Java API类库 来自商业机构和开源社区的第三方Java类库 JDK是用于支持Java开发的最小环境，JRE是支持Java程序运行的标准环境，整个Java体系如下所示： Java发展史 JDK 1.0: Java虚拟机、Applet、AWT等； JDK 1.1：JAR文件格式、JDBC、JavaBeans、RMI、内部类、反射； JDK 1.2：拆分为J2SE/J2EE/J2ME、内置JIT编译器、一系列Collections集合类； JDK 1.3：JNDI服务、使用CORBA IIOP实现RMI通信协议、Java 2D改进； JDK 1.4：正则表达式、异常链、NIO、日志类、XML解析器和XSLT转换器； JDK 1.5：自动装箱、泛型、动态注解、枚举、可变参数、遍历循环、改进了Java内存模型、提供了java.util.concurrent并发包； JDK 1.6：提供动态语言支持、提供编译API和微型HTTP服务器API、虚拟机优化（锁与同步、垃圾收集、类加载等）； JDK 1.7：G1收集器、加强对Java语言的调用支持、升级类加载架构； JDK 1.8：Lambda表达式等； Java虚拟机发展史 Sun Classic/Exact VM：Classic VM是第一款商用虚拟机，纯解析器方式来执行Java代码，如果要使用JIT编译器就必须进行外挂，解析器和编译器不能配合工作，编译器执行效率非常差；Exact VM是Sun虚拟机团队曾在Solaris平台发布的虚拟机，支持两级即时编译器、编译器和解释器混合工作、使用准确内存管理（虚拟机可以知道内存中某个位置的数据具体是什么类型），但很快就被HotSpot VM所取代； Sun HotSpot VM：Sun JDK和OpenJDK所带的虚拟机，目前使用范围最广；继承了前两款虚拟机的优点，还支持热点代码探测技术（通过计数器找出最具编译价值的代码）；2006年Sun公司宣布JDK包括HotSpot VM开源，在此基础上建立OpenJDK； Sun Mobile-Embedded VM/Meta-Circular VM：还有一些Sun开发的面对移动和嵌入式发布的和实验性质的虚拟机； BEA JRockit/IBM J9 VM：JRockit VM号称是世界上最快的Java虚拟机，专注于服务器端应用，不包含解析器实现，全部靠即时编译器编译执行；J9 VM定位于HotSpot比较接近，主要目的是作为IBM公司各种Java产品的执行平台； Azul VM/BEA Liquid VM：特定硬件平台专有的高性能虚拟机； Apache Harmony/Google Android Dalvik VM：Apache Harmony包含自己的虚拟机和Java库，但没有通过TCK认证；Dalvik VM是Android平台的核心组成部分，其并没有遵循Java虚拟机规范，不能直接执行Class文件，使用的是寄存器架构而不是JVM常见的栈架构； Microsoft JVM及其他：微软曾经是Java技术的铁杆支持者，开发过Windows下性能最好的Java虚拟机，但后来被Sun起诉终止其发展； 展望Java技术的未来 模块化 混合语言 多核并行 进一步丰富语法 64位虚拟机 实战：自己编译JDK 下载OpenJDK：https://jdk7.java.net/source.html 系统需求：Ubuntu 64位、5GB的磁盘、1G内存； 构建编译环境：需要Bootstrap JDK(JDK6以上)/Ant(1.7.1以上)/GCC。 sudo apt-get install build-essential gawk m4 openjdk-6-jdk libasound2-dev libcups2-dev libxrender-dev xorg-dev xutils-dev x11proto-print-dev binutils libmotif3 libmotif-dev ant 进行编译：设置环境变量、make sanity检查、make编译、复制到JAVA_HOME、编辑env.sh 在IDE工具中进行源码调试NetBeans（支持C/C++开发的版本） 本章小结本章介绍了Java技术体系的过去、现在以及未来的一些发展趋势，并独立编译一个OpenJDK 7的版本。 Part 2 自动内存管理机制第二章 Java内存区域与内存溢出异常概述对于Java程序员来说，在虚拟机自动内存管理机制下，不需要为new操作去写配对的delete/free代码，不容易出现内存泄漏。但是如果出现内存泄漏问题，如果不了解虚拟机的机制，便难以定位。 运行时数据区域 程序计数器 一块较小的内存，可以看作是当前线程所执行的字节码的行号指示器； 在虚拟机概念模型（各种虚拟机实现可能不一样）中，字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令； 程序计数器是属于线程私有的内存； 如果执行的是Java方法，该计数器记录的是正在执行的虚拟机字节码指令的地址；如果是Native方法则为空； Java虚拟机栈 Java虚拟机栈也是线程私有的； 描述的是Java方法执行的内存模型：每个方法在执行的同时都会创建一个栈帧（Stack Frame）用于存储局部变量表、操作数栈、动态链接、方法出口等信息。每一个方法从调用直至执行完成的过程，就对应着一个栈帧在虚拟机中入栈到出栈的过程； 局部变量表存放了编译器可知的各种基本数据类型、对象引用和returnAddress类型；其所需的内存空间在编辑期完成分配，不会再运行期改变； 可能存在两种异常：StackOverflowError和OutOfMemoryError； 本地方法栈 与虚拟机栈非常相似，只不过是为虚拟机使用到的Native方法服务； 可能存在两种异常：StackOverflowError和OutOfMemoryError； Java堆 Java堆是被所有线程共享的，在虚拟机启动时创建； 此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例都在这分配； 是垃圾收集器管理的主要区域，可以分为新生代和老年代； 可以物理不连续，只要逻辑上是连续的即可； 如果堆中没有内存完成实例分配也无法再扩展时，会抛出OutOfMemoryError异常； 方法区 是线程共享的区域； 用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据； 该区域对于垃圾收集来说条件比较苛刻，但是还是非常有必要要进行回收处理； 当无法满足内存分配需求时，将抛出OutOfMemoryError异常； 运行时常量池 是方法区的一部分； Class文件中除了有类的版本、字段、方法、接口等描述信息外，还有一项信息是常量池，用于存放编译器生成的各种字面量和符号引用，这部分内容将在类加载后进入方法区的运行时常量池中存放； Java虚拟机规范要求较少，通常还会把翻译出来的直接引用也存储在此； 另外一个重要特征是具备动态性，可以在运行期间将新的常量放入池中，如String的intern方法； 可能存在的异常：OutOfMemoryError； 直接内存 并不是虚拟机运行时数据区的一部分，也不是Java虚拟机规范中定义的内存区域； JDK 1.4的NIO引入了基于通道（Channel）和缓冲区（Buffer）的IO方法，可以使用Native函数库直接分配对外内存，然后通过一个存储在Java堆中的DirectByteBuffer对象作为这块内存的引用进行操作以提升性能； HotSpot虚拟机对象探秘进一步了解虚拟机内存中数据的其他细节，比如它们是如何创建、如何布局以及如何访问的。下面以虚拟机HotSpot和常用的内存区域Java堆为例，深入探讨HotSpot虚拟机在Java堆中对象分配、布局和访问的全过程。 对象的创建 虚拟机遇到一条new指令时，先检查指令的参数是否能在常量池中定位到一个类的符号，并且检查这个符号引用代码的类是否已被加载、解析和初始化过。如果没有，那必须先执行相应的类加载过程； 接下来虚拟机将为新生对象分配内存。对象所需的内存大小在类加载完成后便完全确定，为对象分配空间等同于把一块确定大小的内存从Java堆中划分出来。在使用Serial、ParNew等带Compact过程的收集器时，系统采用的分配算法是指针碰撞（内存绝对规整，只要通过指针作为分界点标识）；而使用CMS这种基于Mark-Sweep算法收集器时，通常使用空闲列表（内存不规整，通过维护一个列表记录那块内存是可用的）； 另外一个需要考虑的并发下的线程安全问题，有两种方案：一是分配内存空间的动作进行同步处理（实际上虚拟机采用CAS配上失败重试的方式保证更新操作的原子性）；二是为每个线程分配一小块内存（称为本地线程分配缓冲，TLAB），各个线程独立分配，只有TLAB用完需要分配新的才需要同步锁定，虚拟机通过-XX:+/-UseTLAB参数来设定； 内存分配完后，虚拟机将分配到的内存空间都初始化为零值（不包括对象头），这保证了对象的实例字段在Java代码中可以不赋值就直接使用，程序能访问到这些字段数据类型对应的零值； 接下来设置对象的对象头（Object Header）信息，包括对象是哪个类的实例、如何才能找到类的元数据信息、对象的哈希码、对象GC分代年龄等； 接着执行方法，把对象按照程序员的意愿进行初始化，这样一个真正可用的对象才算完全产生出来； HotSpot解释器的代码片段：略 对象的内存布局 对象在内存中存储的布局可以分为3块区域：对象头（Object Header）、实例数据（Instance Data）和对齐填充（Padding）； 对象头包括两部分信息：第一部分用于存储对象自身的运行时数据，如哈希码、GC分代年龄、锁状态标志、线程持有的锁、偏向线程ID、偏向时间戳等；另一部分是类型指针，即对象指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是哪个类的实例（并不是所有虚拟机都必须在对象数据上保留类型指针）。另外如果对象是一个Java数组，对象头中还必须有一块用于记录数组长度的数据。 实例数据部分是真正存储的有效信息，也是在代码中所定义的各种类型字段内容。无论是父类继承的还是子类中定义的都需要记录下来。这部分存储的顺序会受到虚拟机分配策略参数和字段在Java源码中定义顺序的影响。 对齐填充不是必然存在的，主要是由于HotSpot VM的自动内存管理系统要求对象起始地址必须是8字节的整数倍。 对象的访问定位 栈上的reference类型在虚拟机规范中只规定了一个指向对象的引用，并没有定义这个引用应该通过何种方式去定位、访问堆栈对象的具体位置，目前主流的方式方式有句柄和直接直接两种。 通过句柄：Java堆中划出一块内存作为句柄池，reference中存储的就是对象的句柄地址，而句柄中包含了对象实例数据与类型数据各自的具体地址信息。其最大好处就是reference存储的是稳定的句柄地址，在对象被移到（垃圾收集时移到）只改变实例数据指针，而reference不需要修改； 通过直接指针：Java堆对象的布局中必须考虑如果放置访问类型数据的相关信息，而reference中存在的直接就是对象地址。其最大好处在于速度更快，节省了一次指针定位的时机开销。HotSpot采用该方式进行对象访问，但其他语言和框架采用句柄的也非常常见。 实战：OutOfMemoryError异常 通过代码验证Java虚拟机规范中描述各个运行时区域存储的内容； 在实际遇到内存溢出异常时，能根据异常的信息快速判断是哪个区域内存溢出； Java堆溢出 解决思路：先通过内存映像分析工具对dump出来的堆转储快照进行分析，先分清楚是内存泄漏还是内存溢出；如果是内存泄漏，进一步查看泄漏对象到GC Roots的引用链，从而确认为什么无法回收；如果是内存溢出，则应当检查虚拟机堆参数（-Xmx与-Xmx）或检查是否存在对象生命周期过长、持有状态时间过长的情况； 虚拟机栈和本地方法栈溢出 HotSpot不区分虚拟机栈和本地方法栈； StackOverflowError和OutOfMemoryError存在互相重叠的地方； 栈容量由-Xss参数设定； 虚拟机的默认参数对于通常的方法调用（1000~2000层）完全够用，通常根据异常的堆栈日志就可以很容易定位问题。 方法区和运行时常量池溢出对于这个区域的测试，基本思路是运行时产生大量的类去填满方法区（比如使用反射和动态代理），这里我们借助CGLib直接操作字节码运行时产生大量的动态类（很对主流框架如Spring、Hibernate都会采用类似的字节码技术）。在这里需要特别注意垃圾回收的状况。 本机直接内存溢出 DirectMemory导致的内存溢出，在Heap Dump里不会看见明显的异常。如果发现OouOfMemory之后Dump文件很小，程序又使用了NIO，那就可以检查下是否这方面的原因。 本章小结学习了虚拟机的内存是如何划分的，对象是如何创建、布局和访问的，哪部分区域、什么样的代码和操作可能导致内存的溢出异常。 系列读书笔记 《深入理解Java虚拟机》读书笔记1：Java技术体系、Java内存区域和内存溢出异常 《深入理解Java虚拟机》读书笔记2：垃圾收集器与内存分配策略 《深入理解Java虚拟机》读书笔记3：虚拟机性能监控与调优实战 《深入理解Java虚拟机》读书笔记4：类文件结构 《深入理解Java虚拟机》读书笔记5：类加载机制与字节码执行引擎 《深入理解Java虚拟机》读书笔记6：程序编译与代码优化 《深入理解Java虚拟机》读书笔记7：高效并发]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[阅读随手记 201701]]></title>
      <url>%2Fpost%2Freading_record_201701%2F</url>
      <content type="text"><![CDATA[关键字：架构, 微服务，消息中间件，性能调优，Elasticsearch，缓存，RPC，日志分析，平台化，DistributedLog，监控，负载均衡，高性能，高并发，高可用。 千万级规模高性能、高并发的网络架构经验分享https://mp.weixin.qq.com/s?__biz=MzA3MzYwNjQ3NA==&amp;mid=401628413&amp;idx=1&amp;sn=91abfbad4c7dc882e94939042a8785a4 架构的本质 新浪微博整体架构 微博架构的演变不可能在第一代基础上通过简单的修修补补满足用户量快速增长的，同时线上业务又不能停， 这是我们常说的在飞机上换引擎的问题。建议在做服务化的时候，首先更多是偏向业务的梳理，同时要找准一个很好的切入点，既有架构和服务化上的提升，业务方也要有收益，比如提升性能或者降低维护成本同时升级过程要平滑，建议开始从原子化服务切入，比如基础的用户服务、基础的短消息服务、基础的推送服务。 微博的技术挑战 正交分解法解析架构一个维度是水平的分层拆分，第二从垂直的维度会做拆分。水平的维度从接口层、服务层到数据存储层。垂直怎么拆分，会用业务架构、技术架构、监控平台、服务治理等等来处理。 从业务架构来看，接口层有feed、用户关系、通讯接口；服务层，SOA里有基层服务、原子服务和组合服务，在微博我们只有原子服务和组合服务。原子服务不依赖于任何其他服务，组合服务由几个原子服务和自己的业务逻辑构建而成，资源层负责海量数据 的存储。 技术框架解决独立于业务的海量高并发场景下的技术难题，由众多的技术组件共同构建而成。在接口层，微博使用JERSY框架，帮助你做参数的解析、参数的验证、序列化和反序列化；资源层，主要是缓存、DB相关的各类组件，比如Cache组件和对象库组件。 监控平台和服务治理，完成系统服务的像素级监控，对分布式系统做提前诊断、预警以及治理。包含了SLA 规则的制定、服务监控、服务调用链监控、流量监控、错误异常监控、线上灰度发布上线系统、线上扩容缩容调度系统等。 常见的设计原则 系统架构三个利器：RPC服务组件、消息中间件（交互异步化、流量削峰）、配置管理（灰度发布、降级）； 无状态：接口层最重要的就是无状态，将有状态的数据剥离到数据库或缓存中； 数据层比服务层更需要设计：存储、压缩、索引等； 物理结构与逻辑结构的映射：几个垂直的业务组加上一个基础技术架构组，精细化团队分工，有利于提高沟通协作的效率； 分布式系统，它最终的瓶颈一定落在CPU、内存、存储和网络上； 微博多级双机房缓存架构 微博使用了双层缓存，上面是L1，每个L1上都是一组(包含4-6台机器)，左边的框相当于一个机房，右边又是一个机房。两个机房是互为主备，或者互为热备 L1缓存的作用：增加整个系统的QPS、以低成本灵活扩容的方式增加系统的带宽； 第二级缓存更多的是从容量上来规划，保证请求以较小的比例穿透到后端的数据库中； Feed的存储架构 内容表：每条内容一个索引，每天建一张表； 一级索引的时候会先根据关注的用户，取他们的前条微博ID，然后聚合排序。在做哈希(分库分表)的时候，同时考虑了按照UID哈希（分库）和按照时间维度（分表）。 二级索引，是我们里面一个比较特殊的场景，就是我要快速找到这个人所要发布的某一时段的微博时，通过二级索引快速定位。 分布式服务追踪系统一个请求从用户过来之后，在后台不同的机器之间不停的调用并返回。当你发现一个问题的时候，这些日志落在不同的机器上，你也不知道问题到底出在哪儿，各个服务之间互相隔离，互相之间没有建立关联。所以导致排查问题基本没有任何手段，就是出了问题没法儿解决。 解决办法是用一个请求ID，然后结合RPC框架，服务治理功能使日志联系起来。用JAVA的话就可以用AOP，要做到零侵入的原则，就是对所有相关的中间件打点，从接口层组件 (HTTP Client、HTTP Server)至到服务层组件(RPC Client、RPC Server)，还有数据访问中间件的，这样业务系统只需要少量的配置信息就可以实现全链路监控。 RPC的概念模型与实现解析http://mp.weixin.qq.com/s?__biz=MzAxMTEyOTQ5OQ==&amp;mid=2650610547&amp;idx=1&amp;sn=2cae08dbf62d9a6c2f964ffd440c0077 定义：RPC的全称是 Remote Procedure Call，是一种进程间通信方式。 它允许程序调用另一个地址空间（通常是共享网络的另一台机器上）的过程或函数，而不用程序员显式编码这个远程调用的细节。即程序员无论是调用本地的还是远程的函数，本质上编写的调用代码基本相同。 起源：上世纪80年代由 Bruce Jay Nelson提出； 目标：让构建分布式计算（应用）更容易，在提供强大的远程调用能力时不损失本地调用的语义简洁性。 分类：同步调用和异步调用，区分在于是否等待服务端执行完成并返回结果。 RPC理论模型： 这里User就是Client端。当User发起一个远程调用时，它实际是通过本地调用User-stub。 User-stub负责将调用的接口、方法和参数通过约定的协议规范进行编码并通过本地的RPCRuntime实例传输到远端的实例。远端RPCRuntime实例收到请求后交给 Server-stub进行解码后发起向本地端Server的调用，调用结果再返回给User端。 RPC模型拆解 RPC服务端通过RpcServer去导出远程接口方法，而客户端通过RpcClient去导入远程接口方法。客户端像调用本地方法一样去调用远程接口方法，RPC框架提供接口的代理实现，实际的调用将委托给代理RpcProxy。代理封装调用信息并将调用转交给RpcInvoker去实际执行。在客户端的RpcInvoker通过连接器RpcConnector去维持与服务端的通道RpcChannel，并使用RpcProtocol执行协议编码并将编码后的请求消息通过通道发送给服务端。 RPC服务端接收器RpcAcceptor接收客户端的调用请求，同样使用RpcProtocol执行协议解码。解码后的调用信息传递给RpcProcessor去控制处理调用过程，最后再委托调用给RpcInvoker去实际执行并返回调用结果。 实现：微型RPC框架库craft-atom-rpc Elastic{ON} Dev China Beijing 2016 Keynotehttp://elasticsearch.cn/article/122阅读时结合：http://www.infoq.com/cn/news/2016/08/Elasticsearch-5-0-Elastic 目前ELK下载次数已经达到75M次。 有75%的用户将ELK使用于多个场景，60%的用户使用其数据搜索和分析功能，40%的用户使用日志分析功能。 统一发布5.0版本后，拥有了ELKB和Elastic Cloud的全栈产品线。 Elasticsearch5.0率先集成了Lucene 6版本，其中最重要的特性就是 Dimensional Point Fields，多维浮点字段，ES里面相关的字段如date, numeric，ip 和 Geospatial 都将大大提升性能。 ES5.0在Internal engine级别移除了用于避免同一文档并发更新的竞争锁，带来15%-20%的性能提升。 ElasticSearch采用了更先进的Painless脚本。Painless使用白名单来限制函数与字段的访问，针对ES的场景来进行优化，只做ES数据的操作，更加轻量级，速度要快好几倍，并且支持Java静态类型，语法保持Groove类似，还支持Java的lambda表达式。 毫秒级的Shrink API。它可将分片数进行收缩成它的因数，如之前你是15个分片，你可以收缩成5个或者3个又或者1个，那么我们就可以想象成这样一种场景，在写入压力非常大的收集阶段，设置足够多的索引，充分利用shard的并行写能力，索引写完之后收缩成更少的shard，提高查询性能。 对日志类索引更友好的Rollover API。首先创建一个logs-0001的索引，它有一个别名是logs_write,然后我们给这个logs_write创建了一个rollover规则，即这个索引文档不超过1000个或者最多保存7天的数据，超过会自动切换别名到logs-0002,你也可以设置索引的setting、mapping等参数,剩下的es会自动帮你处理。这个特性对于存放日志数据的场景是极为友好的。 新增了一个Wait for refresh功能。大家知道elasticsearch可以设置refresh时间来保证数据的实时性，refresh时间过于频繁会造成很大的开销，太小会造成数据的延时，之前提供了索引层面的_refresh接口，但是这个接口工作在索引层面，我们不建议频繁去调用，如果你有需要修改了某个文档，需要客户端实时可见怎么办？在 5.0中，Index、Bulk、Delete、Update这些数据新增和修改的接口能够在单个文档层面进行refresh控制了，有两种方案可选，一种是创建一个很小的段，然后进行刷新保证可见和消耗一定的开销，另外一种是请求等待es的定期refresh之后再返回。 新增Ingest Node。之前如果需要对数据进行加工，都是在索引之前进行处理，比如logstash可以对日志进行结构化和转换，现在直接在es就可以处理了，目前es提供了一些常用的诸如convert、grok之类的处理器，在使用的时候，先定义一个pipeline管道，里面设置文档的加工逻辑，在建索引的时候指定pipeline名称，那么这个索引就会按照预先定义好的pipeline来处理了。 这是一个原始的日志： { &quot;message&quot;: &quot;55.3.244.1 GET /index.html 15824 0.043” } 使用Ingest就可以这么定义一个pipeline： 通过我们的pipeline处理之后的文档长什么样呢，我们获取这个文档的内容看看： 另一个和aggregation的改进也是非常大，Instant Aggregations。Elasticsearch已经在Shard层面提供了Aggregation缓存，如果你的数据没有变化，ES能够直接返回上次的缓存结果。 新增了一个Sliced Scroll类型，现在Scroll接口可以并发来进行数据遍历了。每个Scroll请求，可以分成多个Slice请求，可以理解为切片，各Slice独立并行，利用Scroll重建或者遍历要快很多倍。 ES现在提供了Profile API来进行查询的优化，只需要在查询的时候开启profile：true就可以了，一个查询执行过程中的每个组件的性能消耗都能收集到。 还有一个和翻页相关的问题，就是深度分页，现在有一个新的 Search After 机制，其实和scroll类似，也是游标的机制，它的原理是对文档按照多个字段进行排序，然后利用上一个结果的最后一个文档作为起始值，拿size个文档，一般我们建议使用_uid这个字段，它的值是唯一的id。 新增Reindex。关于索引数据，大家之前经常重建，数据源在各种场景，重建起来很是头痛，那就不得不说说现在新加的Reindex接口了，Reindex可以直接在ElasticSearch集群里面对数据进行重建，如果你的mapping因为修改而需要重建，又或者索引设置修改需要重建的时候，借助Reindex可以很方便的异步进行重建，并且支持跨集群间的数据迁移。 ES 5.0里面提供了第一个Java原生的REST客户端SDK，相比之前的TransportClient，版本依赖绑定，集群升级麻烦，不支持跨Java版本的调用等问题，新的基于HTTP协议的客户端对Elasticsearch的依赖解耦，没有jar包冲突，提供了集群节点自动发现、日志处理、节点请求失败自动进行请求轮询，充分发挥Elasticsearch的高可用能力，并且性能不相上下。 另外还介绍了ELKB 5.0包括X-Pack组件的新特性，这里不做具体介绍，大家可以按照自己的兴趣阅读。 基于Kibana和ES的苏宁实时日志分析平台http://elasticsearch.cn/article/122 集群现状 日志平台架构演进 优化总结 – 硬件 优先独立物理理机； 对于实时性要求非常高的需求，优先SSD； 适当调整OS的max_file_descriptors，解决Too many open files 异常； 单服务器运行多个node时，调整max user processes，否则容易易native threadOOM； 关闭swap交换或锁内存 ulimit -l unlimited/bootstrap.mlockall: true 优化总结 – ES 根据数据量合理的规划索引pattern和shard数； disabled _all 节省存储空间、提升索引速度； 不需要分词的字段设成 not_analyzed； 对于不要求100%高可用的内部系统，可不设置副本，提升index速度和减少存储； 设置合理的refresh时间 index.refresh_interval: 300S 设置合理的flush间隔 index.translog.flush_threshold_size: 4g; index.translog.flush_threshold_ops: 50000 合理配置throttling indices.store.throttle.max_bytes_per_sec: 200mb 适当调整bulk队列 threadpool.bulk.queue_size: 1000 有时可能因为gc时间过长，导致该数据节点被主节点踢出集群的情况，导致集群出现不健康的状态，为了解决这样的问题，我们适当的调整ping参数。(master) discovery.zen.fd.ping_timeout: 40s discovery.zen.fd.ping_interval: 5s discovery.zen.fd.ping_retries: 5 数据节点young gc频繁，适当调转新生代（-Xmn3g），降低young gc的频率。 在进行检索和聚合操作时，ES会读取反向索引，并进行反向解析，然后进行排序，将结果保存在内存中。这个处理会消耗很多Heap，有必要进行限制，不然会很容易出现OOM。 Disabled analyzed field fielddata 限制Field Data的Heap Size的使用 indices.fielddata.cache.size: 40% indices.breaker.fielddata.limit: 50% 美团点评搜索平台化实践之路http://elasticsearch.cn/article/122 为什么需要平台化 重复建设严重 使用门槛高 缺少整体解决方案 长期演进不足 平台化：提供一整套的技术、运维方案和开发组件，最大化简化应用开发，提高开发效率、降低成本、提高可靠性。 平台化解决的主要问题 快速部署 -&gt; 代码库管理软件包；管理界面部署、重启、停止等操作； 集群高可用 -&gt; 引用集群组概念；双机房+双集群；双集群写；双集群读； 客户端使用门槛高 -&gt; 支持POJO功能；支持读写监控；支持多集群访问； 开源插件安全性弱、扩展性难 -&gt; 独立研发管理平台； 慢查询日志可视化和告警 -&gt; 通过Logstash抓取日志上报管理平台；管理平台提供查询、分析和统计； 平台技术架构 百度对Elasticsearch的优化改进http://elasticsearch.cn/article/122 分布式SQL查询层 提供标准SQL接口，方便使用，降低学习成本 兼容Mysql协议，原Mysql/DDBS业务无缝迁移 兼容原始的HTTP协议 权限管理 不同用户访问自己的表，增加database逻辑层，兼容ES和MySQL； 权限级别： db， table 用户级别：root， superuser，user 权限类型：read_only，read_write 白名单：IP（通配符）、hostname（BNS） Online schema change DistributedLog 数据一致性 需求背景：部分业务对ES可靠性要求很高；不能容忍脑裂、数据不一致、丢数据等情况； 需解决的问题：元数据一致性（脑裂）、强一致写、强一致读 解决方法：DistributedLog 元数据一致性（脑裂）：Master Leader向DL中写入Cluster State变更；其余的Master节点和所有的DataNode节点从DL中获取变更并向本地Apply； 强一致写：Master指定Primary；Primary将日志写入DL；Replica从DL中读取日志并回放；Translog必须在Lucene引擎内部实现为原子操作（如果先写log，Lucene可能写入不成功；如果先写Lucene，log有可能写入不成功） 强一致读 Lease机制：Primary需要定时从Master获取Lease；读取时首先检查Lease，当Lease Expire时不再提供读取服务；查询只查primary； 多集群数据同步 需求背景：业务要求高可用，两地三中心部署；多个主备集群需实时同步增量数据；主备切换后的冲突处理； 设计实现：每个Doc都有修改的timestmap和version信息；Mirror Maker根据timestamp获取index修改的增量信息；Mirror Maker将增量的更新发往目标集群的Index中；冲突时根据version来判断是否覆盖目标集群里的Doc；默认version使用timestmap； 多租户资源隔离 需求背景：多个业务使用公共集群，CPU、内存、IO、JVM等相互影响；云化部署； 设计实现：每台物理机启动多个ES进程组成大集群；cgroup对每个ES进程进行CPU、内存、IO等隔离；引入tenement 概念，分配不同的ES节点为每个租户创建自己的虚拟集群；Allocation filter限制租户的index只能创建在自己的节点上；每个租户分配不同DB，隔离访问权限；username@tenement，租户命名空间隔离租户信息；根据租户ID隔离settings，templates、nodes等； 整体架构 整体规模 万亿交易量级下的秒级监控https://102.alibaba.com/newsInfo.htm?newsId=26 阿里监控体系：集团层面的占整体80%；各个事业群根据自身特性自主研发了多套监控系统；规模已达到千万量级的监控项、PB级的监控数据、亿级的报警通知； SunFire是一整套海量日志实时分析解决方案，以日志、REST接口、Shell脚本等作为数据采集来源，提供设备、应用、业务等各种视角的监控能力，从而快速发现、定位、分析和解决问题，为线上系统可用率提供有效保障；其利用文件传输、流式计算、分布式文件存储、数据可视化、数据建模等技术，提供实时、智能、可定制、多视角、全方位的监控体系；技术架构如下所示： Agent组件负责日志原始数据的采集，按周期查询日志，要求低耗、智能。 低耗的第一个要素就是避免跨机房传输，SunFire运行时组件自包含在机房内部，需要全量数据才从各机房查询合并； SunFire还利用了zero-copy，即文件传输可以不经过用户态，实现极低CPU占用的文件传输； 要求按周期查询日志这个是Agent工程里最大的难题。RAF里通过指定offset和读取size可以很低耗地读取这部分内容，但是在计算平台周期任务驱动架构里，pull的方式无法提供offset，这个是通过二分法查找来猜； 当日志滚动的时候也是靠穷举的方式来猜offset； 支持两种查询服务：first query和ordinary query。一个周期的查询请求只有第一次需要猜offset。 另外一个是Map Reduce的计算组件，负责对所有采集内容进行加工计算，具备故障自动恢复及弹性伸缩能力； 计算组件的特性：纯异步（使用akka作为协程框架）、周期驱动、任务重试、输入共享； 其他组件：存储（HBase、MongoDB）、展示、自我管控。 万亿级数据洪峰下的分布式消息引擎https://102.alibaba.com/newsInfo.htm?newsId=21 低延迟探索之路：JVM停顿（尽量避免Full GC、关闭偏向锁、输出GC日志到内存文件系统、关闭JVM输出的jstat日志）、利用CAS将RocketMQ核心链路无锁化、通过内核参数（vm.extra_free_kbytes和vm.swappiness）调优避免内存延迟、消除Page Cache延迟（内存预分配、文件预热、读写分离）； 容量保障三大法宝：降级、限流和熔断； RocketMQ高可用：基于多机房部署，利用分布式锁和通知机制，借助Controller组件，设计并实现了Master/Slave结构的高可用架构；消息的读请求会命中Master，然后通过异步方式复制到Slave；消息的读请求优先命中Master，有压力时转移到Slave； 分布式系统通用高可用解决方案 RocketMQ高可用架构 万亿级调用系统：微信序列号生成器架构设计及演变http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2650992918&amp;idx=1&amp;sn=be5121c3c57257291a30715ef7130a90&amp;scene=21 微信立项之初就确立了利用数据版本号实现终端与后台数据增量同步的机制，确保消息可靠送达对方手机；这就需要一个高可用、高可靠的序列号生成器来产生同步数据用的版本号，这个生成器就是seqsvr； 数据版本号有两个性质：递增的64位整型变量；每个用户都有自己独立的64位sequence空间（避免申请互斥）； 架构原型（64位数组，每个用户保存最后一个seq） –&gt; 预分配中间层（放置一定步长的数据在内存，避免频繁更新）–&gt; 分号段共享存储（uid相连的用户属于同一号段，共享max_seq，减少重启时加载过长问题）； 工程实现：存储层（StoreSrv）利用多机NRW策略保证数据持久化不丢失；每个缓存中间层（AllocSrv）负责若干个号段的seq分配；整个系统按uid范围分Set，每个Set都是一个完整的、独立的子系统。 微信开源PhxSQL背后：强一致高可用分布式数据库的设计和实现哲学http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2650994184&amp;idx=1&amp;sn=9be9eb8ab569ad281330b6ceeb490757&amp;scene=21 为何研发：解决强一致性、高可用和serializable事务隔离；完全兼容Mysql； PhxSQL是什么：建立在Paxos的一致性和Mysql的binlog流水的基础上的；提供两个服务端口：强一致读写端口和只读端口； PhxSQL的强一致性指线性一致性，高可用是指只要多余一半机器工作和互联即可在保证线性一致性的质量下正常工作，提供和ZooKeeper相同的强一致性和高可用性； 设计原则：简单可逻辑证明的一致性模型（基于Paxos和binlog流水一致）；最小侵入Mysql原则；简单的架构、部署和运维； 局限性：DDL命令可能存在一致性风险、写入请求量很大主机死机时会有一段时间不可写；另外不支持多写和分表分库。 章文嵩博士和他背后的负载均衡帝国http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2650992803&amp;idx=1&amp;sn=e2a46917301941faacc324af29013877&amp;scene=21 常见的负载均衡技术：DNS轮询（易于实现，但存在会话粘连、DNS缓存滞后、容错、数据热点问题）；引入负载均衡器（集中分发、支持多种分发方式，但存在单点的问题）；健康监测（负载均衡的伴侣）； VIPServer：阿里中间层负载均衡产品；是基于P2P模式的七层负载均衡产品；提供动态域名解析和负载均衡服务；支持一系列流量智能调度和容灾策略、支持多种健康监测协议、支持精细的权重控制；提供多级容灾、具有对称调用、健康阈值保护等功能。 阿里双十一大促，技术准备只做了这两件事情？http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2650995093&amp;idx=1&amp;sn=574f6d83a48c2c596943b1fbeb25e4a7&amp;scene=21 容量规划：在什么时候什么样的系统需要多少服务器？需要给出确定性、量化的数字； 容量规划三阶段：经验判断、线上压测（测性能、估机器、模拟回放、线上分流）和场景化压测，目前还做了全链路压测； 场景化容量评估：造流量，尽量模拟真实场景；流量隔离，通过负载均衡出一个在线集群； 流量评估的流程：数据构造（构造基础数据、业务模型预测、构造压测请求） -&gt; 环节准备（配置压测方案、上传压测数据、业务预热、生效压测passtoken、小流量预跑验证） -&gt; 压测执行&amp;总结（压测用户登录、压测执行&amp;实时调速、动态弹性伸缩、压测报告&amp;问题总结）； 容量评估的总结： Twitter再开源！这回是分布式高性能日志复制服务http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2650992712&amp;idx=1&amp;sn=727ce15ad3651ce43a710a165ed2495a&amp;scene=21 DistributedLog是一个高性能的日志复制服务，提供了持久化、复制以及强一致性的功能，这对于构建可靠的分布式系统都是至关重要的，如复制状态机、通用的发布订阅系统、分布式数据库以及分布式队列； DL会分类维护记录的序列并称其为Log，将记录写入DL Log的进程称之为Writer，从Log中读取并处理记录的进程称之为Reader，其整体架构如下所示： Log：是有序的、不可变的日志记录，它的数据结构如下所示： 日志记录：一个字节序列，会按照序列写入到日志流中，并且会分配一个DLSN的唯一序列号。应用程序还可以设置自己的序列号（称之为TransactionID），便于Reader从特定的日志记录读取； Log分段：Log会被分解为Log分段，每个分段包含了记录的子集，分布式地存储（如BookKeeper）。DL会基于配置好的策略轮询每个Log分段。 命名空间：属于同一组织的Log流会归类在同一命名空间下，便于管理； Writer：序列号由Writer负责的，这意味着对于某个Log，在给定的时间点上，只能有一个激活的Writer；Writer由名为Write Proxy的服务层来提供和管理，Write Proxy用来接受大量客户端的fan-in写入； Reader：Reader会在一个给定的位置（DLSN或TransactionID）开始从Log中严格按顺序读取记录。在同一个Log中，不同的Reader可以在不同的起始位置读取记录。与其他的订阅发布系统不同，DL并不会记录和管理Reader的位置，它将跟踪的任务留给了应用程序本身； 优势总结：高性能（毫秒级延迟）、持久化和一致性、各种工作负载、多租户、分层架构； 如何用十条命令在一分钟内检查Linux服务器性能http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2650994146&amp;idx=1&amp;sn=f6b0987a06831805b4c343c417121827&amp;scene=21 ## 以下命令uptime用于快速查看机器的负载情况，输出1min、5min、15min的平均负载请假 [test@localhost ~]$ uptime 13:56:36 up 32 days, 13:12, 1 user, load average: 0.00, 0.02, 0.00 ## 以下命令用于输出系统日志的最后10行 [test@localhost ~]$ dmesg | tail alloc kstat_irqs on node -1 bnx2 0000:01:00.0: irq 65 for MSI/MSI-X alloc irq_desc for 66 on node -1 alloc kstat_irqs on node -1 bnx2 0000:01:00.0: irq 66 for MSI/MSI-X alloc irq_desc for 67 on node -1 alloc kstat_irqs on node -1 bnx2 0000:01:00.0: irq 67 for MSI/MSI-X bnx2 0000:01:00.0: em1: using MSIX bnx2 0000:01:00.0: em1: NIC Copper Link is Up, 100 Mbps full duplex, receive &amp; transmit flow control ON ## 以下命令用于输出一些系统核心指标，后面的参数1表示每秒输出一次统计信息。 [test@localhost ~]$ vmstat 1 procs -----------memory---------- ---swap-- -----io---- --system-- -----cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 0 0 0 9118956 221536 10690528 0 0 0 19 0 2 2 0 98 0 0 0 0 0 9118816 221536 10690560 0 0 0 0 1180 2939 0 0 100 0 0 0 0 0 9118816 221536 10690584 0 0 0 464 1199 2798 0 0 99 0 0 0 0 0 9118824 221536 10690616 0 0 0 0 1089 2839 0 0 100 0 0 ## 以下命令用于显示每个CPU的占用情况 [test@localhost ~]$ mpstat -P ALL 1 Linux 2.6.32-431.el6.x86_64 (localhost.localdomain) 01/19/2017 _x86_64_ (8 CPU) 02:04:08 PM CPU %usr %nice %sys %iowait %irq %soft %steal %guest %idle 02:04:09 PM all 0.25 0.00 0.25 0.00 0.00 0.00 0.00 0.00 99.50 02:04:09 PM 0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 100.00 02:04:09 PM 1 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 100.00 02:04:09 PM 2 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 99.00 02:04:09 PM 3 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 100.00 02:04:09 PM 4 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 100.00 02:04:09 PM 5 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 100.00 02:04:09 PM 6 0.00 0.00 0.99 0.00 0.00 0.00 0.00 0.00 99.01 02:04:09 PM 7 0.99 0.00 0.00 0.00 0.00 0.00 0.00 0.00 99.01 ## 以下命令用于输出进程的CPU占用率 [test@localhost ~]$ pidstat 1 Linux 2.6.32-431.el6.x86_64 (localhost.localdomain) 01/19/2017 _x86_64_ (8 CPU) 02:05:38 PM PID %usr %system %guest %CPU CPU Command 02:05:39 PM 2004 0.99 0.00 0.00 0.99 4 mysqld 02:05:39 PM 14777 0.99 0.99 0.00 1.98 0 pidstat 02:05:39 PM 25506 0.99 0.99 0.00 1.98 6 java 02:05:39 PM 25922 0.99 0.99 0.00 1.98 6 java ## 以下命令用于查看机器磁盘IO情况 [test@localhost ~]$ iostat -xz 1 Linux 2.6.32-431.el6.x86_64 (localhost.localdomain) 01/19/2017 _x86_64_ (8 CPU) avg-cpu: %user %nice %system %iowait %steal %idle 1.72 0.00 0.26 0.18 0.00 97.84 Device: rrqm/s wrqm/s r/s w/s rsec/s wsec/s avgrq-sz avgqu-sz await r_await w_await svctm %util sda 0.01 31.80 0.04 6.35 2.17 305.21 48.09 0.09 14.38 2.52 14.46 3.55 2.27 ## 以下命令用于查看系统内存的使用情况 [test@localhost ~]$ free -m total used free shared buffers cached Mem: 32092 23216 8876 0 216 10458 -/+ buffers/cache: 12540 19551 Swap: 4095 0 4095 ## 以下命令用于查看网络设备的吞吐率 [test@localhost ~]$ sar -n DEV 1 Linux 2.6.32-431.el6.x86_64 (localhost.localdomain) 01/19/2017 _x86_64_ (8 CPU) 02:11:06 PM IFACE rxpck/s txpck/s rxkB/s txkB/s rxcmp/s txcmp/s rxmcst/s 02:11:07 PM lo 117.17 117.17 8.28 8.28 0.00 0.00 0.00 02:11:07 PM em1 108.08 120.20 8.11 24.58 0.00 0.00 5.05 02:11:07 PM em2 0.00 0.00 0.00 0.00 0.00 0.00 0.00 ## 以下命令用于查看TCP连接状态 [test@localhost ~]$ sar -n TCP,ETCP 1 Linux 2.6.32-431.el6.x86_64 (localhost.localdomain) 01/19/2017 _x86_64_ (8 CPU) 02:12:25 PM active/s passive/s iseg/s oseg/s 02:12:26 PM 80.81 0.00 585.86 772.73 02:12:25 PM atmptf/s estres/s retrans/s isegerr/s orsts/s 02:12:26 PM 0.00 80.81 0.00 0.00 105.05 ## top命令是前面好几个命令检查内容的汇总 top 不谈架构，看看如何从代码层面优化系统性能！http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2650993458&amp;idx=1&amp;sn=e959385bc0bddb4b7cfab84a1310f9e8&amp;scene=21 数据库死锁问题改进：使用Redis做分布式锁；使用主键防重方法；使用版本号机制防重； 数据库事务占用时间过长：事务代码要尽量小，将不需要事务控制的代码移出； CPU时间被占满：C3P0在大并发下性能差，改成使用AKKA； 日志打印问题：统一日志输出规范、日志输出行号有锁去除行号； 微服务那么热，创业公司怎么选用实践？http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2650994360&amp;idx=1&amp;sn=dd2664e2db6bfc7427a4ea738899840e&amp;scene=21 SOA没有大范围取代单体应用的原因：其好处主要来自项目模块化而非模块服务化；没有解决多服务运维这个核心问题； 微服务的九大特征：服务即组件、按照业务域来组织微服务、按产品而非项目划分微服务、关注业务逻辑而非服务间通讯、分散式管理、分散式数据、基础设施自动化、容错、进化； 何时不需要微服务：你的代码没有模块化、你的服务要求极高性能、你没有一个好的容器编排系统； 技术选型：容器编排系统Kubernetes、编程语言Go、在线监控Prometheus+Grafana、离线数据分析fluentd+ODPS、同步通讯gRPC+HTTP Restful、异步通讯RabbitMQ、持续集成Jenkins、Docker私有仓库Harbor； 深入理解G1垃圾收集器http://ifeve.com/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3g1%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8/ G1相比CMS的优势：压缩空间方面有优势、通过Region避免内存碎片、可设置预期停顿时间避免应用雪崩、可在Young GC中使用； 一次完整的过程：YGC、并发阶段、混合模式、Full GC； ## YGC：在Eden充满时触发，回收之后所有值钱Eden的清空，至少有一个Survivor区，同时有一些数据移到了Old区； 23.430: [GC pause (young), 0.23094400 secs] ... [Eden: 1286M(1286M)-&gt;0B(1212M) Survivors: 78M-&gt;152M Heap: 1454M(4096M)-&gt;242M(4096M)] [Times: user=0.85 sys=0.05, real=0.23 secs] ## 并发阶段：这个阶段主要是发现哪些区域包含可回收的垃圾最多 ## 首先是初始标记阶段，该阶段会Stop-The-World，执行一次YGC 50.541: [GC pause (young) (initial-mark), 0.27767100 secs] [Eden: 1220M(1220M)-&gt;0B(1220M) Survivors: 144M-&gt;144M Heap: 3242M(4096M)-&gt;2093M(4096M)] [Times: user=1.02 sys=0.04, real=0.28 secs] ## 接下来，G1开始扫描根区域（GC Root），这个过程是后台线程并行处理，不暂停应用线程 50.819: [GC concurrent-root-region-scan-start] 51.408: [GC concurrent-root-region-scan-end, 0.5890230] ## 接下来，开始进入并发标记阶段，该阶段也是后台线程执行的 111.382: [GC concurrent-mark-start] .... 120.905: [GC concurrent-mark-end, 9.5225160 sec] ## 然后是二次标记阶段和清理阶段：这两个阶段会暂停应用线程，但实际很短 120.910: [GC remark 120.959: [GC ref-PRC, 0.0000890 secs], 0.0718990 secs] [Times: user=0.23 sys=0.01, real=0.08 secs] 120.985: [GC cleanup 3510M-&gt;3434M(4096M), 0.0111040 secs] [Times: user=0.04 sys=0.00, real=0.01 secs] ## 再之后还有额外的一次并发清理阶段 120.996: [GC concurrent-cleanup-start] 120.996: [GC concurrent-cleanup-end, 0.0004520] ## 混合GC：会同时进行YGC和清理上阶段标记为清理的区域，一直持续到几乎所有的标记区域垃圾对象都被回收 79.826: [GC pause (mixed), 0.26161600 secs] .... [Eden: 1222M(1222M)-&gt;0B(1220M) Survivors: 142M-&gt;144M Heap: 3200M(4096M)-&gt;1964M(4096M)] [Times: user=1.01 sys=0.00, real=0.26 secs] 京东分布式服务跟踪系统-CallGraphhttp://mp.weixin.qq.com/s/gy2a_nbYfUJq7DhlDFio6A 产生背景：SOA化和微服务；基于Google发表的分布式日志跟踪论文；相似的有淘宝鹰眼和新浪WatchMan； 核心概念：调用链包含了从源头请求到最后底层系统的所有环节，中间通过全局唯一的TraceID透传； 特性及使用场景：方法调用关系（单次调用的问题排查）、应用依赖关系（容量规划、调用来源、依赖度量、调用耗时、调用并行度、调用路由）、与业务数据集成（将公司业务与第三方业务进行关联）； 设计目标：低侵入性、低性能影响、灵活的应用策略、时效性； 实现架构：核心包（被各中间件引用，完成具体的埋点逻辑，日志存放在内存磁盘上由Agent收集发送到JMQ）、JMQ（充当日志数据管道）、Storm（对数据日志并行整理和计算）、存储（实时数据存储有JimDB/HBase/ES，离线数据存储包括HDFS和Spark）、CallGraph-UI（用户交互界面）、UCC（存放配置信息并同步到各服务器）、管理元数据（存放链路签名与应用映射关系等）； 埋点和调用上下文透传：前端利用Web容器的Filter机制调用startTrace开启跟踪，调用endTrace结束跟踪；各中间件调用clientSend、serverRecv、serverSend和clientRecv等API；对于进程间的上下文透传，调用上下文放在ThreadLocal；对于异步调用，通过Java字节码增强方式织入，以透明的方式完成线程间上下文的透传。 日志格式设计：固定部分（TraceID、RpcID、开始时间、调用类型、对端IP、调用耗时、调用结果等）、可变部分； 高性能的链路日志输出：开辟专门的内存区域并虚拟成磁盘设备，产生的日志存放在这样的内存设备，完全不占用磁盘IO；专门的日志模块，输出采用批量、异步方式写入，并在日志量过大时采取丢弃日志； TP日志和链路日志分离：链路日志通常开启采样率机制，比如1000次调用只收集1次；但是对于TP指标来说，必须每次记录，因此这两种数据是独立处理互不影响； 实时配置：通过CallGraph-UI和UCC实时配置，支持基于应用、应用分组、服务器IP多维度配置； 秒级监控：针对业务对实时分析的需求，采用JimDB存放实时数据，针对来源分析、入口分析、链路分析等可以提供1小时内的实时分析结果； 未来之路：延迟更低、完善错误发现和报警、借助深度学习挖掘价值。 京东消息中间件的演进http://mp.weixin.qq.com/s/4dsdpL-9SqQWybb02tOpXQ 第一代JMQ 选型：基于ActiveMQ做消息核心，基于MySQL和ZooKeeper做配置中心，管理监控平台自己研发； 如何存储：ActiveMQ使用KahaDB存储引擎进行存储，BTree索引。为了保证可靠性，索引文件和日志文件都需要同步刷盘；一个Topic有多个订阅者，就为每个订阅者创建一个队列，broker会将消息复制多份； 如何支持集群：当时原生的ActiveMQ客户端看是不支持服务集群化的，所以采用ZK进行扩展，使客户端支持了集群的同时还实现了对服务器动态扩展的支持； 推还是拉：ActiveMQ采用的是push模式，消息由producer发送到broker端之后由broker推送给consumer； 如何处理失败消息：原生ActiveMQ会在失败之后将消息放到死信队列，该队列的信息不能被消费者所获取；扩展的方式是拦截错误信息，写入重试服务库之后给队列返回消费成功的ACK，而后consumer通过一定策略消费重试库里面的消息； 其他优化和扩展：生成消息轨迹、优化broker写逻辑提升性能、新的主从复制、新的主从选举、增加监控模块； 第二代JMQ 第一代JMQ的问题：新的跨机房部署问题、Broker的性能随消息积压而急剧下降、Broker对Topic的订阅复制影响性能、Broker逻辑复杂，无法扩展消息回放、顺序消息和广播消息等、重客户端等； 开启了JMQ的自研：JMQ服务端（实现轻量级的存储模型、支持消息回放、支持重试、支持消息轨迹、支持顺序消息、支持广播信息等，并兼容AMQ客户端使用的OpenWire协议）、JMQ客户端（轻量级只和Broker通讯，支持动态接收参数、内置性能采集、支持跨机房）、管理控制平台、HTTP代理（基于Netty，支持跨语言）； 如何解决IO问题：使用Netty 4.0减少服务端开发，在应用层自定义JMQ协议； 如何存储消息：日志文件journal（主要存储消息内容，包括消息所在队列文件的位置）、消息队列文件queue（主要存储消息所在日志文件的全局偏移量）、消费位置文件offset（存储不同订阅者针对某个topic所消费到的队列的一个偏移量）都保存在Broker所在机器的本地磁盘上。 如何容灾：采用一主一从，至少一个备份，主从分布在同一个数据中心、备份分布在其他数据中心，主从复制同步、备份异步复制。 推还是拉：采用pull模式，由consumer主动发起请求去broker上取消息； 如何处理失败消息：JMQ的broker直接就支持重试，consumer处理消息失败时直接向服务端发送一个重试消息命令，服务端接到命令后将此消息入库；consumer在拉取消息时，服务端根据一定策略从库里取出消息给consumer处理； 如何管理元数据：客户端不再直接连接ZooKeeper，连接Broker获取元数据； 第三代JMQ 几个重要的目标：优化JMQ协议、优化复制模型、实现Kafka协议兼容、实现全局负载均衡、实现全新的选举方案、实现资源的弹性调度。 京东JIMDB建设之路http://mp.weixin.qq.com/s/IzYj3R1mfFpd1pq-xYhbQg JimDB的特性：一键创建集群实例、在线全自动弹性伸缩、部分复制扩容、在线平滑升级、全自动故障恢复、支持多语言接入、支持多种读取策略、溶强化部署、增量复制； 主要包括：Server（提供KV服务，支持一主多从和读写分离）、Config Server（复制集群拓扑的维护）、Sentinel（用于判断服务端实例存活状态）、Failover（负责角色切换和故障实例的替换）、Scaler（当内存容量或者流量等达到阈值时对分片进行分裂扩容）、Info Collector（负责监控数据的采集）、Resource Manager（负责物理机资源的管理和容器的创建）； 自研第一版主要解决以下问题：精确的故障检测和自动故障切换（机房不同机架部署多个探测实例，只要有一个探测到存活就是存活的，没有反馈存活且超过半数认为其死亡则认为死亡）、无损扩容（服务端数据按slot进行组织，迁移时以slot为单位进行迁移）、提供监控和报警等服务； 自研第二版：自动弹性调度（利用监控指标和阈值进行扩容和缩容）、服务端升级（引入docker）、资源隔离（物理机分区，集群分区）、大KEY扫描、读策略优化； 现有系统的完善和改进：完善弹性调度、新特性（丰富数据结构、版本号、支持HashTag）、丰富监控和性能统计数据、客户端增加本地缓存功能、新客户端支持异步发送、大KEY的应急处理、支持KEY按范围扫描等。 奇虎360开源其日志搜索引擎，可处理百万亿级的数据http://mp.weixin.qq.com/s/JhJ709gBeVNjViIFbngoRQ Poseidon系统是一个日志搜索平台，可以在百万亿条、100PB 大小的日志数据中快速分析和检索； 设计目标：原始数据不要额外存储、当前的Map/Reduce作业不用变更、自定义分词策略、秒级查询相应； 所用技术：倒排索引（构建日志搜索引擎的核心技术）、Hadoop（用于存放原始数据和索引数据，并用来运行Map/Reduce程序来构建索引）、Java（构建索引时是用Java开发的Map/Reduce程序）、Golang（检索程序是用Golang开发的）、Redis/Memcached（用于存储 Meta 元数据信息）； 为什么说传统分布式事务不再适用于微服务架构？http://mp.weixin.qq.com/s/wPeDzVk7UKMFXNWyzUyugg 传统分布式事务不是微服务中数据一致性的最佳选择：单机数据库的ACID、分布式数据库的两阶段提交协议（2PC）；对于微服务，数据是微服务私有的且SQL和NoSQL混合使用，2PC很难适用； 微服务架构中应满足数据最终一致性原则：所用副本经过一段时间后最终能够达成一致； 微服务架构实现最终一致性的三种模式：可靠事件模式（保证可靠事件投递和避免重复消费）、业务补偿模式（使用一个额外的协调服务来协调各个需要保证一致性的微服务，关键在于业务流水的记录）和TCC模式（一个完整的TCC业务由一个主业务服务和若干个从业务服务组成，主业务服务发起并完成整个业务活动，从服务提供三个接口Try、Confirm和Cancel）； 对账是最后的终极防线 兼顾高可靠和低延迟，Google打算用QUIC协议替代TCP/UDPhttp://mp.weixin.qq.com/s/O01HkvvpluaqzTyoxd7d8g TCP协议连接建立的成本相对较高；UDP协议是无连接协议，这样的好处是在网络传输层无需对数据包进行确认，但存在的问题就是为了确保数据传输的可靠性，应用层协议需要自己完成包传输情况的确认；QUIC协议可以在1到2个数据包内，完成连接的创建（包括TLS）； QUIC协议的主要目的，是为了整合TCP协议的可靠性和UDP协议的速度和效率。对于Google来说优化TCP协议是一个长期目标，QUIC旨在创建几乎等同于TCP的独立连接，但有着低延迟，并对类似SPDY的多路复用流协议有更好的支持。如果QUIC协议的特性被证明是有效的，这些特性以后可能会被迁移入后续版本的TCP和TLS协议（它们都有很长的开发周期）。 QUIC协议特性：避免前序包阻塞、减少数据包、向前纠错、会话重启和并行下载； 配置高性能ElasticSearch集群的9个小贴士http://mp.weixin.qq.com/s/jfXxpQXxvPzpFG_NOd6j0A 规划索引、分片以及集群增长情况 在配置前了解集群的拓扑结构：设置Master Node和Data Node； 内存设置：”bootstrap.mloclall: true”允许ES节点不交换内存； discovery.zen属性控制ElasticSearch的发现协议：discover.zen.fd.ping_timeout属性控制超时、discovery.zen.minimum_master_nodes属性决定了有资格作为master的节点的最小数量、discovery.zen.ping.unicast.hosts属性指定一组通信主机； 当心DELETE _all：通过设置action.destructive_requires_name:true来禁用； 使用Doc Values：本质上是将ES转换成一个列式存储，从而使ES的许多分析类特性在性能上远超预期； ElasticSearch配额类属性设置指南：属性cluster.routing.allocation.cluster_concurrent_rebalance决定了允许并发再平衡的分片数量，属性cluster.routing.allocation.disk.threshold_enabled值为true（默认值），在分配分片到一个节点时将会把可用的磁盘空间算入配额内。 Recovery属性允许快速重启 线程池属性防止数据丢失 基于 Kafka 和 ElasticSearch，LinkedIn是如何构建实时日志分析系统的？http://mp.weixin.qq.com/s/4dkaOWtEw-weLBI73A0JzQ V1方案是ELK（Log通过Logstash读出来放到Elasticsearch中，然后Kibana去读）；存在Logstash Agent维护不理想和log标准化问题； V2引入Kafka后，不需要每个host上都有Agent；通过Java Container Logger处理不同类型的日志； V3按照业务功能拆分ELK Cluster；将Logstash和Elasticsearch分开运行； V4引入Tribe解决跨数据中心Elasticsearch集群性能问题； V5采用冷热分区解决数据访问速度问题； 究竟啥才是互联网架构“高可用”http://mp.weixin.qq.com/s/7nfSvxZ4vJAxpIN5rCdaCw 单点是系统高可用的大敌，高可用保证的原则是集群化或者叫冗余。通过自动故障转移来实现系统的高可用； 整个互联网分层系统架构的高可用，又是通过每一层的冗余+自动故障转移来综合实现的，具体的：（1）【客户端层】到【反向代理层】的高可用，是通过反向代理层的冗余实现的，常见实践是keepalived + virtual IP自动故障转移（2）【反向代理层】到【站点层】的高可用，是通过站点层的冗余实现的，常见实践是nginx与web-server之间的存活性探测与自动故障转移（3）【站点层】到【服务层】的高可用，是通过服务层的冗余实现的，常见实践是通过service-connection-pool来保证自动故障转移（4）【服务层】到【缓存层】的高可用，是通过缓存数据的冗余实现的，常见实践是缓存客户端双读双写，或者利用缓存集群的主从数据同步与sentinel保活与自动故障转移；更多的业务场景，对缓存没有高可用要求，可以使用缓存服务化来对调用方屏蔽底层复杂性（5）【服务层】到【数据库“读”】的高可用，是通过读库的冗余实现的，常见实践是通过db-connection-pool来保证自动故障转移（6）【服务层】到【数据库“写”】的高可用，是通过写库的冗余实现的，常见实践是keepalived + virtual IP自动故障转移 究竟啥才是互联网架构“高并发”http://mp.weixin.qq.com/s/AMPIwgParjbLUBuCxUCYmw 高并发（High Concurrency）是互联网分布式系统架构设计中必须考虑的因素之一，它通常是指，通过设计保证系统能够同时并行处理很多请求。提高系统并发能力的方式，方法论上主要有两种：垂直扩展（Scale Up）与水平扩展（Scale Out）。前者垂直扩展可以通过提升单机硬件性能，或者提升单机架构性能，来提高并发性，但单机性能总是有极限的，互联网分布式架构设计高并发终极解决方案还是后者：水平扩展。互联网分层架构中，各层次水平扩展的实践又有所不同：（1）反向代理层可以通过“DNS轮询”的方式来进行水平扩展；（2）站点层可以通过nginx来进行水平扩展；（3）服务层可以通过服务连接池来进行水平扩展；（4）数据库可以按照数据范围，或者数据哈希的方式来进行水平扩展；各层实施水平扩展后，能够通过增加服务器数量的方式来提升系统的性能，做到理论上的性能无限。 自动化单元测试的落地方法，高效高质量部署并不难！https://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2650995338&amp;idx=1&amp;sn=a9b14ea359a00b48c9bb5483e058b860 是否值得：Martin Fowler在博客中解释了TestPyramid，单元测试是整个金字塔的基石；实施单元测试，并不代表你的生产效率能提高迅猛，反而有时候阻碍了瞬间的生产效率，但是它最直接的是提升产品质量，从而提升市场的形象，间接才会提升生产效率； 关键部分：自动化单元测试有四个关键组成部分要做到统一，如下图所示： 遵循流程：自动化单元测试的典型工作流程如下： 自动化单元测试原则：隔离UI操作；隔离数据库以及文件读写网络开销等操作；使用Mock替身与Spring容器隔离；设计简单的测试；定义测试套件的运行时间； 落地实践：Jenkins配置构建触发器推荐使用PollSCM；在Maven的pom.xml中配置sonar服务器信息；所有单元测试继承MockitoTestContext父类使Mockito相关注解生效； 最后，来一张图总结使用自动化单元测试前后的对比： 十分钟入门RocketMQhttp://jm.taobao.org/2017/01/12/rocketmq-quick-start-in-10-minutes/ 消息中间件需要解决哪些问题：发布订阅、消息优先级、消息有序性、消息过滤、消息持久化、消息可靠性、低延时、消息必须投递一次、消息只能被发送和消费一次、Broker的buffer满了怎么办、回溯消费、消息堆积、分布式事务、定时消息、消息重试； RocketMQ的特点：是一个队列模型的消息中间件，具有高性能、高可靠、高实时、分布式特点；Producer、Consumer、队列都可以分布式；Producer向一些队列轮流发送消息，队列集合称为Topic，Consumer如果做广播消费，则一个consumer实例消费这个Topic对应的所有队列，如果做集群消费，则多个Consumer实例平均消费这个topic对应的队列集合；能够保证严格的消息顺序；提供丰富的消息拉取模式；高效的订阅者水平扩展能力；实时的消息订阅机制；亿级消息堆积能力；较少的依赖； RocketMQ物理部署结构：Name Server是一个几乎无状态节点，可集群部署，节点之间无任何信息同步；Broker部署相对复杂，Broker分为Master与Slave，一个Master可以对应多个Slave，但是一个Slave只能对应一个Master，Master与Slave的对应关系通过指定相同的BrokerName，不同的BrokerId来定义，BrokerId为0表示Master，非0表示Slave，Master也可以部署多个，每个Broker与Name Server集群中的所有节点建立长连接，定时注册Topic信息到所有Name Server；Producer与Name Server集群中的其中一个节点（随机选择）建立长连接，定期从Name Server取Topic路由信息，并向提供Topic服务的Master建立长连接，且定时向Master发送心跳，Producer完全无状态，可集群部署；Consumer与Name Server集群中的其中一个节点（随机选择）建立长连接，定期从Name Server取Topic路由信息，并向提供Topic服务的Master、Slave建立长连接，且定时向Master、Slave发送心跳，Consumer既可以从Master订阅消息，也可以从Slave订阅消息，订阅规则由Broker配置决定； RocketMQ逻辑部署结构：如下图所示，RocketMQ的逻辑部署结构有Producer Group和Consumer Group； Rocket数据存储结构：如下图所示，采取了一种数据与索引分离的存储方法，有效降低文件资源、IO资源、内存资源的消耗。 NetflixOSS：Hollow正式发布http://www.infoq.com/cn/articles/netflixoss-hollow-officially-released Hollow是一种Java库，为中小规模的内存中数据集提供了一套全面的工具，适合从单一生成方到多个消耗方等不同场景下的数据只读访问；它会根据数据集调整自己的规模； Hollow在内存中保留一份完整的、可供使用的只读数据集，借此可规避从不完整的缓存中更新和逐出数据所产生的后果； Hollow不仅有助于改善性能，还可以大幅促进团队处理与数据有关的任务时的敏捷性；Hollow可根据指定的数据规模自动生成自定义API，可以极为迅速地将包含当前数据或过去时点的整个生成数据集分流到本地开发工作站，还包含大量已经开发完成可以使用的工具； 数据集具体变化的时间线可拆分为多个离散的数据状态，每个状态都是数据在特定时间的一个完整快照；Hollow可自动生成不同状态之间的增量，因此消耗方只需做最少量的工作即可保持自己所用数据为最新版本；Hollow会自动进行数据去重，借此将消耗方所有数据集的堆占用空间将至最低； Hollow并未使用POJO作为内存中的具体呈现，而是使用了一种更紧凑的定长强类型数据编码方式；该编码方式可将数据集的堆占用空间和随时访问数据的CPU消耗降至最低；所有编码后的记录会打包为可重用的内存块（Slab），并在JVM堆的基础之上进行池化，借此避免服务器高负载时对GC行为产生影响；以下是Object类型记录在内存中布局方式的一种范例： Hollow技术的核心在于通过不同方式对数据创建的索引，以便可灵活访问数据中的相关记录，并构成强大的访问模式，而无须考虑数据模型最初的设计是否考虑过这种访问模式； Hollow的配套工具非常易于设置和使用，历史工具可用于检测记录在一段时间以来的变化情况； 免责声明：相关链接版本为原作者所有，如有侵权，请告知删除。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[一起读Spring Framework文档（1）：概述与新特性]]></title>
      <url>%2Fpost%2Fspring_framework_reference_notes_part1_overview_and_newfeature%2F</url>
      <content type="text"><![CDATA[主要包括Spring Framework的介绍、快速入门、模块体系以及4.x各个版本的新特性和改进。 阅读说明： Spring经过多年的发展，现在已经有多个成熟的项目，包括Spring Framework、Spring Boot、Spring Cloud、Spring Data等，但是大家最熟知的应该还是Spring Framework，在这一系列的博文中，如无特殊说明，Spring指的皆为Spring Framework； Spring中很大一部分我没有用过，在笔记中可能存在错误描述，欢迎指正。另外如果有些词不好翻译，会保留英文单词； 该笔记阅读的文档是Spring最新的4.3.5版本，官网地址可能随着新版本的发布而替换； 部分内容会因为个人喜好进行增减，如需完整阅读，可以阅读官网原文。 第一部分 Spring概述 轻量级的开源框架； 潜在的一站式企业级应用开发解决方案； 模块化的设计； 非侵入式的设计； 支持声明式的事务管理； 提供全功能的MVC框架； 能将AOP透明地集成； 1. Spring快速入门Spring Framework Reference主要介绍的是框架的详细信息，如果是新手，建议从官的Guides的实例入手（比如Building REST services with Spring这个就超级棒），这些实例很多都是基于Spring Boot来运行。 2. Spring介绍Spring为开发Java提供了基础架构的支持，基于普通的POJO使得开发J2EE更加轻松。 2.1 依赖注入和控制反转通常一个Java程序包含了大量的对象需要管理，而这些对象之间又存在各种依赖关系。这时虽然一些设计模式如工厂模式、构造器、服务定位的方式可以采用，但是更好的方式利用一个统一的模式：我们只需描述是什么以及在哪里使用即可，剩下的一起交给框架去实现。 Spring的控制反转组件（Inversion of Control, IOC）开箱即用，其通过一系列的组件来形成统一的模式来实现。Spring的IOC是Spring最经典的设计之一，越来越多的公司采用这种方案。 关于IOC，应该读读Martin Fowler大神的《Inversion of Control Containers and the Dependency Injection pattern》一文。 2.2 Spring的模块Spring现在已经发展到包括大约20个模块，按功能可分为：核心容器、数据访问与集成、Web开发、面向切面（AOP）、类代理、消息和测试，整体框架图如下所示： 2.2.1 核心容器 spring-core和spring-beans是Spring最基础的模块，提供了控制反转（IOC）和依赖注入（DI）特性。BeanFactory接口解耦了程序中的配置和依赖关系。 spring-context使得访问对象更加便捷，还增加了对国际化、事件传播、资源加载的支持，以及透明化创建上下文的能力。ApplicationContext是该模块的核心接口。 spring-context-support提供了常见的第三方库集成到Spring上下文的支持，如缓存（EhCache, Guava, JCache）、通知（JavaMail）、调度（CommonJ, Quartz）和模板引擎（FreeMarker, JasperReports, Velocity）。 spring-expression提供了一个强大的表达式语言（EL）来查询和操作运行时的对象图。该表达式语言支持属性赋值、方法调用、访问集合、算术和逻辑运算等能力。 2.2.2 AOP和类代理（Instrumentation） spring-aop提供了面向切面编程（Aspect Oriented Programming, AOP）的能力，利用方法拦截器和切入点能使原始方法和新的逻辑完全分离。使用源码级的元数据功能，还可以将行为信息合并到你的代码中。 spring-aspects集成了AspectJ（另外一个流行的AOP利器）； spring-instrument为应用服务器提供了类代理支持（注：这个地方叫做类代码不知道正确与否，主要是参考了Java SE 6 新特性: Instrumentation 新功能一文 随着后面的阅读再来更正）和类加载器实现。 spring-instrument-tomcat模块是为Tomcat服务器实现的类代理。 2.2.3 消息spring-messaging模块为信息通信处理提供了一套包含抽象概念的实现，包括Message，MessageChannel，MessageHandler等，另外还提供了一些易用的注解。 2.2.4 数据访问与集成 spring-jdbc 模块提供了一个JDBC抽象层，消除冗长的JDBC编码和数据库厂商特有的错误码。 spring-tx模块支持编程和声明式事务，而这只需实现特定的接口甚至是简单的POJO。 spring-orm模块集成了流行的对象关系映射（ORM）API，包括JPA、JDO和Hibernate。 spring-oxm模块提供了“对象XML映射”抽象实现，支持JAXB, Castor, XMLBeans, JiBX和XStream。 spring-jms模块包含了生产和消费信息的功能。4.1版本后还提供了与spring-messaging模块的集成。 2.2.5 Web开发 spring-web模块提供了基本的面向Web的集成特性，例如方文件上传功能、使用Servlet的监听器以及一个面向Web应用程序上下文IoC容器的初始化。它还包含一个HTTP客户端和远程访问相关的部分。 spring-webmvc模块包含Spring的模型-视图-控制器（MVC）和REST Web服务实现。Spring MVC框架使得领域模型代码和Web展示的代码完全分离，并能与其他Spring功能无缝集成。 spring-webmvc-portlet模块为Portlet环境提供了Spring MVC实现。 2.2.6 测试spring-test模块支持Spring这些组件基于Junit或TestNG的单元测试和集成测试。它提供了一致的上下文加载和缓存，另外它还使得对象的Mock更加容易。 2.3 Spring的使用场景++典型的基于Spring的Web应用++ 持久化层可以利用spring-orm来实现对象关系映射，用spring-tx来声明事务； 逻辑层可以基于POJO，并使用IOC容器来管理对象；另外还可以使用邮件发送通知，远程服务实现RPC； 展示层可以使用spring-webmvc分离逻辑和展示； ++与其他Web开发框架集成++ ++远程服务调用++ ++EJB集成++ 2.3.1 依赖管理与命名约定 Maven依赖仓库：Maven的中央仓库和一些特别为Spring建立的公共Maven仓库； 命名约定：GroupId为org.springframework，ArtifactId为各个模块名如spring-aop； 最小化依赖原则：不必依赖所有的模块，按需依赖； Maven依赖配置： &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;4.3.5.RELEASE&lt;/version&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; Gradle依赖配置： repositories { mavenCentral() // and optionally... maven { url &quot;http://repo.spring.io/release&quot; } } dependencies { compile(&quot;org.springframework:spring-context:4.3.5.RELEASE&quot;) testCompile(&quot;org.springframework:spring-test:4.3.5.RELEASE&quot;) } 2.3.2 日志框架日志框架依赖的问题对于Spring框架非常的重要，因为： 它是Spring仅有的必须的外部依赖； 应用程序本身也离不开日志； Spring所集成的第三方框架本身也会依赖日志框架； Spring采取的方案是在spring-core模块中依赖commons-logging，其他模块通过传递依赖来依赖该模块。这样的好处是你无须做特别的处理，程序会在classpath或特定的路径寻找最合适的日志框架，即使找不到也会有看着不错的日志输出。 另外一种选择是放弃commons-logging改用SLF4J（Simple Logging Facade for Java，基于门面模式本身并不负责记录日志、支持常用的日志框架），依赖配置如下： &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-core&lt;/artifactId&gt; &lt;version&gt;4.3.5.RELEASE&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;commons-logging&lt;/groupId&gt; &lt;artifactId&gt;commons-logging&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;jcl-over-slf4j&lt;/artifactId&gt; &lt;version&gt;1.5.8&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;1.5.8&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;version&gt;1.5.8&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.14&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 第二部分 Spring框架4.x版本新特性3. Spring 4.0版本新特性与改进Spring自从2004年发布以来，经历了多个重大版本的演变。在Spring 2.0中提供了XML命名空间和AspectJ支持；在Spring 2.5中拥抱注解驱动的配置；在Spring 3.0在框架代码中大量使用Java 5+的特性，如@Configuration。 Spring 4.0是目前的最新主要版本，首次支持Java 8的功能，当前你也可以使用旧版本的Java，但是最低要求是Java 6。 在这个版本中，Spring清除了很多过时的类和方法。具体升级时可以参考migration guide for upgrading to Spring 4.0指南。 3.1 改进快速入门体验网站改版，同时例子补充得更完整更易理解 3.2 移除废弃的包和类同时对第三方依赖的框架也进行了版本升级 3.3 对Java 8的支持包括Lambda表达式、方法引用、java.time； 3.4 对Java EE 6和7的支持 J2EE 6：JPA 2.0, Servlet 3.0; J2EE 7：JMS 2.0, JTA 1.2, JPA 2.1, Bean Validation 1.1, JSR-236; 3.5 使用Groovy的领域专业语言（DSL）定义Bean举个栗子： def reader = new GroovyBeanDefinitionReader(myApplicationContext) reader.beans { dataSource(BasicDataSource) { driverClassName = &quot;org.hsqldb.jdbcDriver&quot; url = &quot;jdbc:hsqldb:mem:grailsDB&quot; username = &quot;sa&quot; password = &quot;&quot; settings = [mynew:&quot;setting&quot;] } sessionFactory(SessionFactory) { dataSource = dataSource } myService(MyService) { nestedBean = { AnotherBean bean -&gt; dataSource = dataSource } } } 3.6 核心容器改进 注入Bean时支持泛型作为一个标识符，如@Autowired Repository customerRepository； Spring的元注解支持定制来实现暴露源注解的特定属性； 集合和数组属性在装配时支持排序； @Lazy注解允许在注入点和@Bean定义时使用； 引入新的@Description注解； 通过@Conditional注解实现了条件过滤Bean，和@Profile不同之处在于@Conditional可以通过自定义编程来过滤； 基于CGLIB的代理类不再需要默认的构造函数，从而实现了无须调用构造函数的代理； 通过LocaleContext支持对时区进行管理； 3.7 Web开发改进 全面采用Servlet 3； 增加@RestController注解，这样无须在RequestMapping的方法上添加@ResponseBody注解； AsyncRestTemplate支持非阻塞异步REST请求； Spring MVC中更易理解的时区机制； 3.8 WebSocket、SockJS以及STOMP通信 增加了spring-websocket模块支持JSR-356; 增加了spring-messaging模块支持STOMP，另外该工程还对消息通信进行了抽象以便给其他模块集成； 3.9 测试方面的改进 几乎所有的Spring注解都可以在测试代码中使用； Active bean definition profiles可以通过自定义编码来实现； spring-core工程的SocketUtils类可以扫描本地空闲的TCP和UDP端口； 大部分的Servlet相关的Mock类都升级到Servlet 3.0版本了； 4. Spring 4.1版本新特性与改进4.1 JMS的改进 Spring 4.1引进了一个更简单的方式来注册JMS监听，即给Bean的方法上增加@JmsListener注解，同时XML命名空间上也增加了jms:annotation-driven，使用JmsListenerConfigurer还可以实现完全自定义编程的注册。 由于spring-messaging模块的抽象定义，消息监听可以变得更灵活。另外像@Payload、@Header、@Headers、@SendTo这样的注解的引入使得代码编写更加清晰； JmsMessageOperations的引入使得JmsTemplate调用更加便捷； JmsTemplate支持同步的请求应答； 可以给多个设置优先级； 支持JMS 2.0的共享消费者； 4.2 缓存改进 在无须更改现有的缓存配置下支持JCache (JSR-107)； 使用CacheResolver支持运行期的缓存，从而缓存的value参数不是必须的了； 支持更多操作级的定制，如缓存使用、缓存管理、缓存key生成； 通过类级别的@CacheConfig注解允许一些配置在类级别共享，而不需要任何额外的缓存操作； 通过CacheErrorHandler更好的异常处理； Cache接口增加了新的方法putIfAbsent； 4.3 Web开发改进 基于资源处理现有的支持ResourceHttpRequestHandler 已经扩展了新的抽象ResourceResolver，ResourceTransformer和ResourceUrlProvider；许多内置的实现支持多版本的URL资源、定位gzip压缩资源、生成HTML 5声明等能力。 @RequestParam、@RequestHeader和@MatrixVariable这些控制器的方法参数支持Java 8的java.util.Optional; 当一个服务已经返回ListenableFuture时，可以使用其作为返回来替代原本采用的DeferredResult； 按照相互依赖的顺序关系来调用@ModelAttribute； Jackson的@JsonView注解可以直接在带有@ResponseBody和ResponseEntity的控制器方法上使用； Jackson支持JSONP； 声明一个@ControllerAdvice的Bean可以在控制器方法已经结束但响应写入之前的时机被调用； HttpMessageConverter支持三个新的选项：Gson、Protobuf以及Jackson基于XML的序列化； JSP这样的视图文件可以通过名字定义指向控制器方法的链接； ResponseEntity提供了一个内建风格的API来声明服务器端的响应，例如方法ResponseEntity.ok()； RequestEntity提供了一个内置风格的API来声明客户端REST代码对HTTP请求； MVC方面：视图解析器现在可以配置内容negotiation、默认已支持视图控制器重定向和设置响应状态、默认已支持自定义的路由； 支持Groovy的标记模板； 4.4 WebSocket通信改进 支持SockJS客户端； 当STOMP客户端订阅和取消订阅时会发布新的上下文事件SessionSubscribeEvent和SessionUnsubscribeEvent； @SendToUser只针对单一会话不需要身份验证； @MessageMapping方法使用“.”替换原来的“/”作为路径分隔符； 4.5 测试方面的改进 通过TestTransaction API可以测试事务； 通过@Sql和@SqlConfig注解为每个类或方法执行SQL脚本； 通过@TestPropertySource注解实现测试属性值覆盖应用和系统的属性值； 5. Spring 4.2版本新特性与改进5.1 核心容器的改进 支持Java 8接口默认方法上的@Bean注解解析； Configration类可以在普通的组件类上使用@Import注解； Confugration类可以通过@Order注解实现排序； @Resource注入点支持@Lazy声明； 事件处理机制引入了基于注解模型的能力； 对注释的属性别名的声明和查找提供了先天的支持； 元注解的查找进行了多项优化； DefaultConversionService和DefaultFormattingConversionService提供了对字符、金额、时区的转换能力； 5.2 数据访问的改进 通过AspectJ支持javax.transaction.Transactional； 支持Hibernate ORM 5.0； 内置的数据库可以自动分配唯一的名称； 5.3 JMS的改进 通过JmsListenerContainerFactory可以控制autoStartup属性； 可以给每个监听的容器配置回复的Destination； 在同一方法上可以配置多个@JmsListener； 5.4 Web开发的改进 支持HTTP流和服务器发送事件； 内置支持CORS，包括全局和本地的配置； HTTP缓存机制优化包括新的CacheControl构造器、ETag/Last-Modified改进； 支持自定义的Mapping注解； 通过AbstractHandlerMethodMapping在运行时注册和取消注册请求Mapping； @Controller方法返回类型支持java.util.concurrent.CompletableFuture； RestTemplate集成okhttp； 5.5 WebSocket通信改进 暴露有关连接用户和订阅状态的信息； 解决跨服务器的集群用户destinations； StompSubProtocolErrorHandler来自定义STOMP错误处理； 5.6 测试方面的改进 基于JUnit的集成测试，现在可以使用JUnit的规则，而不是执行 SpringJUnit4ClassRunner。这使得基于Spring集成测试可以通过JUnit的执行器或第三方执行器（如MockitoJUnitRunner）来执行； 为HtmlUnit提供先天支持； AopTestUtils是一种新的测试工具类，它允许开发者获得隐藏着一个或多个Spring代理基础目标对象的引用； ReflectionTestUtils现在支持设置和获取static属性，包括常量； @Commit替换原来的 @Rollback(false)； 6. Spring 4.3版本新特性与改进6.1 核心容器改进 核心容器异常时提供更丰富的元数据信息； Bean属性的getters/setters支持Java 8的默认方法； 当注入一个Primary的Bean时，懒加载的候选Bean不会被立即创建； @Configuration 类支持构造函数注入； @Scheduled适用于任何范围的Bean； 6.2 数据访问的改进jdbc:initialize-database和jdbc:embedded-database可以给每个脚本应用单独的可配置separator； 6.3 缓存方面的改进 在指定Key上面的并发调用变成同步的，使得缓存值只会被计算一次；该特性需要通过@Cacheable的sync属性开启； Cache接口增加了get(Object key, Callable valueLoader)方法； ConcurrentMapCacheManager和ConcurrentMapCache现在可以通过storeByValue属性进行缓存的序列化； 6.4 JMS的改进 @SendTo现在可以在类级别指定共用的一个回复destination。 @JmsListener和@JmsListeners现在可以用作元注解来创建自定义的支持属性覆盖的复合注解； 6.5 Web开发的改进 内置支持HTTP HEAD和HTTP OPTIONS； 新的注解：@GetMapping、@PostMapping、@PutMapping、@DeleteMapping和@PatchMapping； 新的注解：@RequestScope、@SessionScope和@ApplicationScope； @ResponseStatus支持类级别并被所有方法继承； 在HTTP消息转换时采用一致的字符集处理； AsyncRestTemplate 支持请求拦截； 6.6 WebSocket通信改进@SendTo现在可以在类级别指定共用的一个回复destination。 6.7 测试方面的改进 Spring测试上下文需要Junit 4.12以上的版本； SpringJUnit4ClassRunner增加新的别名SpringRunner； 测试的有关注解现在可以在接口中声明； 6.8 第三方框架版本更新 Hibernate ORM 5.2 Hibernate Validator 5.3 Jackson 2.8 OkHttp 3.x Tomcat 8.5 Netty 4.1 Undertow 1.4 WildFly 10.1]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[一个程序员的小结：2016年的进步、收获与成长]]></title>
      <url>%2Fpost%2Fsummary_for_2016%2F</url>
      <content type="text"><![CDATA[2016余额不足，回顾总结再展望。 离开了工作五年多的通信公司，正式踏入互联网圈，算是离开了自己的舒适圈； 全新的业务、全新的架构、全新的技术栈，感觉又回到毕业时空空白白，不过通过半年的学习，大致了解了一个电商的架构体系、常用的中间件； 遇到了喜欢的开源项目Elasticsearch，也做了一些扩展，希望来年也为社区做点贡献； 以前开开停停的个人博客，今年终于下定决心坚持写下来； 坚持了两个月的番茄工作法，个人工作效率和目标完成上效果不错； 坚持阅读，今年大概阅读了26本书，精读了其中的10本。精度了订阅博客和社区文章50篇左右。 升级了电脑硬件，IDE切换到IDEA，编程环境更舒适了； 最后，适应了MarkDown的写作，基本丢弃了以前Word的写作方式，也算是一个进步； 开始试着参与知乎Live和GitChat这样的在线交流活动，学习一些新的技术或者深入地探讨到技术细节； 全年无通宵熬夜，噢耶~ 知乎链接：https://www.zhihu.com/question/52882320/answer/138164618]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[动手学习Elasticsearch中的Multi Match Query]]></title>
      <url>%2Fpost%2Felasticsearch_multi_match_query%2F</url>
      <content type="text"><![CDATA[在Elasticsearch全文检索中，我们用的比较多的就是Multi Match Query，其支持对多个字段进行匹配。Elasticsearch支持5种类型的Multi Match，我们一起来深入学习下它们的区别。 5种类型的Multi Match Query直接从官网的文档上摘抄一段来： best_fields: (default) Finds documents which match any field, but uses the _score from the best field. most_fields: Finds documents which match any field and combines the _score from each field. cross_fields: Treats fields with the same analyzer as though they were one big field. Looks for each word in any field. phrase: Runs a match_phrase query on each field and combines the _score from each field. phrase_prefix: Runs a match_phrase_prefix query on each field and combines the _score from each field. 这里我们只考虑前面三种，后两种可以另外单独研究，就先忽略了。 创建测试索引，预置测试数据创建gino_product索引PUT /gino_product { &quot;mappings&quot;: { &quot;product&quot;: { &quot;properties&quot;: { &quot;productName&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;analyzer&quot;: &quot;fulltext_analyzer&quot;, &quot;copy_to&quot;: [ &quot;bigSearchField&quot; ] }, &quot;brandName&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;analyzer&quot;: &quot;fulltext_analyzer&quot;, &quot;copy_to&quot;: [ &quot;bigSearchField&quot; ], &quot;fields&quot;: { &quot;brandName_pinyin&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;analyzer&quot;: &quot;pinyin_analyzer&quot;, &quot;search_analyzer&quot;: &quot;standard&quot; }, &quot;brandName_keyword&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;analyzer&quot;: &quot;keyword&quot;, &quot;search_analyzer&quot;: &quot;standard&quot; } } }, &quot;sortName&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;analyzer&quot;: &quot;fulltext_analyzer&quot;, &quot;copy_to&quot;: [ &quot;bigSearchField&quot; ], &quot;fields&quot;: { &quot;sortName_pinyin&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;analyzer&quot;: &quot;pinyin_analyzer&quot;, &quot;search_analyzer&quot;: &quot;standard&quot; } } }, &quot;productKeyword&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;analyzer&quot;: &quot;fulltext_analyzer&quot;, &quot;copy_to&quot;: [ &quot;bigSearchField&quot; ] }, &quot;bigSearchField&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;analyzer&quot;: &quot;fulltext_analyzer&quot; } } } }, &quot;settings&quot;: { &quot;index&quot;: { &quot;number_of_shards&quot;: 1, &quot;number_of_replicas&quot;: 0 }, &quot;analysis&quot;: { &quot;tokenizer&quot;: { &quot;simple_pinyin&quot;: { &quot;type&quot;: &quot;pinyin&quot;, &quot;first_letter&quot;: &quot;none&quot; } }, &quot;analyzer&quot;: { &quot;fulltext_analyzer&quot;: { &quot;type&quot;: &quot;ik&quot;, &quot;use_smart&quot;: true }, &quot;pinyin_analyzer&quot;: { &quot;type&quot;: &quot;custom&quot;, &quot;tokenizer&quot;: &quot;simple_pinyin&quot;, &quot;filter&quot;: [ &quot;word_delimiter&quot;, &quot;lowercase&quot; ] } } } } } 插入一些测试数据POST /gino_product/product/1 { &quot;productName&quot;: &quot;耐克女生运动轻跑鞋&quot;, &quot;brandName&quot;: &quot;耐克&quot;, &quot;sortName&quot;: &quot;鞋子&quot;, &quot;productKeyword&quot;: &quot;耐克，潮流，运动，轻跑鞋&quot; } POST /gino_product/product/2 { &quot;productName&quot;: &quot;耐克女生休闲运动服&quot;, &quot;brandName&quot;: &quot;耐克&quot;, &quot;sortName&quot;: &quot;上衣&quot;, &quot;productKeyword&quot;: &quot;耐克，休闲，运动&quot; } POST /gino_product/product/3 { &quot;productName&quot;: &quot;阿迪达斯女生冬季运动板鞋&quot;, &quot;brandName&quot;: &quot;阿迪达斯&quot;, &quot;sortName&quot;: &quot;鞋子&quot;, &quot;productKeyword&quot;: &quot;阿迪达斯，冬季，运动，板鞋&quot; } POST /gino_product/product/4 { &quot;productName&quot;: &quot;阿迪达斯女生冬季运动夹克外套&quot;, &quot;brandName&quot;: &quot;阿迪达斯&quot;, &quot;sortName&quot;: &quot;上衣&quot;, &quot;productKeyword&quot;: &quot;阿迪达斯，冬季，运动，夹克，外套&quot; } 测试数据总览 分别搜索【运动】POST /gino_product/_search { &quot;query&quot;: { &quot;multi_match&quot;: { &quot;query&quot;: &quot;运动&quot;, &quot;fields&quot;: [ &quot;brandName^100&quot;, &quot;brandName.brandName_pinyin^100&quot;, &quot;brandName.brandName_keyword^100&quot;, &quot;sortName^80&quot;, &quot;sortName.sortName_pinyin^80&quot;, &quot;productName^60&quot;, &quot;productKeyword^20&quot; ], &quot;type&quot;: &lt;multi-match-type&gt;, &quot;operator&quot;: &quot;AND&quot; } } } 发现使用3种type都可以搜索出4条商品数据，而且排序也是一致的。 分别搜索【运动 上衣】POST /gino_product/_search { &quot;query&quot;: { &quot;multi_match&quot;: { &quot;query&quot;: &quot;运动 上衣&quot;, &quot;fields&quot;: [ &quot;brandName^100&quot;, &quot;brandName.brandName_pinyin^100&quot;, &quot;brandName.brandName_keyword^100&quot;, &quot;sortName^80&quot;, &quot;sortName.sortName_pinyin^80&quot;, &quot;productName^60&quot;, &quot;productKeyword^20&quot; ], &quot;type&quot;: &lt;multi-match-type&gt;, &quot;operator&quot;: &quot;AND&quot; } } } 这次搜索只有cross_field才能搜索出数据，而使用best_fields和most_fields不行，为什么？ 使用validate API来比较区别 POST /gino_product/_validate/query?rewrite=true { &quot;query&quot;: { &quot;multi_match&quot;: { &quot;query&quot;: &quot;运动 上衣&quot;, &quot;fields&quot;: [ &quot;brandName^100&quot;, &quot;brandName.brandName_pinyin^100&quot;, &quot;brandName.brandName_keyword^100&quot;, &quot;sortName^80&quot;, &quot;sortName.sortName_pinyin^80&quot;, &quot;productName^60&quot;, &quot;productKeyword^20&quot; ], &quot;type&quot;: &lt;multi-match-type&gt;, &quot;operator&quot;: &quot;AND&quot; } } } best_fields：所有输入的Token必须在一个字段上全部匹配。每个字段匹配时分别使用mapping上定义的analyzer和search_analyzer。 (+brandName:运动 +brandName:上衣)^100.0 | (+brandName.brandName_pinyin:运 +brandName.brandName_pinyin:动 +brandName.brandName_pinyin:上 +brandName.brandName_pinyin:衣)^100.0 | (+brandName.brandName_keyword:运 +brandName.brandName_keyword:动 +brandName.brandName_keyword:上 +brandName.brandName_keyword:衣)^100.0 | (+sortName:运动 +sortName:上衣)^80.0 | (+sortName.sortName_pinyin:运 +sortName.sortName_pinyin:动 +sortName.sortName_pinyin:上 +sortName.sortName_pinyin:衣)^80.0 | (+productName:运动 +productName:上衣)^60.0 | (+productKeyword:运动 +productKeyword:上衣)^20.0 most_fields：所有输入的Token必须在一个字段上全部匹配。与best_fields不同之处在于相关性评分，best_fields取最大匹配得分（max计算），而most_fields取所有匹配之和（sum计算）。 ( (+brandName:运动 +brandName:上衣)^100.0 (+brandName.brandName_pinyin:运 +brandName.brandName_pinyin:动 +brandName.brandName_pinyin:上 +brandName.brandName_pinyin:衣)^100.0 (+brandName.brandName_keyword:运 +brandName.brandName_keyword:动 +brandName.brandName_keyword:上 +brandName.brandName_keyword:衣)^100.0 (+sortName:运动 +sortName:上衣)^80.0 (+sortName.sortName_pinyin:运 +sortName.sortName_pinyin:动 +sortName.sortName_pinyin:上 +sortName.sortName_pinyin:衣)^80.0 (+productName:运动 +productName:上衣)^60.0 (+productKeyword:运动 +productKeyword:上衣)^20.0 ) cross_fields：所有输入的Token必须在同一组的字段上全部匹配。首先ES会对cross_fields进行查询重写分组，分组的依据是search_analyzer。具体到我们的例子中【brandName.brandName_pinyin、brandName.brandName_keyword、sortName.sortName_pinyin】这三个字段的search_analyzer是standard，而其余的字段是fulltext_analyzer，因此最终被分为了两组。 ( ( +(brandName.brandName_pinyin:运^100.0 | sortName.sortName_pinyin:运^80.0 | brandName.brandName_keyword:运^100.0) +(brandName.brandName_pinyin:动^100.0 | sortName.sortName_pinyin:动^80.0 | brandName.brandName_keyword:动^100.0) +(brandName.brandName_pinyin:上^100.0 | sortName.sortName_pinyin:上^80.0 | brandName.brandName_keyword:上^100.0) +(brandName.brandName_pinyin:衣^100.0 | sortName.sortName_pinyin:衣^80.0 | brandName.brandName_keyword:衣^100.0) ) ( +(productKeyword:运动^20.0 | brandName:运动^100.0 | sortName:运动^80.0 | productName:运动^60.0) +(productKeyword:上衣^20.0 | brandName:上衣^100.0 | sortName:上衣^80.0 | productName:上衣^60.0) ) ) 继续探索和思考如何让best_fields和most_fields也可以匹配出商品？最常见的做法就是使用_all字段或者copyTo字段来实现，比如我们mapping里面的bigSearchField字段。 如何改进cross_fields的搜索结果？由于cross_fields需要根据search_analyzer进行分组，因此像搜索【运动 shangyi】这样的输入时是无法匹配到商品的，因此应该尽可能地减少分组既尽量使用统一的search_analyzer，或者在search时强制指定search_analyzer覆盖mapping里定义的search_analyzer。 把operator改成OR会如何？在上面的例子中，我们设置的operator均为AND，意味着所有搜索的Token都必须被匹配。那设置成OR会怎么样以及什么场景下该使用OR呢？ 在使用OR的时候要特别注意，因为只要有一个Token匹配就会把商品搜索出来，比如上面的搜索【运动 上衣】的时候，会把鞋子的商品也匹配出来，这样搜索的准确度会远远降低。 在一些特殊的搜索中，比如我们搜索【耐克 阿迪达斯 上衣】，如果使用operator为AND，则无论使用哪种multi-search-type都无法匹配出商品（想想为什么？），此时我们可以设置operator为OR并且设置minimum_should_match为60%，这样就可以搜索出属于耐克和阿迪达斯的上衣了，这种情况相当于一种智能的搜索降级了。 /gino_product/_search { &quot;query&quot;: { &quot;multi_match&quot;: { &quot;query&quot;: &quot;耐克 阿迪达斯 上衣&quot;, &quot;fields&quot;: [ &quot;brandName^100&quot;, &quot;brandName.brandName_pinyin^100&quot;, &quot;brandName.brandName_keyword^100&quot;, &quot;sortName^80&quot;, &quot;sortName.sortName_pinyin^80&quot;, &quot;productName^60&quot;, &quot;productKeyword^20&quot; ], &quot;type&quot;: &quot;cross_fields&quot;, &quot;operator&quot;: &quot;OR&quot;, &quot;minimum_should_match&quot;: &quot;60%&quot; } } } 再谈相关性评分在Elasticsearch相关性打分机制学习一文中我们曾经探讨过best_fields和cross_fields相关性评分的机制，其中的例子使用的相同的search_analyzer。那对于分组情况下，cross_fields评分又是如何计算的呢？ 我们还是用上面的例子，增加explain参数来看一下。 POST /gino_product/_search { &quot;explain&quot;: true, &quot;query&quot;: { &quot;multi_match&quot;: { &quot;query&quot;: &quot;运动 上衣&quot;, &quot;fields&quot;: [ &quot;brandName^100&quot;, &quot;brandName.brandName_pinyin^100&quot;, &quot;brandName.brandName_keyword^100&quot;, &quot;sortName^80&quot;, &quot;sortName.sortName_pinyin^80&quot;, &quot;productName^60&quot;, &quot;productKeyword^20&quot; ], &quot;type&quot;: &quot;cross_fields&quot;, &quot;operator&quot;: &quot;AND&quot; } } } 详细ES响应报文：cross_fields_scoring.json 通过上述validate API得到的分组信息和explain得到的评分详情信息，可以总结出一个cross_fields评分公式： score(q, d) = coord(q, d) * ∑(∑(max(score(t, f)))) coord(q, d): 分组匹配因子，比如上面我们只有一个分组匹配，coord就是0.5（两个分组中匹配了一个分组）； score(t, f): 搜索的一个Token和一个特定的字段的相关性评分（使用TFIDF）计算； max：搜索的一个Token在所有字段评分中取最大值； 分组内求和：一个分组内搜索的所有Token的最大值进行求和； 分组间求和：所有分组的得分最终进行求和计算； 小结 best_fields对搜索为单个Token的情况下效果更好，比如搜索【耐克】的时候品牌为耐克和商品关键字包含耐克的时候前者相关性得分更高；但是对于都是为多个Token需要跨字段匹配时，只能引进大字段来匹配，这样权重的设置就失去意义了； most_fields和best_fields类似，其优点在于能够尽可能多地匹配，相关性评分机制更合理； cross_fields最大的优点在于能够跨字段匹配，而且充分利用到了各个字段的权重设置。但是需要注意的是匹配时是根据search_analyzer进行分组，不同分组直接的匹配无法跨字段。 参考材料 ElaticSearch Reference &gt; Multi Match Query]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[阅读随手记 201612]]></title>
      <url>%2Fpost%2Freading_record_201612%2F</url>
      <content type="text"><![CDATA[关键字：Ctrip DAL、推荐系统、分布式队列、分布式定时任务、微服务、系统架构、Kafka、DistributedLog、分布式缓存、分布式数据库、Motan、RPC、IDEA、机器学习、深度学习、微信小程序、性能调优、Git、函数式编程、服务化、高性能、高并发、高可用； 携程开源数据库访问框架Ctrip DALhttp://www.infoq.com/cn/news/2016/09/Ctrip-DAL 支持分表分库； 支持多种数据库和多种语言； 有一套管理DAO的界面（Code Gen），但感觉这个很繁琐； Recommender systems explainedhttps://medium.com/recombee-blog/recommender-systems-explained-d98e8221f468#.6vi5y036thttp://www.infoq.com/cn/articles/three-recommendation-systems-principle Knowledge based recommender systems：基于属性（物品和用户都具备一些属性），通过这些属性的相关性进行推荐； Content based recommender systems： 基于用户的行为（显式反馈和隐式反馈）推荐类似的物品； Collaborative filtering： 基于近邻算法，考虑最相似的一组用户进行检查，获取最终推荐最多的物品； 分布式队列编程：从模型、实战到优化http://www.infoq.com/cn/articles/distributed-queue-programme-model-actual-combat-optimization When：同步VS异步；Who：发送者接收者解耦；Where：消息暂存机制；How：如何传递； 抽象模型：点对点模型、生产者消费者模型、发布订阅模型； 挑战：高吞吐量、高可用性、高一致性要求、完整性约束、持久化要求； 生产者优化：内存缓存请求、内存持久化策略（定期、定量写入）、批量写入队列； 基于Mesos的当当作业云Elastic Job Cloudhttp://www.infoq.com/cn/news/2016/09/Mesos-Elastic-Job-Cloud Mesos的核心概念 蓝色是Mesos的基础组件，由Mesos Master、Mesos Agent和运维界面/API组成; 紫色是Mesos Framework的组成部分。Mesos只有基础组件并不能独立使用，需要注册Framework接收Mesos Master分配的资源并决定如何执行，目前常见的Marathon和Chronos都是Mesos Framework。Framework的两个重要组成是Scheduler和Executor; 红色是Scheduler中两个最重要的回调方法，resourceOffers用于将资源转化为任务并调用Executor执行，statusUpdate用于处理Executor回传的状态； Mesos带来的好处 作为部署平台，Mesos可以做到应用分发自动化，省去人工部署成本； Mesos可以将资源收集至统一资源池，做到硬件资源与应用一体化，按需分配资源，自动资源回收，减少资源闲置和浪费； 弃用内置Framework，自研Elastic Job Cloud 实现应用分发和资源分配，并且是高可用的解决方案; 资源分配静态与动态相结合，常驻与瞬时作业分离处理； Elastic Job 1.X的核心理念仍可沿用，包括分片，以及之前未提及的功能，如多种作业类型，事件统计等； 易于定制化需求开发； The Hardest Part About Microservices: Your Datahttp://blog.christianposta.com/microservices/the-hardest-part-about-microservices-data/http://www.infoq.com/cn/news/2016/09/Christian-Posta-Micro-service-da What is the domain? What is reality? –理解上下文，使用DDD建立数据模型、画出边界； Where are the transactional boundaries? – 事务边界是业务不可变性的最小原子单元，要让事务边界尽可能小； How should microservices communicate across boundaries? – 事件机制； What if we just turn the database inside out? – 通过Apache Samza构建事件流处理系统； 美团外卖系统架构演进与稳定性的探索http://www.infoq.com/cn/articles/evolution-and-stability-of-meituan-waimai-architecture 稳定性的架构设计：大系统小做、依赖稳定性原则、设计这个稳定性的时候需要考虑用户的体验； 事前预警： 分层的监控（系统级监控和业务监控）、日志聚合、监控指标可视化； 三种限流的策略：防刷、等待+限时、单机的QPS保护； 总结：你要想稳定性做的非常可靠，灰度、灰度、还是灰度；慢查询往往闯大祸；防御式编程；SOP保平安；你所担心的事一定会发生，而且可能马上会发生； A Technical Review of Kafka and DistributedLoghttp://distributedlog.incubator.apache.org/technical-review/2015/09/19/kafka-vs-distributedlog.htmlhttp://www.infoq.com/cn/articles/technology-comparison-of-kafka-and-distributedlog MTDDL——美团点评分布式数据访问层中间件 实现了MySQL动态数据源、读写分离、分布式唯一主键生成器、分库分表、连接池及SQL监控、动态化配置等一系列功能，支持分库分表算法、分布式唯一主键生成算法的高可扩展性，而且支持全注解的方式接入，业务方不需要引入任何配置文件。 分布式ID生成系统Leaf: 基于DB的Ticket服务，通过一张通用的Ticket表来实现分布式ID的持久化，执行update更新语句来获取一批Ticket，这些获取到的Ticket会在内存中进行分配，分配完之后再从DB获取下一批Ticket。 分库分表目前默认使用的是取模算法，分表算法为 (#shard_key % (group_shard_num table_shard_num))，分库算法为 (#shard_key % (group_shard_num table_shard_num)) / table_shard_num，其中group_shard_num为分库个数，table_shard_num为每个库的分表个数。 为了尽可能地方便业务方接入，MTDDL采用全注解方式使用分库分表功能，通过ShardInfo、ShardOn、IDGen三个注解实现。 采用Spring AOP技术对所有DAO方法进行功能增强处理，通过美团点评分布式会话跟踪组件MTrace进行SQL调用数据埋点及上报，进而实现从客户端角度对SQL执行耗时、QPS、调用量、超时率、失败率等指标进行监控。 动态化配置：在Spring容器启动的时候自动注册数据源及分库分表相关配置到美团点评的统一配置中心MCC，在MCC配置管理页面可以进行动态调整，MCC客户端在感知到变更事件后会刷新本地配置； 常见性能优化策略的总结http://tech.meituan.com/performance_tunning.html 代码：第一步就应该是分析相关的代码，找出相应的瓶颈，再来考虑具体的优化策略； 数据库：SQL调优（慢查询、explain、profile）、架构层面的调优（读写分离、多从库负载均衡、水平和垂直分库分表）、连接池调优（结合当前使用连接池的原理、具体的连接池监控数据和当前的业务量作一个综合的判断）； 缓存：分类（本地和分布式）、场景（读多写少、热点数据）、关键点（更新策略、缓存淘汰、缓存击穿）； 异步：BlockingQueue+异步线程、消息中间件； NoSQL：如果业务数据不需要和其他数据作关联，不需要事务或者外键之类的支持，而且有可能写入会异常频繁，这个时候就比较适合用NoSQL（比如HBase）。 JVM调优：CPU使用率与Load值偏大（thread count以及gc count）、关键接口响应时间很慢（gc time以及gc log中的stop the world的时间）、发生full gc或者old cms gc非常频繁（内存泄露）； 多线程与分布式：单机多线程（引入线程池）、多机多线程（调度、拆分和分发）； 度量系统：确定指标、采集数据、计算数据，存储结果、展现和分析； Intellij IDEA 一些不为人知的技巧http://www.jianshu.com/p/364b94a664ff 用 Control + E 来找到最近访问的文件； 在行中任意位置使用Control + Shift + Enter 来快速补全分号； Intellij IDEA 里面内置了一个 Rest Client，大家可以通过 Control + Shift + A，然后搜索 Rest Client 来找到； 通过 Control + Shift + V 访问历史粘贴板； 使用Language Injection 的功能（Alt + Enter）将一个字符串标记为JSON、正则。HTML等进行编辑； 使用Smart Step Into( Shift + F7)可以选择到底要 Debug 进入哪一个方法; 技术框架与组件使用http://weibo.com/ttarticle/p/show?id=2309404038032062872510 Motan是微博研发的开源RPC服务框架，基于java语言开发，主要特点是轻量级、高性能、高扩展性。Motan提供了实用的服务治理功能，包括服务发现、负载均衡、HA策略、流量控制等。通过SPI机制能够支持不同RPC协议、传输以及序列化协议，并且可以方便的增加不同的filter功能，便于二次开发。 RPC（Remote Procedure Call）指远程过程调用，是一种通过网络调用远程过程的协议，简单地说就是能使应用像调用本地方法一样的调用远程的过程或服务。 RPC协议只规定了Client与Server之间的点对点调用流程，包括Stub、通信协议、RPC消息解析等部分, 而RPC框架一般是指能够完成服务调用的解决方案，除了具体通信的RPC协议，还包括服务的发现与注销、提供服务的多台Server的负载均衡、服务的高可用等服务治理的功能。Motan就是这样一个RPC框架。 Motan中有服务提供方RPC Server，服务调用方RPC Client和服务注册中心Registry三个角色。 注册中心Vintage是基于Group的，server和client必须在相同group下才能进行访问。 Motan server：解析配置文件生成对应的protocol信息、打开并监听声明的RPC服务端口、向Vintage(即Registry)注册RPC服务、服务验证通过后打开心跳开关； Motan client： 解析配置文件生成对应的protocol信息、向Registry订阅所需的RPC服务、对每个Server建立初始链接； 分布式缓存架构基础http://weibo.com/ttarticle/p/show?id=2309404022116222639373&amp;mod=zwenzhang Memached Multiget-Hole（在Memcached采用数据分片方式部署的情况下，对于multiget命令来说，部署部署更多的节点，并不能提升multiget的承载量，甚至出现效率反而降低）：使用多副本的方式扩容、multiget的keys尽可能放在同一个节点上； 反向Cache就是将一个不存在的key放在缓存中，也就是在缓存中存一个空值，减少对DB的穿透。 缓存Fail-Fast：当出现故障节点时，标识故障节点为不可用节点（策略举例：连续N次请求都出现超时，标识M时间段内为不可用)，读写不可用节点快速返回。 缓存无过期（缓存中存储全量数据，不存在数据穿透的情况）：适合总体数据量很小，但是访问量巨大的业务场景； dog-pile effect狗桩效应 (极热访问的缓存数据失效，大量请求发现没有缓存，进而穿透至DB)：从代码层面就要考虑到并发穿透的情况，保证一个进程只有一次穿透； 极热点数据场景（缓存资源遇到性能瓶颈）：前端使用local cache, 以缓解后端缓存的压力；考虑引入L1结构，通过部署多组小容量的L1缓存来应对突然的访问量增长； 避免雪崩（缓存服务器宕机等原因导致命中率降低，大量的请求穿透到数据库）：缓存高可用、降级和流控、清楚后端资源容量； 数据一致性（CAP）-&gt; 采用数据最终一致性：Master与副本一致性、Cache与Storage一致性、业务各维度缓存数据一致性； 缓存容量规划：考虑请求量、命中率、网络带宽、存储容量、连接数； 微博缓存中间件CacheService：代理层/资源层/客户端/配置中心/集群管理系统; 从优化性能到应对峰值流量：微博缓存服务化的设计与实践http://weibo.com/ttarticle/p/show?id=2309404013728432540615 微博使用的缓存主要是 Memcache 和 Redis，因为 Memcache 的使用场景、容量更大，而且目前推的缓存服务化也是优先基于 Memcache，然后再扩展到 Redis、 ssdcache 等其他缓存； 如果某个时间点，核心业务的多个缓存节点不可用，大量请求穿透会给 DB 带来巨大的压力，极端情况会导致雪崩场景。于是我们引入 Main-HA 双层架构。 对于集群内的扩缩，线上操作最多的是增减 L1 组或扩容 main 层。 S4LRU分成四个子 LRU： LRU0-LRU3。 Key miss 或新写入一个 key 时，把这个 key 放在第一层 LRU0，如果后来被命中则移到 LRU1 ；如果在 LRU1 又一次被命中则移到 LRU2，依此类推，一直升级到 LU3。如果它四次以上命中，就会一直把它放在 LU3。如果发现 LU3 的数据量太多需要 evict，我们先把待 evict 的 key 降级到 LU2 上，如此类推。同时每个 kv 有过期时间，如果发现它过期就清理。 LS4LRU 是在 S4LRU 的基础上增加一个分级的过期时间，每个 KV 有两个过期时间 exp1 和 exp2。比如说某业务， exp1 是一秒， xep2 是三秒， LS4LRU 被命中的时候，如果发现它是在一秒内的数据，则直接反给客户端的，如果是在 1 秒到 3 秒的时候，则会首先返回到客户端，然后再从异步获取最新的数据并更新。如果是 3 秒以上的，就直接去清理，走 key miss 流程。 海量数据存储基础http://weibo.com/ttarticle/p/show?id=2309404025046866781970&amp;mod=zwenzhang 关系型数据库：局限于服务器性能、局限于数据复杂度、常见的SSD磁盘服务器，单机读取性能可达万级/s； NoSQL(Not only SQL)数据库：存储非结构化数据、半结构化数据、单机QPS在10万级别； 微博平台核心业务的数据都存储在MySQL上，目前具备千台规模的集群，单个核心业务数据突破千亿级，单个核心业务QPS峰值可达10万级每秒，写入也是万级每秒。 从单机到集群的架构变迁：SQL优化，硬件升级-》垂直拆分-》读写分离-》水平拆分 分布式数据库设计：hash拆分方式、时间拆分方式 当一台服务器宕机怎么办：Slave宕机（切换、扩容、降级）；Master宕机（快速下线master提升slave为master）； redis过期机制：被动过期、主动过期（volatile-lru、volatile-ttl、volatile-random、allkeys-lru、allkeys-random）； 定制CounterService：修改hash table为增量扩展式的hash tables、废弃expire； 分布式Redis架构如何实现高可用：采用M-S高可用方案、服务域名化是必要的； 微博平台采用如下几个层级的组建来进行分布式数据库操作： 分布式架构下的redis client访问: Java性能调优工程的几点建议http://www.infoq.com/cn/news/2016/10/javaPerformance-guide-byMonica 能优化工程由两部分组成：性能需求分析及规划、性能结果分析。两者构成闭环，使得性能得到不断的提升。 需要站在用户的角度去思考QoS；将QoS标准量化为可测量的指标，即SLA服务等级协议；然后对SLA性能指标进行定义、梳理并排列优先级（吞吐量、响应时间、容量、请求足迹、CPU使用率等）。 性能调优执行的两种实现方式：自上而下、自下而上。 不论哪一种方向，均可以分为四步：第一步监控、第二步归纳、第三步分析、第四步调优和应用。 机器学习：发展与未来 – 周志华http://www.leiphone.com/news/201610/rZ9EHIpeSwBv2Tvq.html 现在是大数据时代，但是大数据不等于大价值。 人工智能的三个阶段：推理期（自动定理证明系统）、知识器（专家系统）、学习期（机器学习）； 计算学习理论中最重要的一个理论模型就是概率近似正确模型（PAC）； 从技术层面来看，深度学习其实就是很多层的神经网络，最著名的深度学习模型叫做卷积神经网络（CNN）； 提升模型的复杂度可以提升学习能力，增加模型深度比宽度更有效，但提升模型的复杂度并不一定有利，因为存在过拟合和计算开销大的问题。 深度学习最重要的作用是表示学习； 传统的机器学习任务大都是在给定参数的封闭静态环境中，而现在正在慢慢转向开放动态环境。 其实机器学习的形态就是算法 + 数据。 学件（Learnware）= 模型（model）+规约（specification）；“可重用”的特性能够获取大量不同的样本、“可演进”的特性可以适应环境的变化、“可了解”的特性能有效地了解模型的能力； Git from the inside outhttps://codewords.recurse.com/issues/two/git-from-the-inside-out 一篇非常有意思的介绍Git原理的文章，虽然比较长，但图文结合，花个半个小时能看懂。 Core Functional Programming Conceptshttps://thesocietea.org/2016/12/core-functional-programming-concepts/ Functions are Pure：幂等性、无副作用； Functions are First-Class and can be Higher-Order：一等公民、高阶函数； Variables are Immutable：变量不可变性； Functions have Referential Transparency：如果程序中任意两处具有相同输入值的函数调用能够互相置换，而不影响程序的动作，那么该程序就具有引用透明性； Functional Programming is Based on Lambda Calculus：Lambda算子（匿名、柯里化） The JVM Architecture Explainedhttps://dzone.com/articles/jvm-architecture-explained Class Loader Subsystem: Boot Strap ClassLoader/Extension ClassLoader/Application ClassLoader;Linking: Verify -&gt; Prepare -&gt; Resolve; Runtime Data Area: Method Area/Heap Area/Stack Area/PC Registers/Native Method stacks; Execution Engine: Interpreter/JIT Compiler/Garbage Collector; One Sure-Fire Way to Improve Your Codinghttps://changelog.com/posts/one-sure-fire-way-to-improve-your-coding 代码写少读多，多读代码能显著提升编码水平； 读什么：正在用的插件、库和框架；惊艳你的代码；崇拜的大神的代码；从小项目开始； 怎么读：看整体项目结构、给代码加注释、跑自带的测试用例、尝试更改或添加特性、不断尝试； Best Practices for Building a Microservice Architecturehttp://www.vinaysahni.com/best-practices-for-building-a-microservice-architecture The Platform Your platform is a set of standards combined with supporting tools （统一控制面板管理） Service Essentials Independently develop &amp; deploy services （独立开发部署、谨慎对待共享库） Services should have their own private data （公用数据库导致服务不独立、共享数据库服务器） Keep Services small enough to stay focused and big enough to add value （低耦合、高内聚、单一边界上下文） Store data in databases, not ephemeral service instances （无状态服务实例、便于扩容和负载均衡） Eventual consistency is your friend （最终一致性带来松耦合、便于使用异步通信） Offload work to asynchronous workers whenever possible （异步通信加快响应、降低错误、便于扩容） Keep helpful documentation for all services in a common place （统一维护接口文档） Distribute work with load balancers （使用客户端的负载均衡代替接收方的负载均衡） Aggregation services on network boundaries can translate for the outside world （聚合服务负责收集来自其他服务的数据，处理任何特殊的编码或压缩要求） Layer your security and don’t write your own crypto code! （分层式安全、使用自动化的安全更新、加固底层操作系统、不要自行编写加密代码） Service Interactions Transport data over HTTP, serialized using JSON or protobuf （同步通信用HTTP、异步用消息队列、用JSON或protobuf序列化） For HTTP services, 500 series errors or timeouts mean the service is unhealthy（统一服务异常码） APIs should be simple and effective （使用优雅的面向资源的REST接口设计） A service discovery mechanism makes it easy for services to find each other （服务注册、服务发现、负载均衡+DNS） Prefer decentralized interactions over centralized orchestrators （去中心化的交互耦合更低） Version all APIs, colocating multiple versions within the same service instances （支持多版本的接口） Use limits on resources to fail fast before a service gets overloaded （快速失败） Connection pools can reduce downstream impact of sudden request spikes （平滑化请求、相互隔离） Timouts minimize impact from downstream delays and failures （更短的超时避免级联失败） Be tolerant of unrelated downstream API changes （向前兼容的接口变更） Circuit breakers give downstream services a break during tough times （断路器直接失败） Correlation IDs help you track requests across service logs （为请求生成唯一标识便于跟踪） Make sure you can guarantee eventual consistency （稍后修复、每块数据都应该有一个单一数据源） Authenticating all API calls provides a clearer picture of usage patterns Auto retry failed requests with random retry intervals （使用随机化的重试避免惊群效应） Only talk to a services through exposed and documented APIs Economic forces encourage efficient usage of available resources Client libraries can handle all the basics, so you can focus on what matters Development Use a common source control platform for all services （每个服务都该有自己的代码库） Either mimic prod in dev or use isolated cloud based dev environments （本地开发和运行的服务与云中运行的隔离环境结合） Push working code to mainline often （尽快将开发中的代码与主线分支进行集成） Release less, release it faster （持续交付的目标在于更快速地发布小规模变更） Warning: shared libraries are painful to update （无法控制何时将共享库的更新部署到使用它的服务上是使用共享库带来的最大挑战，最佳做法是发布弃用时间表） Your service templates should cover the fundamentals out of the box （除了核心业务逻辑，服务还管理一系列其他附加任务，例如服务注册、监控、客户端负载均衡、限制管理、断路。团队应能通过这些模板快速实现服务自举以处理所有常见任务，并与平台进行恰当集成。） Simple services are also easy to replace Deployment Use a system image for a deployment package （标准化的部署程序包是自动化部署流水线中重要的组成部件） Have a way to automatically deploy any version of any service to any environment （零停机更新） Feature flags decouple code deployment from feature deployment （有效地实现代码部署和功能部署间的解耦） Configuration should be managed outside of the deployment package Operations Manage all logs in one place （任何日志都不应存储在实例中） Use a common monitoring platform for all services （监控解决方案必须能将不同实例的衡量值汇总在一起） Stateless services are easy to auto scale （很多微服务平台为实例数量的处理提供了声明性接口，这种功能非常易用） Dependent services that don’t run on your platform also need automation People Service teams develop, deploy &amp; operate their own services （服务团队需要拥有、运维并完善自己构建的服务） Teams should be autonomous in daily operations （拥有这些服务的团队必须具备开发、部署，以及运维这些服务所需的全部技能和工具） Best Practices for Designing a Pragmatic RESTful APIhttp://www.vinaysahni.com/best-practices-for-a-pragmatic-restful-api An API is a user interface for a developer - so put some effort into making it pleasant Use RESTful URLs and actions Use SSL everywhere, no exceptions An API is only as good as its documentation - so have great documentation Version via the URL, not via headers Use query parameters for advanced filtering, sorting &amp; searching Provide a way to limit which fields are returned from the API Return something useful from POST, PATCH &amp; PUT requests HATEOAS isn’t practical just yet Use JSON where possible, XML only if you have to You should use camelCase with JSON, but snake_case is 20% easier to read Pretty print by default &amp; ensure gzip is supported Don’t use response envelopes by default Consider using JSON for POST, PUT and PATCH request bodies Paginate using Link headers Provide a way to autoload related resource representations Provide a way to override the HTTP method Provide useful response headers for rate limiting Use token based authentication, transported over OAuth2 where delegation is needed Include response headers that facilitate caching Define a consumable error payload Effectively use HTTP Status codes 服务化框架技术选型与京东JSF解密http://mp.weixin.qq.com/s/hureIGrLVKO7FLDOWdNjWA 基本的服务化框架包括如下模块：统一的RPC框架，服务注册中心，管理平台。 RPC框架选型需要考量的因素： 代码规范：例如是对已有代码透明，还是代码生成； 通讯协议：例如是TCP还是HTTP； 序列化协议：例如是二进制还是文本，是否需要跨语言，性能； IO模型：异步/同步，阻塞/非阻塞； 负载均衡：客户端软负载，代理模式，服务端负载。 另外如果是从开源里面选择，那么我们还需要考量： 成熟度：包括学习成本，社区热度，文档数，是否有团队维护，稳定性（盲目追求的不一定是最适合）； 可扩展性：是否有SPI支持扩展，是否支持上下兼容； 跨语言：是否支持跨语言； 性能：要想作为RPC框架，性能一般都不会太差。 如果需要与前端交互的，适合短链接、跨语言的RPC框架，例如RESTful、gRPC等； 如果纯粹后台交互的，适合长链接、序列化为二进制的RPC框架，例如thrift、dubbo等更高效； 如果是小公司，新公司从头开始推广服务化框架的，可以选择规范化的RPC框架，例如thrift、RESTful、gRPC； 如果是已有大量业务代码的再推广服务框架的，那么最好选择无代码入侵的RPC框架，例如dubbo、RESTful。 注册中心选型选择注册中心基本要考量： 服务注册：接收注册信息的方式； 服务订阅：返回订阅信息的方式，推还是拉； 状检测：检测服务端存活状态。 如果是从开源里面选择，那么还需要考量： 成熟度：包括学习成本，社区热度，文档数（盲目追求的不一定是最适合）； 维护成本：注册中心维护； 数据解构：是否能快速定位结果，是否能遍历； 性能和稳定性； CAP原则：CP（关注一致性）还是AP（关注可用性）。 规模小选择CP，RPC框架可以直接接入数据源； 规模大选择AP， RPC框架不可以直接接入数据源； 存在跨机房，跨地域的尽量不要选有强一致性协议的注册中心； RPC框架必须要有注册中心不可用的容灾策略； 服务状态检测十分重要。 完善的服务化框架 接口文档管理：提供一个接口文档管理以及接口查询的入口。这里可以定义接口的文档，包括接口描述，方法定义，字段定义。可以定义接口的SLA，包括支持的并发数，tp99多少，建议配置是什么。 配置中心：这里说的配置主要指的是服务相关的一些配置。配置包括分组配置、路由策略、黑白名单、降级开关、限流信息、超时时间、重试次数等等，任何可以动态变更的所有数据。 监控中心：监控服务关注接口维度，实例维度的数据。RPC框架可以定时上报调用次数，耗时，异常等信息。监控中心可以统计出服务质量信息，也可以进行监控报警。 分布式跟踪：区别于监控中心，以调用链的模式对服务进行。RPC框架作为分布式跟踪系统的一个天然埋点，可以很好的进行一个数据输出。 服务治理（重点）：服务路由、调用授权、动态分组、调用限流、灰度部署、配置下发、服务降级； 网关：RPC框架大部分场景都是自己调用的，网关可以提供如下功能：统一的鉴权服务、限流服务、协议转换、Mock（服务测试，降级等）、其它一些统一处理逻辑（例如请求解析，响应包装）。 服务注册中心Plus：需要逻辑处理能力，例如对数据进行筛选过滤整合，计算服务路由等功能同时还需要有与RPC框架交互的功能。 管理端Plus：管理端除了之前的简单服务管理功能外，还需要提供配置信息展示，监控信息展示，各种维度的数据展示。 京东实践 JSF注册中心 京东的注册中心是自研的，基于DB做的数据最终一致; 注册中心主要实现的就是服务列表的注册订阅推送，服务配置的获取下发，服务状态的实时查看等功能; 注册中心节点是无状态的，可水平扩展的; 每个机房部署多个注册中心节点。同机房的RPC框架会优先连本机房的注册中心节点; 引入Index服务概念：该服务就是一个最简单HTTP的服务，用于找注册中心节点，RPC框架会优先连该服务拿注册中心地址，这样子的好处是注册中心地址变化后，RPC框架不用修改任何设置； 注册中心内存有服务列表全量缓存，连不上数据库也保证可读； 数据库的数据结构更适合各种维度展示、过滤、分析等，例如根据分组/IP/应用/机房等不同维度； 注册中心就是个JSF服务，监控到压力大即可进行动态水平扩展，dogfooding，注册中心其实是第一个JSF接口； 服务列表推送逻辑改进：例如原来100个Provider，现在加1个节点，之前的SAF是需要下发101个节点，自己判断加了哪个节点，进行长链接建立；现在的改进是：修改为下发一个add事件，告知RPC框架加了1个节点，RPC框架进行长链接建立；这样做大大减少了推送的数据量； 注册中心与RPC框架可各种交互：注册中心和RPC框架是长链接，而且JSF是支持Callback的，注册中心可以调用RPC框架进行服务列表变化之外的操作；例如查看状态，查看配置，配置下发等。 JSF RPC框架 JSF的RPC框架主要分为图中的几个模块，下面大概列下一些功能特性： Config：Spring/API/Annotation Proxy: Javassist/JDK Invoker/Filter：内置+自定义，Filter可扩展 Client：Failover（默认）/FailFast/TransportPinpoint/MultiClientProxy 调用方式：同步（默认）/异步并行/异步回调/Callback/泛化 Loadbalance：Random（默认）/Roundrobin/ConsistentHash/ LocalPreference/LeastActiveCall 路由：参数路由，分组路由，（IP级别路由逻辑在注册中心做） 连接维护：可用/死亡/亚健康 协议：JSF（默认）/SAF(dubbo)/HTTP/Telnet/HTTP2 第三方：REST/Webservice 序列化：MsgPack（默认）/Hessian/Json/Java/protobuf(c++) 压缩：Snappy/LZMA 网络：基于Netty4.0，长连接复用 线程模型：BOSS+WORKER+BIZ 容灾：本地文件 请求上下文：IP，参数，隐式传参 事件监听：响应事件，连接事件，状态事件 分布式跟踪支持：进行数据埋点 JSF管理平台提供强大管理功能，包括服务管理，监控管理，注册中心管理等功能。 高性能高并发系统的稳定性保障https://mp.weixin.qq.com/s/YMgIwaz8YC_zNPh_Jf98HA 性能、并发、稳定性三者关系 高性能：高吞吐量、低延时；吞吐量包括QPS， TPS，OPS等； 并发：并不是越高越好，需要考虑TP99； 用户角度：系统是个黑盒，复杂系统中的任何一环到会导致稳定性问题。 SLA：在某种吞吐量下能提供TP99为n毫秒的服务能力。 降低延时，会提高吞吐量，但是延时的考核是TP99这样的稳定的延时。 如何改善延时 关键路径：“28原则”（20%的代码影响了80%的性能问题，抓重点）、“过早优化是万恶之源”。不同解读； 优化代码：空间换时间：各级缓存；时间换空间：比如传输压缩，解决网络传输的瓶颈；多核并行：减少锁竞争；lesscode；各类语言、框架、库的trick；算法+数据结构，保持代码的清晰、可读、可维护和扩展； 通过性能测试和监控找出瓶颈； 内存分配: 显式分配器(jemalloc/tcmalloc代替默认的ptmalloc)、隐式分配器（JVM GC的各种调优）、是否使用hugepagen预分配和重用（Netty的Pooled ByteBuf）、减少拷贝（new ArrayList(int), new StringBuilder(int)）； 减少系统调用：批处理:（buffer io，pipeline）、使用用户态的等价函数:（gettimeofday -&gt;clock_gettime ）、减少锁竞争、RWMutex 、CAS、Thread local、最小化锁范围、最小化状态、不变类； 减少上下文切换：触发（中断、系统调用、时间片耗尽、IO阻塞等）、危害（L1/L2 Cache Missing，上下文保存/恢复）、单线程、ThreadPool的配置； 网络：内核TCP Tuning参数和SocketOption、TCP Socket连接池、网络I/O模型、传输压缩、编解码效率、超时心跳和重试机制； 如何提高吞吐量 复制：通过复制提高读吞吐量、容灾、异构；通过数据分片提高写吞吐量；程序双写（同步双写、异步双写、Change Data Capture如Canal）；底层存储复制机制； 扩容：扩容规划；扩容checklist、应用扩容、垂直扩容、水平扩容； 异步化：解耦利器、削峰填谷、页面异步化、系统异步化、JMQ、状态机（worker）+DB、本地队列、集中式缓存队列； 缓存：多级缓存、缓存前置、一致性延迟权衡、缓存主节点负责写、通过CDC监听数据库binlog主动更新缓存、CPU不是瓶颈，网络才是、优化编码，减少尺寸、优化操作、优化拓扑； 如何保障稳定性 提高可用性：衡量指标（几个9）、减少故障、减少故障修复时间、冗余复制灾备切换、快速切换、监控； 分组和隔离：网络流量隔离、业务系统隔离、流量分组、存储的分组； 限流：谨慎使用、区分正常流量和超预期流量、读少限，写多限、客户端配合限流、不同分组的限流阈值、多层限流； 降级：保证用户的核心需求、需要有预案和开关、非关键业务屏蔽、业务功能模块降级、数据降级（动态降级到静态，远程服务降级到本地缓存）； 监控和故障切换：网络流量；操作系统指标；服务接口调用量、TP99、错误率…；日志；业务量变化； 小结 如何更好的设计RESTful APIhttps://zhuanlan.zhihu.com/p/24592119 你的API越简单明了，使用的人就越多； HTTP动词GET（SELECT）：从服务器检索特定资源，或资源列表。 HTTP动词POST（CREATE）：在服务器上创建一个新的资源。 HTTP动词PUT（UPDATE）：更新服务器上的资源，提供整个资源。 HTTP动词DELETE（DELETE）：从服务器删除资源。 HTTP动词PATCH （UPDATE）：更新服务器上的资源，仅提供更改的属性。 HTTP动词HEAD - 检索有关资源的元数据，例如数据的哈希或上次更新时间。 HTTP动词OPTIONS - 检索关于客户端被允许对资源做什么的信息。 版本控制：一个好的RESTful API设计将跟踪URL中的版本； 分析：跟踪客户端使用的API的版本/端点； 将内容设为API根目录是个好主意； 端点是您的API中指向特定资源或资源集合的URL，当引用每个端点可以做什么时，您需要列出有效的HTTP动词和端点组合。 当客户端请求对象列表时，请务必为它们提供符合所请求条件的每个对象的列表。然而，重要的是，您确实为客户端提供了指定某种过滤/结果限制的能力。 作为RESTful API，使用正确的HTTP状态代码非常重要;他们是一个标准！ 认证：OAuth 2.0提供了一个很好的方法。对于每个请求，您可以确定知道哪个客户正在发出请求，代表他们请求哪个用户，并提供一种（大部分）标准化的方式来过期访问或允许用户撤消来自客户端的访问权，需要第三方客户端知道用户登录凭据。 内容类型：目前，最令人兴奋的API提供来自RESTful接口的JSON数据。 超媒体API：超媒体API很可能是RESTful API设计的未来。超媒体API概念的工作方式与人类相同。请求API的根返回一个URL列表，它可能指向每个信息集合，并以客户端可以理解的方式描述每个集合。为每个资源提供ID并不重要（或必需），只要提供了一个URL即可。 微信小程序实战，从入门到弃坑http://www.jianshu.com/p/4433d46e6235 微信小程序的内容部分是hybrid模式，并非原生； 一个完整的微信小程序是由一个App实例和多个Page实例构成，其中App实例表示该小程序应用，多个Page表示该小程序的多个页面。 微信小程序并没有提供自定义组件的方式，这就导致微信小程序在开发较复杂应用时，可能会比较艰难。 两种配置文件：app.json（应用的全局配置文件）、page.json（页面的全局配置文件）； 两个核心函数：App() （小程序注册入口，全局唯一）、Page() （页面注册入口）； 生命周期及调用流程： 小程序虽然是hybrid模式，但并不使用HTML渲染。WXML支持数据绑定、条件渲染、循环、模块化等功能。但问题却不少：不能跨浏览器、富文本解析困难，iframe视频不支持，没办法外链跳转。 页面间的跳转见下表： 事件绑定： // bindtap 和 catchtap的区别在于 // bindtap 不会阻止事件冒泡 // catchtap会冒泡事件冒泡 &lt;view id=&quot;tapTest&quot; data-hi=&quot;WeChat&quot; bindtap=&quot;tapName&quot;&gt; Click me! &lt;/view&gt; &lt;view id=&quot;tapTest&quot; data-hi=&quot;WeChat&quot; catchtap=&quot;tapName&quot;&gt; Click me! &lt;/view&gt; // 绑定的函数tapName只是一个函数名称，默认接受一个event对象作为参数 Page({ tapName: function(event) { console.log(event) } }) 事件传参：将参数绑定到wxml标签上，然后通过event.target.dataset获取；直接使用Page.data或其他数据； 官方组件：https://mp.weixin.qq.com/debug/wxadoc/dev/component/?t=20161222 官方API：https://mp.weixin.qq.com/debug/wxadoc/dev/api/?t=20161222 Java 模块化技术演进和对现有应用微服务化的意义https://mp.weixin.qq.com/s/7H71uPSKcb2hVam7GU0T6g Java模块化需求：语言定义粒度缺失的一环、顺序加载带来的问题、多版本组件部署； Java模块化的实现：OSGI、Jigsaw、JBoss Module； Java语言的模块化：Jigsaw项目、Java Platform Module System(JSR 376)、推迟到Java9时发布、JDK9 EA 111合并入Jigsaw； JDK 9模块化实例： 3种模块化实现特性比较 模块化技术对微服务的意义:设计更加灵活、模块化是必须的基础设施、和容器技术配合（运用Docker的层叠式文件系统、Kubernetes自动扩展机制的结合） 开发架构的变化：应用架构按照层次进行模块化改造、语言级进行支持、逐步适应、长期的过程； 京东亿级商品搜索核心技术解密http://mp.weixin.qq.com/s/N2va4w1XERoEIh7ZwT4AUQ 京东商品搜索引擎是搜索推荐部自主研发的商品搜索引擎，主要功能是为海量京东用户提供精准、快速的购物体验。 自身显著的业务特点：结构化的商品数据、极高的召回率要求、商品信息的及时更新、逻辑复杂的商品业务、用户购物的个性化需求； 系统架构分为四个部分：1. 爬虫系统、2. 离线信息处理系统、3. 索引系统、4. 搜索服务系统。 从上到下共分为3层: 最上层是由搜索的前端UI层，负责页面展示。 中间层是由搜索索引服务、SUG搜索、相关搜索、划词服务和兜底服务组成。其中，SUG搜索提供输入框下拉提示词功能；相关搜索提供与query相关的其他搜索词服务；划词服务提供去除query部分词的功能；兜底服务用于索引服务异常情况下提供托底，保证用户基本的搜索可用。 最下层是索引生产端，主要功能是对接商品、库存、价格、促销、仓储等众多外部系统，整合相关数据生产全量和增量数据的索引，为在线检索服务集群提供全量索引和实时索引数据。 爬虫系统：利用大数据平台的数据库抽取接口和中间件系统，实现了站内商品爬虫系统，用来抽取数据库中的商品信息和及时发现变化的商品信息。 离线信息处理系统：主要功能是用来建立商品搜索引擎的待索引数据，包括全量待索引数据和增量待索引数据。对来源分散的数据在商品维度进行合并，生成“商品全量待索引宽表”。该表不仅应用于搜索引擎服务，还同时应用于个性化推荐等其他产品服务当中。但是仅生成宽表是无法完成搜索引擎的索引需求的，因此我们利用Hadoop/MapReduce计算框架对宽表数据进行清洗，并且依照离线业务逻辑规则对数据进行二次“加工”，最终生成一份全量待索引数据。对于增量索引，采用和全量索引类似的方法对数据进行处理，生成增量待索引数据。为了保证增量数据的及时性和准确性，离线信息处理系统会实时调用各商品信息接口获取数据，完成增量待索引数据的在线组装和生产。 索引系统：索引系统是商品搜索引擎的核心，主要功能是把以商品为维度进行存储的待索引数据，转换成以关键字为维度进行存储的数据，用于搜索引擎上层服务进行调用。 搜索服务系统：Query Processor服务（用户查询意图分析）、User Profile服务（查询用户标签）、detail服务（结果包装、基于缓存云实现的商品信息KV查询服务）、检索服务进行分片化处理； 进一步优化：多级缓存策略、截断策略、均匀分片策略、业务优化； 搜索技术的新发展：场景搜索、图像搜索； 解读2016之深度学习篇：开源深度学习框架发展展望http://www.infoq.com/cn/articles/interpretation-of-2016-deeplearning 单台机器CPU -&gt; 专门的图形处理器（Graphics Processing Unit，GPU）-&gt; 通用图像处理器（General Purpose GPU，GPGPU）; 大量的研究表明：增加训练样本数或模型参数的数量，或同时增加训练样本数和模型参数的数量，都能够极大地提升最终分类的准确性； 近年来，存储设备和网络在性能方面的提升远超CPU，CPU性能瓶颈极大地限制了分布式环境下，单台节点的处理速度。许多优秀的开源深度学习框架都在尝试将开源大数据处理技术框架如Spark和GPGPU结合起来，通过提高集群中每台机器的处理性能来加速深度学习算法模型的训练。 深度学习提升性能的三种方式：纵向扩展（给机器加GPU）、横向扩展（通过集群训练模型）及融合扩展（在分布式的基础上，给每个集群中的Worker节点加GPU）； 单机开源深度学习框架：Theano（蒙特利尔理工学院）、Torch（Facebook 和 Twitter主推）、TensorFlow（Google开源的）、Caffe（加州大学伯克利分校）、CNTK（微软开源的）； 分布式开源深度学习框架：DSSTNE（亚马逊开源的）、Paddle（百度开源的）、SparkNet（AMPLab开源的）、Deeplearning4J（Skymind开源的）、Caffe On Spark（Yahoo开源的）、Tensorflow on Spark（Arimo）、TensorFrames（Databricks开源的）、Angel（腾讯开源的）； 开源深度学习框架的发展有以下几个趋势：a.分布式深度学习框架特别是构建在Hadoop生态体系内的分布式深度学习框架（基于Java或Scala语言实现）会越来越流行，并通过融合扩展的方式加速深度学习算法模型的训练。b.在分布式深度学习方面，大数据的本质除了常说的4V特性之外，还有一个重要的本质那就是Online，数据随时可更新可应用，而且数据本质上具备天然的流式特征，因此具备实时在线、模型可更新算法的深度学习框架也是未来发展的方向。c. 当待训练的深度学习算法模型参数较多时，Spark与开源分布式内存文件系统Tachyon结合使用是提升性能的有效手段。 技术人员的发展之路http://coolshell.cn/articles/17583.html 一个重要阶段和标志：人生中的一个非常重要的阶段——20到30岁；这个阶段的首要任务，就是提升自己学习能力和解决难题的能力。这是一个非常非常关键的时间段！这个时间段几乎决定着你的未来。 高效的学习能力和解决问题的能力； 个人发展的三个方向：在职场中打拼、去经历有意义有价值的事、追求一种自由的生活； 免责声明：相关链接版本为原作者所有，如有侵权，请告知删除。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[《Git权威指南》读书笔记]]></title>
      <url>%2Fpost%2Fgit_definitive_guide_notes%2F</url>
      <content type="text"><![CDATA[Git是现在最流行的版本控制工具，其开创性的分布式版本控制特性使其快速拥有一大群粉丝，是Linus Torvalds的又一大作。除了常见的命令，学习其设计理念和原理机制也很有必要。 版本控制的前世和今生略。 爱上Git的理由略。 Git的安装和使用略。 Git初始化创建版本库及第一次提交配置用户名和Email git config --global user.name &quot;Gino Zhang&quot; git config --global user.email &quot;zf_zmc@163.com&quot; 设置常用的一些命令别名 git config --global alias.st status git config --global alias.ci commit git config --global alias.co checkout git config --global alias.br branch 开启颜色显示 git config --global color.ui true 初始化版本库 git init demo Initialized empty Git repository in D:/git_learning/demo/.git/ 创建第一个文件welcome.txt echo &quot;Hello.&quot; &gt; welcome.txt 将新建的文件添加到暂存区中 git add welcome.txt 将新建的文件添加到版本库中 git ci -m &quot;initialized.&quot; [master (root-commit) 0e2185a] initialized. 1 file changed, 1 insertion(+) create mode 100644 welcome.txt 为什么工作区跟目录下有一个.git目录 对于Git来说，.git目录即为本地的版本库，子目录下不存在其他跟踪文件或目录； 这样设计可以使得大部分的操作（除了远程版本库操作外）都可以在本地完成； 当在子目录执行git命令时会递归查找到根目录的.git目录； git config命令的各个参数版本库级别的配置文件git config -e /d/git_learning/demo/.git/config [core] repositoryformatversion = 0 filemode = false bare = false logallrefupdates = true symlinks = false ignorecase = true 全局配置文件git config -e --global ~/.gitconfig [user] name = Gino Zhang email = zf_zmc@163.com [alias] st = status ci = commit co = checkout br = branch [color] ui = true 系统级配置文件git config -e --system /mingw64/etc/gitconfig [credential] helper = manager 备份本章的工作成果/d/git_learning $ git clone demo demo-step-1 Cloning into &apos;demo-step-1&apos;... done. 暂存区修改不能直接提交吗给welcome.txt追加一行 echo &quot;Nice to meet you.&quot; &gt;&gt; welcome.txt 使用git diff命令比较下差别 git diff diff --git a/welcome.txt b/welcome.txt index 18832d3..fd3c069 100644 --- a/welcome.txt +++ b/welcome.txt @@ -1 +1,2 @@ Hello. +Nice to meet you. 试试直接提交到版本库 $ git ci -m &quot;Append a new line.&quot; On branch master Changes not staged for commit: modified: welcome.txt no changes added to commit 提示告诉我们修改的文件也必须先使用git add添加之后才能使用git commit提交。那就将修改“提交”到提交任务中去。 git add welcome.txt git diff 但是执行git diff时没有输出，难道是提交成功了？试试git status和git diff HEAD呢。 git status On branch master Changes to be committed: (use &quot;git reset HEAD &lt;file&gt;...&quot; to unstage) modified: welcome.txt git diff HEAD diff --git a/welcome.txt b/welcome.txt index 18832d3..fd3c069 100644 --- a/welcome.txt +++ b/welcome.txt @@ -1 +1,2 @@ Hello. +Nice to meet you. 说明当前与HEAD（或MASTER分支）是存在差别的。这时如果使用git commit就可以提交入库了，但是为了试验，我们继续修改welcome.txt文件试试看。 echo &quot;Bye-Bye.&quot; &gt;&gt; welcome.txt git status On branch master Changes to be committed: (use &quot;git reset HEAD &lt;file&gt;...&quot; to unstage) modified: welcome.txt Changes not staged for commit: (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory) modified: welcome.txt 这就说明不但版本库的最新提交与暂存区存在差别，当前工作区与暂存区也存在差别。即存在三个版本的文件：一个在工作区、一个在等待提交的暂存区、一个是版本库中最新提交的。那三者之间如何比较呢？ git diff //比较的是工作区与暂存区的差别 git diff HEAD //比较的是工作区和版本库最新提交的差别 git diff --cached //比较的是暂存区和版本库最新提交的差别 这时候使用git commit提交会怎么样呢？试试看 git ci -m &quot;Which version checked in?&quot; [master 3231bdc] Which version checked in? 1 file changed, 1 insertion(+) 这时候提交到版本库的会是哪个版本呢？使用上面的diff命令可以看出来，真正提交的是暂存区中的版本。 理解Git暂存区暂存区的设计是Git最成功的设计之一，也是最难理解的。 .git目录下有一个index文件，我们来玩一个和它有关的小实验。 先把之前未提交的welcome.txt撤销掉。 git co -- welcome.txt 然后查看下.git/index文件的时间戳 ls --full-time .git/index -rw-r--r-- 1 35360 197609 145 2016-12-14 17:30:32.674746800 +0800 .git/index //注意这个时间17:30:32 我们使用touch命令但是不改变它的内容，然后使用git status -s查看状态，再观察时间戳 touch welcome.txt git status -s ls --full-time .git/index -rw-r--r-- 1 35360 197609 145 2016-12-14 17:35:24.975946900 +0800 .git/index //注意时间戳改变了 这说明执行git status命令扫描工作区变动时，先根据.git/index文件中记录的时间戳、长度等信息来判断工作区文件是否变动，如果有变动则打开文件对比原始文件，否则则将该文件新的时间戳记录到.git/index中。这种方式使得Git的扫描能更加快速高效。 文件.git/index实际上就是一个包含文件索引的目录树，其记录了文件和文件状态而非内容。文件的内容尽量在Git对象库.git/objects目录中，文件索引建立了文件和对象实体之间的关系。 工作区、版本库、暂存区原理图 HEAD实际是指向master分支的一个“游标”； 执行git add时，暂存区目录树会被更新，文件内容被写入到对象库，对象的ID记录在文件索引中； 执行git commit时，暂存区的目录树写到版本库，master分支做相应的更新； 执行git reset HEAD时，暂存区的目录树被master分支指向的目录树替换，工作区不受影响； 执行git rm –cached时，会直接从暂存区删除，工作区不受影响； 执行git checkout .时，工作区的目录树被暂存区的替换； 执行git checkout HEAD . 或git checkout HEAD时，会用HEAD指向的master分支替换工作区和暂存区的文件； Git Diff魔法先清理暂存区和工作区，使其和master分支保存一致。 git clean -fd git checkout . 添加一个文件目录和文件，并修改welcome.txt的内容，然后添加到暂存区 echo &quot;Bye-Bye.&quot; &gt;&gt; welcome.txt mkdir subdir echo &quot;Hello.&quot; &gt; subdir/hello.txt git add . 然后再次修改hello.txt文件的内容，是的工作区、暂存区和版本库的目录树均不同 echo &quot;Bye-bye.&quot; &gt;&gt; subdir/hello.txt git status -s AM subdir/hello.txt M welcome.txt 下面来看如果用git diff比较他们之间的区别 git diff：工作区和暂存区的比较； git diff –cached：暂存区和HEAD比较； git diff HEAD：工作区和HEAD比较； 不用使用git commit -a这个命令可以直接将工作区的修改和删除文件直接提交版本库，但是会有两个问题：一个是没有包括未被版本库跟踪的文件；另外一个是丢掉了暂存区带来的好处（对提交内容进行控制的能力）。 搁置问题，暂存状态git status On branch master Changes to be committed: (use &quot;git reset HEAD &lt;file&gt;...&quot; to unstage) new file: subdir/hello.txt modified: welcome.txt Changes not staged for commit: (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory) modified: subdir/hello.txt git stash Saved working directory and index state WIP on master: 3231bdc Which version checked in? HEAD is now at 3231bdc Which version checked in? git status On branch master nothing to commit, working tree clean Git对象Git对象库探秘为什么一个提交里有三个SHA1的对象ID： git log -1 --pretty=raw commit 3231bdcee2336b13684a283960c0bba366519dfa #本次提交标识 tree f58da9a820e3fd9d84ab2ca2f1b467ac265038f9 #本次提交对应的目录树 parent 0e2185a617f030d147e7092d34c478125da861b1 #本次提交的父提交（上一次提交） author Gino Zhang &lt;zf_zmc@163.com&gt; 1481706190 +0800 committer Gino Zhang &lt;zf_zmc@163.com&gt; 1481706190 +0800 Which version checked in? 研究Git对象ID的一个重量级武器就是git cat-file命令。 git cat-file -p 3231 tree f58da9a820e3fd9d84ab2ca2f1b467ac265038f9 parent 0e2185a617f030d147e7092d34c478125da861b1 author Gino Zhang &lt;zf_zmc@163.com&gt; 1481706190 +0800 committer Gino Zhang &lt;zf_zmc@163.com&gt; 1481706190 +0800 Which version checked in? git cat-file -p f58d 100644 blob fd3c069c1de4f4bc9b15940f490aeb48852f3c42 welcome.txt #blob对应的文件继续看 git cat-file -p fd3c # 对应的是welcome.txt的文件内容 Hello. Nice to meet you. git cat-file -p 0e21 tree 190d840dd3d8fa319bdec6b8112b0957be7ee769 author Gino Zhang &lt;zf_zmc@163.com&gt; 1481703025 +0800 committer Gino Zhang &lt;zf_zmc@163.com&gt; 1481703025 +0800 initialized. 这些对象保存在.git/object目录下（ID的前2位是目录，后38位是文件名）。这些对象之间形成了一条跟踪链，可以通过git log查看。 git log --pretty=raw --graph 3231 * commit 3231bdcee2336b13684a283960c0bba366519dfa | tree f58da9a820e3fd9d84ab2ca2f1b467ac265038f9 | parent 0e2185a617f030d147e7092d34c478125da861b1 | author Gino Zhang &lt;zf_zmc@163.com&gt; 1481706190 +0800 | committer Gino Zhang &lt;zf_zmc@163.com&gt; 1481706190 +0800 | | Which version checked in? | * commit 0e2185a617f030d147e7092d34c478125da861b1 tree 190d840dd3d8fa319bdec6b8112b0957be7ee769 author Gino Zhang &lt;zf_zmc@163.com&gt; 1481703025 +0800 committer Gino Zhang &lt;zf_zmc@163.com&gt; 1481703025 +0800 initialized. SHA1哈希值到底是什么，如何生成的？可以总结为综合了“对象类型（commit, blob, tree）、对象大小（提交说明字符长度、文件大小、目录树内容大小）、对象内容（提交说明、文件内容、目录树内容）”因素生成的一个标识，冲突概率很低很低。 为什么不用顺序的数字来表示提交？主要是由于Git是分布式的版本库，如果采用顺序的数字来标识提交，不可避免会造成冲突。 Git重置分支游标master探秘新增一个文件，观察文件.git/ref/heads/master的内容如何改变 $ cat .git/refs/heads/master 3231bdcee2336b13684a283960c0bba366519dfa $ touch new-commit.txt $ git add new-commit.txt $ git commit -m &quot;does master follow this new commit?&quot; [master cfa86f4] does master follow this new commit? 1 file changed, 0 insertions(+), 0 deletions(-) create mode 100644 new-commit.txt $ cat .git/refs/heads/master cfa86f4465926576eb3a9e0bc77e115af0467551 #指向了新的提交 $ git log --graph --oneline * cfa86f4 does master follow this new commit? * 3231bdc Which version checked in? * 0e2185a initialized. 重置版本库（危险动作） $ git reset --hard HEAD^ HEAD is now at 3231bdc Which version checked in? $ git reset --hard 0e2185a HEAD is now at 0e2185a initialized. $ cat welcome.txt Hello. 重置命令很危险，会彻底底丢弃历史。还能通过浏览历史的办法找回历史的提交ID然后再恢复吗？不能！因为提交历史也没了。 用reflog挽救错误的重置$ tail -10 .git/logs/refs/heads/master 0000000000000000000000000000000000000000 0e2185a617f030d147e7092d34c478125da861b1 Gino Zhang &lt;zf_zmc@163.com&gt; 1481703025 +0800 commit (initial): initialized. 0e2185a617f030d147e7092d34c478125da861b1 3231bdcee2336b13684a283960c0bba366519dfa Gino Zhang &lt;zf_zmc@163.com&gt; 1481706190 +0800 commit: Which version checked in? 3231bdcee2336b13684a283960c0bba366519dfa cfa86f4465926576eb3a9e0bc77e115af0467551 Gino Zhang &lt;zf_zmc@163.com&gt; 1482915083 +0800 commit: does master follow this new commit? cfa86f4465926576eb3a9e0bc77e115af0467551 3231bdcee2336b13684a283960c0bba366519dfa Gino Zhang &lt;zf_zmc@163.com&gt; 1482915422 +0800 reset: moving to HEAD^ 3231bdcee2336b13684a283960c0bba366519dfa 0e2185a617f030d147e7092d34c478125da861b1 Gino Zhang &lt;zf_zmc@163.com&gt; 1482915443 +0800 reset: moving to 0e2185a To be continued…]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Elasticsearch相关性打分机制学习]]></title>
      <url>%2Fpost%2Felasticsearch_relevancy_score%2F</url>
      <content type="text"><![CDATA[Elasticsearch全文搜索默认采用的是相关性打分TFIDF，在实际的运用中，我们采用Multi-Match给各个字段设置权重、使用should给特定文档权重或使用更高级的Function_Score来自定义打分，借助于Elasticsearch的explain功能，我们可以深入地学习一下其中的机制。 创建一个索引curl -s -XPUT &apos;http://localhost:9200/gino_test/&apos; -d &apos;{ &quot;mappings&quot;: { &quot;tweet&quot;: { &quot;properties&quot;: { &quot;text&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;term_vector&quot;: &quot;with_positions_offsets_payloads&quot;, &quot;store&quot; : true, &quot;analyzer&quot; : &quot;fulltext_analyzer&quot; }, &quot;fullname&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;term_vector&quot;: &quot;with_positions_offsets_payloads&quot;, &quot;analyzer&quot; : &quot;fulltext_analyzer&quot; } } } }, &quot;settings&quot; : { &quot;index&quot; : { &quot;number_of_shards&quot; : 1, &quot;number_of_replicas&quot; : 0 }, &quot;analysis&quot;: { &quot;analyzer&quot;: { &quot;fulltext_analyzer&quot;: { &quot;type&quot;: &quot;custom&quot;, &quot;tokenizer&quot;: &quot;whitespace&quot;, &quot;filter&quot;: [ &quot;lowercase&quot;, &quot;type_as_payload&quot; ] } } } } }&apos; 插入测试数据： _index _type _id text fullname gino_test tweet 1 hello world gino zhang gino_test tweet 2 gino like world cup gino li gino_test tweet 3 my cup jsper li 简单情况：单字段匹配打分POST http://192.168.102.216:9200/gino_test/_search { &quot;explain&quot;: true, &quot;query&quot;: { &quot;match&quot;: { &quot;text&quot;: &quot;my cup&quot; } } } 查询结果： score_simple.json 打分分析: Elasticsearch目前采用的默认相关性打分采用的是Lucene的TF-IDF技术。 我们来深入地分析一下这个公式： score(q,d) = queryNorm(q) · coord(q,d) · ∑ (tf(t,d) · idf(t)² · t.getBoost() · norm(t,d)) score(q,d) 是指查询输入Q和当前文档D的相关性得分； queryNorm(q) 是查询输入归一化因子，其作用是使最终的得分不至于太大，从而具有一定的可比性； coord(q,d) 是协调因子，表示输入的Token被文档匹配到的比例； tf(t,d) 表示输入的一个Token在文档中出现的频率，频率越高，得分越高； idf(t) 表示输入的一个Token的频率级别，它具体的计算与当前文档无关，而是与索引中出现的频率相关，出现频率越低，说明这个词是个稀缺词，得分会越高； t.getBoost() 是查询时指定的权重. norm(t,d) 是指当前文档的Term数量的一个权重，它在索引阶段就已经计算好，由于存储的关系，它最终值是0.125的倍数。 注意：在计算过程中，涉及的变量应该考虑的是document所在的分片而不是整个index。 score(q,d) = _score(q,d.f) --------- ① = queryNorm(q) · coord(q,d) · ∑ (tf(t,d) · idf(t)² · t.getBoost() · norm(t,d)) = coord(q,d) · ∑ (tf(t,d) · idf(t)² · t.getBoost() · norm(t,d) · queryNorm(q)) = coord(q,d.f) · ∑ _score(q.ti, d.f) [ti in q] --------- ② = coord(q,d.f) · (_score(q.t1, d.f) + _score(q.t2, d.f)) ① 相关性打分其实是查询与某个文档的某个字段之间的相关性打分，而不是与文档的相关性； ② 根据公式转换，就变成了查询的所有Term与文档中字段的相关性求和，如果某个Term不相关，则需要处理coord系数； multi-match多字段匹配打分（best_fields模式）POST http://192.168.102.216:9200/gino_test/_search { &quot;explain&quot;: true, &quot;query&quot;: { &quot;multi_match&quot;: { &quot;query&quot;: &quot;gino cup&quot;, &quot;fields&quot;: [ &quot;text^8&quot;, &quot;fullname^5&quot; ] } } } 查询结果：score_bestfields.json 打分分析： score(q,d) = max(_score(q, d.fi)) = max(_score(q, d.f1), _score(q, d.f2)) = max(coord(q,d.f1) · (_score(q.t1, d.f1) + _score(q.t2, d.f1)), coord(q,d.f2) · (_score(q.t1, d.f2) + _score(q.t2, d.f2))) 对于multi-field的best_fields模式来说，相当于是对每个字段对查询分别进行打分，然后执行max运算获取打分最高的。 在计算query weight的过程需要乘上字段的权重，在计算fieldNorm的时候也需要乘上字段的权重。 默认operator为or，如果使用and，打分机制也是一样的，但是搜索结果会不一样。 multi-match多字段匹配打分（cross_fields模式）POST http://192.168.102.216:9200/gino_test/_search { &quot;explain&quot;: true, &quot;query&quot;: { &quot;multi_match&quot;: { &quot;query&quot;: &quot;gino cup&quot;, &quot;type&quot;: &quot;cross_fields&quot;, &quot;fields&quot;: [ &quot;text^8&quot;, &quot;fullname^5&quot; ] } } } 查询结果：score_crossfields.json 打分分析： score(q, d) = ∑ (_score(q.ti, d.f)) = ∑ (_score(q.t1, d.f), _score(q.t1, d.f)) = ∑ (max(coord(q.t1,d.f) · _score(q.t1, d.f1), coord(q.t1,d.f) · _score(q.t1, d.f2)), max(coord(q.t2,d.f) · _score(q.t2, d.f1), coord(q.t2,d.f) · _score(q.t2, d.f2))) coord(q.t1,d.f)函数表示搜索的Term(如gino)在multi-field中有多少比率的字段匹配到；best_fields模式中coord(q,d.f1)表示搜索的所以Term(如gino和cup)有多少比率存在与特定的field字段（如text字段）里； 对于multi-field的cross_fields模式来说，相当于是对每个查询的Term进行打分（每个Term执行best_fields打分，即看下哪个field匹配更高），然后执行sum运算。 默认operator为or，如果使用and，打分机制也是一样的，但是搜索结果会不一样。这是一个使用operator为or的报文：score_crossfields_or.json should增加权重打分为了增加filter的测试，给gino_test/tweet增加一个tags的字段。 PUT /gino_test/_mapping/tweet { &quot;properties&quot;: { &quot;tags&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;analyzer&quot;: &quot;fulltext_analyzer&quot; } } } 增加tags的标签 _index _type _id text fullname tags gino_test tweet 1 hello world gino zhang new, gino gino_test tweet 2 gino like world cup gino li hobby, gino gino_test tweet 3 my cup jsper li goods, jasper POST http://192.168.102.216:9200/gino_test/_search { &quot;explain&quot;: true, &quot;query&quot;: { &quot;bool&quot;: { &quot;must&quot;: { &quot;bool&quot;: { &quot;must&quot;: { &quot;multi_match&quot;: { &quot;query&quot;: &quot;gino cup&quot;, &quot;fields&quot;: [ &quot;text^8&quot;, &quot;fullname^5&quot; ], &quot;type&quot;: &quot;best_fields&quot;, &quot;operator&quot;: &quot;or&quot; } }, &quot;should&quot;: [ { &quot;term&quot;: { &quot;tags&quot;: { &quot;value&quot;: &quot;goods&quot;, &quot;boost&quot;: 6 } } }, { &quot;term&quot;: { &quot;tags&quot;: { &quot;value&quot;: &quot;hobby&quot;, &quot;boost&quot;: 3 } } } ] } } } } } 查询结果：score_should.json 打分分析： 增加了should的权重之后，相当于多了一个打分参考项，打分的过程见上面的计算过程。 function_score高级打分机制DSL格式： { &quot;function_score&quot;: { &quot;query&quot;: {}, &quot;boost&quot;: &quot;boost for the whole query&quot;, &quot;functions&quot;: [ { &quot;filter&quot;: {}, &quot;FUNCTION&quot;: {}, &quot;weight&quot;: number }, { &quot;FUNCTION&quot;: {} }, { &quot;filter&quot;: {}, &quot;weight&quot;: number } ], &quot;max_boost&quot;: number, &quot;score_mode&quot;: &quot;(multiply|max|...)&quot;, &quot;boost_mode&quot;: &quot;(multiply|replace|...)&quot;, &quot;min_score&quot; : number } } 支持四种类型发FUNCTION: script_score: 自定义的高级打分机制，涉及的字段只能是数值类型的 weight: 权重打分，一般结合filter一起使用，表示满足某种条件加多少倍的分 random_score： 生成一个随机分数，比如应该uid随机打乱排序 field_value_factor： 根据index里的某个字段值影响打分，比如销量（涉及的字段只能是数值类型的） decay functions: 衰减函数打分，比如越靠近市中心的打分越高 来做一个实验。先给index增加一个查看数的字段： PUT /gino_test/_mapping/tweet { &quot;properties&quot;: { &quot;views&quot;: { &quot;type&quot;: &quot;long&quot;, &quot;doc_values&quot;: true, &quot;fielddata&quot;: { &quot;format&quot;: &quot;doc_values&quot; } } } 给三条数据分别加上查看数的值： POST gino_test/tweet/1/_update { &quot;doc&quot; : { &quot;views&quot; : 56 } } 最终数据的样子： _index _type _id text fullname tags views gino_test tweet 1 hello world gino zhang new, gino 56 gino_test tweet 2 gino like world cup gino li hobby, gino 21 gino_test tweet 3 my cup jsper li goods, jasper 68 执行一个查询： { &quot;explain&quot;: true, &quot;query&quot;: { &quot;function_score&quot;: { &quot;query&quot;: { &quot;multi_match&quot;: { &quot;query&quot;: &quot;gino cup&quot;, &quot;type&quot;: &quot;cross_fields&quot;, &quot;fields&quot;: [ &quot;text^8&quot;, &quot;fullname^5&quot; ] } }, &quot;boost&quot;: 2, &quot;functions&quot;: [ { &quot;field_value_factor&quot;: { &quot;field&quot;: &quot;views&quot;, &quot;factor&quot;: 1.2, &quot;modifier&quot;: &quot;sqrt&quot;, &quot;missing&quot;: 1 } }, { &quot;filter&quot;: { &quot;term&quot;: { &quot;tags&quot;: { &quot;value&quot;: &quot;goods&quot; } } }, &quot;weight&quot;: 4 } ], &quot;score_mode&quot;: &quot;multiply&quot;, &quot;boost_mode&quot;: &quot;multiply&quot; } } } 查询结果：score_function.json 打分分析： score(q,d) = score_query(q,d) * (score_fvf(`view`) * score_filter(`tags:goods`)) score_mode表示多个FUNCTION之间打分的运算法则，需要注意不同的FUNCTION的打分的结果级别可能相差很大； boost_mode表示function_score和query_score打分的运算法则，也需要注意打分结果的级别； rescore重打分机制ES官网介绍: https://www.elastic.co/guide/en/elasticsearch/reference/2.3/search-request-rescore.html 重打分机制并不会应用到所有的数据中。比如需要查询前10条数据，那么所有的分片先按默认规则查询出前10条数据，然后应用rescore规则进行重打分返回给master节点进行综合排序返回给用户。 rescore支持多个规则计算，以及与原先的默认打分进行运算（权重求和等）。 rescore因为计算的打分的document较少，性能应该会更好一点，但是这个涉及到全局排序，实际运用的场景要注意。 参考材料 Elasticsearch官方文档 ElasticDearch权威指南 Lucene TFIDF算法]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[高效工作摆脱拖延的神器 -- 番茄工作法]]></title>
      <url>%2Fpost%2Fpomodoro_technique_notes%2F</url>
      <content type="text"><![CDATA[在工作中你是否曾有过这样的感受：一天浑浑噩噩不知道干了什么，或者整天被他人打断而不能专注，又或者从早到晚的忙导致下班头脑发胀？在试用了一个月的番茄工作法之后，我准备把它推荐给大家，希望能真正的帮大家高效工作摆脱拖延。 先给大家分享下我写的《番茄工作法图解-简单易行的时间管理方法》读书笔记，在后面我会整理下我实践过程中使用的工具和流程。 一次只做一件事为什么要用番茄工作法？我想大部分的人和我一样，一定是经常无法专注于做自己当前最重要的事情。而番茄工作法利用一个个短短的番茄钟来使自己专注于当前的25分钟，并努力保护番茄钟不被破坏，从而利用惯性的力量使自己更新专注。 背景为什么番茄工作法能够奏效？作者从生理、心理和一些有趣的现象入手，帮我们了解专注和习惯的力量。 方法来，我们一步步到手开始真正的玩一把番茄工作法。 中断在使用番茄工作法的过程中，总是难免碰到打扰，使得我们不得不停止当前手头的事情，是逃避愤怒还是实事求是地面对？ 预估使用番茄工作法一个容易被忽视的问题就是预估，到底这个任务应该需要多少个番茄，我今天应该给自己安排多少个番茄？我想这些都要随着自己一段时间的使用慢慢地更加精确，不过前提是你要行动起来并且随时记录和总结。 应变每个人的情况不同，周边能使用的工具不同，因此很有必要根据自己的喜好制定出一套属于自己的番茄工作流程。 团队番茄工作法也适合团队，特别是对于多人会议，是不是被那些杂乱而拖沓的会议烦扰，能否大家会前立个规，这次我们只来它三个番茄？ PS：我的番茄工作法我的番茄工作流程就像《番茄工作法图解》一书中第六章应变所述，我们没有必要刻板地按照书本的方法一一执行，而是应该选择适合自己的工具的流程。 这是一个月前我刚试用番茄工作法时给自己画的一个简易流程： 我采用的工具Chrome的Any.DO插件：负责维护近期的活动清单。Any.DO的最大特点是功能强大又易操作。 Chrome的番茄计时器插件: 这款计时插件界面简洁还提供了统计和云同步功能。 一个简单的Excel表格，用于记录每天番茄的完成情况。 我碰到的问题 最多的就是容易被外部中断，这个最重要的还是要诚实地去记录。我一般的处理方法是会在特定的时间主动地和周边去讨论近期的计划和安排，另外一个就是新记录一个活动清单，然后再按优先级去完成； 另外一个就是番茄钟完成后当前手头的工作意犹未尽，不想打断思路，所以会经常出现放弃休息的情况。我现在的处理方式是放弃4个番茄钟长休息之前的那个休息时间，这样一天下来我大概有三个连续的番茄钟，其他时间则强迫自己休息； 最后一个就是优先级的问题，一般来说，我一天会给自己安排七八个番茄钟的活动，其中有两三个是自我学习的番茄钟。这样我总是把工作相关的和临时紧急的事情排在前面，自我学习相关的做延后处理。 写在最后当我回头看，番茄工作法确实让我能更加专注于当天的任务，很多原本不知道如何开始的工作便一步步开始了。同时番茄工作法很强调大脑的休息和放松，这样才能让自己更高效更轻松，从而实现良性循环。希望大家和我一起动起来，开始试试这个神器。 点击下载完整的XMind思维导图：下载]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[《Java 8函数式编程》读书笔记]]></title>
      <url>%2Fpost%2Fjava8_lambda_notes%2F</url>
      <content type="text"><![CDATA[Java 8版本可能是Java 5版本之后最令人激动的一次，其中的Lambda表达式和对集合的流操作被许多Java程序员推荐。目前我们项目组采用了最新的Java 8版本，很多原来的老式的操作集合的代码使用集合Steam进行了重构。为了深入学习下相关特性，花了一周的时间阅读了这本《Java 8函数式编程》。 简介Java 为什么需要引入函数式编程 Lambda 表达式认识Lambda表达式、函数接口、JDK核心函数接口 流感受Java 8集合流操作的魅力 类库深入Java 8的类库，了解最新特性 高级集合类和收集器全书的最大核心部分，深入讲解集合如何进行函数式编程，感受Cellectors收集器的强大 数据并行化使用集合是流操作之后，数据并行化变得轻而易举，再也不用写一堆烦人的fork-join了。 测试、调试和重构说实话，流的引入对于代码的调试是个挑战，作者提供的思路重点在peek方法 设计和架构的原则函数式编程如何简化常见设计模式和设计原则 使用Lambda 表达式编写并发程序使用Vert.X和RxJava框架介绍了异步消息和响应式编程，以及引入函数式编程的代码简化 个人感受在最近开发的一个搜索词自动联想的功能中，我大量使用了Java 8的集合流操作，确实给人耳目一新的感觉，代码变得更加易读，并行化也非常容易。一百多页的书，大概两三个小时就可以读完，加上译者的水平不错，读起来非常流畅，非常值得一读，五星推荐。 点击下载完整的XMind思维导图：下载]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Mysql增量订阅和消费组件Canal学习小记]]></title>
      <url>%2Fpost%2Flearning_canal%2F</url>
      <content type="text"><![CDATA[在电商的搜索模块中，需要支持全量索引和增量索引。全量搜索是指根据数据库的数据重建索引里的数据，一般耗时耗性能，可能一天甚至几天重建一次；而增量索引是根据业务库数据的变化，准实时地更新该部分内容到索引中。对于增量索引而言，就需要能够监听业务的表数据变化，阿里开源的Canal组件便是一个不错的选择。 什么是Mysql的二进制日志（binlog）？ binlog记录了数据库变更的事件（如create table、update records）。由一组二进制日志文件（如log-bin.000010）和一个索引文件（如log-bin.index）组成。 binlog有两个重要用途：主从复制和数据恢复； 通过log-bin参数开启binlog，通过binlog-do-db和binlog-ignore-db参数配置记录和忽略的schema； 使用RESET MASTER命令可以清除已记录的binlog； 使用Mysql内置的mysqlbinlog工具可以查看和操作binlog； binlog有三种格式：基于语句的、基于行数据和混合模式； 关于记录binlog的时机，官网上如是说： Binary logging is done immediately after a statement or transaction completes but before any locks are released or any commit is done. This ensures that the log is logged in commit order. Within an uncommitted transaction, all updates (UPDATE, DELETE, or INSERT) that change transactional tables such as InnoDB tables are cached until a COMMIT statement is received by the server. At that point, mysqld writes the entire transaction to the binary log before the COMMIT is executed. binlog的Event结构（目前主要用的是v4版本），下表中的字段名后的两个用冒号分隔的是offset和length。 +=====================================+ | event | timestamp 0 : 4 | | header +----------------------------+ | | type_code 4 : 1 | = FORMAT_DESCRIPTION_EVENT = 15 | +----------------------------+ | | server_id 5 : 4 | | +----------------------------+ | | event_length 9 : 4 | &gt;= 91 | +----------------------------+ | | next_position 13 : 4 | | +----------------------------+ | | flags 17 : 2 | +=====================================+ | event | binlog_version 19 : 2 | = 4 | data +----------------------------+ | | server_version 21 : 50 | | +----------------------------+ | | create_timestamp 71 : 4 | | +----------------------------+ | | header_length 75 : 1 | | +----------------------------+ | | post-header 76 : n | = array of n bytes, one byte per event | | lengths for all | type that the server knows about | | event types | +=====================================+ 什么是Canal？Canal是由Alibaba开源的一个基于binlog的增量日志组件，其核心原理是Canal伪装成Mysql的slave，发送dump协议获取binlog，解析并存储起来给客户端消费。 Canal的核心架构整体架构 Canal是一个CS的架构，Client和Server基于netty进行通信，采用的protobuf协议； Server包含了一个或多个Instance（一个Instance可以想象为监听一个数据库的binlog），Server将Client的请求转至具体的Instance处理； 一个Instance又包含了EventParser、EventSink和EventStore三个核心部件。EventParser负责获取Mysql的binlog并进行解析，然后将解析的Event交给EventSink进行过滤和路由处理，处理之后的数据交由EventStore进行存储。而Server转过来的请求最终就会获取或者操作EventStore上存储的数据。 EventParser设计处理流程 MysqlConnection为EventParser的核心组件，它通过dump协议不断地从Mysql获取binlog，并交给EventParser处理； 如果配置了Mysql的配置了standby，MysqlConnection支持失效转移，从standby的数据库获取binlog； MysqlEventParser为核心组件，接收到的Binaly Log的通过Binlog parser进行协议解析，补充一些特定信息； MysqlEventParser传递给EventSink模块进行数据存储，是一个阻塞操作，直到存储成功； 存储成功后，由CanalLogPositionManager定时记录Binaly Log位置，其持久化策略支持内存的、文件的、ZK的以及混合式的； 代码设计 核心源码解析以下核心源码为AbstractEventParser类的start方法，具体的执行流程见代码注释： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144public void start() &#123; super.start(); MDC.put("destination", destination); // 配置transaction buffer // 初始化缓冲队列 transactionBuffer.setBufferSize(transactionSize);// 设置buffer大小 transactionBuffer.start(); // 构造bin log parser binlogParser = buildParser();// 初始化一下BinLogParser binlogParser.start(); // 启动工作线程 parseThread = new Thread(new Runnable() &#123; public void run() &#123; MDC.put("destination", String.valueOf(destination)); ErosaConnection erosaConnection = null; while (running) &#123; try &#123; // 开始执行replication // 1. 构造Erosa连接 erosaConnection = buildErosaConnection(); // 2. 启动一个心跳线程 startHeartBeat(erosaConnection); // 3. 执行dump前的准备工作 preDump(erosaConnection); erosaConnection.connect();// 链接 // 4. 获取最后的位置信息 final EntryPosition startPosition = findStartPosition(erosaConnection); if (startPosition == null) &#123; throw new CanalParseException("can't find start position for " + destination); &#125; logger.info("find start position : &#123;&#125;", startPosition.toString()); // 重新链接，因为在找position过程中可能有状态，需要断开后重建 erosaConnection.reconnect(); final SinkFunction sinkHandler = new SinkFunction&lt;EVENT&gt;() &#123; private LogPosition lastPosition; public boolean sink(EVENT event) &#123; try &#123; CanalEntry.Entry entry = parseAndProfilingIfNecessary(event); if (!running) &#123; return false; &#125; if (entry != null) &#123; exception = null; // 有正常数据流过，清空exception transactionBuffer.add(entry); // 记录一下对应的positions this.lastPosition = buildLastPosition(entry); // 记录一下最后一次有数据的时间 lastEntryTime = System.currentTimeMillis(); &#125; return running; &#125; catch (TableIdNotFoundException e) &#123; throw e; &#125; catch (Exception e) &#123; // 记录一下，出错的位点信息 processError(e, this.lastPosition, startPosition.getJournalName(), startPosition.getPosition()); throw new CanalParseException(e); // 继续抛出异常，让上层统一感知 &#125; &#125; &#125;; // 4. 开始dump数据 if (StringUtils.isEmpty(startPosition.getJournalName()) &amp;&amp; startPosition.getTimestamp() != null) &#123; erosaConnection.dump(startPosition.getTimestamp(), sinkHandler); &#125; else &#123; erosaConnection.dump(startPosition.getJournalName(), startPosition.getPosition(), sinkHandler); &#125; &#125; catch (TableIdNotFoundException e) &#123; exception = e; // 特殊处理TableIdNotFound异常,出现这样的异常，一种可能就是起始的position是一个事务当中，导致tablemap // Event时间没解析过 needTransactionPosition.compareAndSet(false, true); logger.error(String.format("dump address %s has an error, retrying. caused by ", runningInfo.getAddress().toString()), e); &#125; catch (Throwable e) &#123; exception = e; if (!running) &#123; if (!(e instanceof java.nio.channels.ClosedByInterruptException || e.getCause() instanceof java.nio.channels.ClosedByInterruptException)) &#123; throw new CanalParseException(String.format("dump address %s has an error, retrying. ", runningInfo.getAddress().toString()), e); &#125; &#125; else &#123; logger.error(String.format("dump address %s has an error, retrying. caused by ", runningInfo.getAddress().toString()), e); sendAlarm(destination, ExceptionUtils.getFullStackTrace(e)); &#125; &#125; finally &#123; // 关闭一下链接 afterDump(erosaConnection); try &#123; if (erosaConnection != null) &#123; erosaConnection.disconnect(); &#125; &#125; catch (IOException e1) &#123; if (!running) &#123; throw new CanalParseException(String.format("disconnect address %s has an error, retrying. ", runningInfo.getAddress().toString()), e1); &#125; else &#123; logger.error("disconnect address &#123;&#125; has an error, retrying., caused by ", runningInfo.getAddress().toString(), e1); &#125; &#125; &#125; // 出异常了，退出sink消费，释放一下状态 eventSink.interrupt(); transactionBuffer.reset();// 重置一下缓冲队列，重新记录数据 binlogParser.reset();// 重新置位 if (running) &#123; // sleep一段时间再进行重试 try &#123; Thread.sleep(10000 + RandomUtils.nextInt(10000)); &#125; catch (InterruptedException e) &#123; &#125; &#125; &#125; MDC.remove("destination"); &#125; &#125;); parseThread.setUncaughtExceptionHandler(handler); parseThread.setName(String.format("destination = %s , address = %s , EventParser", destination, runningInfo == null ? null : runningInfo.getAddress().toString())); parseThread.start();&#125; EventSink设计处理流程 数据过滤：支持通配符的过滤模式，表名、字段内容等； 数据路由/分发：解决1:n (1个parser对应多个store的模式)； 数据归并：解决n:1 (多个parser对应1个store)； 数据加工：在进入store之前进行额外的处理，比如join； 代码设计 核心源码解析以下核心代码为EntryEventSink类的sinkData方法：1234567891011121314151617181920212223242526272829303132333435363738private boolean sinkData(List&lt;CanalEntry.Entry&gt; entrys, InetSocketAddress remoteAddress) throws InterruptedException &#123; boolean hasRowData = false; boolean hasHeartBeat = false; List&lt;Event&gt; events = new ArrayList&lt;Event&gt;(); for (CanalEntry.Entry entry : entrys) &#123; Event event = new Event(new LogIdentity(remoteAddress, -1L), entry); if (!doFilter(event)) &#123; continue; &#125; events.add(event); hasRowData |= (entry.getEntryType() == EntryType.ROWDATA); hasHeartBeat |= (entry.getEntryType() == EntryType.HEARTBEAT); &#125; if (hasRowData) &#123; // 存在row记录 return doSink(events); &#125; else if (hasHeartBeat) &#123; // 存在heartbeat记录，直接跳给后续处理 return doSink(events); &#125; else &#123; // 需要过滤的数据 if (filterEmtryTransactionEntry &amp;&amp; !CollectionUtils.isEmpty(events)) &#123; long currentTimestamp = events.get(0).getEntry().getHeader().getExecuteTime(); // 基于一定的策略控制，放过空的事务头和尾，便于及时更新数据库位点，表明工作正常 if (Math.abs(currentTimestamp - lastEmptyTransactionTimestamp) &gt; emptyTransactionInterval || lastEmptyTransactionCount.incrementAndGet() &gt; emptyTransctionThresold) &#123; lastEmptyTransactionCount.set(0L); lastEmptyTransactionTimestamp = currentTimestamp; return doSink(events); &#125; &#125; // 直接返回true，忽略空的事务头和尾 return true; &#125;&#125; EventStore设计处理流程 EventStore借鉴了Disruptor的RingBuffer的实现思路，定义了3个cursor：Put（Sink模块进行数据存储的最后一次写入位置）、Get（数据订阅获取的最后一次提取位置）、Ack（数据消费成功的最后一次消费位置）； 目前仅实现了Memory内存模式，暂不支持文件等方式存储； 这里Ack的设计主要是为了确保消息被客户端正常消费，当客户端调用rollback时，对应的Get会重置到Ack的位置； 代码设计 核心源码解析以下核心代码为MemoryEventStoreWithBuffer类的几个重要操作方法： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169/** * 执行具体的put操作 */private void doPut(List&lt;Event&gt; data) &#123; long current = putSequence.get(); long end = current + data.size(); // 先写数据，再更新对应的cursor,并发度高的情况，putSequence会被get请求可见，拿出了ringbuffer中的老的Entry值 for (long next = current + 1; next &lt;= end; next++) &#123; entries[getIndex(next)] = data.get((int) (next - current - 1)); &#125; putSequence.set(end); // 记录一下gets memsize信息，方便快速检索 if (batchMode.isMemSize()) &#123; long size = 0; for (Event event : data) &#123; size += calculateSize(event); &#125; putMemSize.getAndAdd(size); &#125; // tell other threads that store is not empty notEmpty.signal();&#125;private Events&lt;Event&gt; doGet(Position start, int batchSize) throws CanalStoreException &#123; LogPosition startPosition = (LogPosition) start; long current = getSequence.get(); long maxAbleSequence = putSequence.get(); long next = current; long end = current; // 如果startPosition为null，说明是第一次，默认+1处理 if (startPosition == null || !startPosition.getPostion().isIncluded()) &#123; // 第一次订阅之后，需要包含一下start位置，防止丢失第一条记录 next = next + 1; &#125; if (current &gt;= maxAbleSequence) &#123; return new Events&lt;Event&gt;(); &#125; Events&lt;Event&gt; result = new Events&lt;Event&gt;(); List&lt;Event&gt; entrys = result.getEvents(); long memsize = 0; if (batchMode.isItemSize()) &#123; end = (next + batchSize - 1) &lt; maxAbleSequence ? (next + batchSize - 1) : maxAbleSequence; // 提取数据并返回 for (; next &lt;= end; next++) &#123; Event event = entries[getIndex(next)]; if (ddlIsolation &amp;&amp; isDdl(event.getEntry().getHeader().getEventType())) &#123; // 如果是ddl隔离，直接返回 if (entrys.size() == 0) &#123; entrys.add(event);// 如果没有DML事件，加入当前的DDL事件 end = next; // 更新end为当前 &#125; else &#123; // 如果之前已经有DML事件，直接返回了，因为不包含当前next这记录，需要回退一个位置 end = next - 1; // next-1一定大于current，不需要判断 &#125; break; &#125; else &#123; entrys.add(event); &#125; &#125; &#125; else &#123; long maxMemSize = batchSize * bufferMemUnit; for (; memsize &lt;= maxMemSize &amp;&amp; next &lt;= maxAbleSequence; next++) &#123; // 永远保证可以取出第一条的记录，避免死锁 Event event = entries[getIndex(next)]; if (ddlIsolation &amp;&amp; isDdl(event.getEntry().getHeader().getEventType())) &#123; // 如果是ddl隔离，直接返回 if (entrys.size() == 0) &#123; entrys.add(event);// 如果没有DML事件，加入当前的DDL事件 end = next; // 更新end为当前 &#125; else &#123; // 如果之前已经有DML事件，直接返回了，因为不包含当前next这记录，需要回退一个位置 end = next - 1; // next-1一定大于current，不需要判断 &#125; break; &#125; else &#123; entrys.add(event); memsize += calculateSize(event); end = next;// 记录end位点 &#125; &#125; &#125; PositionRange&lt;LogPosition&gt; range = new PositionRange&lt;LogPosition&gt;(); result.setPositionRange(range); range.setStart(CanalEventUtils.createPosition(entrys.get(0))); range.setEnd(CanalEventUtils.createPosition(entrys.get(result.getEvents().size() - 1))); // 记录一下是否存在可以被ack的点 for (int i = entrys.size() - 1; i &gt;= 0; i--) &#123; Event event = entrys.get(i); if (CanalEntry.EntryType.TRANSACTIONBEGIN == event.getEntry().getEntryType() || CanalEntry.EntryType.TRANSACTIONEND == event.getEntry().getEntryType() || isDdl(event.getEntry().getHeader().getEventType())) &#123; // 将事务头/尾设置可被为ack的点 range.setAck(CanalEventUtils.createPosition(event)); break; &#125; &#125; if (getSequence.compareAndSet(current, end)) &#123; getMemSize.addAndGet(memsize); notFull.signal(); return result; &#125; else &#123; return new Events&lt;Event&gt;(); &#125;&#125;public void rollback() throws CanalStoreException &#123; final ReentrantLock lock = this.lock; lock.lock(); try &#123; getSequence.set(ackSequence.get()); getMemSize.set(ackMemSize.get()); &#125; finally &#123; lock.unlock(); &#125;&#125;public void ack(Position position) throws CanalStoreException &#123; cleanUntil(position);&#125;public void cleanUntil(Position position) throws CanalStoreException &#123; final ReentrantLock lock = this.lock; lock.lock(); try &#123; long sequence = ackSequence.get(); long maxSequence = getSequence.get(); boolean hasMatch = false; long memsize = 0; for (long next = sequence + 1; next &lt;= maxSequence; next++) &#123; Event event = entries[getIndex(next)]; memsize += calculateSize(event); boolean match = CanalEventUtils.checkPosition(event, (LogPosition) position); if (match) &#123;// 找到对应的position，更新ack seq hasMatch = true; if (batchMode.isMemSize()) &#123; ackMemSize.addAndGet(memsize); // 尝试清空buffer中的内存，将ack之前的内存全部释放掉 for (long index = sequence + 1; index &lt; next; index++) &#123; entries[getIndex(index)] = null;// 设置为null &#125; &#125; if (ackSequence.compareAndSet(sequence, next)) &#123;// 避免并发ack notFull.signal(); return; &#125; &#125; &#125; if (!hasMatch) &#123;// 找不到对应需要ack的position throw new CanalStoreException("no match ack position" + position.toString()); &#125; &#125; finally &#123; lock.unlock(); &#125;&#125; Instance设计代码设计 核心源码解析CanalInstance的设计比较简单，主要是包含了上述三个组件： 1234567891011121314151617181920212223/** * 代表单个canal实例，比如一个destination会独立一个实例 */public interface CanalInstance extends CanalLifeCycle &#123; String getDestination(); CanalEventParser getEventParser(); CanalEventSink getEventSink(); CanalEventStore getEventStore(); CanalMetaManager getMetaManager(); CanalAlarmHandler getAlarmHandler(); /** * 客户端发生订阅/取消订阅行为 */ boolean subscribeChange(ClientIdentity identity);&#125; Servers设计代码设计 nettyCanal Server是基于netty和protobuf实现的，netty是一个很流行的nio框架，其官网描述为： Netty is a NIO client server framework which enables quick and easy development of network applications such as protocol servers and clients. It greatly simplifies and streamlines network programming such as TCP and UDP socket server. client 传递过来的消息会有个Channel对象接收，然后产生一个Channel事件，并发送到ChannelPipeline。 ChannelPipeline会选择一个ChannelHandler进行处理。 ChannelHandler处理之后，可能会产生新的ChannelEvent，并流转到下一个ChannelHandler。 protobufprotobuf是Google开发的一种数据描述语言语言，能够将结构化的数据序列化，可用于数据存储，通信协议等方面，官方版本支持C++、Java、Python，社区版本支持更多语言。相对于JSON和XML具有以下优点： 简洁； 体积小：消息大小只需要XML的1/10 ~ 1/3； 速度快：解析速度比XML快20 ~ 100倍； 使用Protobuf的编译器，可以生成更容易在编程中使用的数据访问代码； 更好的兼容性，Protobuf设计的一个原则就是要能够很好的支持向下或向上兼容； Canal主要包含以下两个protobuf定义：CanalProtocol.proto 和 EntryProtocol.proto 核心源码解析以下核心源码为CanalServerWithNetty类的start方法，其中piplines包含了四个Handler，最主要的Handler为SessionHandler。 123456789101112131415161718192021222324252627282930313233public void start() &#123; super.start(); if (!embeddedServer.isStart()) &#123; embeddedServer.start(); &#125; this.bootstrap = new ServerBootstrap(new NioServerSocketChannelFactory(Executors.newCachedThreadPool(), Executors.newCachedThreadPool())); // 构造对应的pipeline bootstrap.setPipelineFactory(new ChannelPipelineFactory() &#123; public ChannelPipeline getPipeline() throws Exception &#123; ChannelPipeline pipelines = Channels.pipeline(); pipelines.addLast(FixedHeaderFrameDecoder.class.getName(), new FixedHeaderFrameDecoder()); pipelines.addLast(HandshakeInitializationHandler.class.getName(), new HandshakeInitializationHandler()); pipelines.addLast(ClientAuthenticationHandler.class.getName(), new ClientAuthenticationHandler(embeddedServer)); SessionHandler sessionHandler = new SessionHandler(embeddedServer); pipelines.addLast(SessionHandler.class.getName(), sessionHandler); return pipelines; &#125; &#125;); // 启动 if (StringUtils.isNotEmpty(ip)) &#123; this.serverChannel = bootstrap.bind(new InetSocketAddress(this.ip, this.port)); &#125; else &#123; this.serverChannel = bootstrap.bind(new InetSocketAddress(this.port)); &#125;&#125; Client设计核心源码解析Canal Client支持对Server集群的处理，其是基于zk来实现的。对于Client最主要是负责封装请求发送给Server处理，以下为SimpleCanalConnector类的getWithoutAck方法： 1234567891011121314151617181920212223242526272829public Message getWithoutAck(int batchSize, Long timeout, TimeUnit unit) throws CanalClientException &#123; waitClientRunning(); try &#123; int size = (batchSize &lt;= 0) ? 1000 : batchSize; long time = (timeout == null || timeout &lt; 0) ? -1 : timeout; // -1代表不做timeout控制 if (unit == null) &#123; unit = TimeUnit.MILLISECONDS; &#125; writeWithHeader(channel, Packet.newBuilder() .setType(PacketType.GET) .setBody(Get.newBuilder() .setAutoAck(false) .setDestination(clientIdentity.getDestination()) .setClientId(String.valueOf(clientIdentity.getClientId())) .setFetchSize(size) .setTimeout(time) .setUnit(unit.ordinal()) .build() .toByteString()) .build() .toByteArray()); return receiveMessages(); &#125; catch (IOException e) &#123; throw new CanalClientException(e); &#125;&#125; Consumer样例代码我们的Consumer代码通过Client就可以向Canal Server获取binlog的Event进行增量消费了，以下是一个样例代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253 public void startClient() &#123; for (ZkClusterCanalConfig instance : canalInstances) &#123; threadPool.execute(() -&gt; &#123; Thread.currentThread().setName(Thread.currentThread().getName() + "-" + instance.getDestination()); initConnector(instance); while (!threadPool.isShutdown() &amp;&amp; !threadPool.isTerminated()) &#123; try &#123; handleCanalMessage(canalInstConnectors.get(instance.getDestination()), instance); &#125; catch (CanalClientException e) &#123; reConnectCanal(instance); &#125; catch (Throwable e) &#123; try &#123; Thread.sleep(1000); &#125; catch (InterruptedException ie) &#123; Thread.currentThread().interrupt(); &#125; &#125; &#125; &#125;); &#125;&#125;private void initConnector(ZkClusterCanalConfig instance) &#123; CanalConnector canalConnector = CanalConnectors.newClusterConnector(instance.getZkAddress(), instance.getDestination(), instance.getUsername(), instance.getPassword()); canalConnector.connect(); canalConnector.subscribe(instance.getSubscribeChannel()); canalConnector.rollback(); canalInstConnectors.put(instance.getDestination(), canalConnector);&#125;public void handleCanalMessage(CanalConnector connector, CanalConfig config) throws CanalClientException&#123; long batchId = 0; try &#123; Message message = connector.getWithoutAck(config.getFetchSize(), 10L, TimeUnit.SECONDS); batchId = message.getId(); int size = message.getEntries().size(); if (batchId &gt;= 0 &amp;&amp; size &gt; 0) &#123; List&lt;Entry&gt; entryList = message.getEntries(); for (Entry entry : entryList) &#123; handleEntry(entry); &#125; &#125; &#125; catch (CanalClientException e) &#123; throw e; &#125; catch (Exception e) &#123; logger.error(e.getMessage(), e); &#125; finally &#123; connector.ack(batchId); &#125;&#125; 参考资源 MySQL 5.7 Reference Manual – The Binary Log MySQL Internals Manual – The Binary Log MySQL 5.7 Reference Manual – mysqlbinlog Utility for Processing Binary Log Files alibaba/canal – 阿里巴巴mysql数据库binlog的增量订阅&amp;消费组件]]></content>
    </entry>

    
  
  
</search>
