<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>GinoBeFunny</title>
  <subtitle>Be a creator of knowledge.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://ginobefunny.com/"/>
  <updated>2017-06-14T08:57:56.492Z</updated>
  <id>http://ginobefunny.com/</id>
  
  <author>
    <name>Gino Zhang</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>面试小结之综合篇</title>
    <link href="http://ginobefunny.com/post/java_other_interview_questions/"/>
    <id>http://ginobefunny.com/post/java_other_interview_questions/</id>
    <published>2017-06-14T08:38:59.000Z</published>
    <updated>2017-06-14T08:57:56.492Z</updated>
    
    <content type="html"><![CDATA[<p>最近面试一些公司，被问到的关于编程基础、数据库、Redis和系统设计相关的问题，以及自己总结的回答。<br><a id="more"></a></p>
<h2 id="介绍一下你熟悉的几种排序算法以及它们的时间复杂度。"><a href="#介绍一下你熟悉的几种排序算法以及它们的时间复杂度。" class="headerlink" title="介绍一下你熟悉的几种排序算法以及它们的时间复杂度。"></a>介绍一下你熟悉的几种排序算法以及它们的时间复杂度。</h2><h3 id="冒泡排序"><a href="#冒泡排序" class="headerlink" title="冒泡排序"></a>冒泡排序</h3><ul>
<li>它重复地走访过要排序的数列，一次比较两个元素，如果他们的顺序错误就把他们交换过来。走访数列的工作是重复地进行直到没有再需要交换。</li>
<li>算法的平均时间复杂度为O(n^2)。</li>
<li>但是若在某趟排序中未发现气泡位置的交换，则说明待排序的无序区中所有气泡均满足轻者在上，重者在下的原则，即为正序。则冒泡排序过程可在此趟扫描后就终止，基于这种考虑，提出了第一种改进的算法。</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">bubbleSort</span><span class="params">(<span class="keyword">int</span>[] numbers)</span> </span>&#123;</div><div class="line">    <span class="keyword">int</span> size = numbers.length;</div><div class="line">    <span class="keyword">boolean</span> isSorted = <span class="keyword">false</span>;</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; size - <span class="number">1</span> &amp;&amp; !isSorted; i++) &#123;</div><div class="line">        isSorted = <span class="keyword">true</span>;</div><div class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; size - <span class="number">1</span> - i; j++) &#123;</div><div class="line">            <span class="keyword">if</span> (numbers[j] &gt; numbers[j + <span class="number">1</span>]) &#123;</div><div class="line">                swap(numbers, j, j + <span class="number">1</span>);</div><div class="line">                isSorted = <span class="keyword">false</span>;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="快速排序"><a href="#快速排序" class="headerlink" title="快速排序"></a>快速排序</h3><ul>
<li>快速排序是一种排序算法，对包含n个数的输入数组，平均时间为O（nlgn），最坏情况是O（n^2）。</li>
<li>快速排序时基于分治模式处理的，在数据集之中，选择一个元素作为”基准”（pivot），所有小于”基准”的元素，都移到”基准”的左边；所有大于”基准”的元素，都移到”基准”的右边。这个操作称为分区 (partition) 操作，分区操作结束后，基准元素所处的位置就是最终排序后它的位置。对”基准”左边和右边的两个子集，不断重复第一步和第二步，直到所有子集只剩下一个元素为止。</li>
<li>快排的局限性：快排是一个效率很高的排序算法，但是对于长度很小的序列，快排效率低；pivot选择不当，将导致树的不平衡，这样导致快排的时间复杂度为o(n^2)；当数组中有大量重复的元素，快排效率将非常之低；改进方法可以参考java.util.DualPivotQuicksort：小数组使用插入排序、双枢轴（快速三向切分）、划分策略优化（五取样划分）。</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">quick</span><span class="params">(<span class="keyword">int</span>[] numbers)</span> </span>&#123;</div><div class="line">    <span class="keyword">if</span> (numbers.length &gt; <span class="number">0</span>) &#123;</div><div class="line">        quickSort(numbers, <span class="number">0</span>, numbers.length - <span class="number">1</span>);</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">quickSort</span><span class="params">(<span class="keyword">int</span>[] numbers, <span class="keyword">int</span> low, <span class="keyword">int</span> high)</span> </span>&#123;</div><div class="line">    <span class="keyword">if</span> (low &lt; high) &#123;</div><div class="line">        <span class="keyword">int</span> middle = getMiddle(numbers, low, high); <span class="comment">//将numbers数组进行一分为二</span></div><div class="line">        quickSort(numbers, low, middle - <span class="number">1</span>);   <span class="comment">//对低字段表进行递归排序</span></div><div class="line">        quickSort(numbers, middle + <span class="number">1</span>, high); <span class="comment">//对高字段表进行递归排序</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">getMiddle</span><span class="params">(<span class="keyword">int</span>[] numbers, <span class="keyword">int</span> low, <span class="keyword">int</span> high)</span> </span>&#123;</div><div class="line">    <span class="keyword">int</span> temp = numbers[low]; <span class="comment">//数组的第一个作为中轴</span></div><div class="line">    <span class="keyword">while</span> (low &lt; high) &#123;</div><div class="line">        <span class="keyword">while</span> (low &lt; high &amp;&amp; numbers[high] &gt; temp) &#123;</div><div class="line">            high--;</div><div class="line">        &#125;</div><div class="line">        numbers[low] = numbers[high];<span class="comment">//比中轴小的记录移到低端</span></div><div class="line">        <span class="keyword">while</span> (low &lt; high &amp;&amp; numbers[low] &lt; temp) &#123;</div><div class="line">            low++;</div><div class="line">        &#125;</div><div class="line">        numbers[high] = numbers[low]; <span class="comment">//比中轴大的记录移到高端</span></div><div class="line">    &#125;</div><div class="line">    numbers[low] = temp; <span class="comment">//中轴记录到尾</span></div><div class="line">    <span class="keyword">return</span> low; <span class="comment">// 返回中轴的位置</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="堆排序"><a href="#堆排序" class="headerlink" title="堆排序"></a>堆排序</h3><ul>
<li>堆（二叉堆）可以视为一棵完全的二叉树，完全二叉树的一个“优秀”的性质是，除了最底层之外，每一层都是满的，这使得堆可以利用数组来表示（普通的一般的二叉树通常用链表作为基本容器表示），每一个结点对应数组中的一个元素。</li>
<li>二叉堆一般分为两种：最大堆和最小堆。最大堆：最大堆中的最大元素值出现在根结点（堆顶）；堆中每个父节点的元素值都大于等于其孩子结点（如果存在）；</li>
<li>堆排序就是把最大堆堆顶的最大数取出，将剩余的堆继续调整为最大堆，再次将堆顶的最大数取出，这个过程持续到剩余数只有一个时结束。在堆中定义以下几种操作：</li>
</ul>
<pre><code>最大堆调整（Max-Heapify）：将堆的末端子节点作调整，使得子节点永远小于父节点
创建最大堆（Build-Max-Heap）：将堆所有数据重新排序，使其成为最大堆
堆排序（Heap-Sort）：移除位在第一个数据的根节点，并做最大堆调整的递归运算
Parent(i) = floor((i-1)/2)，i 的父节点下标
Left(i) = 2i + 1，i 的左子节点下标
Right(i) = 2(i + 1)，i 的右子节点下标
</code></pre><ul>
<li>堆排序其实也是一种选择排序，是一种树形选择排序。只不过直接选择排序中，为了从R[1…n]中选择最大记录，需比较n-1次，然后从R[1…n-2]中选择最大记录需比较n-2次。事实上这n-2次比较中有很多已经在前面的n-1次比较中已经做过，而树形选择排序恰好利用树形的特点保存了部分前面的比较结果，因此可以减少比较次数。对于n个关键字序列，最坏情况下每个节点需比较log2(n)次，因此其最坏情况下时间复杂度为nlogn。堆排序为不稳定排序，不适合记录较少的排序。</li>
</ul>
<h2 id="谈谈对Spring-IoC的理解。"><a href="#谈谈对Spring-IoC的理解。" class="headerlink" title="谈谈对Spring IoC的理解。"></a>谈谈对Spring IoC的理解。</h2><ul>
<li>IoC不是什么技术，而是一种设计思想，它意味着将你设计好的对象交给容器控制，而不是传统的在你的对象内部直接控制。</li>
<li>理解好Ioc的关键是要明确：</li>
</ul>
<pre><code>谁控制谁 -- IoC容器控制了对象
控制什么 -- 主要控制了外部资源获取（不只是对象，包括比如文件等）
为何是反转 -- 由容器帮我们查找及注入依赖对象，对象只是被动的接受依赖对象
哪些方面反转了 -- 依赖对象的获取被反转了
</code></pre><p>传统做法</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3709321-f70145540ae48678.JPG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="传统做法"></p>
<p>IoC做法</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3709321-fa7206a6cbeb3200.JPG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="IoC做法"></p>
<ul>
<li>IoC能做什么：把创建和查找依赖对象的控制权交给了容器，由容器进行注入组合对象，所以对象与对象之间是松散耦合，这样也方便测试，利于功能复用，更重要的是使得程序的整个体系结构变得非常灵活；IoC很好的体现了面向对象设计法则之一 –  好莱坞法则；</li>
<li>IoC和DI是同一个概念的不同角度描述，“依赖注入”明确描述了“被注入对象依赖IoC容器配置依赖对象”。</li>
<li>在Spring里，BeanFactory提供了IoC容器最基本功能，而 ApplicationContext 则增加了更多支持企业级功能支持。ApplicationContext完全继承BeanFactory，因而BeanFactory所具有的语义也适用于ApplicationContext。ApplicationContext实现包括FileSystemXmlApplicationContext、ClassPathXmlApplicationContext、WebXmlApplicationContext；</li>
<li>Spring框架中bean的生命周期：</li>
</ul>
<pre><code>Spring容器从XML文件中读取bean的定义，并实例化bean。
Spring根据bean的定义填充所有的属性。
如果bean实现了BeanNameAware接口，Spring传递bean的ID到setBeanName方法。
如果Bean实现了BeanFactoryAware接口，Spring传递beanfactory给setBeanFactory方法。
如果有任何与bean相关联的BeanPostProcessors，Spring会在postProcesserBeforeInitialization()方法内调用它们。
如果bean实现IntializingBean了，调用它的afterPropertySet方法，如果bean声明了初始化方法，调用此初始化方法。
如果有BeanPostProcessors和bean关联，这些bean的postProcessAfterInitialization() 方法将被调用。
如果bean实现了DisposableBean，它将调用destroy()方法。
</code></pre><h2 id="谈谈对Spring-AOP的理解。"><a href="#谈谈对Spring-AOP的理解。" class="headerlink" title="谈谈对Spring AOP的理解。"></a>谈谈对Spring AOP的理解。</h2><ul>
<li>方面（Aspect）：一个关注点的模块化，这个关注点实现可能另外横切多个对象。事务管理是J2EE应用中一个很好的横切关注点例子。方面用Spring的Advisor或拦截器实现。 </li>
<li>连接点（Joinpoint）: 程序执行过程中明确的点，如方法的调用或特定的异常被抛出；</li>
<li>通知（Advice）: 在特定的连接点，AOP框架执行的动作。许多AOP框架包括Spring都是以拦截器做通知模型，维护一个“围绕”连接点的拦截器链。Spring中定义了四个advice: BeforeAdvice, AfterAdvice, ThrowAdvice和DynamicIntroductionAdvice；</li>
<li>切入点（Pointcut）: 指定一个通知将被引发的一系列连接点的集合。AOP框架必须允许开发者指定切入点：例如，使用正则表达式。 Spring定义了Pointcut接口，用来组合MethodMatcher和ClassFilter，可以通过名字很清楚的理解，MethodMatcher是用来检查目标类的方法是否可以被应用此通知，而ClassFilter是用来检查Pointcut是否应该应用到目标类上；</li>
<li><p>切点标志符(designator)：execution、within、this 与 target、bean、args、@annotation；</p>
</li>
<li><p>JDK动态代理和CGLib代理。</p>
</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// 定义接口</span></div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">ForumService</span> </span>&#123;</div><div class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">removeTopic</span><span class="params">(<span class="keyword">int</span> topic)</span></span>;</div><div class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">removeForum</span><span class="params">(<span class="keyword">int</span> forumId)</span></span>;</div><div class="line">	</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// 接口实现类</span></div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ForumServiceImpl</span> <span class="keyword">implements</span> <span class="title">ForumService</span></span>&#123;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">removeTopic</span><span class="params">(<span class="keyword">int</span> topic)</span></span>&#123;</div><div class="line">    	System.out.println(<span class="string">"模拟删除主题"</span>+topic);</div><div class="line">    	<span class="keyword">try</span>&#123;</div><div class="line">    		Thread.currentThread().sleep(<span class="number">20</span>);</div><div class="line">    	&#125;<span class="keyword">catch</span>(Exception e)&#123;</div><div class="line">    		<span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(e);</div><div class="line">    	&#125;</div><div class="line">    </div><div class="line">    &#125;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">removeForum</span><span class="params">(<span class="keyword">int</span> forumId)</span></span>&#123;</div><div class="line">    	System.out.println(<span class="string">"模拟删除论坛"</span>+forumId);</div><div class="line">    	<span class="keyword">try</span>&#123;</div><div class="line">    		Thread.currentThread().sleep(<span class="number">20</span>);</div><div class="line">    	&#125;<span class="keyword">catch</span>(Exception e)&#123;</div><div class="line">    		<span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(e);</div><div class="line">    	&#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// 工具类用于打印接口调用信息</span></div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PerformanceMonitor</span> </span>&#123;</div><div class="line">    <span class="keyword">private</span> <span class="keyword">static</span> ThreadLocal&lt;MethodPerformance&gt; performanceRecord=<span class="keyword">new</span> ThreadLocal&lt;MethodPerformance&gt;();</div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">begin</span><span class="params">(String method)</span></span>&#123;</div><div class="line">    	System.out.println(<span class="string">"begin monitor..."</span>);</div><div class="line">    	MethodPerformance mp = <span class="keyword">new</span> MethodPerformance(method);</div><div class="line">    	performanceRecord.set(mp);</div><div class="line">    &#125;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">end</span><span class="params">()</span></span>&#123;</div><div class="line">    	System.out.println(<span class="string">"end monitor..."</span>);</div><div class="line">    	MethodPerformance mp = performanceRecord.get();</div><div class="line">    	mp.printPerformance();</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MethodPerformance</span> </span>&#123;</div><div class="line">    <span class="keyword">private</span> <span class="keyword">long</span> begin;</div><div class="line">    <span class="keyword">private</span> <span class="keyword">long</span> end;</div><div class="line">    <span class="keyword">private</span> String serviceMethod;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="title">MethodPerformance</span><span class="params">(String serviceMethod)</span></span>&#123;</div><div class="line">        <span class="keyword">this</span>.serviceMethod = serviceMethod;</div><div class="line">        <span class="keyword">this</span>.begin = System.currentTimeMillis();</div><div class="line">    &#125;</div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">printPerformance</span><span class="params">()</span></span>&#123;</div><div class="line">        end = System.currentTimeMillis();</div><div class="line">        <span class="keyword">long</span> elapse = end - begin;</div><div class="line">        System.out.println(serviceMethod + <span class="string">"花费"</span> + elapse + <span class="string">"毫秒"</span>);</div><div class="line">    &#125;</div><div class="line">&#125; </div><div class="line"></div><div class="line"><span class="comment">// JDK动态代理需要实现InvocationHandler</span></div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PerformanceHandler</span> <span class="keyword">implements</span> <span class="title">InvocationHandler</span> </span>&#123;</div><div class="line">	<span class="keyword">private</span> Object target;</div><div class="line"></div><div class="line">	<span class="function"><span class="keyword">public</span> <span class="title">PerformanceHandler</span><span class="params">(Object object)</span> </span>&#123;</div><div class="line">		<span class="keyword">this</span>.target = object;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	<span class="meta">@Override</span></div><div class="line">	<span class="function"><span class="keyword">public</span> Object <span class="title">invoke</span><span class="params">(Object proxy, Method method, Object[] args)</span></span></div><div class="line">			<span class="keyword">throws</span> Throwable &#123;</div><div class="line">		PerformanceMonitor.begin(target.getClass().getName() + <span class="string">"."</span> + method.getName());</div><div class="line">		Object obj = method.invoke(target, args);</div><div class="line">		PerformanceMonitor.end();</div><div class="line">		<span class="keyword">return</span> obj;</div><div class="line">	&#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// JDK动态代理使用</span></div><div class="line">ForumServiceImpl target = <span class="keyword">new</span> ForumServiceImpl();</div><div class="line">PerformanceHandler handler = <span class="keyword">new</span> PerformanceHandler(target);</div><div class="line">ForumService proxy = (ForumService)Proxy.newProxyInstance(target.getClass().getClassLoader(),</div><div class="line">    target.getClass().getInterfaces(), handler);</div><div class="line">proxy.removeForum(<span class="number">23</span>);</div><div class="line">proxy.removeTopic(<span class="number">678</span>);</div><div class="line">		</div><div class="line"><span class="comment">// CGlib动态代理需要实现MethodInterceptor</span></div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CglibProxy</span> <span class="keyword">implements</span> <span class="title">MethodInterceptor</span></span>&#123;</div><div class="line">    <span class="keyword">private</span> Enhancer enhancer = <span class="keyword">new</span> Enhancer();</div><div class="line">    <span class="function"><span class="keyword">public</span> Object <span class="title">getProxy</span><span class="params">(Class clazz)</span></span>&#123;</div><div class="line">    	enhancer.setSuperclass(clazz);</div><div class="line">    	enhancer.setCallback(<span class="keyword">this</span>);</div><div class="line">    	<span class="keyword">return</span> enhancer.create();</div><div class="line">    &#125;</div><div class="line">    </div><div class="line">    <span class="meta">@Override</span></div><div class="line">    <span class="function"><span class="keyword">public</span> Object <span class="title">intercept</span><span class="params">(Object proxy, Method method, Object[] args,</span></span></div><div class="line">			MethodProxy methodProxy) <span class="keyword">throws</span> Throwable &#123;</div><div class="line">        PerformanceMonitor.begin(proxy.getClass().getName() + <span class="string">"."</span> + method.getName());</div><div class="line">        Object obj = methodProxy.invoke(proxy, args);</div><div class="line">        PerformanceMonitor.end();</div><div class="line">        <span class="keyword">return</span> obj;</div><div class="line">	&#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// CGlib动态代理使用</span></div><div class="line">CglibProxy proxy2 = <span class="keyword">new</span> CglibProxy();</div><div class="line">ForumServiceImpl forumServiceImpl = (ForumServiceImpl)proxy2.getProxy(ForumServiceImpl.class);</div><div class="line">forumServiceImpl.removeForum(<span class="number">456</span>);</div><div class="line">forumServiceImpl.removeTopic(<span class="number">987</span>);</div></pre></td></tr></table></figure>
<ul>
<li>两种动态代理的比较：CGLib所创建的动态代理对象的性能比JDK所创建的代理对象性能高不少，大概10倍，但CGLib在创建代理对象时所花费的时间却比JDK动态代理多大概8倍，所以对于singleton的代理对象或者具有实例池的代理，因为无需频繁的创建新的实例，所以比较适合CGLib动态代理技术，反之则适用于JDK动态代理技术。另外，由于CGLib采用动态创建子类的方式生成代理对象，所以不能对目标类中的final，private等方法进行处理。所以，大家需要根据实际的情况选择使用什么样的代理了。同样的，Spring的AOP编程中相关的ProxyFactory代理工厂内部就是使用JDK动态代理或CGLib动态代理的，通过动态代理，将增强（advice)应用到目标类中。</li>
</ul>
<h2 id="Spring-MVC的核心流程是什么样的？"><a href="#Spring-MVC的核心流程是什么样的？" class="headerlink" title="Spring MVC的核心流程是什么样的？"></a>Spring MVC的核心流程是什么样的？</h2><p><img src="http://upload-images.jianshu.io/upload_images/3709321-c123e66f595dbeea?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="核心流程"></p>
<p>具体步骤：</p>
<ol>
<li>首先用户发送请求——&gt;DispatcherServlet，前端控制器收到请求后自己不进行处理，而是委托给其他的解析器进行处理，作为统一访问点，进行全局的流程控制；</li>
<li>DispatcherServlet——&gt;HandlerMapping，HandlerMapping 将会把请求映射为 HandlerExecutionChain 对象（包含一个 Handler 处理器（页面控制器）对象、多个 HandlerInterceptor 拦截器）对象，通过这种策略模式，很容易添加新的映射策略；</li>
<li>DispatcherServlet——&gt;HandlerAdapter，HandlerAdapter 将会把处理器包装为适配器，从而支持多种类型的处理器，即适配器设计模式的应用，从而很容易支持很多类型的处理器；</li>
<li>HandlerAdapter——&gt;处理器功能处理方法的调用，HandlerAdapter 将会根据适配的结果调用真正的处理器的功能处理方法，完成功能处理；并返回一个 ModelAndView 对象（包含模型数据、逻辑视图名）；</li>
<li>ModelAndView 的逻辑视图名——&gt; ViewResolver， ViewResolver 将把逻辑视图名解析为具体的 View，通过这种策略模式，很容易更换其他视图技术；</li>
<li>View——&gt;渲染，View 会根据传进来的 Model 模型数据进行渲染，此处的 Model 实际是一个 Map 数据结构，因此很容易支持其他视图技术；</li>
<li>返回控制权给 DispatcherServlet，由 DispatcherServlet 返回响应给用户，到此一个流程结束。</li>
</ol>
<h2 id="Redis和Memcached的异同。"><a href="#Redis和Memcached的异同。" class="headerlink" title="Redis和Memcached的异同。"></a>Redis和Memcached的异同。</h2><h3 id="Memcached"><a href="#Memcached" class="headerlink" title="Memcached"></a>Memcached</h3><ol>
<li>可以利用多核优势，单实例吞吐量极高，可以达到几十万QPS；</li>
<li>只支持简单的key/value数据结构，不像Redis可以支持丰富的数据类型。</li>
<li>无法进行持久化，数据不能备份，只能用于缓存使用，且重启后数据全部丢失。</li>
<li>无法进行数据同步，不能将MC中的数据迁移到其他MC实例中。</li>
<li>Memcached内存分配采用Slab Allocation机制管理内存，value大小分布差异较大时会造成内存利用率降低，    并引发低利用率时依然出现踢出等问题。需要用户注重value设计。</li>
</ol>
<h3 id="Redis"><a href="#Redis" class="headerlink" title="Redis"></a>Redis</h3><ol>
<li>支持多种数据结构，如string、 list、hash、set、zset等；</li>
<li>支持持久化操作，可以进行aof及rdb数据持久化到磁盘，从而进行数据备份或数据恢复等操作，较好的防止数据丢失的手段；</li>
<li>支持通过Replication进行数据复制，通过master-slave机制，可以实时进行数据的同步复制来实现HA；</li>
<li>单线程请求，所有命令串行执行，并发情况下不需要考虑数据一致性问题，但性能受限于CPU性能，故单实例CPU最高才可能达到5-6wQPS每秒；</li>
<li>支持pub/sub消息订阅机制，可以用来进行消息订阅与通知；</li>
<li>Redis在string类型上会消耗较多内存，可以使用hash表压缩存储以降低内存耗用。</li>
</ol>
<h2 id="Redis作为分布式缓存可能会存在哪些问题，怎么解决？"><a href="#Redis作为分布式缓存可能会存在哪些问题，怎么解决？" class="headerlink" title="Redis作为分布式缓存可能会存在哪些问题，怎么解决？"></a>Redis作为分布式缓存可能会存在哪些问题，怎么解决？</h2><ul>
<li>缓存穿透预防及优化：缓存穿透是指查询一个根本不存在的数据，缓存层和存储层都不会命中；缓存穿透将导致不存在的数据每次请求都要到存储层去查询，失去了缓存保护后端存储的意义；解决方法：缓存空对象和布隆过滤器拦截；</li>
<li>缓存雪崩问题优化：由于缓存层承载着大量请求，有效的保护了存储层，但是如果缓存层由于某些原因整体不能提供服务，于是所有的请求都会达到存储层，存储层的调用量会暴增，造成存储层也会挂掉的情况；预防和解决缓存雪崩问题，可以从以下三个方面进行着手：保证缓存层服务高可用性、依赖隔离组件为后端限流并降级、提前演练。</li>
<li>缓存热点key重建优化：开发人员使用缓存和过期时间的策略既可以加速数据读写，又保证数据的定期更新，这种模式基本能够满足绝大部分需求。但如果热点Key和重建缓存耗时两个问题同时出现，可能就会对应用造成致命的危害；解决方法：互斥锁（只允许一个线程重建缓存）、永远不过期（唯一不足的就是重构缓存期间会出现数据不一致的情况）。</li>
</ul>
<h2 id="谈谈对Mysql索引的认识。"><a href="#谈谈对Mysql索引的认识。" class="headerlink" title="谈谈对Mysql索引的认识。"></a>谈谈对Mysql索引的认识。</h2><ul>
<li>主键查询走索引，我们一般使用的索引都是Btree索引；</li>
<li>MyISAM和InnoDB索引结构有很大差异，这里以InnoDB为例，InnoDB的叶节点存储的是数据的行，而除了主键之外的列索引存储的是主键key，也就是说在查询的时候需要二次查询，先通过列索引找到主键，再通过主键索引找到row。</li>
</ul>
<p><img src="http://upload-images.jianshu.io/upload_images/2177145-37c20115cb0950c1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="MySQL索引"></p>
<ul>
<li>针对我们经常查询的多列场景，我们可以建组合索引，组合索引在可以尽可能多的运用列的查询规则。说到组合索引那么必须说一下最左前缀原则：指的的是在sql where子句中一些条件或表达式中出现的列的顺序要保持和多索引的一致或以多列索引顺序出现，只要出现非顺序出现、断层都无法利用到多列索引。</li>
<li>索引的区分度，主要是衡量索引值不相同的程度，区分度越大，越有利于索引的查询。</li>
<li>组合索引的顺序和区分度：对于多个列构成的组合索引，在查询过滤的时候也是和列的位置有关的，这也是最左前缀规则说的事情，也就是说如果在第一次能过滤掉大量的数据，那么后续的索引匹配就能减少很多消耗。所以在选择索引顺序的时候最好是要考虑到区分度的问题，将区分度比较高的列放在前面。</li>
<li>只有当索引的列顺序和Order By子句的顺序完全一致时，并且所有的列的排序方向都一样时，才能使用索引对子句进行排序。</li>
<li>应该注意的几点：在使用索引查询的时候，需要保证索引类型和查询的数据类型一致，经常混用的是用int型查询varchar类型的数据或反过来，这样会导致索引失效；range查询要尽量放在后面，因为在range后面的查询不会走索引，这一点在设计索引时要注意；Like查询不能前缀模糊匹配，也就是说不可以like ‘%123’，因为like的后缀模糊 like ‘123%’可以转化为range查询，但是前缀模糊不可以；索引不是越多越好，索引十分大时不仅会影响查询效率，同时会为数据的插入造成很大的负担；对于重复索引需要删除，规划好索引是高效率的前提。</li>
<li>主键索引和唯一索引的区别：主键一定会创建一个唯一索引，但是有唯一索引的列不一定是主键；主键不允许为空值，唯一索引列允许空值；一个表只能有一个主键，但是可以有多个唯一索引主键可以被其他表引用为外键，唯一索引列不可以；主键是一种约束，而唯一索引是一种索引，是表的冗余数据结构，两者有本质的差别。</li>
</ul>
<h2 id="MySql的事务隔离级别有哪几种？"><a href="#MySql的事务隔离级别有哪几种？" class="headerlink" title="MySql的事务隔离级别有哪几种？"></a>MySql的事务隔离级别有哪几种？</h2><ul>
<li>隔离级别用于表述并发事务之间的相互干扰程度，其基于锁机制进行并发控制。</li>
<li>可序列化（Serializable）：事务一个接一个的执行，完全相互独立；实现可序列化要求在选定对象上的读锁和写锁保持直到事务结束后才能释放；在SELECT的查询中使用一个WHERE子句来描述一个范围时应该获得一个“范围锁”。</li>
<li>可重复度（Repeatable Read）：事务A读取数据之后，对涉及的数据加锁，不允许其他事务进行修改，由于其他事务会插入新的数据，因此会产生幻读；对选定对象的读锁和写锁一直保持到事务结束，但不要求“范围锁”，因此可能会发生幻读；可重复读是MySQL的默认事务隔离级别。</li>
<li>读取已提交（Read Committed）：只能看到其他事务已经提交的数据，避免了脏读，但存在不可重复读、幻读；DBMS需要对选定对象的写锁(write locks)一直保持到事务结束，但是读锁(read locks)在SELECT操作完成后马上释放，且不要求“范围锁”。</li>
<li>读取未提交（Read Uncommitted）：可以看到其他事务没有提交的数据，出现脏读、不可重复读、幻读；</li>
</ul>
<p><em>[PS补充]<br>不可重复读的重点是修改：同样的条件，读取过的数据，再次读取出来发现值不一样了。<br>幻读的重点在于新增或者删除：同样的条件，第1次和第2次读出来的记录数不一样。</em></p>
<h2 id="MySql的MVVC有什么作用？"><a href="#MySql的MVVC有什么作用？" class="headerlink" title="MySql的MVVC有什么作用？"></a>MySql的MVVC有什么作用？</h2><ul>
<li>多版本并发控制（MVCC），来实现MySQL上的多事务并发访问时，隔离级别控制；</li>
<li>数据版本：并发事务执行时，同一行数据有多个版本；</li>
<li>事务版本：每个事务都有一个事务版本；</li>
<li>版本有序：版本是通过时间来标识的；</li>
<li>通过MVCC实现的效果是同一时刻、同一张表、多个并发事务，看到的数据是不同的。</li>
<li>MVCC本质使用了copy-on-write，为每个数据保留多份snapshot，不同snapshot之间，使用指针连接成链表；update操作，能看到的snapshot是受限的，是链表上小于等于当前事务版本的最大版本（读取已提交：离当前事务最近的已提交版本）；</li>
</ul>
<h2 id="如何设计高性能、高并发、高可用的系统。"><a href="#如何设计高性能、高并发、高可用的系统。" class="headerlink" title="如何设计高性能、高并发、高可用的系统。"></a>如何设计高性能、高并发、高可用的系统。</h2><ul>
<li>系统架构三个利器：RPC服务组件、消息中间件（交互异步化、流量削峰）、配置管理（灰度发布、降级）；</li>
<li>无状态：接口层最重要的就是无状态，将有状态的数据剥离到数据库或缓存中；</li>
<li>如何改善延时：找关键路径（“28原则”）、空间换时间，如各级缓存；时间换空间，如传输压缩，解决网络传输的瓶颈；多核并行，减少锁竞争；更适合的算法和数据结构；通过性能测试和监控找出瓶颈；减少系统调用和上下文切换；</li>
<li>如何提高吞吐量：复制、扩容、异步化、缓存；</li>
<li>如何保障稳定性：提高可用性、分组和隔离、限流、降级、监控和故障切换；</li>
<li>理解高可用系统：要做到数据不丢，就必需要持久化；要做到服务高可用，就必需要有备用（复本），无论是应用结点还是数据结点；要做到复制，就会有数据一致性的问题；我们不可能做到100%的高可用，也就是说，我们能做到几个9个的SLA；</li>
</ul>
<h2 id="其他面试小结"><a href="#其他面试小结" class="headerlink" title="其他面试小结"></a>其他面试小结</h2><ul>
<li><a href="http://ginobefunny.com/post/elasticsearch_interview_questions/">面试小结之Elasticsearch篇</a></li>
<li><a href="http://ginobefunny.com/post/jvm_interview_questions/">面试小结之JVM篇</a></li>
<li><a href="http://ginobefunny.com/post/java_concurrent_interview_questions/">面试小结之并发篇</a></li>
<li><a href="http://ginobefunny.com/post/java_nio_interview_questions/">面试小结之IO篇</a></li>
<li><a href="http://ginobefunny.com/post/java_other_interview_questions/">面试小结之综合篇</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近面试一些公司，被问到的关于编程基础、数据库、Redis和系统设计相关的问题，以及自己总结的回答。&lt;br&gt;
    
    </summary>
    
      <category term="CodingLife" scheme="http://ginobefunny.com/categories/CodingLife/"/>
    
    
      <category term="职场" scheme="http://ginobefunny.com/tags/%E8%81%8C%E5%9C%BA/"/>
    
      <category term="Spring" scheme="http://ginobefunny.com/tags/Spring/"/>
    
      <category term="面试" scheme="http://ginobefunny.com/tags/%E9%9D%A2%E8%AF%95/"/>
    
      <category term="MySql" scheme="http://ginobefunny.com/tags/MySql/"/>
    
      <category term="Redis" scheme="http://ginobefunny.com/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>面试小结之IO篇</title>
    <link href="http://ginobefunny.com/post/java_nio_interview_questions/"/>
    <id>http://ginobefunny.com/post/java_nio_interview_questions/</id>
    <published>2017-06-14T08:38:50.000Z</published>
    <updated>2017-06-14T08:58:06.824Z</updated>
    
    <content type="html"><![CDATA[<p>最近面试一些公司，被问到的关于Java NIO编程的问题，以及自己总结的回答。<br><a id="more"></a></p>
<h2 id="谈谈对Java-IO的认识。"><a href="#谈谈对Java-IO的认识。" class="headerlink" title="谈谈对Java IO的认识。"></a>谈谈对Java IO的认识。</h2><ul>
<li>对于I/O操作来说, 其根本的作用在于传输数据。输入和输出指的仅是数据的流向，实际传输是通过某些具体的媒介来完成的，其中最主要的是文件系统和网络连接；</li>
<li>早期的java.io包把I/O操作抽象成数据的流动，进而有了流的概念；在Java NIO中，则把I/O操作抽象成端到端的一个数据连接，这就有了通道（channel）的概念；</li>
<li>Java中最基本的流是在字节这个层次上进行操作的；在read方法的调用是阻塞的，这可能会成为应用中的瓶颈（可以通过available方法获取在不阻塞的情况下可以获取到的字节数）；流无法重新使用，BufferedInputStream通过mark和reset操作可以实现流中部分内容的重复读取；另外一种重用输入流的方式是把它转换成数据来使用；</li>
<li>输出流是通过write方法把数据存放在缓冲区（缓冲区满了会自动执行写入），使用flush方法强制进行实际的写入操作；</li>
<li>其他常用流：FileInput(Output)Stream、ByteArrayInput(Output)Stream、字符流（new BufferedReader(new InputStreamReader(inputStream))）；</li>
</ul>
<h2 id="介绍一下Java-NIO中的Buffer、Channel和Selector的概念和作用。"><a href="#介绍一下Java-NIO中的Buffer、Channel和Selector的概念和作用。" class="headerlink" title="介绍一下Java NIO中的Buffer、Channel和Selector的概念和作用。"></a>介绍一下Java NIO中的Buffer、Channel和Selector的概念和作用。</h2><ul>
<li>Java NIO的缓冲区：使用数组的方式不够灵活且性能差，Java NIO的缓冲区功能更加强大；容量(capacity)表示缓冲区的额定大小，需要在创建时指定（allocate静态方法）；读写限制(limit)表示缓冲区在进行读写操作时的最大允许位置；读写位置(position)表示当前进行读写操作时的位置；缓冲区的很多操作（clear、flip、rewind）都是操作limit和position的值来实现重复读写；</li>
<li>Java NIO的通道：channel表示为一个已经建立好的到支持I/O操作的实体（如文件和网络）的连接，在此连接上进行数据的读写操作，使用的是缓冲区来实现读写；</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">openAndWrite</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</div><div class="line">    FileChannel channel = FileChannel.open(Paths.get(<span class="string">"my.txt"</span>), StandardOpenOption.CREATE, StandardOpenOption.WRITE);</div><div class="line">    ByteBuffer buffer = ByteBuffer.allocate(<span class="number">64</span>);</div><div class="line">    buffer.putChar(<span class="string">'A'</span>).flip();</div><div class="line">    channel.write(buffer);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<ul>
<li>Socket和ServerSocket类中提供的建立连接和数据传输相关的方法都是阻塞式的；对服务端通常使用线程池的方式来调用ServerSocket.accept方法来监听连接请求；Java NIO提供了非阻塞式和多路复用的套接字连接；</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">startSimpleServer</span><span class="params">()</span> <span class="keyword">throws</span> IOException</span>&#123;</div><div class="line">    ServerSocketChannel channel = ServerSocketChannel.open();</div><div class="line">    channel.bind(<span class="keyword">new</span> InetSocketAddress(<span class="string">"localhost"</span>, <span class="number">10800</span>));</div><div class="line">    <span class="keyword">while</span>(<span class="keyword">true</span>)&#123;</div><div class="line">        <span class="keyword">try</span>(SocketChannel sc = channel.accept())&#123;</div><div class="line">            sc.write(ByteBuffer.wrap(<span class="string">"Hello"</span>.getBytes(<span class="string">"UTF-8"</span>)));</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<ul>
<li>套接字通道的多路复用的思想比较简单，通过一个专门的选择器（Selector）来同时对多个套接字通道进行监听；当其中的某些套接字通道上有它感兴趣的事件发生时，这些通道就会变为可用状态，可以在选择器的选择操作中被选中；可用通道的选择一般是通过操作系统提供的底层操作系统调用来实现的，性能也比较高；</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LoadWebPageUseSelector</span> </span>&#123;</div><div class="line"></div><div class="line">    <span class="comment">// 通过Selector同时下载多个网页的内容</span></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">load</span><span class="params">(Set&lt;URL&gt; urls)</span> <span class="keyword">throws</span> IOException </span>&#123;</div><div class="line">        Map&lt;SocketAddress, String&gt; mapping = urlToSocketAddress(urls);</div><div class="line"></div><div class="line">        <span class="comment">// 1. 创建Selector</span></div><div class="line">        Selector selector = Selector.open();</div><div class="line"></div><div class="line">        <span class="comment">// 2. 将套接字Channel注册到Selector上</span></div><div class="line">        <span class="keyword">for</span> (SocketAddress address : mapping.keySet()) &#123;</div><div class="line">            register(selector, address);</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="keyword">int</span> finished = <span class="number">0</span>;</div><div class="line">        <span class="keyword">int</span> total = mapping.size();</div><div class="line"></div><div class="line">        ByteBuffer buffer = ByteBuffer.allocate(<span class="number">32</span> * <span class="number">1024</span>);</div><div class="line">        <span class="keyword">int</span> len = -<span class="number">1</span>;</div><div class="line">        <span class="keyword">while</span> (finished &lt; total) &#123;</div><div class="line">            <span class="comment">// 3. 调用select方法进行通道选择，该方法会阻塞，直到至少有一个他们所感兴趣的事件发生，然后可以通过selectedKeys获取被选中的通道的对象集合</span></div><div class="line">            selector.select();</div><div class="line">            Iterator&lt;SelectionKey&gt; iterator = selector.selectedKeys().iterator();</div><div class="line">            <span class="keyword">while</span> (iterator.hasNext()) &#123;</div><div class="line">                SelectionKey key = iterator.next();</div><div class="line">                iterator.remove();</div><div class="line">                <span class="keyword">if</span> (key.isValid() &amp;&amp; key.isConnectable()) &#123;</div><div class="line">                    SocketChannel channel = (SocketChannel) key.channel();</div><div class="line"></div><div class="line">                    <span class="comment">// 4. 如果连接成功，则发送HTTP请求；失败则取消该连接；</span></div><div class="line">                    <span class="keyword">boolean</span> success = channel.finishConnect();</div><div class="line">                    <span class="keyword">if</span> (!success) &#123;</div><div class="line">                        finished++;</div><div class="line">                        key.cancel();</div><div class="line">                    &#125; <span class="keyword">else</span> &#123;</div><div class="line">                        InetSocketAddress address = (InetSocketAddress) channel.getRemoteAddress();</div><div class="line">                        String path = mapping.get(address);</div><div class="line">                        String request = <span class="string">"GET"</span> + path + <span class="string">"HTTP/1.0\r\n\r\nHost:"</span> + address.getHostString() + <span class="string">"\r\n\r\n"</span>;</div><div class="line">                        ByteBuffer header = ByteBuffer.wrap(request.getBytes(<span class="string">"UTF-8"</span>));</div><div class="line">                        channel.write(header);</div><div class="line">                    &#125;</div><div class="line">                &#125; <span class="keyword">else</span> <span class="keyword">if</span> (key.isValid() &amp;&amp; key.isReadable()) &#123;</div><div class="line">                    <span class="comment">// 5. 当channel处于可读时则读取channel的数据并写入文件</span></div><div class="line">                    SocketChannel channel = (SocketChannel) key.channel();</div><div class="line">                    InetSocketAddress address = (InetSocketAddress) channel.getRemoteAddress();</div><div class="line">                    String filename = address.getHostName() + <span class="string">".txt"</span>;</div><div class="line">                    FileChannel destChannel = FileChannel.open(Paths.get(filename), StandardOpenOption.APPEND, StandardOpenOption.CREATE);</div><div class="line">                    buffer.clear();</div><div class="line"></div><div class="line">                    <span class="comment">// 6. 当返回0时表示本次没有数据可读不需要操作；如果为-1则表示所有数据亿级读取完毕，可以关闭；</span></div><div class="line">                    <span class="keyword">while</span> ((len = channel.read(buffer)) &gt; <span class="number">0</span> || buffer.position() != <span class="number">0</span>) &#123;</div><div class="line">                        buffer.flip();</div><div class="line">                        destChannel.write(buffer);</div><div class="line">                        buffer.compact();</div><div class="line">                    &#125;</div><div class="line"></div><div class="line">                    <span class="keyword">if</span> (len == -<span class="number">1</span>) &#123;</div><div class="line">                        finished++;</div><div class="line">                        key.cancel();</div><div class="line">                    &#125;</div><div class="line">                &#125;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">register</span><span class="params">(Selector selector, SocketAddress address)</span> <span class="keyword">throws</span> IOException </span>&#123;</div><div class="line">        SocketChannel channel = SocketChannel.open();</div><div class="line"></div><div class="line">        <span class="comment">// 设置为非阻塞模式</span></div><div class="line">        channel.configureBlocking(<span class="keyword">false</span>);</div><div class="line">        channel.connect(address);</div><div class="line"></div><div class="line">        <span class="comment">// 注册时需要指定感兴趣的事件类型</span></div><div class="line">        channel.register(selector, SelectionKey.OP_CONNECT | SelectionKey.OP_READ);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">private</span> Map&lt;SocketAddress, String&gt; <span class="title">urlToSocketAddress</span><span class="params">(Set&lt;URL&gt; urls)</span> </span>&#123;</div><div class="line">        Map&lt;SocketAddress, String&gt; mapping = <span class="keyword">new</span> HashMap&lt;&gt;();</div><div class="line">        <span class="keyword">for</span> (URL url : urls) &#123;</div><div class="line">            <span class="keyword">int</span> port = url.getPort() != -<span class="number">1</span> ? url.getPort() : url.getDefaultPort();</div><div class="line">            SocketAddress address = <span class="keyword">new</span> InetSocketAddress(url.getHost(), port);</div><div class="line">            String path = url.getPath();</div><div class="line">            <span class="keyword">if</span> (url.getQuery() != <span class="keyword">null</span>) &#123;</div><div class="line">                path = path + <span class="string">"?"</span> + url.getQuery();</div><div class="line">            &#125;</div><div class="line"></div><div class="line">            mapping.put(address, path);</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="keyword">return</span> mapping;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="Java-7的版本对Java-NIO有哪些增强？"><a href="#Java-7的版本对Java-NIO有哪些增强？" class="headerlink" title="Java 7的版本对Java NIO有哪些增强？"></a>Java 7的版本对Java NIO有哪些增强？</h2><ul>
<li>Java 7中的NIO.2进一步增强，主要包括文件系统访问和异步I/O通道；</li>
<li>引入Path接口作为文件系统中路径的一种抽象，来代替之前字符串处理的方式，更加语义化；引入DirectoryStream来支持目录下子目录和文件的遍历，它的优势在于它渐进式地遍历，每次只读取一定数量的内容，从而可以降低遍历时的开销（DirectoryStream<path></path> stream = Files.newDirectoryStream(path, “*.java”)）；如果要递归地遍历子目录下的子目录，对整个目录树进行遍历，可以使用FileVisitor；通过引入文件视图FileAttributeView来获取和设置文件的各种属性；另外还提供了新的目录监视服务，当指定目录下的子目录或文件被创建、更新或删除时可以得到事件通知；Files工具类提供了一系列静态方法可以满足常见的需求；</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">calculate</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</div><div class="line">    WatchService service = FileSystems.getDefault().newWatchService();</div><div class="line">    Path path = Paths.get(<span class="string">""</span>).toAbsolutePath();</div><div class="line">    path.register(service, StandardWatchEventKinds.ENTRY_CREATE);</div><div class="line">    <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</div><div class="line">        WatchKey watchKey = service.take();</div><div class="line">        <span class="keyword">for</span> (WatchEvent&lt;?&gt; event : watchKey.pollEvents()) &#123;</div><div class="line">            Path createdPath = (Path) event.context();</div><div class="line">            createdPath = path.resolve(createdPath);</div><div class="line">            <span class="keyword">long</span> size = Files.size(createdPath);</div><div class="line">            System.out.println(createdPath + <span class="string">"=&gt;"</span> + size);</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        watchKey.reset();</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">manipulateFiles</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</div><div class="line">    Path newFile = Files.createFile(Paths.get(<span class="string">"new.txt"</span>).toAbsolutePath());</div><div class="line">    List&lt;String&gt; content = Arrays.asList(<span class="string">"Hello"</span>, <span class="string">"World"</span>);</div><div class="line">    Files.write(newFile, content, Charset.forName(<span class="string">"UTF-8"</span>));</div><div class="line">    Files.size(newFile);</div><div class="line"></div><div class="line">    <span class="keyword">byte</span>[] bytes = Files.readAllBytes(newFile);</div><div class="line">    ByteArrayOutputStream outputStream = <span class="keyword">new</span> ByteArrayOutputStream();</div><div class="line">    Files.copy(newFile, outputStream);</div><div class="line">    Files.delete(newFile);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<ul>
<li>异步I/O通道一般提供两种使用方式：一会走是通过Future类的对象来表示异步操作的结果，另外一种是在执行操作时传入一个CompletionHandler接口的实现对象作为操作完成时的回调方法；异步文件通道由AsynchronousFileChannel类表示，它没有当前读写位置的概念。</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">asyncWrite</span><span class="params">()</span> <span class="keyword">throws</span> IOException, ExecutionException, InterruptedException </span>&#123;</div><div class="line">    AsynchronousFileChannel channel = AsynchronousFileChannel.open(Paths.get(<span class="string">"large.bin"</span>),</div><div class="line">            StandardOpenOption.CREATE, StandardOpenOption.WRITE);</div><div class="line">    ByteBuffer buffer = ByteBuffer.allocate(<span class="number">32</span> * <span class="number">1024</span> * <span class="number">1024</span>);</div><div class="line">    Future&lt;Integer&gt; result = channel.write(buffer, <span class="number">0</span>);</div><div class="line">    Integer len = result.get();</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">startAsyncSimpleServer</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</div><div class="line">    AsynchronousChannelGroup group = AsynchronousChannelGroup.withFixedThreadPool(<span class="number">10</span>, Executors.defaultThreadFactory());</div><div class="line">    <span class="keyword">final</span> AsynchronousServerSocketChannel serverChannel = AsynchronousServerSocketChannel.open(group).bind(<span class="keyword">new</span> InetSocketAddress(<span class="number">10080</span>));</div><div class="line">    serverChannel.accept(<span class="keyword">null</span>, <span class="keyword">new</span> CompletionHandler&lt;AsynchronousSocketChannel, Void&gt;() &#123;</div><div class="line">        <span class="meta">@Override</span></div><div class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">completed</span><span class="params">(AsynchronousSocketChannel result, Void attachment)</span> </span>&#123;</div><div class="line">            serverChannel.accept(<span class="keyword">null</span>, <span class="keyword">this</span>);</div><div class="line">            <span class="comment">// 使用clientChannel</span></div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="meta">@Override</span></div><div class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">failed</span><span class="params">(Throwable exc, Void attachment)</span> </span>&#123;</div><div class="line">            <span class="comment">// 错误处理</span></div><div class="line">        &#125;</div><div class="line">    &#125;);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="其他面试小结"><a href="#其他面试小结" class="headerlink" title="其他面试小结"></a>其他面试小结</h2><ul>
<li><a href="http://ginobefunny.com/post/elasticsearch_interview_questions/">面试小结之Elasticsearch篇</a></li>
<li><a href="http://ginobefunny.com/post/jvm_interview_questions/">面试小结之JVM篇</a></li>
<li><a href="http://ginobefunny.com/post/java_concurrent_interview_questions/">面试小结之并发篇</a></li>
<li><a href="http://ginobefunny.com/post/java_nio_interview_questions/">面试小结之IO篇</a></li>
<li><a href="http://ginobefunny.com/post/java_other_interview_questions/">面试小结之综合篇</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近面试一些公司，被问到的关于Java NIO编程的问题，以及自己总结的回答。&lt;br&gt;
    
    </summary>
    
      <category term="CodingLife" scheme="http://ginobefunny.com/categories/CodingLife/"/>
    
    
      <category term="Java" scheme="http://ginobefunny.com/tags/Java/"/>
    
      <category term="职场" scheme="http://ginobefunny.com/tags/%E8%81%8C%E5%9C%BA/"/>
    
      <category term="面试" scheme="http://ginobefunny.com/tags/%E9%9D%A2%E8%AF%95/"/>
    
      <category term="NIO" scheme="http://ginobefunny.com/tags/NIO/"/>
    
  </entry>
  
  <entry>
    <title>面试小结之并发篇</title>
    <link href="http://ginobefunny.com/post/java_concurrent_interview_questions/"/>
    <id>http://ginobefunny.com/post/java_concurrent_interview_questions/</id>
    <published>2017-06-14T08:38:44.000Z</published>
    <updated>2017-06-15T08:44:29.181Z</updated>
    
    <content type="html"><![CDATA[<p>最近面试一些公司，被问到的关于Java并发编程的问题，以及自己总结的回答。<br><a id="more"></a></p>
<h2 id="Java线程的状态及如何转换。"><a href="#Java线程的状态及如何转换。" class="headerlink" title="Java线程的状态及如何转换。"></a>Java线程的状态及如何转换。</h2><p><img src="http://upload-images.jianshu.io/upload_images/3709321-cb615ced8545bfa2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="线程状态及其转换图"></p>
<h2 id="多个线程之间如何协调？"><a href="#多个线程之间如何协调？" class="headerlink" title="多个线程之间如何协调？"></a>多个线程之间如何协调？</h2><ul>
<li>wait()、notify()、notifyAll()：这三个方法用于协调多个线程对共享数据的存取，所以必须在同步语句块内使用。wait方法要等待notify/notifyAll的线程释放锁后才能开始继续往下执行。</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// 等待方</span></div><div class="line"><span class="keyword">synchronized</span>(lockObj)&#123;</div><div class="line">    <span class="keyword">while</span>(condition is <span class="keyword">false</span>)&#123;</div><div class="line">        lockObj.wait();</div><div class="line">    &#125;</div><div class="line">    </div><div class="line">    <span class="comment">// do business</span></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// 通知方</span></div><div class="line"><span class="keyword">synchronized</span>(lockObj)&#123;</div><div class="line">    <span class="comment">// change condition</span></div><div class="line">    </div><div class="line">    lockObj.notifyAll();</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="说说Java的线程池是如何实现的？"><a href="#说说Java的线程池是如何实现的？" class="headerlink" title="说说Java的线程池是如何实现的？"></a>说说Java的线程池是如何实现的？</h2><ul>
<li>创建线程要花费昂贵的资源和时间，如果任务来了才创建线程那么响应时间会变长，而且一个进程能创建的线程数有限。为了避免这些问题，在程序启动的时候就创建若干线程来响应处理，它们被称为线程池，里面的线程叫工作线程。</li>
<li>maximumPoolSize和corePoolSize的区别：这个概念很重要，maximumPoolSize为线程池最大容量，也就是说线程池最多能起多少Worker。corePoolSize是核心线程池的大小，当corePoolSize满了时，同时workQueue full（ArrayBolckQueue是可能满的） 那么此时允许新建Worker去处理workQueue中的Task，但是不能超过maximumPoolSize。超过corePoolSize之外的线程会在空闲超时后终止。可以通过beforeExecute和afterExecute实现线程池的监听；</li>
</ul>
<p><img src="http://upload-images.jianshu.io/upload_images/3709321-10647ceea1f53aff.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="线程池"></p>
<p><img src="http://upload-images.jianshu.io/upload_images/3709321-aad65d89876a30f7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="线程池处理流程"></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> ExecutorService <span class="title">newFixedThreadPool</span><span class="params">(<span class="keyword">int</span> nThreads)</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> <span class="keyword">new</span> ThreadPoolExecutor(nThreads, nThreads,</div><div class="line">                                  <span class="number">0L</span>, TimeUnit.MILLISECONDS,</div><div class="line">                                  <span class="keyword">new</span> LinkedBlockingQueue&lt;Runnable&gt;());</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="关于BlockingQueue和TransferQueue的异同。"><a href="#关于BlockingQueue和TransferQueue的异同。" class="headerlink" title="关于BlockingQueue和TransferQueue的异同。"></a>关于BlockingQueue和TransferQueue的异同。</h2><ul>
<li>TransferQueue继承了BlockingQueue并扩展了一些新方法。BlockingQueue是Java 5中加入的接口，它是指这样的一个队列：当生产者向队列添加元素但队列已满时，生产者会被阻塞；当消费者从队列移除元素但队列为空时，消费者会被阻塞。</li>
<li>TransferQueue则更进一步，生产者会一直阻塞直到所添加到队列的元素被某一个消费者所消费（不仅仅是添加到队列里就完事），新添加的transfer方法用来实现这种约束。顾名思义，阻塞就是发生在元素从一个线程transfer到另一个线程的过程中，它有效地实现了元素在线程之间的传递（以建立Java内存模型中的happens-before关系的方式）。</li>
<li>TransferQueue还包括了其他的一些方法：两个tryTransfer方法，一个是非阻塞的，另一个带有timeout参数设置超时时间的。还有两个辅助方法hasWaitingConsumer()和getWaitingConsumerCount()。</li>
<li>TransferQueue相比SynchronousQueue用处更广、更好用，因为你可以决定是使用BlockingQueue的方法（例如put方法）还是确保一次传递完成（即transfer方法）。在队列中已有元素的情况下，调用transfer方法，可以确保队列中被传递元素之前的所有元素都能被处理。Doug Lea说从功能角度来讲，LinkedTransferQueue实际上是ConcurrentLinkedQueue、SynchronousQueue（公平模式）和LinkedBlockingQueue的超集。而且LinkedTransferQueue更好用，因为它不仅仅综合了这几个类的功能，同时也提供了更高效的实现。</li>
</ul>
<h2 id="谈谈HashMap的实现。"><a href="#谈谈HashMap的实现。" class="headerlink" title="谈谈HashMap的实现。"></a>谈谈HashMap的实现。</h2><ul>
<li>从结构实现来讲，HashMap是数组+链表+红黑树（JDK1.8增加了红黑树部分）实现的，如下如所示。</li>
</ul>
<p><img src="http://upload-images.jianshu.io/upload_images/3709321-c219f054ec125188.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="HashMap"></p>
<pre><code>从源码可知，HashMap类中有一个非常重要的字段，就是 Node[] table，即哈希桶数组；
Node是HashMap的一个内部类，实现了Map.Entry接口，本质是就是一个映射(键值对)。
HashMap就是使用哈希表来存储的。为解决冲突，Java中HashMap采用了链地址法，简单来说，就是数组加链表的结合。在每个数组元素上都一个链表结构，当数据被Hash后，得到数组下标，把数据放在对应下标元素的链表上。
Node[] table的初始化长度默认值是16，Load factor为负载因子(默认值是0.75)，threshold是HashMap所能容纳的最大数据量的Node(键值对)个数。threshold = length * Load factor。也就是说，在数组定义好长度之后，负载因子越大，所能容纳的键值对个数越多。
</code></pre><ul>
<li>确定哈希桶数组索引位置：取key的hashCode值、高位运算(通过hashCode()的高16位异或低16位实现的)、取模运算。</li>
</ul>
<pre><code>static final int hash(Object key) {
    int h;
    return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);
}
</code></pre><p><img src="http://upload-images.jianshu.io/upload_images/3709321-a3f8b575f7b7fb8d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="数组索引位置"></p>
<ul>
<li>HashMap的put方法执行过程可以通过下图来理解。</li>
</ul>
<p><img src="http://upload-images.jianshu.io/upload_images/3709321-9c2a3d684ede2890.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="put方法执行过程"></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> V <span class="title">put</span><span class="params">(K key, V value)</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> putVal(hash(key), key, value, <span class="keyword">false</span>, <span class="keyword">true</span>);</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">final</span> V <span class="title">putVal</span><span class="params">(<span class="keyword">int</span> hash, K key, V value, <span class="keyword">boolean</span> onlyIfAbsent,</span></span></div><div class="line">               <span class="keyword">boolean</span> evict) &#123;</div><div class="line">    Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; <span class="keyword">int</span> n, i;</div><div class="line">    <span class="comment">// 步骤①：tab为空则创建</span></div><div class="line">    <span class="keyword">if</span> ((tab = table) == <span class="keyword">null</span> || (n = tab.length) == <span class="number">0</span>)</div><div class="line">        n = (tab = resize()).length;</div><div class="line">    <span class="comment">// 步骤②：计算index，并对null做处理 </span></div><div class="line">    <span class="keyword">if</span> ((p = tab[i = (n - <span class="number">1</span>) &amp; hash]) == <span class="keyword">null</span>)</div><div class="line">        tab[i] = newNode(hash, key, value, <span class="keyword">null</span>);</div><div class="line">    <span class="keyword">else</span> &#123;</div><div class="line">        Node&lt;K,V&gt; e; K k;</div><div class="line">        <span class="comment">// 步骤③：节点key存在，直接覆盖value</span></div><div class="line">        <span class="keyword">if</span> (p.hash == hash &amp;&amp;</div><div class="line">            ((k = p.key) == key || (key != <span class="keyword">null</span> &amp;&amp; key.equals(k))))</div><div class="line">            e = p;</div><div class="line">        <span class="comment">// 步骤④：判断该链为红黑树</span></div><div class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (p <span class="keyword">instanceof</span> TreeNode)</div><div class="line">            e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(<span class="keyword">this</span>, tab, hash, key, value);</div><div class="line">        <span class="comment">// 步骤⑤：该链为链表</span></div><div class="line">        <span class="keyword">else</span> &#123;</div><div class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> binCount = <span class="number">0</span>; ; ++binCount) &#123;</div><div class="line">                <span class="keyword">if</span> ((e = p.next) == <span class="keyword">null</span>) &#123;</div><div class="line">                    p.next = newNode(hash, key, value, <span class="keyword">null</span>);</div><div class="line">                    <span class="comment">//链表长度大于8转换为红黑树进行处理</span></div><div class="line">                    <span class="keyword">if</span> (binCount &gt;= TREEIFY_THRESHOLD - <span class="number">1</span>) <span class="comment">// -1 for 1st</span></div><div class="line">                        treeifyBin(tab, hash);</div><div class="line">                    <span class="keyword">break</span>;</div><div class="line">                &#125;</div><div class="line">                <span class="comment">// key已经存在直接覆盖value</span></div><div class="line">                <span class="keyword">if</span> (e.hash == hash &amp;&amp;</div><div class="line">                    ((k = e.key) == key || (key != <span class="keyword">null</span> &amp;&amp; key.equals(k))))</div><div class="line">                    <span class="keyword">break</span>;</div><div class="line">                p = e;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">if</span> (e != <span class="keyword">null</span>) &#123; <span class="comment">// existing mapping for key</span></div><div class="line">            V oldValue = e.value;</div><div class="line">            <span class="keyword">if</span> (!onlyIfAbsent || oldValue == <span class="keyword">null</span>)</div><div class="line">                e.value = value;</div><div class="line">            afterNodeAccess(e);</div><div class="line">            <span class="keyword">return</span> oldValue;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">    ++modCount;</div><div class="line">    <span class="comment">// 步骤⑥：超过最大容量 就扩容</span></div><div class="line">    <span class="keyword">if</span> (++size &gt; threshold)</div><div class="line">        resize();</div><div class="line">    afterNodeInsertion(evict);</div><div class="line">    <span class="keyword">return</span> <span class="keyword">null</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<ul>
<li>扩容机制：扩容(resize)就是重新计算容量，向HashMap对象里不停的添加元素，而HashMap对象内部的数组无法装载更多的元素时，对象就需要扩大数组的长度，以便能装入更多的元素。当然Java里的数组是无法自动扩容的，方法是使用一个新的数组代替已有的容量小的数组。在扩充HashMap的时候，不需要像JDK1.7的实现那样重新计算hash，只需要看看原来的hash值新增的那个bit是1还是0就好了，是0的话索引没变，是1的话索引变成“原索引+oldCap”。这个设计确实非常的巧妙，既省去了重新计算hash值的时间，而且同时，由于新增的1bit是0还是1可以认为是随机的，因此resize的过程，均匀的把之前的冲突的节点分散到新的bucket了。</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">final</span> Node&lt;K,V&gt;[] resize() &#123;</div><div class="line">        Node&lt;K,V&gt;[] oldTab = table;</div><div class="line">        <span class="keyword">int</span> oldCap = (oldTab == <span class="keyword">null</span>) ? <span class="number">0</span> : oldTab.length;</div><div class="line">        <span class="keyword">int</span> oldThr = threshold;</div><div class="line">        <span class="keyword">int</span> newCap, newThr = <span class="number">0</span>;</div><div class="line">        <span class="keyword">if</span> (oldCap &gt; <span class="number">0</span>) &#123;</div><div class="line">            <span class="comment">// 超过最大值就不再扩充了，就只好随你碰撞去吧</span></div><div class="line">            <span class="keyword">if</span> (oldCap &gt;= MAXIMUM_CAPACITY) &#123;</div><div class="line">                threshold = Integer.MAX_VALUE;</div><div class="line">                <span class="keyword">return</span> oldTab;</div><div class="line">            &#125;</div><div class="line">            <span class="comment">// 没超过最大值，就扩充为原来的2倍</span></div><div class="line">            <span class="keyword">else</span> <span class="keyword">if</span> ((newCap = oldCap &lt;&lt; <span class="number">1</span>) &lt; MAXIMUM_CAPACITY &amp;&amp;</div><div class="line">                     oldCap &gt;= DEFAULT_INITIAL_CAPACITY)</div><div class="line">                newThr = oldThr &lt;&lt; <span class="number">1</span>; <span class="comment">// double threshold</span></div><div class="line">        &#125;</div><div class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (oldThr &gt; <span class="number">0</span>) <span class="comment">// initial capacity was placed in threshold</span></div><div class="line">            newCap = oldThr;</div><div class="line">        <span class="keyword">else</span> &#123;               <span class="comment">// zero initial threshold signifies using defaults</span></div><div class="line">            newCap = DEFAULT_INITIAL_CAPACITY;</div><div class="line">            newThr = (<span class="keyword">int</span>)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY);</div><div class="line">        &#125;</div><div class="line">        </div><div class="line">        <span class="comment">// 计算新的resize上限</span></div><div class="line">        <span class="keyword">if</span> (newThr == <span class="number">0</span>) &#123;</div><div class="line">            <span class="keyword">float</span> ft = (<span class="keyword">float</span>)newCap * loadFactor;</div><div class="line">            newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (<span class="keyword">float</span>)MAXIMUM_CAPACITY ?</div><div class="line">                      (<span class="keyword">int</span>)ft : Integer.MAX_VALUE);</div><div class="line">        &#125;</div><div class="line">        threshold = newThr;</div><div class="line">        <span class="meta">@SuppressWarnings</span>(&#123;<span class="string">"rawtypes"</span>,<span class="string">"unchecked"</span>&#125;)</div><div class="line">            Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])<span class="keyword">new</span> Node[newCap];</div><div class="line">        table = newTab;</div><div class="line">        <span class="keyword">if</span> (oldTab != <span class="keyword">null</span>) &#123;</div><div class="line">            <span class="comment">// 把每个bucket都移动到新的buckets中</span></div><div class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; oldCap; ++j) &#123;</div><div class="line">                Node&lt;K,V&gt; e;</div><div class="line">                <span class="keyword">if</span> ((e = oldTab[j]) != <span class="keyword">null</span>) &#123;</div><div class="line">                    oldTab[j] = <span class="keyword">null</span>;</div><div class="line">                    <span class="keyword">if</span> (e.next == <span class="keyword">null</span>)</div><div class="line">                        newTab[e.hash &amp; (newCap - <span class="number">1</span>)] = e;</div><div class="line">                    <span class="keyword">else</span> <span class="keyword">if</span> (e <span class="keyword">instanceof</span> TreeNode)</div><div class="line">                        ((TreeNode&lt;K,V&gt;)e).split(<span class="keyword">this</span>, newTab, j, oldCap);</div><div class="line">                    <span class="keyword">else</span> &#123; <span class="comment">// preserve order</span></div><div class="line">                        Node&lt;K,V&gt; loHead = <span class="keyword">null</span>, loTail = <span class="keyword">null</span>;</div><div class="line">                        Node&lt;K,V&gt; hiHead = <span class="keyword">null</span>, hiTail = <span class="keyword">null</span>;</div><div class="line">                        Node&lt;K,V&gt; next;</div><div class="line">                        <span class="keyword">do</span> &#123;</div><div class="line">                            next = e.next;</div><div class="line">                            <span class="comment">// 原索引</span></div><div class="line">                            <span class="keyword">if</span> ((e.hash &amp; oldCap) == <span class="number">0</span>) &#123;</div><div class="line">                                <span class="keyword">if</span> (loTail == <span class="keyword">null</span>)</div><div class="line">                                    loHead = e;</div><div class="line">                                <span class="keyword">else</span></div><div class="line">                                    loTail.next = e;</div><div class="line">                                loTail = e;</div><div class="line">                            &#125;</div><div class="line">                            <span class="comment">// 原索引+oldCap</span></div><div class="line">                            <span class="keyword">else</span> &#123;</div><div class="line">                                <span class="keyword">if</span> (hiTail == <span class="keyword">null</span>)</div><div class="line">                                    hiHead = e;</div><div class="line">                                <span class="keyword">else</span></div><div class="line">                                    hiTail.next = e;</div><div class="line">                                hiTail = e;</div><div class="line">                            &#125;</div><div class="line">                        &#125; <span class="keyword">while</span> ((e = next) != <span class="keyword">null</span>);</div><div class="line">                        </div><div class="line">                        <span class="comment">// 原索引放到bucket里</span></div><div class="line">                        <span class="keyword">if</span> (loTail != <span class="keyword">null</span>) &#123;</div><div class="line">                            loTail.next = <span class="keyword">null</span>;</div><div class="line">                            newTab[j] = loHead;</div><div class="line">                        &#125;</div><div class="line">                        <span class="comment">// 原索引+oldCap放到bucket里</span></div><div class="line">                        <span class="keyword">if</span> (hiTail != <span class="keyword">null</span>) &#123;</div><div class="line">                            hiTail.next = <span class="keyword">null</span>;</div><div class="line">                            newTab[j + oldCap] = hiHead;</div><div class="line">                        &#125;</div><div class="line">                    &#125;</div><div class="line">                &#125;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">return</span> newTab;</div><div class="line">    &#125;</div></pre></td></tr></table></figure>
<h2 id="谈谈线程安全的ConcurrentHashMap的实现原理。"><a href="#谈谈线程安全的ConcurrentHashMap的实现原理。" class="headerlink" title="谈谈线程安全的ConcurrentHashMap的实现原理。"></a>谈谈线程安全的ConcurrentHashMap的实现原理。</h2><ul>
<li>ConcurrentHashMap在jdk1.8中主要做了2方面的改进：改进一是取消segments字段，直接采用transient volatile HashEntry<k,v>[] table保存数据，采用table数组元素作为锁，从而实现了对每一行数据进行加锁，进一步减少并发冲突的概率；改进二是将原先table数组＋单向链表的数据结构，变更为table数组＋单向链表＋红黑树的结构，对于hash表来说，最核心的能力在于将key hash之后能均匀的分布在数组中，如果hash之后散列的很均匀，那么table数组中的每个队列长度主要为0或者1。但实际情况并非总是如此理想，虽然ConcurrentHashMap类默认的加载因子为0.75，但是在数据量过大或者运气不佳的情况下，还是会存在一些队列长度过长的情况，如果还是采用单向列表方式，那么查询某个节点的时间复杂度为O(n)；因此，对于个数超过8(默认值)的列表，jdk1.8中采用了红黑树的结构，那么查询的时间复杂度可以降低到O(logN)，可以改进性能。</k,v></li>
<li>TreeNode类：树节点类，另外一个核心的数据结构。当链表长度过长的时候，会转换为TreeNode。但是与HashMap不相同的是，它并不是直接转换为红黑树，而是把这些结点包装成TreeNode放在TreeBin对象中，由TreeBin完成对红黑树的包装。而且TreeNode在ConcurrentHashMap继承自Node类，而并非HashMap中的集成自LinkedHashMap.Entry。</li>
<li>二叉查找树，也称有序二叉树（ordered binary tree），是指一棵空树或者具有下列性质的二叉树：</li>
</ul>
<pre><code>1.若任意节点的左子树不空，则左子树上所有结点的值均小于它的根结点的值；
2.若任意节点的右子树不空，则右子树上所有结点的值均大于它的根结点的值；
3.任意节点的左、右子树也分别为二叉查找树。
4.没有键值相等的节点（no duplicate nodes）。
</code></pre><ul>
<li>红黑树虽然本质上是一棵二叉查找树，但它在二叉查找树的基础上增加了着色和相关的性质使得红黑树相对平衡，从而保证了红黑树的查找、插入、删除的时间复杂度最坏为O(log n)。但它是如何保证一棵n个结点的红黑树的高度始终保持在logn的呢？这就引出了红黑树的5个性质：</li>
</ul>
<pre><code>1.每个结点要么是红的要么是黑的。  
2.根结点是黑的。  
3.每个叶结点（叶结点即指树尾端NIL指针或NULL结点）都是黑的。  
4.如果一个结点是红的，那么它的两个儿子都是黑的。  
5.对于任意结点而言，其到叶结点树尾端NIL指针的每条路径都包含相同数目的黑结点。
</code></pre><h2 id="什么是一致性哈希？"><a href="#什么是一致性哈希？" class="headerlink" title="什么是一致性哈希？"></a>什么是一致性哈希？</h2><ul>
<li>环形Hash空间：按照常用的hash算法来将对应的key哈希到一个具有2^32次方个桶的空间中，即0~(2^32)-1的数字空间中。现在我们可以将这些数字头尾相连，想象成一个闭合的环形；</li>
<li>把数据通过一定的hash算法处理后映射到环上；</li>
<li>将机器通过hash算法映射到环上（一般情况下对机器的hash计算是采用机器的IP或者机器唯一的别名作为输入值），然后以顺时针的方向计算，将所有对象存储到离自己最近的机器中；</li>
<li>机器的删除与添加：普通hash求余算法最为不妥的地方就是在有机器的添加或者删除之后会照成大量的对象存储位置失效，这样就大大的不满足单调性了。通过对节点的添加和删除的分析，一致性哈希算法在保持了单调性的同时，还是数据的迁移达到了最小，这样的算法对分布式集群来说是非常合适的，避免了大量数据迁移，减小了服务器的的压力。</li>
<li>平衡性：在一致性哈希算法中，为了尽可能的满足平衡性，其引入了虚拟节点。它实际上是节点在hash空间的复制品，一实际个节点对应了若干个“虚拟节点”，这个对应个数也成为“复制个数”，“虚拟节点”在hash空间中以hash值排列。</li>
</ul>
<p><img src="http://upload-images.jianshu.io/upload_images/3709321-7ff1b37dfd99950a?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="一致性哈希"></p>
<h2 id="Java有哪些实现锁的方式？"><a href="#Java有哪些实现锁的方式？" class="headerlink" title="Java有哪些实现锁的方式？"></a>Java有哪些实现锁的方式？</h2><ul>
<li>synchronized同步锁：它无法中断一个正在等候获得锁的线程，也无法通过投票得到锁，如果不想等下去，也就没法得到锁。但除非对锁的某个高级特性有明确的需要，或者有明确的证据表明在特定情况下，同步已经成为瓶颈，否则还是应当继续使用synchronized。</li>
<li>volatile是比synchronized更轻量，因为它没有上下文切换；其实现是通过lock指令将缓存行数据写到系统内存且让其他缓存数据无效；</li>
<li>ReentrantLock可以支持公平锁，当然公平锁性能会有影响，默认为非公平的；</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// 对于非公平锁，会执行该方法:</span></div><div class="line"><span class="function"><span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">nonfairTryAcquire</span><span class="params">(<span class="keyword">int</span> acquires)</span> </span>&#123;</div><div class="line">    <span class="keyword">final</span> Thread current = Thread.currentThread();</div><div class="line">    <span class="keyword">int</span> c = getState();<span class="comment">//获取状态变量</span></div><div class="line">    <span class="keyword">if</span> (c == <span class="number">0</span>) &#123;<span class="comment">//表明没有线程占有该同步状态</span></div><div class="line">        <span class="keyword">if</span> (compareAndSetState(<span class="number">0</span>, acquires)) &#123;<span class="comment">//以原子方式设置该同步状态</span></div><div class="line">            setExclusiveOwnerThread(current);<span class="comment">//该线程拥有该FairSync同步状态</span></div><div class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">else</span> <span class="keyword">if</span> (current == getExclusiveOwnerThread()) &#123;<span class="comment">//当前线程已经拥有该同步状态</span></div><div class="line">        <span class="keyword">int</span> nextc = c + acquires;</div><div class="line">        <span class="keyword">if</span> (nextc &lt; <span class="number">0</span>) <span class="comment">// overflow</span></div><div class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> Error(<span class="string">"Maximum lock count exceeded"</span>);</div><div class="line">        setState(nextc);<span class="comment">//重复设置状态变量（锁的可重入特性）</span></div><div class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</div><div class="line">    &#125;</div><div class="line">    </div><div class="line">    <span class="keyword">return</span> <span class="keyword">false</span>;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// 而对于公平锁，该方法则是这样：</span></div><div class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">tryAcquire</span><span class="params">(<span class="keyword">int</span> acquires)</span> </span>&#123;</div><div class="line">    <span class="keyword">final</span> Thread current = Thread.currentThread();</div><div class="line">    <span class="keyword">int</span> c = getState();</div><div class="line">    <span class="keyword">if</span> (c == <span class="number">0</span>) &#123;</div><div class="line">        <span class="comment">//先判断该线程节点是否是队列的头结点</span></div><div class="line">        <span class="comment">//是则以原子方式设置同步状态，获取锁</span></div><div class="line">        <span class="comment">//否则失败返回</span></div><div class="line">        <span class="keyword">if</span> (!hasQueuedPredecessors() &amp;&amp; compareAndSetState(<span class="number">0</span>, acquires)) &#123;</div><div class="line">            setExclusiveOwnerThread(current);</div><div class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">else</span> <span class="keyword">if</span> (current == getExclusiveOwnerThread()) &#123;<span class="comment">//重入</span></div><div class="line">        <span class="keyword">int</span> nextc = c + acquires;</div><div class="line">        <span class="keyword">if</span> (nextc &lt; <span class="number">0</span>)</div><div class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> Error(<span class="string">"Maximum lock count exceeded"</span>);</div><div class="line">        setState(nextc);</div><div class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</div><div class="line">    &#125;</div><div class="line">    </div><div class="line">    <span class="keyword">return</span> <span class="keyword">false</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<ul>
<li>AQS管理的FIFO等待队列，获取锁状态失败的线程会被放入该队列，等待再次尝试获取锁。而state成员变量，代表着锁的同步状态，一个线程成功获得锁，这个行为的实质就是该线程成功的设置了state变量的状态。</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">void</span> <span class="title">acquire</span><span class="params">(<span class="keyword">int</span> arg)</span> </span>&#123;</div><div class="line">    <span class="keyword">if</span> (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg))</div><div class="line">        selfInterrupt();</div><div class="line">&#125;</div><div class="line">    </div><div class="line"><span class="function"><span class="keyword">private</span> Node <span class="title">addWaiter</span><span class="params">(Node mode)</span> </span>&#123;</div><div class="line">    Node node = <span class="keyword">new</span> Node(Thread.currentThread(), mode);</div><div class="line">    <span class="comment">// Try the fast path of enq; backup to full enq on failure</span></div><div class="line">    Node pred = tail;</div><div class="line">    <span class="comment">// 这个if分支其实是一种优化：CAS操作失败的话才进入enq中的循环。</span></div><div class="line">    <span class="keyword">if</span> (pred != <span class="keyword">null</span>) &#123;</div><div class="line">        node.prev = pred;</div><div class="line">        <span class="keyword">if</span> (compareAndSetTail(pred, node)) &#123;</div><div class="line">            pred.next = node;</div><div class="line">            <span class="keyword">return</span> node;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">    enq(node);</div><div class="line">    <span class="keyword">return</span> node;</div><div class="line">&#125; </div><div class="line"></div><div class="line"><span class="function"><span class="keyword">private</span> Node <span class="title">enq</span><span class="params">(<span class="keyword">final</span> Node node)</span> </span>&#123;</div><div class="line">    <span class="keyword">for</span> (;;) &#123;</div><div class="line">        Node t = tail;</div><div class="line">        <span class="keyword">if</span> (t == <span class="keyword">null</span>) &#123; <span class="comment">// Must initialize</span></div><div class="line">            <span class="keyword">if</span> (compareAndSetHead(<span class="keyword">new</span> Node()))</div><div class="line">                tail = head;</div><div class="line">        &#125; <span class="keyword">else</span> &#123;</div><div class="line">            node.prev = t;</div><div class="line">            <span class="keyword">if</span> (compareAndSetTail(t, node)) &#123;</div><div class="line">                t.next = node;</div><div class="line">                <span class="keyword">return</span> t;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">acquireQueued</span><span class="params">(<span class="keyword">final</span> Node node, <span class="keyword">int</span> arg)</span> </span>&#123;</div><div class="line">    <span class="keyword">boolean</span> failed = <span class="keyword">true</span>;</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">        <span class="keyword">boolean</span> interrupted = <span class="keyword">false</span>;</div><div class="line">        <span class="keyword">for</span> (;;) &#123;</div><div class="line">            <span class="keyword">final</span> Node p = node.predecessor();</div><div class="line">            <span class="keyword">if</span> (p == head &amp;&amp; tryAcquire(arg)) &#123;</div><div class="line">                setHead(node);</div><div class="line">                p.next = <span class="keyword">null</span>; <span class="comment">// help GC</span></div><div class="line">                failed = <span class="keyword">false</span>;</div><div class="line">                <span class="keyword">return</span> interrupted;</div><div class="line">            &#125;</div><div class="line">            </div><div class="line">            <span class="keyword">if</span> (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt())</div><div class="line">                interrupted = <span class="keyword">true</span>;</div><div class="line">        &#125;</div><div class="line">    &#125; <span class="keyword">finally</span> &#123;</div><div class="line">        <span class="keyword">if</span> (failed)</div><div class="line">            cancelAcquire(node);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<ul>
<li>ReentrantReadWriteLock对重入锁再进一步分离为读锁和写锁，在读多写少的场景下能显著提升性能。</li>
<li>ReadLock可以被多个线程持有并且在作用时排斥任何的WriteLock，而WriteLock则是完全的互斥。</li>
<li>写线程获取写入锁后可以获取读取锁，然后释放写入锁，这样就从写入锁变成了读取锁，从而实现锁降级的特性。读取锁是不能直接升级为写入锁的。因为获取一个写入锁需要释放所有读取锁，所以如果有两个读取锁视图获取写入锁而都不释放读取锁时就会发生死锁。</li>
<li>如果读取执行情况很多，写入很少的情况下，使用ReentrantReadWriteLock可能会使写入线程遭遇饥饿问题，也就是写入线程吃吃无法竞争到锁定而一直处于等待状态。</li>
<li>StampedLock控制锁有三种模式（写，读，乐观读），一个StampedLock状态是由版本和模式两个部分组成，锁获取方法返回一个数字作为票据stamp，它用相应的锁状态表示并控制访问，数字0表示没有写锁被授权访问。在读锁上分为悲观锁和乐观锁。所谓的乐观读模式，也就是若读的操作很多，写的操作很少的情况下，你可以乐观地认为，写入与读取同时发生几率很少，因此不悲观地使用完全的读取锁定，程序可以查看读取资料之后，是否遭到写入执行的变更，再采取后续的措施（重新读取变更信息，或者抛出异常） ，这一个小小改进，可大幅度提高程序的吞吐量！</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Point</span> </span>&#123;</div><div class="line">   <span class="keyword">private</span> <span class="keyword">double</span> x, y;</div><div class="line">   <span class="keyword">private</span> <span class="keyword">final</span> StampedLock sl = <span class="keyword">new</span> StampedLock();</div><div class="line">   <span class="function"><span class="keyword">void</span> <span class="title">move</span><span class="params">(<span class="keyword">double</span> deltaX, <span class="keyword">double</span> deltaY)</span> </span>&#123; <span class="comment">// an exclusively locked method</span></div><div class="line">     <span class="keyword">long</span> stamp = sl.writeLock();</div><div class="line">     <span class="keyword">try</span> &#123;</div><div class="line">       x += deltaX;</div><div class="line">       y += deltaY;</div><div class="line">     &#125; <span class="keyword">finally</span> &#123;</div><div class="line">       sl.unlockWrite(stamp);</div><div class="line">     &#125;</div><div class="line">   &#125;</div><div class="line">   </div><div class="line">   <span class="comment">//下面看看乐观读锁案例</span></div><div class="line">   <span class="function"><span class="keyword">double</span> <span class="title">distanceFromOrigin</span><span class="params">()</span> </span>&#123; <span class="comment">// A read-only method</span></div><div class="line">     <span class="keyword">long</span> stamp = sl.tryOptimisticRead(); <span class="comment">//获得一个乐观读锁</span></div><div class="line">     <span class="keyword">double</span> currentX = x, currentY = y; <span class="comment">//将两个字段读入本地局部变量</span></div><div class="line">     <span class="keyword">if</span> (!sl.validate(stamp)) &#123; <span class="comment">//检查发出乐观读锁后同时是否有其他写锁发生？</span></div><div class="line">        stamp = sl.readLock(); <span class="comment">//如果没有，我们再次获得一个读悲观锁</span></div><div class="line">        <span class="keyword">try</span> &#123;</div><div class="line">          currentX = x; <span class="comment">// 将两个字段读入本地局部变量</span></div><div class="line">          currentY = y; <span class="comment">// 将两个字段读入本地局部变量</span></div><div class="line">        &#125; <span class="keyword">finally</span> &#123;</div><div class="line">           sl.unlockRead(stamp);</div><div class="line">        &#125;</div><div class="line">     &#125;</div><div class="line">     <span class="keyword">return</span> Math.sqrt(currentX * currentX + currentY * currentY);</div><div class="line">   &#125;</div><div class="line"></div><div class="line">    <span class="comment">//下面是悲观读锁案例</span></div><div class="line">   <span class="function"><span class="keyword">void</span> <span class="title">moveIfAtOrigin</span><span class="params">(<span class="keyword">double</span> newX, <span class="keyword">double</span> newY)</span> </span>&#123; <span class="comment">// upgrade</span></div><div class="line">     <span class="comment">// Could instead start with optimistic, not read mode</span></div><div class="line">     <span class="keyword">long</span> stamp = sl.readLock();</div><div class="line">     <span class="keyword">try</span> &#123;</div><div class="line">       <span class="keyword">while</span> (x == <span class="number">0.0</span> &amp;&amp; y == <span class="number">0.0</span>) &#123; <span class="comment">//循环，检查当前状态是否符合</span></div><div class="line">         <span class="keyword">long</span> ws = sl.tryConvertToWriteLock(stamp); <span class="comment">//将读锁转为写锁</span></div><div class="line">         <span class="keyword">if</span> (ws != <span class="number">0L</span>) &#123; <span class="comment">//这是确认转为写锁是否成功</span></div><div class="line">           stamp = ws; <span class="comment">//如果成功 替换票据</span></div><div class="line">           x = newX; <span class="comment">//进行状态改变</span></div><div class="line">           y = newY; <span class="comment">//进行状态改变</span></div><div class="line">           <span class="keyword">break</span>;</div><div class="line">         &#125;</div><div class="line">         <span class="keyword">else</span> &#123; <span class="comment">//如果不能成功转换为写锁</span></div><div class="line">           sl.unlockRead(stamp); <span class="comment">//我们显式释放读锁</span></div><div class="line">           stamp = sl.writeLock(); <span class="comment">//显式直接进行写锁 然后再通过循环再试</span></div><div class="line">         &#125;</div><div class="line">       &#125;</div><div class="line">     &#125; <span class="keyword">finally</span> &#123;</div><div class="line">       sl.unlock(stamp); <span class="comment">//释放读锁或写锁</span></div><div class="line">     &#125;</div><div class="line">   &#125;</div><div class="line"> &#125;</div></pre></td></tr></table></figure>
<h2 id="AtomicLong、LongAdder和LongAccumulator的实现有何不同？"><a href="#AtomicLong、LongAdder和LongAccumulator的实现有何不同？" class="headerlink" title="AtomicLong、LongAdder和LongAccumulator的实现有何不同？"></a>AtomicLong、LongAdder和LongAccumulator的实现有何不同？</h2><ul>
<li>AtomicLong是Java 5引入的基于CAS的无锁的操作长整形值的工具类；</li>
<li>LongAdder是Java 8提供的累加器，基于Striped64实现。它常用于状态采集、统计等场景。AtomicLong也可以用于这种场景，但在线程竞争激烈的情况下，LongAdder要比AtomicLong拥有更高的吞吐量，但会耗费更多的内存空间。</li>
<li>Striped64的设计核心思路就是通过内部的分散计算来避免竞争(比如多线程CAS操作时的竞争)。Striped64内部包含一个基础值和一个单元哈希表。没有竞争的情况下，要累加的数会累加到这个基础值上；如果有竞争的话，会将要累加的数累加到单元哈希表中的某个单元里面。所以整个Striped64的值包括基础值和单元哈希表中所有单元的值的总和。</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/** </span></div><div class="line"> * 存放Cell的hash表，大小为2的幂。 </div><div class="line"> */  </div><div class="line"><span class="keyword">transient</span> <span class="keyword">volatile</span> Cell[] cells;  </div><div class="line"></div><div class="line"><span class="comment">/** </span></div><div class="line"> * 基础值，没有竞争时会使用(更新)这个值，同时做为初始化竞争失败的回退方案。 </div><div class="line"> * 原子更新。 </div><div class="line"> */  </div><div class="line"><span class="keyword">transient</span> <span class="keyword">volatile</span> <span class="keyword">long</span> base;  </div><div class="line"></div><div class="line"><span class="comment">/** </span></div><div class="line"> * 自旋锁，通过CAS操作加锁，用于保护创建或者扩展Cell表。 </div><div class="line"> */  </div><div class="line"><span class="keyword">transient</span> <span class="keyword">volatile</span> <span class="keyword">int</span> cellsBusy;</div></pre></td></tr></table></figure>
<ul>
<li>LongAccumulator和LongAdder类似，也基于Striped64实现。但要比LongAdder更加灵活(要传入一个函数接口)，LongAdder相当于是LongAccumulator的一种特例。</li>
</ul>
<h2 id="CompletableFuture对比Future有哪些改进，怎么用？"><a href="#CompletableFuture对比Future有哪些改进，怎么用？" class="headerlink" title="CompletableFuture对比Future有哪些改进，怎么用？"></a>CompletableFuture对比Future有哪些改进，怎么用？</h2><ul>
<li>Future对象代表一个尚未完成异步操作的结果。从Java 5以来，JUC包一直提供着最基本的Future，不过它太鸡肋了，除了get、cancel、isDone和isCancelled方法之外就没有其他的操作了，对于结果的获取很不方便，只能通过阻塞或者轮询的方式得到任务的结果。阻塞的方式显然和我们的异步编程的初衷相违背，轮询的方式又会耗费无谓的CPU资源，而且也不能及时地得到计算结果，这样很不方便。</li>
<li>好在Java 8中引入了具有函数式风格的CompletableFuture，支持一系列的函数式的组合、运算操作，非常方便，可以写出函数式风格的代码而摆脱callback hell。</li>
<li>主动完成计算：CompletableFuture类实现了CompletionStage和Future接口，所以你还是可以像以前一样通过阻塞或者轮询的方式获得结果，尽管这种方式不推荐使用。</li>
<li>主要的API如下所示：</li>
</ul>
<pre><code>supplyAsync/runAsync -- 创建CompletableFuture对象；
whenComplete/whenCompleteAsync/exceptionally -- 计算完成或者抛出异常的时可以执行特定的Action；
thenApply/thenApplyAsync -- 对数据进行一些处理或变换；
thenAccept/thenAcceptAsync -- 纯消费，不返回新的计算值；
thenAcceptBoth/thenAcceptBothAsync/runAfterBoth -- 当两个CompletionStage都正常完成计算的时候，就会执行提供的Action；
thenCompose/thenComposeAsync -- 这个Function的输入是当前的CompletableFuture的计算值，返回结果将是一个新的CompletableFuture。
记住，thenCompose返回的对象并不一是函数fn返回的对象，如果原来的CompletableFuture还没有计算出来，
它就会生成一个新的组合后的CompletableFuture。可以用来实现异步pipline；
thenCombine/thenCombineAsync - 并行执行的，它们之间并没有先后依赖顺序，和thenAcceptBoth的区别在于有返回值；
allOf/anyOf -- 所有的/其中一个CompletableFuture都执行完后执行计算；
</code></pre><h2 id="其他面试小结"><a href="#其他面试小结" class="headerlink" title="其他面试小结"></a>其他面试小结</h2><ul>
<li><a href="http://ginobefunny.com/post/elasticsearch_interview_questions/">面试小结之Elasticsearch篇</a></li>
<li><a href="http://ginobefunny.com/post/jvm_interview_questions/">面试小结之JVM篇</a></li>
<li><a href="http://ginobefunny.com/post/java_concurrent_interview_questions/">面试小结之并发篇</a></li>
<li><a href="http://ginobefunny.com/post/java_nio_interview_questions/">面试小结之IO篇</a></li>
<li><a href="http://ginobefunny.com/post/java_other_interview_questions/">面试小结之综合篇</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近面试一些公司，被问到的关于Java并发编程的问题，以及自己总结的回答。&lt;br&gt;
    
    </summary>
    
      <category term="CodingLife" scheme="http://ginobefunny.com/categories/CodingLife/"/>
    
    
      <category term="职场" scheme="http://ginobefunny.com/tags/%E8%81%8C%E5%9C%BA/"/>
    
      <category term="锁" scheme="http://ginobefunny.com/tags/%E9%94%81/"/>
    
      <category term="并发" scheme="http://ginobefunny.com/tags/%E5%B9%B6%E5%8F%91/"/>
    
      <category term="面试" scheme="http://ginobefunny.com/tags/%E9%9D%A2%E8%AF%95/"/>
    
      <category term="ConcurrentHashMap" scheme="http://ginobefunny.com/tags/ConcurrentHashMap/"/>
    
  </entry>
  
  <entry>
    <title>面试小结之JVM篇</title>
    <link href="http://ginobefunny.com/post/jvm_interview_questions/"/>
    <id>http://ginobefunny.com/post/jvm_interview_questions/</id>
    <published>2017-06-14T08:38:35.000Z</published>
    <updated>2017-06-14T08:57:31.963Z</updated>
    
    <content type="html"><![CDATA[<p>最近面试一些公司，被问到的关于Java虚拟机的问题，以及自己总结的回答。<br><a id="more"></a></p>
<h2 id="Java内存区域是如何划分的？"><a href="#Java内存区域是如何划分的？" class="headerlink" title="Java内存区域是如何划分的？"></a>Java内存区域是如何划分的？</h2><ul>
<li>Java堆：线程共享的，唯一目的就是用于存放<strong>对象实例</strong>，是垃圾收集器管理的主要区域；</li>
<li>Java虚拟机栈：线程私有的，每个方法在执行的同时都会创建一个栈帧用于存储局部变量等，局部变量表存放了编译器可知的各种基本数据类型和<strong>对象引用</strong>；</li>
<li>本地方法栈：和虚拟机栈类似，不过它是为Native方法服务；</li>
<li>程序计数器：线程私有的，可以看作是当前线程所执行的字节码的行号指示器，以便线程切换后恢复执行使用；</li>
<li>方法区：线程共享的，用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据；该区域的内存回收主要是针对常量池的回收和类型的卸载（特别是要注意一些动态字节码框架和自定义ClassLoader的场景下）；在HotSpot里经常被称为永久代，<a href="http://www.infoq.com/cn/articles/Java-PERMGEN-Removed" target="_blank" rel="external">在Java 8里已被废除了，被元空间取代</a>；</li>
</ul>
<p><img src="http://upload-images.jianshu.io/upload_images/3709321-06db2c16d6f37c2e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Java内存区域"></p>
<h2 id="对象是否可用以及引用类型。"><a href="#对象是否可用以及引用类型。" class="headerlink" title="对象是否可用以及引用类型。"></a>对象是否可用以及引用类型。</h2><ul>
<li>由于引用计数法无法解决循环引用的问题，所以一般都是使用可达性分析来判断的，即通过一系列称为“GC Roots”的对象（比如虚拟机栈引用的对象、方法区中的类静态属性和常量引用对象）作为起点，从这些节点一直往下搜索，走过的路径称为引用链；而那些没有与引用链相连的对象即为不可达，会被回收；</li>
<li>可以通过覆盖finalize方法来实现对象的“自救”，避免在标记后被回收，但通常不建议这么做；</li>
<li>对象的引用类型可分为：强引用、软引用（在内存溢出前会将这种类型的对象进行第二次回收）、弱引用（弱引用对象只能生存到下次垃圾回收之前）、虚引用（不会对生存时间存在影响，也无法通过它获取对象，主要目的就是在回收时收到一个系统通知）；</li>
</ul>
<h2 id="有哪些常见的垃圾收集算法？"><a href="#有哪些常见的垃圾收集算法？" class="headerlink" title="有哪些常见的垃圾收集算法？"></a>有哪些常见的垃圾收集算法？</h2><ul>
<li>标记-清除算法：首先标记出所有需要回收的对象，然后统一回收所有被标记的对象；缺点是效率不高且容易产生大量不连续的内存碎片；</li>
<li>复制算法：将可用内存分为大小相等的两块，每次只使用其中一块；当这一块用完了，就将还活着的对象复制到另一块上，然后把已使用过的内存清理掉。在HotSpot里，考虑到大部分对象存活时间很短，将内存分为Eden和两块Survivor，默认比例为8:1:1。代价是存在部分内存空间浪费，且可能存在空间不够需要分配担保的情况，所以适合在新生代使用；</li>
<li>标记-整理算法：首先标记出所有需要回收的对象，然后让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存。适用于老年代。</li>
<li>分代收集算法：一般把Java堆分新生代和老年代，在新生代用复制算法，在老年代用标记-清理或标记-整理算法，是现代虚拟机通常采用的算法。</li>
</ul>
<p><em>PS：堆的划分及回收过程详解</em></p>
<pre><code>1. Eden区最大，对外提供堆内存。当Eden区快要满了，则进行Minor GC，把存活对象放入Survivor A区，清空Eden区；
2. Eden区被清空后，继续对外提供堆内存；
3. 当Eden区再次被填满，此时对Eden区和Survivor A区同时进行Minor GC，把存活对象放入Survivor B区，同时清空Eden 区和Survivor A区；
4. Eden区继续对外提供堆内存，并重复上述过程，即在Eden区填满后，把Eden区和某个Survivor区的存活对象放到另一个Survivor区；
5. 当某个Survivor区被填满，且仍有对象未被复制完毕时或者某些对象在反复Survive 15次左右时，则把这部分剩余对象放到Old区；
6. 当Old区也被填满时，进行Major GC，对Old区进行垃圾回收。
</code></pre><h2 id="有哪些常见的垃圾收集器？"><a href="#有哪些常见的垃圾收集器？" class="headerlink" title="有哪些常见的垃圾收集器？"></a>有哪些常见的垃圾收集器？</h2><p>这里讨论JDK 1.7 Update 14之后的HotSpot虚拟机，包含的虚拟机如下图所示（存在连线的表示可以搭配使用）：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3709321-1cbaf6812c70236f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="HotSpot垃圾收集器"></p>
<h3 id="Serial收集器"><a href="#Serial收集器" class="headerlink" title="Serial收集器"></a>Serial收集器</h3><p><img src="http://upload-images.jianshu.io/upload_images/3709321-1ee52c298577695f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Serial收集器"></p>
<ul>
<li>最基本、发展历史最悠久，在JDK 1.3之前是新生代收集的唯一选择；</li>
<li>是一个单线程（只会使用一个收集线程，且必须暂停所有工作线程）的收集器，采用的是复制算法；</li>
<li>现在依然是虚拟机运行在Client模式下的默认新生代收集器，主要就是因为它简单而高效（没有线程交互的开销）；</li>
</ul>
<h3 id="ParNew收集器"><a href="#ParNew收集器" class="headerlink" title="ParNew收集器"></a>ParNew收集器</h3><p><img src="http://upload-images.jianshu.io/upload_images/3709321-13c8878c650dc4f3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="ParNew收集器"></p>
<ul>
<li>其实就是Serial收集器的多线程版本，采用的也是复制算法；</li>
<li>ParNew收集器在单CPU环境中绝对不会有比Serial收集器更好的效果；</li>
<li>是许多运行在Server模式下虚拟机首选的新生代收集器，重要原因就是除了Serial收集器外，只有它能与CMS收集器配合工作；</li>
</ul>
<p><em>PS：关于垃圾收集器的并行和并发</em></p>
<ul>
<li>并行（Parallel）：指多条垃圾收集线程并行工作，但此时用户线程仍处于等待状态；</li>
<li>并发（Concurrent）：指用户线程与垃圾收集线程同时执行，用户线程在继续执行而垃圾收集程序运行在另外一个CPU上；</li>
</ul>
<h3 id="CMS收集器"><a href="#CMS收集器" class="headerlink" title="CMS收集器"></a>CMS收集器</h3><p><img src="http://upload-images.jianshu.io/upload_images/3709321-7e268ca70788cd32.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="CMS收集器"></p>
<ul>
<li>是一种以获取最短回收停顿时间为目标的收集器，特别适合互联网站或者B/S的服务端；</li>
<li>它是基于<strong>标记-清除</strong> 算法实现的，主要包括4个步骤：初始标记（STW，只是初始标记一下GC Roots能直接关联到的对象，速度很快）、并发标记（非STW，执行GC RootsTracing，耗时比较长）、重新标记（STW，修正并发标记期间因用户程序继续导致变动的那一部分对象标记）和并发清除（非STW，耗时较长）；</li>
<li>还有3个明显的缺点：CMS收集器对CPU非常敏感（占用部分线程及CPU资源，影响总吞吐量）、无法处理浮动垃圾（默认达到92%就触发垃圾回收）、大量内存碎片产生（可以通过参数启动压缩）；</li>
</ul>
<h2 id="介绍一下G1收集器的原理和实现。"><a href="#介绍一下G1收集器的原理和实现。" class="headerlink" title="介绍一下G1收集器的原理和实现。"></a>介绍一下G1收集器的原理和实现。</h2><p><img src="http://upload-images.jianshu.io/upload_images/3709321-e1fd8b5e091d6285.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="G1收集器"></p>
<ul>
<li>一款面向服务端应用的垃圾收集器，后续会替换掉CMS垃圾收集器；</li>
<li>特点：</li>
</ul>
<pre><code>并行与并发（充分利用多核多CPU缩短STW时间）
分代收集（独立管理整个Java堆，但针对不同年龄的对象采取不同的策略）
空间整合（局部看是基于复制算法，从整体来看是基于标记-整理算法，都不会产生内存碎片）
可预测的停顿（可以明确指定在一个长度为M毫秒的时间片内垃圾收集不会超过N毫秒）
</code></pre><ul>
<li>将堆分为大小相等的独立区域，避免全区域的垃圾收集；新生代和老年代不再物理隔离，只是部分Region的集合；</li>
<li>G1跟踪各个Region垃圾堆积的价值大小，在后台维护一个优先列表，根据允许的收集时间优先回收价值最大的Region；</li>
<li>Region之间的对象引用以及其他收集器中的新生代与老年代之间的对象引用，采用Remembered Set来避免全堆扫描；</li>
<li>分为几个步骤，和CMS的过程比较类似：</li>
</ul>
<pre><code>初始标记（标记一下GC Roots能直接关联的对象并修改TAMS值，需要STW但耗时很短）
并发标记（从GC Root从堆中对象进行可达性分析找存活的对象，耗时较长但可以与用户线程并发执行）
最终标记（为了修正并发标记期间产生变动的那一部分标记记录，这一期间的变化记录在Remembered 
Set Log里，然后合并到Remembered Set里，该阶段需要STW但是可并行执行）
筛选回收（对各个Region回收价值排序，根据用户期望的GC停顿时间制定回收计划来回收）；
</code></pre><h2 id="你们的服务配置的虚拟机参数是怎么样的？"><a href="#你们的服务配置的虚拟机参数是怎么样的？" class="headerlink" title="你们的服务配置的虚拟机参数是怎么样的？"></a>你们的服务配置的虚拟机参数是怎么样的？</h2><p>我们的服务的虚拟机参数：</p>
<pre><code>-server       --启用能够执行优化的编译器，显著提高服务器的性能
-Xmx4000M     --堆最大值
-Xms4000M     --堆初始大小
-Xmn600M      --年轻代大小
-XX:PermSize=200M         --持久代初始大小
-XX:MaxPermSize=200M      --持久代最大值
-Xss256K                  --每个线程的栈大小
-XX:+DisableExplicitGC    --关闭System.gc()
-XX:SurvivorRatio=1       --年轻代中Eden区与两个Survivor区的比值
-XX:+UseConcMarkSweepGC   --使用CMS内存收集
-XX:+UseParNewGC          --设置年轻代为并行收集
-XX:+CMSParallelRemarkEnabled        --降低标记停顿
-XX:+UseCMSCompactAtFullCollection   --在FULL GC的时候，对年老代进行压缩，可能会影响性能，但是可以消除碎片
-XX:CMSFullGCsBeforeCompaction=0     --此值设置运行多少次GC以后对内存空间进行压缩、整理
-XX:+CMSClassUnloadingEnabled        --回收动态生成的代理类 SEE：http://stackoverflow.com/questions/3334911/what-does-jvm-flag-cmsclassunloadingenabled-actually-do
-XX:LargePageSizeInBytes=128M        --内存页的大小不可设置过大， 会影响Perm的大小
-XX:+UseFastAccessorMethods          --原始类型的快速优化
-XX:+UseCMSInitiatingOccupancyOnly   --使用手动定义初始化定义开始CMS收集，禁止hostspot自行触发CMS GC
-XX:CMSInitiatingOccupancyFraction=80  --使用cms作为垃圾回收，使用80％后开始CMS收集
-XX:SoftRefLRUPolicyMSPerMB=0          --每兆堆空闲空间中SoftReference的存活时间
-XX:+PrintGCDetails                    --输出GC日志详情信息
-XX:+PrintGCApplicationStoppedTime     --输出垃圾回收期间程序暂停的时间
-Xloggc:$WEB_APP_HOME/.tomcat/logs/gc.log  --把相关日志信息记录到文件以便分析.
-XX:+HeapDumpOnOutOfMemoryError            --发生内存溢出时生成heapdump文件
-XX:HeapDumpPath=$WEB_APP_HOME/.tomcat/logs/heapdump.hprof  --heapdump文件地址
</code></pre><h2 id="如何进行性能调优以及常用的JDK的命令行工具有哪些？"><a href="#如何进行性能调优以及常用的JDK的命令行工具有哪些？" class="headerlink" title="如何进行性能调优以及常用的JDK的命令行工具有哪些？"></a>如何进行性能调优以及常用的JDK的命令行工具有哪些？</h2><ul>
<li>JVM调优：CPU使用率与Load值偏大（Thread count以及GC count）、关键接口响应时间很慢（GC time以及GC log中的STW的时间）、发生Full GC或者Old CMS GC非常频繁（内存泄露）；</li>
<li>JVM停顿（尽量避免Full GC、关闭偏向锁、输出GC日志到内存文件系统、关闭JVM输出的jstat日志）；</li>
<li>将Java性能优化分为4个层级：应用层、数据库层、框架层、JVM层。每层优化难度逐级增加，涉及的知识和解决的问题也会不同。比如应用层需要理解代码逻辑，通过Java线程栈定位有问题代码行等；数据库层面需要分析SQL、定位死锁等；框架层需要懂源代码，理解框架机制；JVM 层需要对GC的类型和工作机制有深入了解，对各种 JVM 参数作用了然于胸；</li>
<li>围绕Java性能优化，有两种最基本的分析方法：现场分析法和事后分析法。现场分析法通过保留现场，再采用诊断工具分析定位。现场分析对线上影响较大，部分场景不太合适。事后分析法需要尽可能多收集现场数据，然后立即恢复服务，同时针对收集的现场数据进行事后分析和复现。</li>
<li>OS 的诊断主要关注的是 CPU、Memory、I/O 三个方面。top、vmstat、 free –m、iostat；常用的Java应用诊断包括线程、堆栈、GC 等方面的诊断，可以使用jstack 、jstat、jmap；</li>
</ul>
<p><img src="http://upload-images.jianshu.io/upload_images/3709321-500753a685492eb6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="JDK的命令行工具"></p>
<h2 id="类的加载器是什么？"><a href="#类的加载器是什么？" class="headerlink" title="类的加载器是什么？"></a>类的加载器是什么？</h2><ul>
<li>虚拟机设计团队把类加载阶段的“通过一个类的全限定名来获取描述此类的二进制字节流”这个动作放到虚拟机外部去实现，实现这个动作的代码模块称为类加载器；这种设计给Java语言带来了非常强大的灵活性；</li>
<li>双亲委派模型要求除了顶层的启动类加载器外，其他的类加载器都应当有自己的父类加载器，如果一个类加载器收到了类加载的请求，它首先不会自己去尝试加载这个类，而是把这个请求委派给父类加载器去完成，只有父类加载器反馈自己无法完成这个加载请求时，子加载器才会尝试自己去加载；这对于保证程序的稳定运作很重要；</li>
</ul>
<p><img src="http://upload-images.jianshu.io/upload_images/3709321-9a7cb0ebb779f8e8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="类加载器"></p>
<ul>
<li>OSGI实现模块化热部署的关键是它自定义的类加载机制的实现，每个Bundle（通过Import-Package和Export-Package导入和导出依赖）都有自己的类加载器，类加载器之间形成了更加复杂的网状结构；</li>
</ul>
<h2 id="谈谈你对Java内存模型的理解。"><a href="#谈谈你对Java内存模型的理解。" class="headerlink" title="谈谈你对Java内存模型的理解。"></a>谈谈你对Java内存模型的理解。</h2><ul>
<li>虚拟机规范视图通过JMM来屏蔽掉各种硬件和操作系统的内存访问差异，主要目标是定义程序中各个变量的访问限制，即在虚拟机将变量存储到内存和从内存中取出变量这样的底层细节；</li>
<li>主内存与工作内存：Java内存模型规定了所有的变量都存储在主内存中，每个线程还有自己的工作内存，线程的工作内存中保存了被该线程使用到的变量的主内存副本拷贝，线程对变量的所有操作都必须在工作内存中进行，而不能直接读写主内存中的变量；</li>
</ul>
<p><img src="http://upload-images.jianshu.io/upload_images/3709321-484204d5e376eb01.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="主内存与工作内存"></p>
<ul>
<li>可见性（主内存和工作内存）、原子性（volatile的long是具备原子性的）、有序性（happen—before规则）；</li>
<li>Java语言中有一个“先行发生”（happen—before）的规则，它是Java内存模型中定义的两项操作之间的偏序关系，如果操作A先行发生于操作B，其意思就是说，在发生操作B之前，操作A产生的影响都能被操作B观察到，“影响”包括修改了内存中共享变量的值、发送了消息、调用了方法等，它与时间上的先后发生基本没有太大关系。下面是Java内存模型中的八条可保证happen—before的规则，它们无需任何同步器协助就已经存在，可以在编码中直接使用。如果两个操作之间的关系不在此列，并且无法从下列规则推导出来的话，它们就没有顺序性保障，虚拟机可以对它们进行随机地重排序。</li>
</ul>
<pre><code>1、程序次序规则：在一个单独的线程中，按照程序代码的执行流顺序，（时间上）先执行的操作happen—before（时间上）后执行的操作。
2、管理锁定规则：一个unlock操作happen—before后面（时间上的先后顺序，下同）对同一个锁的lock操作。
3、volatile变量规则：对一个volatile变量的写操作happen—before后面对该变量的读操作。
4、线程启动规则：Thread对象的start（）方法happen—before此线程的每一个动作。
5、线程终止规则：线程的所有操作都happen—before对此线程的终止检测，可以通过Thread.join（）方法结束、Thread.isAlive（）的返回值等手段检测到线程已经终止执行。
6、线程中断规则：对线程interrupt（）方法的调用happen—before发生于被中断线程的代码检测到中断时事件的发生。
7、对象终结规则：一个对象的初始化完成（构造函数执行结束）happen—before它的finalize（）方法的开始。
8、传递性：如果操作A happen—before操作B，操作B happen—before操作C，那么可以得出A happen—before操作C。
</code></pre><ul>
<li>比如双重检查实现单例模式可能存在并发问题，可以使用内部静态类实现。<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Singleton</span> </span>&#123;  </div><div class="line">  </div><div class="line">  <span class="function"><span class="keyword">private</span> <span class="title">Singleton</span><span class="params">()</span> </span>&#123;&#125;  </div><div class="line">  </div><div class="line">  <span class="comment">// Lazy initialization holder class idiom for static fields  </span></div><div class="line">  <span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">InstanceHolder</span> </span>&#123;  </div><div class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Singleton instance = <span class="keyword">new</span> Singleton();  </div><div class="line">  &#125;  </div><div class="line">  </div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Singleton <span class="title">getSingleton</span><span class="params">()</span> </span>&#123;   </div><div class="line">    <span class="keyword">return</span> InstanceHolder.instance;   </div><div class="line">  &#125;  </div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="是否了解偏向锁？"><a href="#是否了解偏向锁？" class="headerlink" title="是否了解偏向锁？"></a>是否了解偏向锁？</h2><ul>
<li>JVM锁有4种状态：无锁、偏向锁（通过MarkWord的线程ID）、轻量级锁（通过MarkWord的锁记录指针）、重量级锁；</li>
</ul>
<p><img src="http://upload-images.jianshu.io/upload_images/3709321-966e0972d28d930f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="锁的优缺点对比"></p>
<h2 id="其他面试小结"><a href="#其他面试小结" class="headerlink" title="其他面试小结"></a>其他面试小结</h2><ul>
<li><a href="http://ginobefunny.com/post/elasticsearch_interview_questions/">面试小结之Elasticsearch篇</a></li>
<li><a href="http://ginobefunny.com/post/jvm_interview_questions/">面试小结之JVM篇</a></li>
<li><a href="http://ginobefunny.com/post/java_concurrent_interview_questions/">面试小结之并发篇</a></li>
<li><a href="http://ginobefunny.com/post/java_nio_interview_questions/">面试小结之IO篇</a></li>
<li><a href="http://ginobefunny.com/post/java_other_interview_questions/">面试小结之综合篇</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近面试一些公司，被问到的关于Java虚拟机的问题，以及自己总结的回答。&lt;br&gt;
    
    </summary>
    
      <category term="CodingLife" scheme="http://ginobefunny.com/categories/CodingLife/"/>
    
    
      <category term="职场" scheme="http://ginobefunny.com/tags/%E8%81%8C%E5%9C%BA/"/>
    
      <category term="JVM" scheme="http://ginobefunny.com/tags/JVM/"/>
    
      <category term="垃圾收集" scheme="http://ginobefunny.com/tags/%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86/"/>
    
      <category term="面试" scheme="http://ginobefunny.com/tags/%E9%9D%A2%E8%AF%95/"/>
    
      <category term="JMM" scheme="http://ginobefunny.com/tags/JMM/"/>
    
  </entry>
  
  <entry>
    <title>面试小结之Elasticsearch篇</title>
    <link href="http://ginobefunny.com/post/elasticsearch_interview_questions/"/>
    <id>http://ginobefunny.com/post/elasticsearch_interview_questions/</id>
    <published>2017-06-14T08:33:23.000Z</published>
    <updated>2017-06-14T08:57:17.788Z</updated>
    
    <content type="html"><![CDATA[<p>最近面试一些公司，被问到的关于Elasticsearch和搜索引擎相关的问题，以及自己总结的回答。<br><a id="more"></a></p>
<h2 id="Elasticsearch是如何实现Master选举的？"><a href="#Elasticsearch是如何实现Master选举的？" class="headerlink" title="Elasticsearch是如何实现Master选举的？"></a>Elasticsearch是如何实现Master选举的？</h2><ul>
<li>Elasticsearch的选主是ZenDiscovery模块负责的，主要包含Ping（节点之间通过这个RPC来发现彼此）和Unicast（单播模块包含一个主机列表以控制哪些节点需要ping通）这两部分；</li>
<li>对所有可以成为master的节点（<strong>node.master: true</strong>）根据nodeId字典排序，每次选举每个节点都把自己所知道节点排一次序，然后选出第一个（第0位）节点，暂且认为它是master节点。</li>
<li>如果对某个节点的投票数达到一定的值（可以成为master节点数n/2+1）并且该节点自己也选举自己，那这个节点就是master。否则重新选举一直到满足上述条件。</li>
<li><em>补充：master节点的职责主要包括集群、节点和索引的管理，不负责文档级别的管理；data节点可以关闭http功能</em>。</li>
</ul>
<h2 id="Elasticsearch中的节点（比如共20个），其中的10个选了一个master，另外10个选了另一个master，怎么办？"><a href="#Elasticsearch中的节点（比如共20个），其中的10个选了一个master，另外10个选了另一个master，怎么办？" class="headerlink" title="Elasticsearch中的节点（比如共20个），其中的10个选了一个master，另外10个选了另一个master，怎么办？"></a>Elasticsearch中的节点（比如共20个），其中的10个选了一个master，另外10个选了另一个master，怎么办？</h2><ul>
<li>当集群master候选数量不小于3个时，可以通过设置最少投票通过数量（<strong>discovery.zen.minimum_master_nodes</strong>）超过所有候选节点一半以上来解决脑裂问题；</li>
<li>当候选数量为两个时，只能修改为唯一的一个master候选，其他作为data节点，避免脑裂问题。</li>
</ul>
<h2 id="客户端在和集群连接时，如何选择特定的节点执行请求的？"><a href="#客户端在和集群连接时，如何选择特定的节点执行请求的？" class="headerlink" title="客户端在和集群连接时，如何选择特定的节点执行请求的？"></a>客户端在和集群连接时，如何选择特定的节点执行请求的？</h2><ul>
<li>TransportClient利用transport模块远程连接一个elasticsearch集群。它并不加入到集群中，只是简单的获得一个或者多个初始化的transport地址，并以 <strong>轮询</strong> 的方式与这些地址进行通信。</li>
</ul>
<h2 id="详细描述一下Elasticsearch索引文档的过程。"><a href="#详细描述一下Elasticsearch索引文档的过程。" class="headerlink" title="详细描述一下Elasticsearch索引文档的过程。"></a>详细描述一下Elasticsearch索引文档的过程。</h2><ul>
<li>协调节点默认使用文档ID参与计算（也支持通过routing），以便为路由提供合适的分片。</li>
</ul>
<pre><code>shard = hash(document_id) % (num_of_primary_shards)
</code></pre><ul>
<li>当分片所在的节点接收到来自协调节点的请求后，会将请求写入到Memory Buffer，然后定时（默认是每隔1秒）写入到Filesystem Cache，这个从Momery Buffer到Filesystem Cache的过程就叫做refresh；</li>
<li>当然在某些情况下，存在Momery Buffer和Filesystem Cache的数据可能会丢失，ES是通过translog的机制来保证数据的可靠性的。其实现机制是接收到请求后，同时也会写入到translog中，当Filesystem cache中的数据写入到磁盘中时，才会清除掉，这个过程叫做flush；</li>
<li>在flush过程中，内存中的缓冲将被清除，内容被写入一个新段，段的fsync将创建一个新的提交点，并将内容刷新到磁盘，旧的translog将被删除并开始一个新的translog。</li>
<li>flush触发的时机是定时触发（默认30分钟）或者translog变得太大（默认为512M）时；</li>
</ul>
<p><img src="http://upload-images.jianshu.io/upload_images/3709321-2084bd0268a42ae1.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Elasticsearch索引文档的过程"></p>
<p><em>补充：关于Lucene的Segement：</em></p>
<ul>
<li>Lucene索引是由多个段组成，段本身是一个功能齐全的倒排索引。</li>
<li>段是不可变的，允许Lucene将新的文档增量地添加到索引中，而不用从头重建索引。</li>
<li>对于每一个搜索请求而言，索引中的所有段都会被搜索，并且每个段会消耗CPU的时钟周、文件句柄和内存。这意味着段的数量越多，搜索性能会越低。</li>
<li>为了解决这个问题，Elasticsearch会合并小段到一个较大的段，提交新的合并段到磁盘，并删除那些旧的小段。</li>
</ul>
<h2 id="详细描述一下Elasticsearch更新和删除文档的过程。"><a href="#详细描述一下Elasticsearch更新和删除文档的过程。" class="headerlink" title="详细描述一下Elasticsearch更新和删除文档的过程。"></a>详细描述一下Elasticsearch更新和删除文档的过程。</h2><ul>
<li>删除和更新也都是写操作，但是Elasticsearch中的文档是不可变的，因此不能被删除或者改动以展示其变更；</li>
<li>磁盘上的每个段都有一个相应的.del文件。当删除请求发送后，文档并没有真的被删除，而是在.del文件中被标记为删除。该文档依然能匹配查询，但是会在结果中被过滤掉。当段合并时，在.del文件中被标记为删除的文档将不会被写入新段。</li>
<li>在新的文档被创建时，Elasticsearch会为该文档指定一个版本号，当执行更新时，旧版本的文档在.del文件中被标记为删除，新版本的文档被索引到一个新段。旧版本的文档依然能匹配查询，但是会在结果中被过滤掉。</li>
</ul>
<h2 id="详细描述一下Elasticsearch搜索的过程。"><a href="#详细描述一下Elasticsearch搜索的过程。" class="headerlink" title="详细描述一下Elasticsearch搜索的过程。"></a>详细描述一下Elasticsearch搜索的过程。</h2><ul>
<li>搜索被执行成一个两阶段过程，我们称之为 Query Then Fetch；</li>
<li>在初始<em>查询阶段</em>时，查询会广播到索引中每一个分片拷贝（主分片或者副本分片）。 每个分片在本地执行搜索并构建一个匹配文档的大小为 from + size 的优先队列。<em>PS：在搜索的时候是会查询Filesystem Cache的，但是有部分数据还在Memory Buffer，所以搜索是近实时的。</em></li>
<li>每个分片返回各自优先队列中 <strong>所有文档的 ID 和排序值</strong> 给协调节点，它合并这些值到自己的优先队列中来产生一个全局排序后的结果列表。</li>
<li>接下来就是 <em>取回阶段</em>，协调节点辨别出哪些文档需要被取回并向相关的分片提交多个 GET 请求。每个分片加载并 <em>丰富</em> 文档，如果有需要的话，接着返回文档给协调节点。一旦所有的文档都被取回了，协调节点返回结果给客户端。</li>
<li><em>补充：Query Then Fetch的搜索类型在文档相关性打分的时候参考的是本分片的数据，这样在文档数量较少的时候可能不够准确，DFS Query Then Fetch增加了一个预查询的处理，询问Term和Document frequency，这个评分更准确，但是性能会变差。</em></li>
</ul>
<p><img src="http://upload-images.jianshu.io/upload_images/3709321-88f589037638c93d.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Elasticsearch执行搜索的过程"></p>
<h2 id="在Elasticsearch中，是怎么根据一个词找到对应的倒排索引的？"><a href="#在Elasticsearch中，是怎么根据一个词找到对应的倒排索引的？" class="headerlink" title="在Elasticsearch中，是怎么根据一个词找到对应的倒排索引的？"></a>在Elasticsearch中，是怎么根据一个词找到对应的倒排索引的？</h2><p>SEE：</p>
<ul>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2009/12/14/1623597.html" target="_blank" rel="external">Lucene的索引文件格式(1)</a>  </li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2009/12/14/1623599.html" target="_blank" rel="external">Lucene的索引文件格式(2)</a></li>
</ul>
<h2 id="Elasticsearch在部署时，对Linux的设置有哪些优化方法？"><a href="#Elasticsearch在部署时，对Linux的设置有哪些优化方法？" class="headerlink" title="Elasticsearch在部署时，对Linux的设置有哪些优化方法？"></a>Elasticsearch在部署时，对Linux的设置有哪些优化方法？</h2><ul>
<li>64 GB 内存的机器是非常理想的， 但是32 GB 和16 GB 机器也是很常见的。少于8 GB 会适得其反。</li>
<li>如果你要在更快的 CPUs 和更多的核心之间选择，选择更多的核心更好。多个内核提供的额外并发远胜过稍微快一点点的时钟频率。</li>
<li>如果你负担得起 SSD，它将远远超出任何旋转介质。 基于 SSD 的节点，查询和索引性能都有提升。如果你负担得起，SSD 是一个好的选择。</li>
<li>即使数据中心们近在咫尺，也要避免集群跨越多个数据中心。绝对要避免集群跨越大的地理距离。</li>
<li>请确保运行你应用程序的 JVM 和服务器的 JVM 是完全一样的。 在 Elasticsearch 的几个地方，使用 Java 的本地序列化。</li>
<li>通过设置gateway.recover_after_nodes、gateway.expected_nodes、gateway.recover_after_time可以在集群重启的时候避免过多的分片交换，这可能会让数据恢复从数个小时缩短为几秒钟。</li>
<li>Elasticsearch 默认被配置为使用单播发现，以防止节点无意中加入集群。只有在同一台机器上运行的节点才会自动组成集群。最好使用单播代替组播。</li>
<li>不要随意修改垃圾回收器（CMS）和各个线程池的大小。</li>
<li>把你的内存的（少于）一半给 Lucene（但不要超过 32 GB！），通过ES_HEAP_SIZE 环境变量设置。</li>
<li>内存交换到磁盘对服务器性能来说是致命的。如果内存交换到磁盘上，一个 100 微秒的操作可能变成 10 毫秒。 再想想那么多 10 微秒的操作时延累加起来。 不难看出 swapping 对于性能是多么可怕。</li>
<li>Lucene 使用了<em>大量的</em>文件。同时，Elasticsearch 在节点和 HTTP 客户端之间进行通信也使用了大量的套接字。 所有这一切都需要足够的文件描述符。你应该增加你的文件描述符，设置一个很大的值，如 64,000。</li>
</ul>
<p><em>补充：索引阶段性能提升方法</em></p>
<ul>
<li>使用批量请求并调整其大小：每次批量数据 5–15 MB 大是个不错的起始点。</li>
<li>存储：使用 SSD</li>
<li>段和合并：Elasticsearch 默认值是 20 MB/s，对机械磁盘应该是个不错的设置。如果你用的是 SSD，可以考虑提高到 100–200 MB/s。如果你在做批量导入，完全不在意搜索，你可以彻底关掉合并限流。另外还可以增加 index.translog.flush_threshold_size 设置，从默认的 512 MB 到更大一些的值，比如 1 GB，这可以在一次清空触发的时候在事务日志里积累出更大的段。</li>
<li>如果你的搜索结果不需要近实时的准确度，考虑把每个索引的index.refresh_interval 改到30s。</li>
<li>如果你在做大批量导入，考虑通过设置index.number_of_replicas: 0 关闭副本。</li>
</ul>
<h2 id="对于GC方面，在使用Elasticsearch时要注意什么？"><a href="#对于GC方面，在使用Elasticsearch时要注意什么？" class="headerlink" title="对于GC方面，在使用Elasticsearch时要注意什么？"></a>对于GC方面，在使用Elasticsearch时要注意什么？</h2><ul>
<li>SEE：<a href="https://elasticsearch.cn/article/32" target="_blank" rel="external">https://elasticsearch.cn/article/32</a></li>
<li>倒排词典的索引需要常驻内存，无法GC，需要监控data node上segment memory增长趋势。</li>
<li>各类缓存，field cache, filter cache, indexing cache, bulk queue等等，要设置合理的大小，并且要应该根据最坏的情况来看heap是否够用，也就是各类缓存全部占满的时候，还有heap空间可以分配给其他任务吗？避免采用clear cache等“自欺欺人”的方式来释放内存。</li>
<li>避免返回大量结果集的搜索与聚合。确实需要大量拉取数据的场景，可以采用scan &amp; scroll api来实现。</li>
<li>cluster stats驻留内存并无法水平扩展，超大规模集群可以考虑分拆成多个集群通过tribe node连接。</li>
<li>想知道heap够不够，必须结合实际应用场景，并对集群的heap使用情况做持续的监控。</li>
</ul>
<h2 id="Elasticsearch对于大数据量（上亿量级）的聚合如何实现？"><a href="#Elasticsearch对于大数据量（上亿量级）的聚合如何实现？" class="headerlink" title="Elasticsearch对于大数据量（上亿量级）的聚合如何实现？"></a>Elasticsearch对于大数据量（上亿量级）的聚合如何实现？</h2><ul>
<li>Elasticsearch 提供的首个近似聚合是cardinality 度量。它提供一个字段的基数，即该字段的<em>distinct</em>或者<em>unique</em>值的数目。它是基于HLL算法的。HLL 会先对我们的输入作哈希运算，然后根据哈希运算的结果中的 bits 做概率估算从而得到基数。其特点是：可配置的精度，用来控制内存的使用（更精确 ＝ 更多内存）；小的数据集精度是非常高的；我们可以通过配置参数，来设置去重需要的固定内存使用量。无论数千还是数十亿的唯一值，内存使用量只与你配置的精确度相关。</li>
</ul>
<h2 id="在并发情况下，Elasticsearch如果保证读写一致？"><a href="#在并发情况下，Elasticsearch如果保证读写一致？" class="headerlink" title="在并发情况下，Elasticsearch如果保证读写一致？"></a>在并发情况下，Elasticsearch如果保证读写一致？</h2><ul>
<li>可以通过版本号使用乐观并发控制，以确保新版本不会被旧版本覆盖，由应用层来处理具体的冲突；</li>
<li>另外对于写操作，一致性级别支持quorum/one/all，默认为quorum，即只有当大多数分片可用时才允许写操作。但即使大多数可用，也可能存在因为网络等原因导致写入副本失败，这样该副本被认为故障，分片将会在一个不同的节点上重建。</li>
<li>对于读操作，可以设置replication为sync(默认)，这使得操作在主分片和副本分片都完成后才会返回；如果设置replication为async时，也可以通过设置搜索请求参数_preference为primary来查询主分片，确保文档是最新版本。</li>
</ul>
<h2 id="如何监控Elasticsearch集群状态？"><a href="#如何监控Elasticsearch集群状态？" class="headerlink" title="如何监控Elasticsearch集群状态？"></a>如何监控Elasticsearch集群状态？</h2><ul>
<li>Marvel 让你可以很简单的通过 Kibana 监控 Elasticsearch。你可以实时查看你的集群健康状态和性能，也可以分析过去的集群、索引和节点指标。</li>
</ul>
<h2 id="介绍下你们电商搜索的整体技术架构。"><a href="#介绍下你们电商搜索的整体技术架构。" class="headerlink" title="介绍下你们电商搜索的整体技术架构。"></a>介绍下你们电商搜索的整体技术架构。</h2><p><img src="http://upload-images.jianshu.io/upload_images/3709321-3a013fdb77aa88fc.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="整体技术架构"></p>
<h2 id="介绍一下你们的个性化搜索方案？"><a href="#介绍一下你们的个性化搜索方案？" class="headerlink" title="介绍一下你们的个性化搜索方案？"></a>介绍一下你们的个性化搜索方案？</h2><p>SEE <a href="http://ginobefunny.com/post/personalized_search_implemention_based_word2vec_and_elasticsearch/">基于word2vec和Elasticsearch实现个性化搜索</a></p>
<h2 id="是否了解字典树？"><a href="#是否了解字典树？" class="headerlink" title="是否了解字典树？"></a>是否了解字典树？</h2><ul>
<li>常用字典数据结构如下所示：</li>
</ul>
<p><img src="http://upload-images.jianshu.io/upload_images/3709321-7b6f0fab6f412f51.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="常用字典数据结构"></p>
<ul>
<li>Trie的核心思想是空间换时间，利用字符串的公共前缀来降低查询时间的开销以达到提高效率的目的。它有3个基本性质：</li>
</ul>
<pre><code>根节点不包含字符，除根节点外每一个节点都只包含一个字符。
从根节点到某一节点，路径上经过的字符连接起来，为该节点对应的字符串。
每个节点的所有子节点包含的字符都不相同。
</code></pre><p><img src="https://github.com/julycoding/The-Art-Of-Programming-By-July/raw/master/ebook/images/8/8.4/1.jpg" alt="字典树"></p>
<ul>
<li>可以看到，trie树每一层的节点数是26^i级别的。所以为了节省空间，我们还可以用动态链表，或者用数组来模拟动态。而空间的花费，不会超过单词数×单词长度。</li>
<li>实现：对每个结点开一个字母集大小的数组，每个结点挂一个链表，使用左儿子右兄弟表示法记录这棵树；</li>
<li>对于中文的字典树，每个节点的子节点用一个哈希表存储，这样就不用浪费太大的空间，而且查询速度上可以保留哈希的复杂度O(1)。</li>
</ul>
<h2 id="拼写纠错是如何实现的？"><a href="#拼写纠错是如何实现的？" class="headerlink" title="拼写纠错是如何实现的？"></a>拼写纠错是如何实现的？</h2><ul>
<li>拼写纠错是基于编辑距离来实现；编辑距离是一种标准的方法，它用来表示经过插入、删除和替换操作从一个字符串转换到另外一个字符串的最小操作步数；</li>
<li>编辑距离的计算过程：比如要计算batyu和beauty的编辑距离，先创建一个7×8的表（batyu长度为5，coffee长度为6，各加2），接着，在如下位置填入黑色数字。其他格的计算过程是取以下三个值的最小值：</li>
</ul>
<pre><code>如果最上方的字符等于最左方的字符，则为左上方的数字。否则为左上方的数字+1。（对于3,3来说为0）
左方数字+1（对于3,3格来说为2）
上方数字+1（对于3,3格来说为2）
</code></pre><p>最终取右下角的值即为编辑距离的值3。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3709321-31bef8a5bbf14a13.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="编辑距离"></p>
<ul>
<li>对于拼写纠错，我们考虑构造一个度量空间（Metric Space），该空间内任何关系满足以下三条基本条件：</li>
</ul>
<pre><code>d(x,y) = 0 -- 假如x与y的距离为0，则x=y
d(x,y) = d(y,x)  -- x到y的距离等同于y到x的距离
d(x,y) + d(y,z) &gt;= d(x,z) -- 三角不等式
</code></pre><ul>
<li>根据三角不等式，则满足与query距离在n范围内的另一个字符转B，其与A的距离最大为d+n，最小为d-n。</li>
<li>BK树的构造就过程如下：每个节点有任意个子节点，每条边有个值表示编辑距离。所有子节点到父节点的边上标注n表示编辑距离恰好为n。比如，我们有棵树父节点是”book”和两个子节点”cake”和”books”，”book”到”books”的边标号1，”book”到”cake”的边上标号4。从字典里构造好树后，无论何时你想插入新单词时，计算该单词与根节点的编辑距离，并且查找数值为d(neweord, root)的边。递归得与各子节点进行比较，直到没有子节点，你就可以创建新的子节点并将新单词保存在那。比如，插入”boo”到刚才上述例子的树中，我们先检查根节点，查找d(“book”, “boo”) = 1的边，然后检查标号为1的边的子节点，得到单词”books”。我们再计算距离d(“books”, “boo”)=2，则将新单词插在”books”之后，边标号为2。</li>
<li>查询相似词如下：计算单词与根节点的编辑距离d，然后递归查找每个子节点标号为d-n到d+n（包含）的边。假如被检查的节点与搜索单词的距离d小于n，则返回该节点并继续查询。比如输入cape且最大容忍距离为1，则先计算和根的编辑距离d(“book”, “cape”)=4，然后接着找和根节点之间编辑距离为3到5的，这个就找到了cake这个节点，计算d(“cake”, “cape”)=1，满足条件所以返回<strong>cake</strong>，然后再找和cake节点编辑距离是0到2的，分别找到cape和cart节点，这样就得到<strong>cape</strong>这个满足条件的结果。</li>
</ul>
<p><img src="http://upload-images.jianshu.io/upload_images/3709321-7cdb109e6f73c192.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="BK树"></p>
<h2 id="其他面试小结"><a href="#其他面试小结" class="headerlink" title="其他面试小结"></a>其他面试小结</h2><ul>
<li><a href="http://ginobefunny.com/post/elasticsearch_interview_questions/">面试小结之Elasticsearch篇</a></li>
<li><a href="http://ginobefunny.com/post/jvm_interview_questions/">面试小结之JVM篇</a></li>
<li><a href="http://ginobefunny.com/post/java_concurrent_interview_questions/">面试小结之并发篇</a></li>
<li><a href="http://ginobefunny.com/post/java_nio_interview_questions/">面试小结之IO篇</a></li>
<li><a href="http://ginobefunny.com/post/java_other_interview_questions/">面试小结之综合篇</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近面试一些公司，被问到的关于Elasticsearch和搜索引擎相关的问题，以及自己总结的回答。&lt;br&gt;
    
    </summary>
    
      <category term="CodingLife" scheme="http://ginobefunny.com/categories/CodingLife/"/>
    
    
      <category term="Elasticsearch" scheme="http://ginobefunny.com/tags/Elasticsearch/"/>
    
      <category term="职场" scheme="http://ginobefunny.com/tags/%E8%81%8C%E5%9C%BA/"/>
    
      <category term="面试" scheme="http://ginobefunny.com/tags/%E9%9D%A2%E8%AF%95/"/>
    
  </entry>
  
  <entry>
    <title>分布式调用跟踪系统调研笔记</title>
    <link href="http://ginobefunny.com/post/learning_distributed_systems_tracing/"/>
    <id>http://ginobefunny.com/post/learning_distributed_systems_tracing/</id>
    <published>2017-05-25T12:26:51.000Z</published>
    <updated>2017-05-25T12:34:20.262Z</updated>
    
    <content type="html"><![CDATA[<p>随着微服务架构的流行，服务之间的调用关系愈加复杂，如何跟踪它们之间的调用关系以及对依赖进行分析显得更加重要。而分布式调用跟踪系统就是为了解决这个问题而生的，本文为调研学习已有DST系统的学习笔记。<br><a id="more"></a></p>
<h1 id="分布式调用链跟踪系统设计目标"><a href="#分布式调用链跟踪系统设计目标" class="headerlink" title="分布式调用链跟踪系统设计目标"></a>分布式调用链跟踪系统设计目标</h1><ul>
<li>低侵入性 – 作为非业务组件，应当尽可能少侵入或者无侵入其他业务系统，对于使用方透明，减少开发人员的负担；</li>
<li>灵活的应用策略 – 可以（最好随时）决定所收集数据的范围和粒度；</li>
<li>时效性 – 从数据的收集和产生，到数据计算和处理，再到最终展现，都要求尽可能快；</li>
<li>决策支持 – 这些数据是否能在决策支持层面发挥作用，特别是从 DevOps 的角度；</li>
<li>可视化才是王道。</li>
</ul>
<h1 id="Twitter-zipkin"><a href="#Twitter-zipkin" class="headerlink" title="Twitter zipkin"></a>Twitter zipkin</h1><p>SEE：<a href="http://zipkin.io/pages/architecture.html" target="_blank" rel="external">http://zipkin.io/pages/architecture.html</a><br>SEE：<a href="https://github.com/openzipkin/zipkin" target="_blank" rel="external">https://github.com/openzipkin/zipkin</a></p>
<ul>
<li>Zipkin 是一款开源的分布式实时数据追踪系统），基于Google Dapper的论文设计而来，由Twitter公司开发贡献。其主要功能是聚集来自各个异构系统的实时监控数据，用来追踪微服务架构下的系统延时问题。应用系统需要进行instrument以向Zipkin报告数据。Zipkin的用户界面可以呈现一幅关联图表，以显示有多少被追踪的请求通过了每一层应用。</li>
<li>Zipkin以Trace结构表示对一次请求的追踪，又把每个Trace拆分为若干个有依赖关系的Span。在微服务架构中，一次用户请求可能会由后台若干个服务负责处理，那么每个处理请求的服务就可以理解为一个Span。当然这个服务也可能继续请求其他的服务，因此Span是一个树形结构，以体现服务之间的调用关系。</li>
<li>Zipkin的Span模型几乎完全仿造了Dapper中Span模型的设计，Zipkin中的Span主要包含三个数据部分：基础数据（包括traceId、spanId、parentId、name、timestamp和duration，主要用于跟踪树中节点的关联和界面展示）、 Annotation（用来记录请求特定事件相关信息）、BinaryAnnotation（提供一些额外信息，一般以key-value对出现）。</li>
<li>Zipkin架构组成：Zipkin的收集器负责将各系统报告过来的追踪数据进行接收；而数据存储默认使用Cassandra，也可以替换为 MySQL等；查询服务用来向其他服务提供数据查询的能力，而Web服务是官方默认提供的一个图形用户界面。</li>
</ul>
<p><img src="http://zipkin.io/public/img/architecture-1.png" alt="Zipkin架构"></p>
<h1 id="淘宝EagleEye"><a href="#淘宝EagleEye" class="headerlink" title="淘宝EagleEye"></a>淘宝EagleEye</h1><p>SEE：<a href="https://www.slideshare.net/terryice/eagleeye-with-taobaojavaone" target="_blank" rel="external">https://www.slideshare.net/terryice/eagleeye-with-taobaojavaone</a><br>SEE：<a href="http://jm.taobao.org/2014/03/04/3465/" target="_blank" rel="external">http://jm.taobao.org/2014/03/04/3465/</a><br>SEE：<a href="http://www.myroute.cn/show.php?md=%2Fali%2FeagleEye%2FeagleEye%E6%BA%90%E7%A0%81%E5%88%9D%E6%8E%A2.md" target="_blank" rel="external">http://www.myroute.cn/show.php?md=%2Fali%2FeagleEye%2FeagleEye%E6%BA%90%E7%A0%81%E5%88%9D%E6%8E%A2.md</a></p>
<ul>
<li>日趋复杂的分布式系统：RPC、消息通讯、数据库分表分库、分布式缓存、分布式文件系统……</li>
<li>EagleEye是基于日志的分布式调用跟踪系统，脱胎于Google Dapper论文。其核心是调用链，每次请求都生成一个全局唯一的ID（TraceId），通过它将不同系统的“孤立的”日志串在一起，重组还原出更多有价值的信息。目前已覆盖了淘宝主要的有网络通信的中间件，包括前端请求接入Tengine、分布式会话tbsession、远程服务调用框架HSF、异步消息通讯Notify、数据库中间件TDDL、分布式缓存Tair、分布式文件系统TFS、特定功能的客户端如搜索等；</li>
<li>处理过程：在前端请求到达服务器时，应用容器在执行实际业务处理之前，会先执行EagleEye的埋点逻辑（类似Filter的机制），埋点逻辑为这个前端请求分配一个全局唯一的调用链ID。这个ID在EagleEye 里面被称为 TraceId，埋点逻辑把TraceId 放在一个调用上下文对象里面，而调用上下文对象会存储在ThreadLocal里面。调用上下文里还有一个ID非常重要，在EagleEye里面被称作RpcId。RpcId用于区分同一个调用链下的多个网络调用的发生顺序和嵌套层次关系。对于前端收到请求，生成的RpcId固定都是0。当这个前端执行业务处理需要发起RPC调用时，淘宝的RPC调用客户端HSF会首先从当前线程ThreadLocal上面获取之前EagleEye设置的调用上下文。然后，把RpcId递增一个序号。在EagleEye里使用多级序号来表示RpcId，比如前端刚接到请求之后的RpcId是0，那么它第一次调用RPC服务A时，会把RpcId改成0.1。之后，调用上下文会作为附件随这次请求一起发送到远程的HSF服务器。服务的逻辑全部处理完毕之后，HSF在返回响应对象之前，会把这次调用情况以及TraceId、RpcId都打印到它的访问日志之中，同时会从ThreadLocal清理掉调用上下文。</li>
<li>EagleEye的使用场景：调用链跟踪（与异常监控集成）；链路分析（容量规划、调用来源、依赖度量、调用耗时、调用并行度、调用路由情况）；透明数据传输（本身需要传递TraceID等上下文信息、透明传输业务数据）；</li>
<li>EagleEye的实现之埋点和输出日志：中间件创建调用上下文生成日志埋点、放在ThreadLocal对业务透明、在网络请求中传递；TraceID、RpcID（Dapper里的SpanID）、开始时间、耗时、调用类型、IP、处理结果等；性能影响改进（利用无锁环形队列做日志异步写入来避免hang住业务线程、调节日志输出缓冲大小控制每秒写日志的IO次数、做采样和开关控制）；</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/19_apm/EagleEye_1.png" alt="埋点和输出日志"></p>
<p><img src="http://oi46mo3on.bkt.clouddn.com/19_apm/EagleEye_2.png" alt="调用链示意图"></p>
<ul>
<li>EagleEye的实现之收集和存储日志：EagleEye的埋点日志会输出到统一的日志目录下，与业务无关的RPC日志会写文件eagleeye.log，业务埋点日志会写文件biz-eagleeye.log。随后，tlog服务器会通过每台应用机器上的哈勃agent实时的不断抓取EagleEye日志，按照日志类型不同，将会有不同的处理方式：</li>
</ul>
<pre><code>1. 全量原始日志都会直接存到HDFS中；
2. 带有实时标记的EagleEye原始日志，放入HBase中存储；
3. 业务日志有一部分会被直接处理，存储到HBase，有一部分会作为消息发送出去，由感兴趣的业务系统订阅处理；
4. 全量日志存储到HDFS之后，EagleEye服务器会每小时创建MapReduce任务，在云梯上完成对全量日志的调用链合并、分析
以及统计，合并好的调用链依旧保存在HDFS，分析和统计的结果也输出到HDFS，EagleEye服务器在任务执行完毕之后，会将
结果拉取到本地建立索引，并保存到HBase。在保存完毕后，就可以在EagleEye的数据展现页面看到分析结果了。
5. 另外，EagleEye 还做了调用的实时统计，有分钟级别的实时链路大盘、应用大盘、服务大盘和入口大盘，帮助快速定位故障问题所在。
</code></pre><p><img src="http://oi46mo3on.bkt.clouddn.com/19_apm/EagleEye_3.png" alt="收集和存储日志"></p>
<ul>
<li>EagleEye的实现之分析调用链：基于入口对调用链做链路分析；</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/19_apm/EagleEye_4.png" alt="分析调用链"></p>
<h1 id="微博Watchman"><a href="#微博Watchman" class="headerlink" title="微博Watchman"></a>微博Watchman</h1><p>SEE： <a href="http://www.infoq.com/cn/articles/weibo-watchman" target="_blank" rel="external">http://www.infoq.com/cn/articles/weibo-watchman</a></p>
<ul>
<li>Watchman系统架构</li>
</ul>
<p><img src="http://cdn1.infoqstatic.com/statics_s1_20170523-0350/resource/articles/weibo-watchman/zh/resources/6-1.png" alt="Watchman系统架构图"></p>
<p><img src="http://oi46mo3on.bkt.clouddn.com/19_apm/WatchMan.png" alt="系统组成"></p>
<ul>
<li>watchman-runtime组件利用字节码增强的方式在载入期织入增强逻辑（load-time weaving），为了跨进程/线程传递请求上下文，对于跨线程watchman-enhance组件通过javaagent的方式在应用启动并载入class时修改了JDK自身的几种线程池（ThreadPool或几类Executor）实现，在客户代码提交（execute或submit）时对传入的runnable/callable对象包上具有追踪能力的实现（proxy-pattern），并且从父线程上去继承或初始化请求上下文（request-context）；</li>
<li>而对于跨进程的RPC场景，则动态增强传输层的客户端和服务端的逻辑。微博平台使用的Motan RPC框架有着类似filter-chain的流程，watchman-aspect会插入自己的filter实现；实现的逻辑就是在RPC请求前获取请求方的请求上下文，序列化后装配近请求体中，服务方获取请求后，再从请求体中反序列化请求上下文，同时设置到线程上下文中（ThreadLocal）。</li>
<li>普通Java调用的处理方式（埋点/追踪）则是通过AspectJ的静态织入，相信广大读者对AspectJ都不陌生，它提供非常强大的AOP的能力，我们使用AspectJ来定义几类切面，分别针对WeiboAuth、HTTP接口、资源客户端的下行方法等。再利用AspectJ的语法定义各个切点，形如：</li>
</ul>
<pre><code>@AfterReturning(value=&quot;execution(public $type $signature($param))&quot;,returning=&quot;$return&quot;)
</code></pre><ul>
<li>watchman-core组件内置几类策略，分别用来控制收集数据的范围、收集数据的采样率、以及几种控制策略。</li>
<li>watchman-aspect组件通过异步日志（async-logger）会在各个节点上输出日志文件，如何将这些分散的日志源源不断的收集汇总并计算？通过watchman-prism组件（基于Scribe），将日志推送到watchman-stream组件（基于Storm），利用这两个业界成熟的系统以流式的方式处理数据，stream中bolt会根据需求进行聚合、统计等计算（针对性能数据），规范化、排序（针对调用链数据），之后写入HBase中。这个过程通过benchmark反映出的结果来看，完全能达到准实时的要求（30s左右）。</li>
<li>服务质量保障是Watchman系统的另一特点，在面向服务的架构（SOA）中，各个服务的提供方需要给出SLA（service level agreement）数据，量化服务的各种指标（如吞吐、承载）和服务质量（如99.99% &lt;50ms）。运行时watchman-stream将不断计算得出的性能数据与通过watchman-registry获得的各服务的SLA数据进行比对。结果会反映到Dashboard上，这里与运维的告警系统等集成，可以及时将状况推送出去。</li>
</ul>
<h1 id="京东CallGraph"><a href="#京东CallGraph" class="headerlink" title="京东CallGraph"></a>京东CallGraph</h1><ul>
<li>产生背景：SOA化和微服务；基于Google发表的分布式日志跟踪论文；相似的有淘宝鹰眼和新浪WatchMan；</li>
<li>核心概念：调用链包含了从源头请求到最后底层系统的所有环节，中间通过全局唯一的TraceID透传；</li>
<li>特性及使用场景：方法调用关系（单次调用的问题排查）、应用依赖关系（容量规划、调用来源、依赖度量、调用耗时、调用并行度、调用路由）、与业务数据集成（将公司业务与第三方业务进行关联）；</li>
<li>设计目标：低侵入性、低性能影响、灵活的应用策略、时效性；</li>
<li>实现架构：核心包（被各中间件引用，完成具体的埋点逻辑，日志存放在内存磁盘上由Agent收集发送到JMQ）、JMQ（充当日志数据管道）、Storm（对数据日志并行整理和计算）、存储（实时数据存储有JimDB/HBase/ES，离线数据存储包括HDFS和Spark）、CallGraph-UI（用户交互界面）、UCC（存放配置信息并同步到各服务器）、管理元数据（存放链路签名与应用映射关系等）；</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/9_reading_201701/jd_callgraph.png" alt="CallGraph实现架构"></p>
<ul>
<li>埋点和调用上下文透传：前端利用Web容器的Filter机制调用startTrace开启跟踪，调用endTrace结束跟踪；各中间件调用clientSend、serverRecv、serverSend和clientRecv等API；对于进程间的上下文透传，调用上下文放在ThreadLocal；对于异步调用，通过Java字节码增强方式织入，以透明的方式完成线程间上下文的透传。</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/9_reading_201701/jd_callgraph2.png" alt="CallGraph透传"></p>
<ul>
<li>日志格式设计：固定部分（TraceID、RpcID、开始时间、调用类型、对端IP、调用耗时、调用结果等）、可变部分；</li>
<li>高性能的链路日志输出：开辟专门的内存区域并虚拟成磁盘设备，产生的日志存放在这样的内存设备，完全不占用磁盘IO；专门的日志模块，输出采用批量、异步方式写入，并在日志量过大时采取丢弃日志；</li>
<li>TP日志和链路日志分离：链路日志通常开启采样率机制，比如1000次调用只收集1次；但是对于TP指标来说，必须每次记录，因此这两种数据是独立处理互不影响；</li>
<li>实时配置：通过CallGraph-UI和UCC实时配置，支持基于应用、应用分组、服务器IP多维度配置；</li>
<li>秒级监控：针对业务对实时分析的需求，采用JimDB存放实时数据，针对来源分析、入口分析、链路分析等可以提供1小时内的实时分析结果；</li>
<li>未来之路：延迟更低、完善错误发现和报警、借助深度学习挖掘价值。</li>
</ul>
<h1 id="美团MTrace"><a href="#美团MTrace" class="headerlink" title="美团MTrace"></a>美团MTrace</h1><p>SEE：<a href="https://www.slideshare.net/meituan/08-63406714" target="_blank" rel="external">https://www.slideshare.net/meituan/08-63406714</a><br>SEE：<a href="http://tech.meituan.com/mt-mtrace.html" target="_blank" rel="external">http://tech.meituan.com/mt-mtrace.html</a></p>
<ul>
<li>MTrace，美团点评内部的分布式会话跟踪系统，其核心理念就是调用链：通过一个全局的ID将分布在各个服务节点上的同一次请求串联起来，还原原有的调用关系、追踪系统问题、分析调用数据、统计系统指标。这套系统借鉴了2010年Google发表的一篇论文《dapper》，并参考了Twitter的Zipkin以及阿里的Eagle Eye的实现。</li>
<li>使用场景：网络优化（通过IP查询一次分布式请求是否有跨机房调用）；瓶颈查询（通过可视化了解整个系统调用的耗时，优化整个系统的效率）；优化链路（一个服务对相同接口多次请求进行优化，比如改成批量接口或者提高整个系统调用的并行度）；异常log绑定（可以将请求的参数、异常log等信息通过traceId进行绑定，很容易地就把这些信息聚合到了一起，方便业务端查询问题）；透明传输数据（put和putOnce）；</li>
<li>系统架构：提供统一的SDK，在各个中间件中埋点，生成traceID等核心数据，上报服务的调用数据信息。尽量在各个统一的中间件中进行显式埋点，虽然会导致代码间耦合度增加，但是方便后续定位问题。Agent仅仅会转发数据，由Agent判断将数据转发到哪里，这样就可以通过Agent做数据路由、流量控制等操作。数据埋点的四个阶段：Client Send、Server Recieve、Server Send和Client Recieve；上下文归档，会把上下文数据异步上传到后端，为了减轻对业务端的影响，上下文上报采用的是异步队列的方式，数据不会落地，直接通过网络形式传递到后端服务，在传递之前会对数据做一层压缩，主要是压缩比很可观，可以达到10倍以上，所以就算牺牲一点CPU资源也是值得的。在SDK与后端服务之间加了一层Kafka，这样做既可以实现两边工程的解耦，又可以实现数据的延迟消费。调用链路数据的实时查询主要是通过Hbase，使用traceID作为RowKey，能天然的把一整条调用链聚合在一起，提高查询效率。离线数据主要是使用Hive，可以通过SQL进行一些结构化数据的定制分析。前端展示，主要遇到的问题是NTP同步的问题，因为调用链的数据是从不同机器上收集上来的，那么聚合展示的时候就会有NTP时间戳不同步的问题。</li>
</ul>
<p><img src="http://tech.meituan.com/img/mt-mtrace/mtrace6.png" alt="MTrace系统架构"></p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><ol>
<li>分布式调用链跟踪的核心基本都是Google Dapper论文所述，使用全局TraceID表示一条调用链，连接各个服务调用（用SPAN表示），通过分析SPAN之间的父子关系形成跟踪树。另外通过中间件的埋点和业务自定义的Annotation，记录日志并采用收集器进行离线和在线分析，从而实现调用链跟踪、优化决策等信息。</li>
<li>实现的核心在于埋点、日志收集、调用链分析，难点在于调用链分析，如何实现可视化以及和告警系统等对接是判断是否好用的核心标准。</li>
<li>另外，为了最低限度降低对业务的影响，采样率、开关控制和日志优化手段也很有必要。</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;随着微服务架构的流行，服务之间的调用关系愈加复杂，如何跟踪它们之间的调用关系以及对依赖进行分析显得更加重要。而分布式调用跟踪系统就是为了解决这个问题而生的，本文为调研学习已有DST系统的学习笔记。&lt;br&gt;
    
    </summary>
    
      <category term="OpenSource" scheme="http://ginobefunny.com/categories/OpenSource/"/>
    
    
      <category term="分布式调用跟踪" scheme="http://ginobefunny.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E8%B0%83%E7%94%A8%E8%B7%9F%E8%B8%AA/"/>
    
      <category term="zipkin" scheme="http://ginobefunny.com/tags/zipkin/"/>
    
      <category term="EagleEye" scheme="http://ginobefunny.com/tags/EagleEye/"/>
    
      <category term="Watchman" scheme="http://ginobefunny.com/tags/Watchman/"/>
    
      <category term="CallGraph" scheme="http://ginobefunny.com/tags/CallGraph/"/>
    
      <category term="MTrace" scheme="http://ginobefunny.com/tags/MTrace/"/>
    
  </entry>
  
  <entry>
    <title>阅读随手记 201704</title>
    <link href="http://ginobefunny.com/post/reading_record_201704/"/>
    <id>http://ginobefunny.com/post/reading_record_201704/</id>
    <published>2017-04-08T05:25:52.000Z</published>
    <updated>2017-05-02T09:34:39.170Z</updated>
    
    <content type="html"><![CDATA[<p>关键字：微服务, 架构, Elasticsearch, 分布式队列, 搜索引擎, 推荐系统, 机器学习, 人工智能, Java 9, CQRS, Event Source, Kafka, 高可用。<br><a id="more"></a></p>
<h3 id="Elasticsearch前沿：ES-5-x改进详解与ES6展望-曾勇"><a href="#Elasticsearch前沿：ES-5-x改进详解与ES6展望-曾勇" class="headerlink" title="Elasticsearch前沿：ES 5.x改进详解与ES6展望  曾勇"></a><a href="https://mp.weixin.qq.com/s/yVbZfE7oWGmnfmcFTeSV4w" target="_blank" rel="external">Elasticsearch前沿：ES 5.x改进详解与ES6展望</a>  曾勇</h3><ul>
<li>如果你的场景是日志，那么基本上数据进去之后是不需要进行修改的，所以现在ES新增了一个append-only的索引模式，也就是当ES是自动生成ID的时候，ES可以跳过不必要的版本检测，<strong>大概可以提升20%左右的索引性能</strong>。</li>
<li>在数据结构方面，新增了多个range字段类型，现在你可以计算连续数据的交并集，可以是时间范围，也可以是数值范围。</li>
<li>下一个特性值得介绍的就是<strong>_all 字段的移除</strong>，去掉_all字段之后，磁盘占用少了不少，索引性能也有一些提升。</li>
<li>现在有一个新的高亮器：Unified Highlighter，大家可能会问，目前ES默认已经提供了3种不同的高亮器，为什么还会有一个新的轮子呢，因为之前的用法有点复杂, 用户选择起来比较困难，新的unified highlighter目的就是简化高亮的使用，可以支持前面3种高亮类型的自动选择。</li>
<li>还有一个是keyword类型可以通过normalizer来进行标准化了，keyword类型相比text类型就是不能分词，但是可能同样需要进行相应的标准化处理，比如统一转成小写，移除标点符号等等，使用方式和analyzer一样。</li>
<li>另一个就是Multi-Word Synonyms，之前是不支持同义词中间有空格分割的，分词的时候会帮你切分开，搜索的时候不能正确处理词组这种同义词。</li>
<li>同义词现在可以支持词组了，也就是说同义词如果是由多个词组成的，不会在分词的时候被傻傻的拆开，而是正确的处理。</li>
<li>还一个就是字段折叠（ Field collapsing），这个特性比较有意思，你可以在搜索的时候，按某个字段作为维度进行去重，我这里写过一篇详细的博客，有兴趣的可以去看看：<a href="http://elasticsearch.cn/article/132" target="_blank" rel="external">http://elasticsearch.cn/article/132</a></li>
<li>ES的搜索，对于一些耗时较长的查询，现在可以通过ES的任务管理机制来进行取消了，感兴趣的可以查看文档：<a href="https://www.elastic.co/guide/en/elasticsearch/reference/5.3/search.html#global-search-cancellation" target="_blank" rel="external">https://www.elastic.co/guide/en/elasticsearch/reference/5.3/search.html#global-search-cancellation</a></li>
<li>现在term aggs提供了一种分区的概念，你可以对一个字段，分n次进行聚合，分而治之，有兴趣的可以看看文档：<a href="https://www.elastic.co/guide/en/elasticsearch/reference/5.3/search-aggregations-bucket-terms-aggregation.html#_filtering_values_with_partitions" target="_blank" rel="external">https://www.elastic.co/guide/en/elasticsearch/reference/5.3/search-aggregations-bucket-terms-aggregation.html#_filtering_values_with_partitions</a></li>
<li>当你的集群变红的时候，新增的 /_cluster/allocation/explain 接口能够直接告诉你哪里出了问题；</li>
<li>Java REST Client也有了更新，ES之前提供了一个偏底层的Java HTTP REST client，但是用起来太费劲，需要手动拼 JSON，现在Java REST Client分成了Java High Level REST Client和Java Low Level REST Client， High Level基于Low Level来实现，顾名思义，提供更多用户方便的接口调用，High Level REST Client将提供和Transport Client类似的接口，不用手动去拼接QueryDSL的JSON了，目测在 5.5 版本提供。</li>
<li>以前的搜索的reduce操作都是要等到把每个分片的结果都拿到本地之后再做合并，现在新增的batched search reduce phases提供了分批进行reduce的行为，也就是不用全部拿到之后再做reduce，而是拿到足够的分配（默认512）之后就开始做reduce，然后拿到合并结果，释放相关的资源。</li>
<li>ES 6.0展望：稀疏性Doc Values的支持；Index sorting，即在索引阶段的排序，即我们查询的时候有时候会根据某个字段的值进行排序，比如时间、编号等等；顺序号的支持，每个es的操作都有一个顺序编号，这个属于es内部的一个功能，可以提供快速的分片副本恢复或同步、跨数据中心的节点恢复、甚至提供一个Changes API 等；无缝滚动升级，使之能够从5的最后一个版本滚动升级到6的最后一个版本，不需要集群的完整重启。</li>
</ul>
<h3 id="Elasticsearch-5-x-字段折叠的使用-medcl"><a href="#Elasticsearch-5-x-字段折叠的使用-medcl" class="headerlink" title="Elasticsearch 5.x 字段折叠的使用 medcl"></a><a href="http://elasticsearch.cn/article/132" target="_blank" rel="external">Elasticsearch 5.x 字段折叠的使用</a> medcl</h3><ul>
<li>什么是字段折叠，可以理解就是按特定字段进行合并去重，比如我们有一个菜谱搜索，我希望按菜谱的“菜系”字段进行折叠，即返回结果每个菜系都返回一个结果，也就是按菜系去重，我搜索关键字“鱼”，要去返回的结果里面各种菜系都有，有湘菜，有粤菜，有中餐，有西餐，别全是湘菜，就是这个意思，通过按特定字段折叠之后，来丰富搜索结果的多样性。</li>
<li>有人肯定会想到，使用term agg+ top hits agg来实现啊，这种组合两种聚和的方式可以实现上面的功能，不过也有一些局限性，比如，不能分页、结果不够精确以及数据量大的情况下聚合比较慢。</li>
<li>而新的的字段折叠的方式是怎么实现的的呢，有这些要点：折叠+取inner_hits分两阶段执行，所以top hits永远是精确的；字段折叠只在top hits层执行，不需要每次都在完整的结果集上对为每个折叠主键计算实际的doc values值，和term agg 相比要节省很多内存；因为只在top hits上进行折叠，所以相比组合聚合的方式，速度要快很多；折叠top docs不需要使用全局序列来转换string，相比agg这也节省了很多内存；分页成为可能，和常规搜索一样，具有相同的局限，先获取from+size的内容，再合并；search_after和scroll暂未实现，不过具备可行性；折叠只影响搜索结果，不影响聚合，搜索结果的total是所有的命中纪录数，去重的结果数未知（无法计算）。</li>
<li>来个例子：</li>
</ul>
<pre><code>PUT recipes
POST recipes/type/_mapping
{
  &quot;properties&quot;: {
    &quot;name&quot;:{
      &quot;type&quot;: &quot;text&quot;
    },
    &quot;rating&quot;:{
      &quot;type&quot;: &quot;float&quot;
    },&quot;type&quot;:{
      &quot;type&quot;: &quot;keyword&quot;
    }
  }
}


POST recipes/type/
{
  &quot;name&quot;:&quot;清蒸鱼头&quot;,&quot;rating&quot;:1,&quot;type&quot;:&quot;湘菜&quot;
}
POST recipes/type/
{
  &quot;name&quot;:&quot;剁椒鱼头&quot;,&quot;rating&quot;:2,&quot;type&quot;:&quot;湘菜&quot;
}
POST recipes/type/
{
  &quot;name&quot;:&quot;红烧鲫鱼&quot;,&quot;rating&quot;:3,&quot;type&quot;:&quot;湘菜&quot;
}
POST recipes/type/
{
  &quot;name&quot;:&quot;鲫鱼汤（辣）&quot;,&quot;rating&quot;:3,&quot;type&quot;:&quot;湘菜&quot;
}
POST recipes/type/
{
  &quot;name&quot;:&quot;鲫鱼汤（微辣）&quot;,&quot;rating&quot;:4,&quot;type&quot;:&quot;湘菜&quot;
}
POST recipes/type/
{
  &quot;name&quot;:&quot;鲫鱼汤（变态辣）&quot;,&quot;rating&quot;:5,&quot;type&quot;:&quot;湘菜&quot;
}
POST recipes/type/
{
  &quot;name&quot;:&quot;广式鲫鱼汤&quot;,&quot;rating&quot;:5,&quot;type&quot;:&quot;粤菜&quot;
}
POST recipes/type/
{
  &quot;name&quot;:&quot;鱼香肉丝&quot;,&quot;rating&quot;:2,&quot;type&quot;:&quot;川菜&quot;
}
POST recipes/type/
{
  &quot;name&quot;:&quot;奶油鲍鱼汤&quot;,&quot;rating&quot;:2,&quot;type&quot;:&quot;西菜&quot;
} 

GET recipes/type/_search
{
  &quot;query&quot;: {
    &quot;match&quot;: {
      &quot;name&quot;: &quot;鱼&quot;
    }
  },
  &quot;collapse&quot;: {
    &quot;field&quot;: &quot;type&quot;,
    &quot;inner_hits&quot;: {
      &quot;name&quot;: &quot;top_rated&quot;,
      &quot;size&quot;: 2,
      &quot;sort&quot;: [
        {
          &quot;rating&quot;: &quot;desc&quot;
        }
      ]
    }
  },
  &quot;sort&quot;: [
    {
      &quot;rating&quot;: {
        &quot;order&quot;: &quot;desc&quot;
      }
    }
  ],
  &quot;size&quot;: 2,
  &quot;from&quot;: 0
}
</code></pre><h3 id="如何做出搜索和推荐深度融合的兴趣引擎架构-田明军"><a href="#如何做出搜索和推荐深度融合的兴趣引擎架构-田明军" class="headerlink" title="如何做出搜索和推荐深度融合的兴趣引擎架构 田明军"></a><a href="https://mp.weixin.qq.com/s/7DOj5Hed3Q4Crr8ng4TeBw" target="_blank" rel="external">如何做出搜索和推荐深度融合的兴趣引擎架构</a> 田明军</h3><p>搜索/推荐工作流程</p>
<p><img src="http://oi46mo3on.bkt.clouddn.com/18_reading_201704/search_recom_flow.png" alt="搜索/推荐工作流程"></p>
<p>一点资讯搜索/推荐架构</p>
<p><img src="http://oi46mo3on.bkt.clouddn.com/18_reading_201704/search_recom_structure.png" alt="一点资讯搜索/推荐架构"></p>
<h3 id="携程实时用户行为系统实践-陈清渠"><a href="#携程实时用户行为系统实践-陈清渠" class="headerlink" title="携程实时用户行为系统实践  陈清渠"></a><a href="https://mp.weixin.qq.com/s/OJdlpP62YWGmVnBWsfpVZw" target="_blank" rel="external">携程实时用户行为系统实践</a>  陈清渠</h3><p>携程实时用户行为系统架构</p>
<p><img src="http://oi46mo3on.bkt.clouddn.com/18_reading_201704/Logic_View.png" alt="携程实时用户行为系统架构"></p>
<ul>
<li>数据有两种流向，分别是处理流和输出流。</li>
<li>在处理流，行为日志会从客户端（App/Online/H5）上传到服务端的Collector Service。Collector Service将消息发送到分布式队列。数据处理模块由流计算框架完成，从分布式队列读出数据，处理之后把数据写入数据层，由分布式缓存和数据库集群组成。</li>
<li>输出流相对简单，web service的后台会从数据层拉取数据，并输出给调用方，有的是内部服务调用，比如推荐系统，也有的是输出到前台，比如浏览历史。</li>
<li>系统实现采用的是Java+Kafka+Storm+Redis+Mysql+Tomcat+Spring的技术栈。</li>
<li>实时性：首先是用storm解决突发流量洪峰的问题，通过storm处理框架，消息能在进入kafka之后毫秒级别被处理；此外storm具有强大的scale out能力，只要通过后台修改worker数量参数，并重启topology，可以马上扩展计算能力，方便应对突发的流量洪峰；实时用户行为系统采用的at least once的策略，这种策略下消息可能会重发，所以程序处理实现了幂等支持；</li>
<li>实时行为双队列设计：在部分情况下数据处理需要重试，但是无法连接一般需要更长时间等待网络或数据库的恢复，这种情况下处理程序不能一直等待，否则会造成数据延迟。实时用户行为系统采用了双队列的设计来解决这个问题。</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/18_reading_201704/double_queue.png" alt="实时行为双队列设计"></p>
<ul>
<li>可用性：首先是系统层面上做了全栈集群化，另外系统在部分模块不可用时通过降级处理保障整个系统的可用性。</li>
</ul>
<h3 id="Spark技术在京东智能供应链预测的应用-杨冬越-郭景瞻"><a href="#Spark技术在京东智能供应链预测的应用-杨冬越-郭景瞻" class="headerlink" title="Spark技术在京东智能供应链预测的应用 杨冬越 郭景瞻"></a><a href="https://mp.weixin.qq.com/s/FK4rjuWyI6S6IZ4pAdMc4g" target="_blank" rel="external">Spark技术在京东智能供应链预测的应用</a> 杨冬越 郭景瞻</h3><ul>
<li>背景：京东的仓库按功能划分为RDC（区域分发中心，中心城市）、FDC（区域运转中心，中小城市及边远地区）、大件中心仓、大件卫星仓、图书仓和城市仓等。京东首先从供货商采购商品到RDC，再根据实际需求调配到FDC，再运往离客户最近的配送站，由快递员送到客户手中。借助机器学习、大数据等技术，技术在很多供应链优化问题上都已经实现系统化，实现全流程自动化，在这其中预测技术起着至关重要的底层支撑作用。</li>
<li>预测系统：主要支持三大业务：销量预测、单量预测和GMV预测。其中销量预测主要支持商品补货、商品调拨；单量预测主要支持仓库、站点的运营管理；GMV预测主要支持销售部门计划的定制。</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/18_reading_201704/forecasting_system.png" alt="预测系统"></p>
<ul>
<li>预测系统架构：先从外部数据源获取我们所需的业务数据，然后对基础数据进行加工清洗，再通过时间序列、机器学习等人工智能技术对数据进行处理分析，最后计算出预测结果并通过多种途径推送给下游系统使用。</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/18_reading_201704/forecasting_system_structure.png" alt="预测系统架构"></p>
<ul>
<li>预测系统核心：基础层（HDFS用来做数据存储，Yarn用来做资源调度，BDP用来做任务调度）、框架层（以Spark RDD、Spark SQL、Hive为主）、工具层（比较常用的包有xgboost、numpy、pandas、sklearn、scipy和hyperopt等）、算法层（时间序列、机器学习和结合业务开发的一些独有的算法）；</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/18_reading_201704/forecasting_system_core.png" alt="预测系统核心"></p>
<ul>
<li>以机器学习算法为主的流程如下：</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/18_reading_201704/forecasting_ml_flow.png" alt="以机器学习算法为主的流程"></p>
<ol>
<li>特征构建：通过数据分析、模型试验确定主要特征，通过一系列任务生成标准格式的特征数据。</li>
<li>模型选择：不同的商品有不同的特性，所以首先会根据商品的销量高低、新品旧品、假节日敏感性等因素分配不同的算法模型。</li>
<li>特征选择：对一批特征进行筛选过滤不需要的特征，不同类型的商品特征不同。</li>
<li>样本分区：对训练数据进行分组，分成多组样本，真正训练时针对每组样本生成一个模型文件。一般是同类型商品被分成一组，比如按品类维度分组，这样做是考虑并行化以及模型的准确性。</li>
<li>模型参数：选择最优的模型参数，合适的参数将提高模型的准确度，因为需要对不同的参数组合分别进行模型训练和预测，所以这一步是非常耗费资源。</li>
<li>模型训练：待特征、模型、样本都确定好后就可以进行模型训练，训练往往会耗费很长时间，训练后会生成模型文件，存储在HDFS中。</li>
<li>模型预测：读取模型文件进行预测执行。</li>
<li>多模型择优：为了提高预测准确度，我们可能会使用多个算法模型，当每个模型的预测结果输出后系统会通过一些规则来选择一个最优的预测结果。</li>
<li>预测值异常拦截：我们发现越是复杂且不易解释的算法越容易出现极个别预测值异常偏高的情况，这种预测偏高无法结合历史数据进行解释，因此我们会通过一些规则将这些异常值拦截下来，并且用一个更加保守的数值代替。</li>
<li>模型评价：计算预测准确度，我们通常用使用mapd来作为评价指标。</li>
<li>误差分析：通过分析预测准确度得出一个误差在不同维度上的分布，以便给算法优化提供参考依据。</li>
</ol>
<h3 id="对CQRS的基础理解-张逸"><a href="#对CQRS的基础理解-张逸" class="headerlink" title="对CQRS的基础理解 张逸"></a><a href="https://mp.weixin.qq.com/s/S2UGCS00gWzr-nuAapR96g" target="_blank" rel="external">对CQRS的基础理解</a> 张逸</h3><ul>
<li>CQRS即Command Query Responsibility Seperation（命令查询职责分离），其设计思想来源于Mayer提出的CQS（Command Query Seperation）。这种命令与查询的分离方式，可以更好地控制请求者的操作。查询操作不会造成数据的修改，因而它属于一种幂等操作，可以反复地发起，而不用担心会对系统造成影响。基于这种特性，我们还可以为其提供缓存，从而改进查询的性能。命令操作则与之相反，它会直接影响系统信息的改变。</li>
<li>查询操作与命令操作对事务的要求也不一样。由于查询操作不会改变系统状态，因而，不会产生最终的数据不一致。从请求响应的角度来看，查询操作常常需要同步请求，实时返回结果；命令操作则不然，因为我们并不期待命令操作必须返回结果，这就可以采用fire-and-forget方式，而这种方式正是运用异步操作的前提。此外，对于大多数软件系统而言，查询操作发起的频率通常要远远高于命令操作。如上种种，都是将命令与查询进行分离的根本原因。</li>
<li>下图是CQRS框架AxonFramework官方文档给出的CQRS架构图。在这个架构图中，最核心的概念是Command、Event。以我的理解，CQRS模式的风格源头就是基于事件的异步状态机模型。抛开命令查询分离这一核心原则，这才是CQRS的基础内容。</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/18_reading_201704/cqrs-arch.png" alt="CQRS架构图"></p>
<ul>
<li>CQRS对设计者的影响，是将领域逻辑，尤其是业务流程，皆看做是一种领域对象状态迁移的过程。这一点与REST将HTTP应用协议看做是应用状态迁移的引擎，有着异曲同工之妙。这种观点（或设计视图）引出了Command与Event的概念。Command是系统中会引起状态变化的活动，通常是一种命令语气，例如注册会议RegisterToConference。至于Event，则描述了某种事件的发生，通常是命令的结果（但并不一定是直接结果，但源头一定是因为发送了命令），例如OrderConfirmed。</li>
</ul>
<h3 id="我对CQRS-EventSourcing架构的思考-汤雪华"><a href="#我对CQRS-EventSourcing架构的思考-汤雪华" class="headerlink" title="我对CQRS/EventSourcing架构的思考 汤雪华"></a><a href="https://mp.weixin.qq.com/s/uZPzDopFApOHTXzwEbfluQ" target="_blank" rel="external">我对CQRS/EventSourcing架构的思考</a> 汤雪华</h3><ul>
<li><strong>聚合</strong>，它通过定义对象之间清晰的所属关系和边界来实现领域模型的内聚，并避免了错综复杂的难以维护的对象关系网的形成。聚合定义了一组具有内聚关系的相关对象的集合，我们把聚合看作是一个修改数据的最小原子单元。</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/18_reading_201704/DDD_Aggregation_Example.png" alt="聚示意图"></p>
<ul>
<li><strong>聚合根</strong>：每个聚合都有一个根对象，根对象管理聚合内的其他子对象（实体、值对象），聚合之间的交互都是通过聚合根来交互，不能绕过聚合根去直接和聚合下的子实体进行交互。</li>
<li><strong>Eventual Consistency</strong>：聚合内的数据修改，是ACID强一致性的；跨聚合的数据修改，是最终一致性的。遵守这个原则，可以让我们最大化的降低并发冲突，从而最大化的提高整个系统的吞吐。</li>
<li><strong>In Memory</strong>：指整个系统中的所有的聚合根对象都活在内存。在In-Memory的架构下，当要修改某个聚合根的状态时，它已经在内存，我们可以直接拿到该对象的引用，且框架会尽量保证聚合根对象的状态就是最新的。聚合根是在内存中的最小计算单元，每个聚合内部都封装了业务规则，并保证数据的强一致性。</li>
<li><strong>Event Sourcing</strong>：不保存对象的最新状态，而是保存对象产生的所有事件；通过事件溯源得到对象最新状态。</li>
<li><strong>Event Sourcing VS CRUD</strong>：对于CRUD，DB的记录可变，可以增删改；对于ES，没有更新、删除，只有Append Event，不可变。</li>
<li><strong>Actor Model</strong>：其核心思想是对象直接不会直接调用来通信，而是通过发消息来通信。每个Actor都有一个Mailbox，它收到的所有的消息都会先放入Mailbox中，然后Actor内部单线程处理Mailbox中的消息。从而保证对同一个Actor的任何消息的处理，都是线性的，无并发冲突。从全局上来看，就是整个系统中，有很多的Actor，每个Actor都在处理自己Mailbox中的消息，Actor之间通过发消息来通信。Akka框架就是实现Actor模型的并行开发框架，并且Akka框架融入了聚合、In-Memory、Event Sourcing这些概念。Actor非常适合作为DDD聚合根。Actor的状态修改是由事件驱动的，事件被持久化起来，然后通过Event Sourcing的技术，还原特定Actor的最新状态到内存。</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/18_reading_201704/Actor_Model.jpg" alt="Actor Model"></p>
<ul>
<li>下图是<strong>CQRS架构的典型架构图</strong>。CQRS架构的核心出发点是将整个系统的架构分割为读和写两部分，从而方便我们对读写两端进行分开优化；CQRS架构的一致性模型为最终一致性。采用CQRS架构的一个前提是，你的系统要接受系统使用者查询到的数据可能不是最新的，而是有几个毫秒的延迟。</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/18_reading_201704/CQRS_ES_Arch.jpg" alt="CQRS架构"></p>
<ul>
<li><strong>CQRS架构的适用场景</strong>：当我们的应用的写模型和读模型差别比较大时；当我们希望实践DDD时，因为CQRS架构可以让我们实现领域模型不受任何ORM框架带来的对象和数据库的阻抗失衡的影响；当我们希望对系统的查询性能和写入性能分开进行优化时，尤其是读/写比非常高的系统，CQ分离是必须的；当我们希望我们的系统同时满足高并发的写、高并发的读的时候；因为CQRS架构可以做到C端最大化的写，Q端非常方便的提供可扩展的读模型。</li>
<li><strong>C端的命令执行流程</strong>：发送命令到分布式MQ；然后命令的订阅者处理命令；订阅者内部根据不同的命令调用不同的Command Handler进行处理；Command Handler内部根据命令所指定的聚合根ID从In-Memory内存中直接获取聚合根对象的引用，然后操作聚合根对象；聚合根对象状态发生变化并产生事件；框架负责自动持久化事件到Event Storage；框架负责将事件发布到Event MQ；Event订阅者订阅事件，然后调用对应的Event Handler进行处理，如更新Data Storage；</li>
<li><strong>Q端的查询执行流程</strong>：调用轻薄的Query Service，传如Query DTO；Query Service从读库进行查询并返回结果；</li>
<li>挖掘出<strong>更多有用的特性</strong>：一个命令只允许修改一个聚合根；命令或事件在分布式MQ的路由根据聚合根ID来路由，也就是同一个聚合根的命令和事件都在一个队列里；引入Command Mailbox，Event Mailbox这两个概念，将聚合根需要处理的命令和产生的事件都队列化去并发，做到架构上最大的并行，将并发降低到最低；引入Group Commit技术，做到整个C端的架构层面支持批量提交聚合根产生的事件，从而极大的提高C端的整体吞吐量；通过引入Saga的概念，做到基于事件驱动的最终一致性；</li>
<li><strong>Event Sourcing的优点</strong>：记录了数据完整变化过程，最详细的日志；可以将系统还原到任何一个时间点的状态；Domain Event非常有业务价值，BI分析事件能预测业务未来发展情况；可以有效解决线上的数据问题，线下重演一遍，就能知道哪里出问题；不再需要用到ORM，所以没有O/R阻抗失衡的问题，领域模型的设计可以更OO；将Command、Event串联起来，可以分析聚合根的整个变化过程，有助于排查分析问题；自动并发冲突检测、命令的幂等处理；</li>
<li><strong>Event Sourcing的缺点</strong>：事件数量巨大，如何存储；如果单个聚合根事件过多，则重演会造成性能问题；领域模型重构被制约，事件的修改必须兼容以前的结构；数据库订正不在有效；架构实践门槛高，没有成熟框架支撑基本无望；需要具备DDD领域建模的能力；事件驱动状态的修改，思维转变难。</li>
</ul>
<h3 id="北大AI公开课第6讲-王俊：DNA是生命数字化的过程，AI改变生命科学-新智元"><a href="#北大AI公开课第6讲-王俊：DNA是生命数字化的过程，AI改变生命科学-新智元" class="headerlink" title="北大AI公开课第6讲 王俊：DNA是生命数字化的过程，AI改变生命科学 新智元"></a><a href="https://mp.weixin.qq.com/s/h7IhHK1vfrHQYDpFLVZ7yA" target="_blank" rel="external">北大AI公开课第6讲 王俊：DNA是生命数字化的过程，AI改变生命科学</a> 新智元</h3><p>视频回放链接：<a href="http://www.iqiyi.com/l_19rrfgal1z.html" target="_blank" rel="external">http://www.iqiyi.com/l_19rrfgal1z.html</a></p>
<h3 id="北大AI公开课第7讲-徐伟：AGI-2050年前实现可能性超50-机器学习研究会"><a href="#北大AI公开课第7讲-徐伟：AGI-2050年前实现可能性超50-机器学习研究会" class="headerlink" title="北大AI公开课第7讲 徐伟：AGI 2050年前实现可能性超50% 机器学习研究会"></a><a href="https://mp.weixin.qq.com/s/mBfXDpqabIFdksCYTixcaw" target="_blank" rel="external">北大AI公开课第7讲 徐伟：AGI 2050年前实现可能性超50%</a> 机器学习研究会</h3><p>视频回放链接：<a href="http://www.iqiyi.com/l_19rrbkb3az.html" target="_blank" rel="external">http://www.iqiyi.com/l_19rrbkb3az.html</a></p>
<h3 id="北大AI公开课第8讲-李航：自然语言处理——理想与现实、机遇与挑战-机器学习研究会"><a href="#北大AI公开课第8讲-李航：自然语言处理——理想与现实、机遇与挑战-机器学习研究会" class="headerlink" title="北大AI公开课第8讲 李航：自然语言处理——理想与现实、机遇与挑战  机器学习研究会"></a><a href="https://mp.weixin.qq.com/s/Kxu_4QUdWQrjue9W1TvfDg" target="_blank" rel="external">北大AI公开课第8讲 李航：自然语言处理——理想与现实、机遇与挑战</a>  机器学习研究会</h3><p>课程回放链接：<a href="http://www.iqiyi.com/w_19rtxrunb9.html" target="_blank" rel="external">http://www.iqiyi.com/w_19rtxrunb9.html</a></p>
<h3 id="北大AI公开课第9讲-叶杰平：深度学习在交通领域应用潜力巨大-新智元"><a href="#北大AI公开课第9讲-叶杰平：深度学习在交通领域应用潜力巨大-新智元" class="headerlink" title="北大AI公开课第9讲 叶杰平：深度学习在交通领域应用潜力巨大  新智元"></a><a href="https://mp.weixin.qq.com/s/RvX1LHAZ9jokCtQ8jt6XHQ" target="_blank" rel="external">北大AI公开课第9讲 叶杰平：深度学习在交通领域应用潜力巨大</a>  新智元</h3><p>课程回放链接：<a href="http://www.iqiyi.com/l_19rrc5qodr.html" target="_blank" rel="external">http://www.iqiyi.com/l_19rrc5qodr.html</a></p>
<h3 id="任何人都能看懂的TensorFlow介绍-Soon-Hin-Khor-机器之心"><a href="#任何人都能看懂的TensorFlow介绍-Soon-Hin-Khor-机器之心" class="headerlink" title="任何人都能看懂的TensorFlow介绍 Soon Hin Khor/机器之心"></a><a href="https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650718466&amp;idx=1&amp;sn=016f111001e8354d49dd4ce279d283cd" target="_blank" rel="external">任何人都能看懂的TensorFlow介绍</a> Soon Hin Khor/机器之心</h3><h3 id="小白也能看懂的TensorFlow介绍-Soon-Hin-Khor-机器之心"><a href="#小白也能看懂的TensorFlow介绍-Soon-Hin-Khor-机器之心" class="headerlink" title="小白也能看懂的TensorFlow介绍 Soon Hin Khor/机器之心"></a><a href="https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650723520&amp;idx=1&amp;sn=d204284574e9e56682b6ed6f9dcaff01" target="_blank" rel="external">小白也能看懂的TensorFlow介绍</a> Soon Hin Khor/机器之心</h3><h3 id="机器学习演化史，方法、应用场景与发展趋势-新智元"><a href="#机器学习演化史，方法、应用场景与发展趋势-新智元" class="headerlink" title="机器学习演化史，方法、应用场景与发展趋势  新智元"></a><a href="https://mp.weixin.qq.com/s/B4RV1iO9fciguqP9J1hCxQ" target="_blank" rel="external">机器学习演化史，方法、应用场景与发展趋势</a>  新智元</h3><h4 id="机器学习演化史：各学派发展融合，最终让自动机器成为可能"><a href="#机器学习演化史：各学派发展融合，最终让自动机器成为可能" class="headerlink" title="机器学习演化史：各学派发展融合，最终让自动机器成为可能"></a>机器学习演化史：各学派发展融合，最终让自动机器成为可能</h4><p><img src="http://oi46mo3on.bkt.clouddn.com/18_reading_201704/ML_evolution.png" alt="机器学习演化史"></p>
<ul>
<li>符号学派（Symbolists）是使用基于规则的符号系统做推理的人。这种方法主要的缺陷在于其脆弱性，因为在边缘情况下，一个僵化的知识库似乎总是不适用，但在现实中存在这种模糊性和不确定性是不可避免的。爱用方法：<strong>规则和决策树</strong>；</li>
<li>贝叶斯学派（Bayesians）是使用概率规则及其依赖关系进行推理的一派。这种方法与符号学方法的相似之处在于可以以某种方式得到对结果的解释，另一个优点是存在可以在结果中表示的不确定性的量度。爱用方法：<strong>朴素贝叶斯或马尔科夫</strong>；</li>
<li>连接学派（Connectionists）的研究者相信智能起源于高度互联的简单机制。这种方法的第一个具体形式是出现于1959年的感知器，最新的形式是深度学习。爱用方法：<strong>神经网络</strong>；</li>
<li>进化学派（Evolutionists）是应用进化的过程，例如交叉和突变以达到一种初期的智能行为的一派。在深度学习中，GA确实有被用来替代梯度下降法，所以它不是一种孤立的方法。爱用方法：<strong>遗传算法</strong>；</li>
<li>类推学派（The analogizers）更多地关注心理学和数学最优化，通过外推来进行相似性判断。类推学派遵循“最近邻”原理进行研究，各种电子商务网站上的产品推荐是类推方法最常见的示例。爱用方法：<strong>支持向量机</strong>；</li>
<li>21世纪的头十年，最显著的就是连接学派和符号学派的结合，由此产生了记忆神经网络以及能够根据知识进行简单推理的智能体，基础架构也向大规模云计算转换。第二个十年，连接学派、符号学派和贝叶斯学派也将融合到一起，而主要的局面将是感知任务由神经网络完成，但涉及到推理和行动还是需要人为编写规则；从2040年以后，根据普华永道的预测，主流学派将成为 Algorithmic convergence，也即各种算法融合在一起，届时机器自主学习，也即元学习（Meta-learning）实现，计算服务将无处不在。</li>
</ul>
<h4 id="机器学习：工作原理及适用场景"><a href="#机器学习：工作原理及适用场景" class="headerlink" title="机器学习：工作原理及适用场景"></a>机器学习：工作原理及适用场景</h4><ul>
<li>机器学习能够通过“学习”大量的数据，在不需要人为编程的情况下，生成以及识别特定的对象，比如人脸。目前，机器学习也是商业应用中最常用的算法。</li>
<li>机器学习是一类关注从数据中找到模式，并根据这些模式进行预测的研究和算法。机器学习属于人工智能，它与数据挖掘、统计学、模式识别等相关领域的关系如下图所示：</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/18_reading_201704/ML_related_AI.png" alt="机器学习和人工智能的关系"></p>
<ul>
<li>机器学习的主要流程步骤：选择数据（训练用数据、验证用数据、测试用数据）、数据建模（使用训练数据构建涉及相关特征的模型）、验证模型（用验证数据验证建立的模型）、调试模型（为了提升模型的性能，使用更多的数据、不同的特征，调整参数，这也是最耗时耗力的一步）、使用模型（部署模型训练好的模型，对新的数据进行预测）、测试模型（使用测试用数据验证模型，并评估模型的性能）；</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/18_reading_201704/how_ML_works.png" alt="机器学习工作原理"></p>
<h4 id="实际应用机器学习：什么才是特定任务的正确算法？"><a href="#实际应用机器学习：什么才是特定任务的正确算法？" class="headerlink" title="实际应用机器学习：什么才是特定任务的正确算法？"></a>实际应用机器学习：什么才是特定任务的正确算法？</h4><p>机器学习中常用的算法有很多，具体需要用哪种，很大程度上取决于你手头的数据及其特征，你的训练目标，尤其是具体的使用场景。除非特殊情况，不必使用最复杂的算法。下面是常见的机器学习算法。</p>
<ul>
<li>决策树（Decision Trees）：是一个决策支持工具，它使用树形图或决策模型以及序列可能性。包括各种偶然事件的后果、资源成本、功效。从商务决策的角度来看，大部分情况下，决策树是一个人为了评估做出正确决定的概率需要问的是/否问题的最小数值。它能让你以一个结构化和系统化的方式来处理这个问题，然后得出一个合乎逻辑的结论。</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/18_reading_201704/ML_decision_trees.png" alt="决策树"></p>
<ul>
<li>支持向量机：是二元分类算法。给定一组两种类型的N维的地方点，SVM产生一个（N - 1）维超平面到这些点分成2组。假设你有两种类型的点，且它们是线性可分的。SVM将找到一条直线将这些点分成2种类型，并且这条直线会尽可能地远离所有的点。在规模上，目前使用SVM（在适当修改的情况下）解决的最大的问题包括显示广告、人类剪接位点识别、基于图像的性别检测和大规模的图像分类等等。</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/18_reading_201704/ML_SVM.png" alt="支持向量机"></p>
<ul>
<li>逻辑回归：是一种强大的统计方法，它能建模出一个二项结果与一个（或多个）解释变量。它通过估算使用逻辑运算的概率，测量分类依赖变量和一个（或多个）独立的变量之间的关系，是累积的逻辑分布情况。</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/18_reading_201704/ML_regression.png" alt="逻辑回归"></p>
<ul>
<li>朴素贝叶斯分类：是一种十分简单的分类算法，方程P(A|B)是后验概率，P(B|A)是可能性，P(A)是类先验概率，而P(B)是预测先验概率。朴素贝叶斯的思想基础是这样的：对于给出的待分类项，求解在此项出现的条件下各个类别出现的概率，哪个最大，就认为此待分类项属于哪个类别。</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/18_reading_201704/ML_native_bayes_classification.png" alt="朴素贝叶斯分类"></p>
<ul>
<li>隐马尔科夫模型通过分析可观察的数据来计算隐藏状态的概率，然后通过分析隐藏状态来估计未来可能观察到的模式。一个例子是，通过分析高气压（或低气压）的概率来预测天气是晴天、雨天或多云的可能性。</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/18_reading_201704/ML_hidden_markov_models.png" alt="隐马尔科夫模型"></p>
<ul>
<li>随机森林算法结合了多个树，使用随机挑选的数据子集，以此提升决策树的分析准确率。下图中的例子展示的是与乳腺癌复发相关的不同基因及其几率。随机深林算法的优势在于能够处理大规模数据集，以及大量看似不相关的数据，可以用于风险评估和客户信息分析。</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/18_reading_201704/ML_random_forest.png" alt="随机森林"></p>
<ul>
<li>递归神经网络：可以描述动态时间行为，因为和前馈神经网络接受较特定结构的输入不同，RNN将状态在自身网络中循环传递，因此可以接受更广泛的时间序列结构输入。手写识别是最早成功利用RNN的研究结果，其他应用还包括图像分类、图说生成和情感分析。</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/18_reading_201704/ML_RNN.png" alt="递归神经网络"></p>
<ul>
<li>长短期记忆和门控循环单元（gated recurrent unit, GRU）神经网络同时具有长期记忆和短期记忆。换句话说，这些较新的 RNN 具有更好的记忆控制，允许先前的值持续保存，或必要时为许多序列步骤重置，避免在步骤到步骤的传递时造成“梯度衰减”（gradient decay）。LSTM 和 GRU 网络通过记忆体组（memory blocks）和被称为“门”（gates）的结构适当地 pass 或 reset 值来实现这种记忆控制。</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/18_reading_201704/ML_LSTM.png" alt="长短期记忆"></p>
<ul>
<li>卷积神经网络：是一种前馈神经网络，它的人工神经元可以响应一部分覆盖范围内的周围单元，对于大型图像处理、药物发现等有出色表现。卷积神经网络由一个或多个卷积层和顶端的全连通层（对应经典的神经网络）组成，同时也包括关联权重和池化层。这一结构使得卷积神经网络能够利用输入数据的二维结构。与其他深度学习结构相比，卷积神经网络在图像和语音识别方面能够给出更优的结果。这一模型也可以使用反向传播算法进行训练。相比较其他深度、前馈神经网络，卷积神经网络需要估计的参数更少。</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/18_reading_201704/ML_CNN.png" alt="卷积神经网络"></p>
<h3 id="关于微服务的发展方向，我们和5位专家聊了聊-Mark-Little-薛命灯"><a href="#关于微服务的发展方向，我们和5位专家聊了聊-Mark-Little-薛命灯" class="headerlink" title="关于微服务的发展方向，我们和5位专家聊了聊 Mark Little/薛命灯"></a><a href="https://mp.weixin.qq.com/s/F4SLNmCs7iIkZe9nxnXDdA" target="_blank" rel="external">关于微服务的发展方向，我们和5位专家聊了聊</a> Mark Little/薛命灯</h3><ul>
<li>使用“微服务架构”这个名字会更恰当些。它是一种架构风格，它把一系列协作的服务组织成一个系统来支撑业务。</li>
<li>组织的沟通结构与服务架构有着更深层次的关系，这个联系比我们先前意识到的要更加紧密。</li>
<li>要做的：监控，监控，监控；做好服务的独立部署；确保你的团队具有DevOps文化；对微服务架构的实施进行评审，并把它作为指南；让微服务仪表化、可调试，并提供度量指标，把测试作为一等公民；对容器部署技术进行评估，容器技术有很大优势。</li>
</ul>
<h3 id="kafka数据可靠性深度解读-朱忠华"><a href="#kafka数据可靠性深度解读-朱忠华" class="headerlink" title="kafka数据可靠性深度解读 朱忠华"></a><a href="https://mp.weixin.qq.com/s/ExzSzf0ue7d-_Qv8q6p9bw" target="_blank" rel="external">kafka数据可靠性深度解读</a> 朱忠华</h3><ul>
<li>Kafka与传统消息系统相比，有以下不同：它被设计为一个分布式系统，易于向外扩展；它同时为发布和订阅提供高吞吐量；它支持多订阅者，当失败时能自动平衡消费者；它将消息持久化到磁盘，因此可用于批量消费，例如ETL以及实时应用程序。</li>
<li>Kafka体系架构：一个典型的Kafka体系架构包括若干Producer、若干broker、若干Consumer (Group)以及一个Zookeeper集群。 一个topic可以认为一类消息，每个topic将被分成多个partition，每个partition在存储层面是append log文件。任何发布到此partition的消息都会被追加到log文件的尾部，每条消息在文件中的位置称为offset，offset为一个long型的数字，它唯一标记一条消息。每条消息都被append到partition中，是顺序写磁盘，因此效率非常高（经验证，顺序写磁盘效率比随机写内存还要高，这是Kafka高吞吐率的一个很重要的保证）。</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/18_reading_201704/Kafka_structure.png" alt="Kafka体系架构"></p>
<ul>
<li>Kafka文件存储机制：partition还可以细分为segment，一个partition物理上由多个segment组成；在Kafka文件存储中，同一个topic下有多个不同的partition，每个partiton为一个目录，partition是实际物理上的概念，而topic是逻辑上的概念；每个partition相当于一个巨型文件被平均分配到多个大小相等的segment数据文件中（每个segment 文件中消息数量不一定相等），这种特性也方已被消费消息的清理，提高磁盘的利用率；segment文件由两部分组成，分别为“.index”文件和“.log”文件，分别表示为segment索引文件和数据文件，”.index”索引文件存储大量的元数据，“.log”数据文件存储大量的消息，索引文件中的元数据指向对应数据文件中message的物理偏移地址。以下图中“.index”索引文件中的元数据[3, 348]为例，在“.log”数据文件表示第3个消息，即在全局partition中表示170410+3=170413个消息，该消息的物理偏移地址为348。怎么知道何时读完本条消息，消息都具有固定的物理结构，可以确定一条消息的大小，即读取到哪里截止。</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/18_reading_201704/Kafka_segment.png" alt="segment文件组成"></p>
<ul>
<li>复制原理和同步方式：为了提高消息的可靠性，Kafka每个topic的partition有N个副本，其中N是topic的复制因子的个数。Kafka通过多副本机制实现故障自动转移，当Kafka集群中一个broker失效情况下仍然保证服务可用。在Kafka中发生复制时确保partition的日志能有序地写到其他节点上，N个replicas中，其中一个replica为leader，其他都为follower，leader处理partition的所有读写请求，与此同时，follower会被动定期地去复制leader上的数据。Kafka提供了数据复制算法保证，如果leader发生故障或挂掉，一个新leader被选举并被接受客户端的消息成功写入。</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/18_reading_201704/Kafka_replica.png" alt="Kafka复制原理"></p>
<ul>
<li>副本同步：HW俗称高水位，HighWatermark的缩写，取一个partition对应的ISR中最小的LEO作为HW，consumer最多只能消费到HW所在的位置。另外每个replica都有HW，leader和follower各自负责更新自己的HW的状态。对于leader新写入的消息，consumer不能立刻消费，leader会等待该消息被所有ISR中的replicas同步后更新HW，此时消息才能被consumer消费。这样就保证了如果leader所在的broker失效，该消息仍然可以从新选举的leader中获取。由此可见，Kafka的复制机制既不是完全的同步复制，也不是单纯的异步复制。</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/18_reading_201704/Kafka_ISR.png" alt="副本同步"></p>
<ul>
<li><p>数据可靠性和持久性保证：当producer向leader发送数据时，可以通过request.required.acks参数来设置数据可靠性的级别，1表示leader已成功收到的数据并得到确认，0表示无需等待确认，-1表示需要等待所有follower确认接收到数据。如果要提高数据的可靠性，在设置request.required.acks=-1的同时，也要min.insync.replicas这个参数(表示ISR中的最小副本数是多少，可以在broker或者topic层面进行设置)的配合，这样才能发挥最大的功效。</p>
</li>
<li><p>关于HW的进一步探讨：一个partition中的ISR列表中，leader的HW是所有ISR列表里副本中最小的那个的LEO。如下图这个时候A机器宕机，如果B成为leader，假如没有HW，在A重新恢复之后会做同步操作，在宕机时log文件之后直接做追加操作，而假如B的LEO已经达到了A的LEO，会产生数据不一致的情况，所以使用HW来避免这种情况。A在做同步操作的时候，先将log文件截断到之前自己的HW的位置，即3，之后再从B中拉取消息进行同步。</p>
</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/18_reading_201704/Kafka_HW.png" alt="关于HW的进一步探讨"></p>
<ul>
<li><p>Leader选举：一个基本的原则就是，如果leader不在了，新的leader必须拥有原来的leader commit的所有消息。Kafka在Zookeeper中为每一个partition动态的维护了一个ISR，这个ISR里的所有replica都跟上了leader，只有ISR里的成员才能有被选为leader的可能（unclean.leader.election.enable=false）。在这种模式下，对于f+1个副本，一个Kafka topic能在保证不丢失已经commit消息的前提下容忍f个副本的失败，在大多数使用场景下，这种模式是十分有利的。事实上，为了容忍f个副本的失败，“少数服从多数”的方式和ISR在commit前需要等待的副本的数量是一样的，但是ISR需要的总的副本的个数几乎是“少数服从多数”的方式的一半。但如果某一个partition的所有replica都挂了，Kafka默认采用选择第一个“活”过来的replica（并不一定是在ISR中）作为leader，可以通过参数unclean.leader.election.enable=false设置为等待ISR中任意一个replica“活”过来，并且选它作为leader；</p>
</li>
<li><p>Kafka的发送模式：由producer端的配置参数producer.type来设置，这个参数指定了在后台线程中消息的发送方式是同步的还是异步的，默认是同步的方式。如果设置成异步的模式，可以是producer以batch的形式push数据，这样会极大的提高broker的性能，但是这样会增加丢失数据的风险。</p>
</li>
<li><p>消息去重：Kafka在producer端和consumer端都会出现消息的重复，这就需要去重处理。Kafka文档中提及GUID(Globally Unique Identifier)的概念，通过客户端生成算法得到每个消息的unique id，同时可映射至broker上存储的地址，即通过GUID便可查询提取消息内容，也便于发送方的幂等性保证，需要在broker上提供此去重处理模块，目前版本尚不支持。建议业务方根据自身的业务特点进行去重，比如业务消息本身具备幂等性，或者借助Redis等其他产品进行去重处理。</p>
</li>
<li><p>要保证数据写入到Kafka是安全的，高可靠的，需要如下的配置：</p>
</li>
</ul>
<pre><code>topic的配置：replication.factor&gt;=3,即副本数至少是3个；2&lt;=min.insync.replicas&lt;=replication.factor
broker的配置：leader的选举条件unclean.leader.election.enable=false
producer的配置：request.required.acks=-1(all)，producer.type=sync
</code></pre><ul>
<li>测试场景总结：</li>
</ul>
<pre><code>当acks=-1时，Kafka发送端的TPS受限于topic的副本数量（ISR中），副本越多TPS越低；
acks=0时，TPS最高，其次为1，最差为-1，即TPS：acks_0 &gt; acks_1 &gt; ack_-1；
min.insync.replicas参数不影响TPS；
partition的不同会影响TPS，随着partition的个数的增长TPS会有所增长，但并不是一直成正比关系，到达一定临界值时，partition数量的增加反而会使TPS略微降低；
Kafka在acks=-1,min.insync.replicas&gt;=1时，具有高可靠性，所有成功返回的消息都可以落盘。
</code></pre><h3 id="聊聊分布式定时任务中间件架构及其实现-张亮"><a href="#聊聊分布式定时任务中间件架构及其实现-张亮" class="headerlink" title="聊聊分布式定时任务中间件架构及其实现 张亮"></a><a href="https://mp.weixin.qq.com/s/pqOujhOQlxw6XR_e7MP02w" target="_blank" rel="external">聊聊分布式定时任务中间件架构及其实现</a> 张亮</h3><ul>
<li>分布式定时任务中间件的关注点从易到难是：集中化 -&gt; 高可用 –&gt; 弹性化。</li>
<li>去中心化架构是指所有的作业节点都是对等的。每个作业从注册中心拉取自己的执行时间并且各自定时执行，执行时均使用作业服务器的本地时钟，在作业无需分片调整时并不会对注册中心产生写操作，进而不会导致注册中心更新缓存，因此执行效率很高，去中心化架构的优点是轻量级，仅提供一个lib就可以与业务代码一同工作，部署成本低，只需搭建注册中心即可，缺点是如果各作业服务器时钟不一致会产生同一作业的不同分片运行有先有后，缺乏统一调度。并且不能跨语言。</li>
<li>中心化架构将系统分为调度节点和执行节点。由调度节点发起作业的分片和执行，然后通过RPC发布给作业执行节点，或者通过写注册中心让监听注册中心的作业执行节点主动触发。中心化架构模式可以解决服务器时间差以及跨语言的问题。缺点是部署和运维稍复杂，需要单独部署调度节点并需要维护其高可用，这也会造成一定的资源浪费。</li>
<li>Elastic-Job是一个纯Java实现的分布式方案，提供无中心化解决方案。它采用all in jar的理念，使用时无需区分主从或调度、执行节点，一切都采用自我协调。Elastic-Job采用ZooKeeper作为注册中心，用于处理作业的高可用、分片、失效转移等分布式协调功能。每个使用Elastic-Job的应用都需要与ZooKeeper建立连接，这样会造成ZooKeeper的连接过多，容易成为分布式ZooKeeper的瓶颈。Elastic-Job的架构图如下：</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/18_reading_201704/Elastic_Job_Structure.png" alt="Elastic-Job的架构图"></p>
<ul>
<li>Elastic-Job-Cloud包括Mesos Framework的Scheduler和Customized Executor两部分。Elastic-Job-Cloud采用中心节点分片，直接将分片任务转化为Mesos的TaskInfo，这样就屏蔽了IP地址的限制。Elastic-Job-Cloud的作业调度采用两个队列，Offer队列用于收集Mesos分配的资源，Job队列用于堆积待执行作业。当待执行作业可以从资源队列中匹配到合适的资源时，才会分配并生成TaskInfo执行。</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/18_reading_201704/Elastic_Job_Cloud_Structure.png" alt="Elastic-Job-Cloud的架构图"></p>
<h3 id="2017年会是Serverless爆发之年吗？-麦克周"><a href="#2017年会是Serverless爆发之年吗？-麦克周" class="headerlink" title="2017年会是Serverless爆发之年吗？ 麦克周"></a><a href="https://mp.weixin.qq.com/s/vQ22_disiOIbfOHz95Ls-w" target="_blank" rel="external">2017年会是Serverless爆发之年吗？</a> 麦克周</h3><ul>
<li>开发人员进行业务开发时，仍然需要关心很多和服务器相关的服务端开发工作，比如缓存、消息服务、Web应用服务器、数据库，以及对服务器进行性能优化，还需要考虑存储和计算资源，考虑负载均衡和横向扩展能力，考虑服务器容灾稳定性等非专业逻辑的开发。这些服务器的运维和开发知识、经验极大地限制了开发者进行业务开发的效率。</li>
<li>2014年，云厂商AWS推出了“无服务器”的范式服务。最初“无服务器”意在帮助开发者摆脱运行后端应用程序所需的服务器设备的设置和管理工作。这项技术的目标并不是为了实现真正意义上的“无服务器”，而是指由第三方供应商负责后端基础结构的维护，以服务的方式为开发者提供所需功能。现在，无服务器架构是指大量依赖第三方服务(也叫做后端即服务，即“BaaS”)或暂存容器中运行的自定义代码(函数即服务，即“FaaS”)的应用程序，函数是无服务器架构中抽象语言运行时的最小单位，在这种架构中，我们并不看重运行一个函数需要多少CPU或RAM或任何其他资源，而是更看重运行函数所需的时间，我们也只为这些函数的运行时间付费。无服务器架构中函数可以多种方式触发，如定期运行函数的定时器、HTTP请求或某些相关服务中的某个事件。</li>
<li>Serverless案例：以带有服务功能逻辑的传统面向客户端的三层应用为例，如果采用Serverless架构来对该应用进行改造，则架构如图所示：删除认证逻辑，用第三方BaaS服务替代；使用另外一个BaaS，允许客户端直接访问第三方上面的数据子库；以前运行在服务端的逻辑转移到客户端中，例如跟踪用户访问，客户端则慢慢转化为单页面应用；计算敏感或者需要访问大量数据的功能，例如搜索这类应用，我们不需要运行一个专用服务，而是通过FaaS模块，通过API Gateway对HTTP访问提供响应；最后，可以将其他功能用另外一个FaaS功能取代，因为安全原因放在服务端还不如在客户端重新实现，当然前端还是API Gateway。</li>
<li>Serverless架构原则： 按需使用计算服务执行代码；编写单一用途的无状态函数；设计基于推送的、事件驱动的管道；创建更强大的前端；与第三方服务集成；</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/18_reading_201704/Serverless_example.jpg" alt="Serverless案例"></p>
<h3 id="Java-9新特性介绍及Jigsaw一览-QCON-杨晓峰"><a href="#Java-9新特性介绍及Jigsaw一览-QCON-杨晓峰" class="headerlink" title="Java 9新特性介绍及Jigsaw一览 QCON/杨晓峰"></a><a href="http://2017.qconbeijing.com/" target="_blank" rel="external">Java 9新特性介绍及Jigsaw一览</a> QCON/杨晓峰</h3><ul>
<li>Jigsaw的目标：提供两方面的基本能力（可靠的配置替换脆弱的classpath机制以及强），带来的好处（可扩展性+可维护性+安全）；</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/18_reading_201704/Java9_Jigsaw.png" alt="Jigsaw项目"></p>
<ul>
<li>模块系统组成：</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/18_reading_201704/Java9_Jigsaw_modular.png" alt="模块系统组成"></p>
<ul>
<li>JDK自身的代码被划分为一组模块：</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/18_reading_201704/Java9_Jigsaw_JDK_modular.png" alt="JDK自身模块组成"></p>
<ul>
<li>典型的模块描述文件：</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/18_reading_201704/Java9_Jigsaw_modular_descriptor.png" alt="模块描述文件"></p>
<ul>
<li>可见性：完全public、只对特定的模块public、只在模块内public、protected、package内可见、private；</li>
<li>Java模块的分类：命名模块、自动命名模块、匿名模块；</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/18_reading_201704/Java9_Jigsaw_module_type.png" alt="Java模块的分类"></p>
<ul>
<li>升级影响：</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/18_reading_201704/Java9_Jigsaw_upgrade.png" alt="Java模块的分类"></p>
<h3 id="打造高性能高可用的搜索服务——爱奇艺搜索架构实践-QCON-陈爱云"><a href="#打造高性能高可用的搜索服务——爱奇艺搜索架构实践-QCON-陈爱云" class="headerlink" title="打造高性能高可用的搜索服务——爱奇艺搜索架构实践 QCON/陈爱云"></a><a href="http://2017.qconbeijing.com/" target="_blank" rel="external">打造高性能高可用的搜索服务——爱奇艺搜索架构实践</a> QCON/陈爱云</h3><ul>
<li>视频搜索难点和痛点：索引量大、高并发、低延时、索引更新速度快；</li>
<li>爱奇艺搜索架构：应用网络抖动（优先访问本机redis）；搜索分级（热门索引和全量索引机器4：1，热门索引可以服务80%的请求，节省机器）；索引更新（全量数据7天更新一次、增量数据每天更新、实时数据实时更新）；从同步到异步（包括http和rpc）；性能优化（基于CPU和内存的优化）；</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/18_reading_201704/Aqy_search.png" alt="爱奇艺搜索架构"></p>
<ul>
<li>高可用：异地多活、降级（client根据server的成功率和响应时间降级，server根据队列长度、平均处理时间和CPU判断是否需要降级，两种降级方法，取长时间缓存和去除一些不重要的计算）、限流、扩容（使用docker自动扩容，使用jenkins一键部署）、监控系统、Trace系统。</li>
</ul>
<h3 id="高速发展业务的架构应对实践-QCON-陈霖"><a href="#高速发展业务的架构应对实践-QCON-陈霖" class="headerlink" title="高速发展业务的架构应对实践 QCON/陈霖"></a><a href="http://2017.qconbeijing.com/" target="_blank" rel="external">高速发展业务的架构应对实践</a> QCON/陈霖</h3><ul>
<li>外卖产品特点：交易量大，峰值集中；交易过程复杂；履约流程较长；</li>
<li>百度外卖架构：</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/18_reading_201704/bdwm_structure.png" alt="百度外卖架构"></p>
<ul>
<li>服务分层情况：</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/18_reading_201704/bdwm_service_split.png" alt="服务分层情况"></p>
<ul>
<li>基础服务优化：流量接入层（七层负载均衡、流量镜像、业务限流）；Redis异地多活（非强一致性、实时读写本region数据、通过MQ同步aof并在当前region进行merge）；服务治理（统一RPC框架、超时重试、过载保护、柔性服务、分布式跟踪、全局超时控制）。</li>
</ul>
<h3 id="美团点评旅游推荐系统的演进-QCON-郑刚"><a href="#美团点评旅游推荐系统的演进-QCON-郑刚" class="headerlink" title="美团点评旅游推荐系统的演进 QCON/郑刚"></a><a href="http://2017.qconbeijing.com/" target="_blank" rel="external">美团点评旅游推荐系统的演进</a> QCON/郑刚</h3><ul>
<li>旅游推荐面临的问题：本异地差距大、推荐形式多样、季节性明显、需求个性化；</li>
<li>用户画像 </li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/18_reading_201704/recom_user_feature.png" alt="用户画像"></p>
<ul>
<li>基于用户画像的推荐</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/18_reading_201704/recom_by_user_feature.png" alt="基于用户画像的推荐"></p>
<ul>
<li>问题建模</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/18_reading_201704/recom_question_model.png" alt="问题建模"></p>
<ul>
<li>特征工程</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/18_reading_201704/recom_feature_engineering.png" alt="特征工程"></p>
<ul>
<li>模型训练</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/18_reading_201704/recom_model_training.png" alt="模型训练"></p>
<ul>
<li>从海量大数据的离线计算到高并发在线服务的推荐引擎架构设计</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/18_reading_201704/recom_structure.png" alt="推荐引擎架构设计"></p>
<ul>
<li>总结</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/18_reading_201704/recom_summary.png" alt="总结"></p>
<h3 id="深度学习在电子商务中的应用-QCON-程进兴"><a href="#深度学习在电子商务中的应用-QCON-程进兴" class="headerlink" title="深度学习在电子商务中的应用 QCON/程进兴"></a><a href="http://2017.qconbeijing.com/" target="_blank" rel="external">深度学习在电子商务中的应用</a> QCON/程进兴</h3><ul>
<li>目前商品搜索中的语义词汇差异问题：如理发器、理发推子、电推子；血糖计和血糖仪；</li>
<li>矢量化搜索模型：传统搜索基于文字匹配，商品包含搜索词或者不包含搜索词；利用深度学习技术，将搜索词和商品全部数值矢量化，将文字匹配转化为数值矢量计算；词语矢量化是进一步进行各种深度学习的基础；</li>
<li>基于词语聚类的矢量化模型：Word2vec等工具可以有效地将词语转化为向量；将句子／段落／文章有效转化为向量则有很大的挑战；电商搜索中遇到的主要是句子／短文分析，可以将短文中的词语聚类，挑选具有代表性的词语聚类结果，来表示整个短文；传统聚类（如Kmeans)在几何距离的基础上进行聚类，效果不好，利用随机过程做词语聚类可以解决这一问题；</li>
<li>基于用户反馈的矢量化：把搜索词和商品文档各自作为整体看待，直接学习训练各自的矢量值；通过分析用户每次访问的行为顺序，构建有“搜索词”和“商品文档”组成的句子；训练集是采用苏宁易购的用户搜索日志作为来源，在经过数据清理之后，按照搜索的时间顺序，结合商品的点击，商品放入购物车，商品的购买这些用户行为，而建立的矢量化训练数据；</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/18_reading_201704/suning_vector_model.png" alt="基于用户反馈的矢量化模型"></p>
<h3 id="微信红包后台系统可用性设计实践-QCON-michaelfang"><a href="#微信红包后台系统可用性设计实践-QCON-michaelfang" class="headerlink" title="微信红包后台系统可用性设计实践 QCON/michaelfang"></a><a href="http://2017.qconbeijing.com/" target="_blank" rel="external">微信红包后台系统可用性设计实践</a> QCON/michaelfang</h3><ul>
<li>微信红包的系统流程：发、包、抢、拆；</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/18_reading_201704/wechat_rp_flow.png" alt="微信红包的系统流程"></p>
<ul>
<li>微信红包的系统架构</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/18_reading_201704/wechat_rp_structure.png" alt="微信红包的系统架构"></p>
<ul>
<li>可用性影响因素：计划外（系统级故障、数据和中介故障、其他）；计划外（日常任务、运维相关、升级相关）；</li>
<li>可用性设计方向：降低意外故障影响（业务逻辑层：部署方案、异步化、降级与柔性；订单存储层：SET化、DB故障自愈能力建设）、平行扩缩容；</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/18_reading_201704/wechat_rp_expand.png" alt="改进后的平行扩容"></p>
<h3 id="异构系统链路追踪——滴滴-trace-实践-QCON"><a href="#异构系统链路追踪——滴滴-trace-实践-QCON" class="headerlink" title="异构系统链路追踪——滴滴 trace 实践 QCON"></a><a href="http://2017.qconbeijing.com/" target="_blank" rel="external">异构系统链路追踪——滴滴 trace 实践</a> QCON</h3><p><img src="http://oi46mo3on.bkt.clouddn.com/18_reading_201704/didi_trace_background.png" alt="背景"></p>
<p><img src="http://oi46mo3on.bkt.clouddn.com/18_reading_201704/didi_trace_reqirement.png" alt="诉求"></p>
<p><img src="http://oi46mo3on.bkt.clouddn.com/18_reading_201704/didi_trace_method.png" alt="方案"></p>
<p><img src="http://oi46mo3on.bkt.clouddn.com/18_reading_201704/didi_trace_mechanism.png" alt="Trace机制"></p>
<p><img src="http://oi46mo3on.bkt.clouddn.com/18_reading_201704/didi_trace_implement.png" alt="Trace落地"></p>
<p><img src="http://oi46mo3on.bkt.clouddn.com/18_reading_201704/didi_trace_development.png" alt="规划"></p>
<h3 id="Leaf——美团点评分布式ID生成系统-照东"><a href="#Leaf——美团点评分布式ID生成系统-照东" class="headerlink" title="Leaf——美团点评分布式ID生成系统 照东"></a><a href="http://tech.meituan.com/MT_Leaf.html" target="_blank" rel="external">Leaf——美团点评分布式ID生成系统</a> 照东</h3><ul>
<li>在复杂分布式系统中，往往需要对大量的数据和消息进行唯一标识。业务系统对ID号的要求有全局唯一性、趋势递增、单调递增、信息安全、可用性要求极高；ID生成系统应该做到如下几点：平均延迟和TP999延迟都要尽可能低；可用性5个9；高QPS。</li>
<li>常见方法介绍：UUID（性能非常高但不易于存储）；类snowflake方案（整个ID都是趋势递增的而且灵活，缺点是强依赖机器时钟）；数据库生成（单调递增且简单，缺点是强依赖DB且会有性能瓶颈）；</li>
<li>Leaf-segment数据库方案：利用proxy server批量获取，每次获取一个segment(step决定大小)号段的值，用完之后再去数据库获取新的号段，可以大大的减轻数据库的压力；各个业务不同的发号需求用biz_tag字段来区分，每个biz-tag的ID获取相互隔离，互不影响，如果以后有性能需求需要对数据库扩容，不需要上述描述的复杂的扩容操作，只需要对biz_tag分库分表就行。数据库表设计如下：</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/18_reading_201704/leaf_biz_tag.png" alt="biz_tag表设计"></p>
<p><img src="http://oi46mo3on.bkt.clouddn.com/18_reading_201704/leaf_biz_usecase.png" alt="使用场景"></p>
<ul>
<li>Leaf-segment数据库方案的优点：Leaf服务可以很方便的线性扩展，性能完全能够支撑大多数业务场景；ID号码是趋势递增的8byte的64位数字，满足上述数据库存储的主键要求；容灾性高，Leaf服务内部有号段缓存，即使DB宕机，短时间内Leaf仍能正常对外提供服务；可以自定义max_id的大小，非常方便业务从原有的ID方式上迁移过来。</li>
<li>Leaf-segment数据库方案的缺点：ID号码不够随机，能够泄露发号数量的信息，不太安全；TP999数据波动大，当号段使用完之后还是会hang在更新数据库的I/O上，tg999数据会出现偶尔的尖刺（优化：采用双buffer的方式，Leaf服务内部有两个号段缓存区segment，当前号段已下发10%时，如果下一个号段未更新，则另启一个更新线程去更新下一个号段，当前号段全部下发完后，如果下个号段准备好了则切换到下个号段为当前segment接着下发，循环往复）；DB宕机会造成整个系统不可用（优化：采用一主两从的方式，同时分机房部署，Master和Slave之间采用半同步方式同步数据）；</li>
<li>Leaf-snowflake方案：完全沿用snowflake方案的bit位设计，即是“1+41+10+12”的方式组装ID号。使用Zookeeper持久顺序节点的特性自动对snowflake节点配置wokerID。Leaf-snowflake是按照下面几个步骤启动的：启动Leaf-snowflake服务，连接Zookeeper，在leaf_forever父节点下检查自己是否已经注册过（是否有该顺序子节点）；如果有注册过直接取回自己的workerID（zk顺序节点生成的int类型ID号），启动服务；如果没有注册过，就在该父节点下面创建一个持久顺序节点，创建成功后取回顺序号当做自己的workerID号，启动服务。</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/18_reading_201704/leaf_snowflake.png" alt="Leaf-snowflake设计"></p>
<ul>
<li>Leaf-snowflake方案要点：弱依赖ZooKeeper（除了每次会去ZK拿数据以外，也会在本机文件系统上缓存一个workerID文件，当ZooKeeper出现问题，恰好机器出现问题需要重启时，能保证服务能够正常启动）；解决时钟问题（如果机器的时钟发生了回拨，那么就会有可能生成重复的ID号，需要解决时钟回退的问题，见下图）。</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/18_reading_201704/leaf_snowflake_timing.png" alt="解决时钟问题"></p>
<h3 id="DevOps详解-Jerome-Kehrli-大愚若智"><a href="#DevOps详解-Jerome-Kehrli-大愚若智" class="headerlink" title="DevOps详解  Jerome Kehrli/大愚若智"></a><a href="http://www.infoq.com/cn/articles/detail-analysis-of-devops" target="_blank" rel="external">DevOps详解</a>  Jerome Kehrli/大愚若智</h3><ul>
<li>DevOps并不是某一套工具，DevOps是一种方法论，其中包含一系列基本原则和实践，而所用的工具只是为了对这样的实践提供支持。</li>
<li>DevOps是一种方法论，是一系列可以帮助开发者和运维人员在实现各自目标的前提下，向自己的客户或用户交付最大化价值及最高质量成果的基本原则和实践。</li>
<li>DevOps鼓励软件开发者和IT运维人员之间所进行的沟通、协作、集成和自动化，借此有助于改善双方在交付软件过程中的速度和质量。</li>
<li>DevOps团队更侧重于通过标准化开发环境和自动化交付流程改善交付工作的可预测性、效率、安全性，以及可维护性。理想情况下，DevOps可以为开发者提供更可控的生产环境，帮助他们更好地理解生产基础架构。</li>
<li>那么核心原则到底是什么？基础架构即代码、持续交付、协作；</li>
</ul>
<h3 id="到底什么时候该使用MQ？-58沈剑"><a href="#到底什么时候该使用MQ？-58沈剑" class="headerlink" title="到底什么时候该使用MQ？ 58沈剑"></a><a href="https://mp.weixin.qq.com/s/Brd-j3IcljcY7BV01r712Q" target="_blank" rel="external">到底什么时候该使用MQ？</a> 58沈剑</h3><ul>
<li>MQ是一个互联网架构中常见的解耦利器。</li>
<li>什么时候不使用MQ？上游实时关注执行结果；</li>
<li>什么时候使用MQ？数据驱动的任务依赖、上游不关心多下游执行结果、异步返回执行时间长；</li>
</ul>
<h3 id="换IP的是你，凭啥重启的却是我？-58沈剑"><a href="#换IP的是你，凭啥重启的却是我？-58沈剑" class="headerlink" title="换IP的是你，凭啥重启的却是我？ 58沈剑"></a><a href="https://mp.weixin.qq.com/s/mCop7WB0wi4mhGy4AHF7tQ" target="_blank" rel="external">换IP的是你，凭啥重启的却是我？</a> 58沈剑</h3><ul>
<li>缘起：数据库换了一个ip，此时往往连接此数据库的上游需要修改配置重启，这是一个“架构耦合”的问题，是一个架构设计上“反向依赖”的问题；</li>
<li>常见的“反向依赖”与优化方案：公共库导致耦合（业务垂直拆分、服务化）；服务化不彻底导致耦合（业务特性代码上浮，业务共性代码下沉，彻底解耦）；notify的不合理实现导致的耦合（通过MQ实现解耦）；配置中的ip导致上下游耦合（通过内网域名而不是ip来进行下游连接）；下游扩容导致上下游耦合；</li>
</ul>
<h3 id="“配置”也有架构演进？看完深有痛感-58沈剑"><a href="#“配置”也有架构演进？看完深有痛感-58沈剑" class="headerlink" title="“配置”也有架构演进？看完深有痛感 58沈剑"></a><a href="https://mp.weixin.qq.com/s/vBr_OrbogMARHtCt8ZaxGg" target="_blank" rel="external">“配置”也有架构演进？看完深有痛感</a> 58沈剑</h3><ul>
<li>缘起：服务化分层后依赖关系会变得非常复杂，为了保证高可用，一个底层服务往往是若干个节点形成一个集群提供服务，当服务集群增减节点的时候，是否存在“反向依赖”，是否“耦合”，是否上游调用方需要修改配置重启，是否能做到上游无感知，即“配置的架构变迁”。</li>
<li>“配置私藏”是配置文件架构的最初级阶段，上游调用下游，每个上游都有一个专属的私有配置文件，记录被调用下游的每个节点配置信息。调用方很痛，容量变化的是你，凭啥修改配置重启的是我？这是一个典型的“反向依赖”架构设计，上下游通过配置耦合，值得优化；服务方很痛，ta不知道有多少个上游调用了自己；</li>
<li>“全局配置”法：对于通用的服务，建立全局配置文件，消除配置私藏：运维层面制定规范，新建全局配置文件，例如/opt/globalconf/global.conf，如果配置较多，注意做好配置的垂直拆分；对于服务方，如果是通用的服务，集群信息配置在global.conf里；对于调用方，调用方禁止配置私藏，必须从global.conf里读取通用下游配置。不足是如果调用方一直不重启，就没有办法将流量迁移到新集群上去了。可以通过文件监控组件和动态连接池组件实现自动流量迁移。</li>
<li>全局配置文件是一个能够快速落地的，解决“修改配置重启”问题的方案，但它仍然解决不了，服务提供方“不知道有多少个上游调用了自己”这个问题，这个可以采用“配置中心”来解决。对比“全局配置”与“配置中心”的架构图，会发现配置由静态的文件 升级为动态的服务：整个配置中心子系统由zk、conf-center服务，DB配置存储与，conf-web配置后台组成；所有下游服务的配置，通过后台设置在配置中心里；所有上游需要拉取配置，需要去配置中心注册，拉取下游服务配置信息。不足是系统复杂度相对较高，对配置中心的可靠性要求较高，一处挂全局挂。</li>
</ul>
<p><strong>免责声明：相关链接版本为原作者所有，如有侵权，请告知删除。</strong></p>
<p><strong>随手记系列：</strong></p>
<ul>
<li><a href="http://ginobefunny.com/post/reading_record_201703/">阅读随手记 201703</a></li>
<li><a href="http://ginobefunny.com/post/reading_record_201702/">阅读随手记 201702</a></li>
<li><a href="http://ginobefunny.com/post/reading_record_201701/">阅读随手记 201701</a></li>
<li><a href="http://ginobefunny.com/post/reading_record_201612/">阅读随手记 201612</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;关键字：微服务, 架构, Elasticsearch, 分布式队列, 搜索引擎, 推荐系统, 机器学习, 人工智能, Java 9, CQRS, Event Source, Kafka, 高可用。&lt;br&gt;
    
    </summary>
    
      <category term="Reading Record" scheme="http://ginobefunny.com/categories/Reading-Record/"/>
    
    
      <category term="Elasticsearch" scheme="http://ginobefunny.com/tags/Elasticsearch/"/>
    
      <category term="搜索引擎" scheme="http://ginobefunny.com/tags/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E/"/>
    
      <category term="推荐系统" scheme="http://ginobefunny.com/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
      <category term="分布式队列" scheme="http://ginobefunny.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E9%98%9F%E5%88%97/"/>
    
      <category term="微服务" scheme="http://ginobefunny.com/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"/>
    
      <category term="Kafka" scheme="http://ginobefunny.com/tags/Kafka/"/>
    
      <category term="机器学习" scheme="http://ginobefunny.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="高可用" scheme="http://ginobefunny.com/tags/%E9%AB%98%E5%8F%AF%E7%94%A8/"/>
    
      <category term="架构" scheme="http://ginobefunny.com/tags/%E6%9E%B6%E6%9E%84/"/>
    
      <category term="人工智能" scheme="http://ginobefunny.com/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
      <category term="Java 9" scheme="http://ginobefunny.com/tags/Java-9/"/>
    
      <category term="CQRS" scheme="http://ginobefunny.com/tags/CQRS/"/>
    
      <category term="Event Source" scheme="http://ginobefunny.com/tags/Event-Source/"/>
    
  </entry>
  
  <entry>
    <title>AMQP学习小记</title>
    <link href="http://ginobefunny.com/post/learning_amqp/"/>
    <id>http://ginobefunny.com/post/learning_amqp/</id>
    <published>2017-03-30T09:21:03.000Z</published>
    <updated>2017-03-30T09:28:13.019Z</updated>
    
    <content type="html"><![CDATA[<p>阅读《RabbitMQ实战》一书第二章<strong>理解消息通信</strong>的读书笔记。<br><a id="more"></a></p>
<p><img src="http://oi46mo3on.bkt.clouddn.com/15_reading_201702/amqp.png" alt="思维导图"></p>
<h2 id="消费者和生产者"><a href="#消费者和生产者" class="headerlink" title="消费者和生产者"></a>消费者和生产者</h2><ul>
<li>生产者（producer）创建消息，然后发送到代理服务器（RabbitMQ）。</li>
<li>消息包含两部分：有效载荷（payload）和标签（label）；有效载荷就是你想要传输的数据（可以是任何格式的任何内容）；标签描述了有效载荷，并且RabbitMQ用它来决定谁将获得消息的拷贝。</li>
<li>消费者（consumer）连接到代理服务器上，并订阅到队列（queue）上；当消费者接收到消息时，它只得到消息的一部分：有效载荷（标签并没有随有效载荷一同传递）。</li>
<li>信道（channel）建立在”真实的”TCP连接内的虚拟连接；不论是发布信息、订阅队列或是接收消息，都是通过信道完成的；不使用TCP连接主要是因为对于操作系统而言建立和销毁TCP会话非常昂贵的开销；在一条TCP连接上创建多少条信道是没有限制的。</li>
</ul>
<h2 id="队列"><a href="#队列" class="headerlink" title="队列"></a>队列</h2><ul>
<li>AMQP消息路由必须有三部分：交换器、队列和绑定。生成者把消息发布到交换器上；消息最终到达队列，并被消费者接收；绑定决定了消息如何从路由器到特定的队列。</li>
<li>消费者和生产者都可以通过queue.declare命令来创建队列。queue-name：队列名称，不指定则随机生产；exclusive：设置为true则为私有队列，只有当前消费者可以订阅；auto-delete：设置为true时最后一个消费者取消订阅将自动移除队列；passive：队列不存在时返回错误。</li>
<li>消费者通过两种方式从特定队列中接收消息：basic.consume（消息一到达队列就自动接收，推荐的方式）和basic.get（从队列里主动获取单条消息）。</li>
<li>当一个队列拥有多个消费者时，队列接收到的消息将以循环（round-robin）的方式发给消费者。每条信息只会发送给一个订阅的消费者。</li>
<li>消费者接收的每一条信息都必须进行确认，可以使用basic.ack命令显式确认或在订阅队列时将auto_ack参数设置为true（一旦接收就自动确认）。如果确认前与RabbitMQ断开连接，则会重新分发给下一个订阅的消费者；如果由于bug等忘记确认的话，RabbitMQ将不会发生新的消息。如果想明确拒绝新消息的话可以使用basic.reject命令（requeue参数为true则发送给其他消费者，为false则从队列中移除）。</li>
</ul>
<h2 id="交换器和绑定"><a href="#交换器和绑定" class="headerlink" title="交换器和绑定"></a>交换器和绑定</h2><ul>
<li>队列通过路由键（routing-key）绑定到交换器。如果消息不匹配任何绑定模式的话，将进入“黑洞”。</li>
<li>四种类型的交换器：direct、fanout、topic和headers。direct类型以队列名称作为routing-key；fanout类型会把消息投递所有附件在此交换器上的队列；topic支持通过“*”和“#”符号匹配消费者发送的topic消息，并投递到指定队列。</li>
</ul>
<h2 id="多租户模式"><a href="#多租户模式" class="headerlink" title="多租户模式"></a>多租户模式</h2><ul>
<li>每个RabbitMQ服务器都能创建虚拟主机（vhost），其本质是一个mini版的RabbitMQ服务器，拥有自己的队列、交换器和绑定，同时也拥有自己的权限机制。</li>
<li>vhost之间是绝对隔离的。</li>
</ul>
<h2 id="持久化策略"><a href="#持久化策略" class="headerlink" title="持久化策略"></a>持久化策略</h2><ul>
<li>队列消息持久化需要满足三个条件：1.发布消息时设置投递模式为2（持久）；2.发送到持久化的交换器（durable属性为true）；3.到达持久化的队列（durable属性为true）；</li>
<li>RabbitMQ通过持久化日志文件（记录在磁盘上）记录和恢复持久化消息；但是持久化日志会导致性能大幅降低，而且內建的集群对于这种方式工作得并不好。</li>
<li>虽然AMQP 0-9-1规范支持了事务，但是使用事务会降低大约2-10倍的吞吐量，而且使生产者应用程序产生同步。RabbitMQ使用了“发送方确认模式”来保证消息投递，其最大的好处是它是异步的方式。</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;阅读《RabbitMQ实战》一书第二章&lt;strong&gt;理解消息通信&lt;/strong&gt;的读书笔记。&lt;br&gt;
    
    </summary>
    
      <category term="OpenSource" scheme="http://ginobefunny.com/categories/OpenSource/"/>
    
    
      <category term="分布式队列" scheme="http://ginobefunny.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E9%98%9F%E5%88%97/"/>
    
      <category term="RabbitMQ" scheme="http://ginobefunny.com/tags/RabbitMQ/"/>
    
      <category term="AMQP" scheme="http://ginobefunny.com/tags/AMQP/"/>
    
      <category term="生产者" scheme="http://ginobefunny.com/tags/%E7%94%9F%E4%BA%A7%E8%80%85/"/>
    
      <category term="消费者" scheme="http://ginobefunny.com/tags/%E6%B6%88%E8%B4%B9%E8%80%85/"/>
    
      <category term="队列" scheme="http://ginobefunny.com/tags/%E9%98%9F%E5%88%97/"/>
    
      <category term="交换器" scheme="http://ginobefunny.com/tags/%E4%BA%A4%E6%8D%A2%E5%99%A8/"/>
    
      <category term="绑定" scheme="http://ginobefunny.com/tags/%E7%BB%91%E5%AE%9A/"/>
    
  </entry>
  
  <entry>
    <title>基于word2vec和Elasticsearch实现个性化搜索</title>
    <link href="http://ginobefunny.com/post/personalized_search_implemention_based_word2vec_and_elasticsearch/"/>
    <id>http://ginobefunny.com/post/personalized_search_implemention_based_word2vec_and_elasticsearch/</id>
    <published>2017-03-28T07:51:02.000Z</published>
    <updated>2017-03-28T11:51:39.026Z</updated>
    
    <content type="html"><![CDATA[<p>在<a href="http://ginobefunny.com/post/learning_word2vec/">word2vec学习小记</a>一文中我们曾经学习了word2vec这个工具，它基于神经网络语言模型并在其基础上进行优化，最终能获取词向量和语言模型。在我们的商品搜索系统里，采用了word2vec的方式来计算用户向量和商品向量，并通过Elasticsearch的function_score评分机制和自定义的脚本插件来实现个性化搜索。</p>
<a id="more"></a> 
<h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h1><p>先来看下<a href="https://en.wikipedia.org/wiki/Personalized_search" target="_blank" rel="external">维基百科</a>上对于个性化搜索的定义和介绍：</p>
<blockquote>
<p>Personalized search refers to web search experiences that are tailored specifically to an individual’s interests by incorporating information about the individual beyond specific query provided. Pitkow et al. describe two general approaches to personalizing search results, one involving modifying the user’s query and the other re-ranking search results.</p>
</blockquote>
<p>由此我们可以得到两个重要的信息：</p>
<ol>
<li>个性化搜索需要充分考虑到用户的偏好，将<strong>用户感兴趣的内容</strong>优先展示给用户；</li>
<li>另外是对于实现个性化的方式上主要有<strong>查询修改和对搜索结果的重排序</strong>两种。</li>
</ol>
<p>而对我们电商网站来说，个性化搜索的重点是当用户搜索某个关键字，如【卫衣】时，能将用户最感兴趣最可能购买的商品（如用户偏好的品牌或款式）优先展示给用户，以提升用户体验和点击转化。</p>
<h1 id="设计思路"><a href="#设计思路" class="headerlink" title="设计思路"></a>设计思路</h1><ol>
<li>在此之前我们曾经有一般的个性化搜索实现，其主要是通过计算用户和商品的一些重要属性（比如品牌、品类、性别等）的权重，然后得到一个用户和商品之间的关联系数，然后根据该系数进行重排序。</li>
<li>但是这一版从效果来看并不是很好，我个人觉得主要的原因有以下几点：用户对商品的各个属性的重视程度并不是一样的，另外考虑的商品的属性并不全，且没有去考虑商品和商品直接的关系；</li>
<li>在新的版本的设计中，我们考虑通过<strong>用户的浏览记录这种时序数据</strong>来获取用户和商品以及商品和商品直接的关联关系，其核心就是通过类似于语言模型的词出现的顺序来训练向量表示结果；</li>
<li>在获取用户向量和商品向量表示后，我们就可以<strong>根据向量直接的距离来计算相关性</strong>，从而将用户感兴趣的商品优先展示；</li>
</ol>
<h1 id="实现细节"><a href="#实现细节" class="headerlink" title="实现细节"></a>实现细节</h1><h2 id="商品向量的计算"><a href="#商品向量的计算" class="headerlink" title="商品向量的计算"></a>商品向量的计算</h2><ul>
<li>根据用户最近某段时间（如30天内）的浏览记录，获取得到浏览SKN的列表并使用空格分隔；核心的逻辑如下面的SQL所示：</li>
</ul>
<pre><code>select concat_ws(&apos; &apos;, collect_set(product_skn)) as skns 
from 
 (select uid, cast(product_skn as string) as product_skn, click_time_stamp 
  from product_click_record 
  where date_id &lt;= $date_id and date_id &gt;= $date_id_30_day_ago
  order by uid, click_time_stamp) as a 
group by uid;
</code></pre><ul>
<li>将该SQL的执行结果写入文件作为word2vec训练的输入；</li>
<li>调用word2vec执行训练，并保存训练的结果：</li>
</ul>
<pre><code>time ./word2vec -train $prepare_file -output $result_file -cbow 1 -size 20 
-window 8 -negative 25 -hs 0 -sample 1e-4 -threads 20 -iter 15 
</code></pre><ul>
<li>读取训练结果的向量，保存到搜索库的商品向量表。</li>
</ul>
<h2 id="用户向量的计算"><a href="#用户向量的计算" class="headerlink" title="用户向量的计算"></a>用户向量的计算</h2><ul>
<li>在计算用户向量时采用了一种简化的处理，即通过用户最近某段时间（如30天内）的商品浏览记录，根据这些商品的向量进行每一维的求平均值处理来计算用户向量，核心的逻辑如下：</li>
</ul>
<pre><code>vec_list = []
for i in range(feature_length):
    vec_list.append(&quot;avg(coalesce(b.vec[%s], 0))&quot; % (str(i)))
vec = &apos;, &apos;.join(vec_list)


select a.uid as uid, array(%s) as vec 
from 
 (select * from product_click_record where date_id &lt;= $date_id and date_id &gt;= $date_id_30_day_ago) as a
left outer join
 (select * from product_w2v where date_id = $date_id) as b
on a.product_skn = b.product_skn
group by a.uid;
</code></pre><ul>
<li>将计算获取的用户向量，保存到Redis里供搜索服务获取。</li>
</ul>
<h2 id="搜索服务时增加个性化评分"><a href="#搜索服务时增加个性化评分" class="headerlink" title="搜索服务时增加个性化评分"></a>搜索服务时增加个性化评分</h2><ul>
<li>商品索引重建构造器在重建索引时设置商品向量到product_index的某个字段中，比如下面例子的productFeatureVector字段；</li>
<li>搜索服务在默认的cross_fields相关性评分的机制下，需要增加个性化的评分，这个可以通过<strong>function_score</strong>来实现。</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">Map&lt;String, Object&gt; scriptParams = <span class="keyword">new</span> HashMap&lt;&gt;();</div><div class="line">scriptParams.put(<span class="string">"field"</span>, <span class="string">"productFeatureVector"</span>);</div><div class="line">scriptParams.put(<span class="string">"inputFeatureVector"</span>, userVector);</div><div class="line">scriptParams.put(<span class="string">"version"</span>, version);</div><div class="line">Script script = <span class="keyword">new</span> Script(<span class="string">"feature_vector_scoring_script"</span>, ScriptService.ScriptType.INLINE, <span class="string">"native"</span>, scriptParams);</div><div class="line">functionScoreQueryBuilder.add(ScoreFunctionBuilders.scriptFunction(script));</div></pre></td></tr></table></figure>
<ul>
<li>这里采用了<a href="https://github.com/ginobefun/elasticsearch-feature-vector-scoring" target="_blank" rel="external">elasticsearch-feature-vector-scoring插件</a>来进行相关性评分，其核心是向量的余弦距离表示，具体见下面一小节的介绍。在脚本参数中，field表示索引中保存商品特征向量的字段；inputFeatureVector表示输入的向量，在这里为用户的向量；</li>
<li>这里把version参数单独拿出来解释一下，因为每天计算出来的向量是不一样的，向量的每一维并没有对应商品的某个具体的属性（至少我们现在看不出来这种关联），因此我们要特别避免不同时间计算出来的向量的之间计算相关性。在实现的时候，我们是通过一个中间变量来表示最新的版本，即在完成商品向量和用户向量的计算和推送给搜索之后，再更新这个中间向量；搜索的索引构造器定期轮询这个中间变量，当发现发生更新之后，就将商品的特征向量批量更新到ES中，在后面的搜索中就可以采用新版本的向量了；</li>
</ul>
<h2 id="elasticsearch-feature-vector-scoring插件"><a href="#elasticsearch-feature-vector-scoring插件" class="headerlink" title="elasticsearch-feature-vector-scoring插件"></a>elasticsearch-feature-vector-scoring插件</h2><p>这是我自己写的一个插件，具体的使用可以看下<a href="https://github.com/ginobefun/elasticsearch-feature-vector-scoring" target="_blank" rel="external">项目主页</a>，其核心也就一个类，我将其主要的代码和注释贴一下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FeatureVectorScoringSearchScript</span> <span class="keyword">extends</span> <span class="title">AbstractSearchScript</span> </span>&#123;</div><div class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> ESLogger LOGGER = Loggers.getLogger(<span class="string">"feature-vector-scoring"</span>);</div><div class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String SCRIPT_NAME = <span class="string">"feature_vector_scoring_script"</span>;</div><div class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">double</span> DEFAULT_BASE_CONSTANT = <span class="number">1.0</span>D;</div><div class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">double</span> DEFAULT_FACTOR_CONSTANT = <span class="number">1.0</span>D;</div><div class="line"></div><div class="line">    <span class="comment">// field in index to store feature vector</span></div><div class="line">    <span class="keyword">private</span> String field;</div><div class="line"></div><div class="line">    <span class="comment">// version of feature vector, if it isn't null, it should match version of index</span></div><div class="line">    <span class="keyword">private</span> String version;</div><div class="line"></div><div class="line">    <span class="comment">// final_score = baseConstant + factorConstant * cos(X, Y)</span></div><div class="line">    <span class="keyword">private</span> <span class="keyword">double</span> baseConstant;</div><div class="line"></div><div class="line">    <span class="comment">// final_score = baseConstant + factorConstant * cos(X, Y)</span></div><div class="line">    <span class="keyword">private</span> <span class="keyword">double</span> factorConstant;</div><div class="line"></div><div class="line">    <span class="comment">// input feature vector</span></div><div class="line">    <span class="keyword">private</span> <span class="keyword">double</span>[] inputFeatureVector;</div><div class="line"></div><div class="line">    <span class="comment">// cos(X, Y) = Σ(Xi * Yi) / ( sqrt(Σ(Xi * Xi)) * sqrt(Σ(Yi * Yi)) )</span></div><div class="line">    <span class="comment">// the inputFeatureVectorNorm is sqrt(Σ(Xi * Xi))</span></div><div class="line">    <span class="keyword">private</span> <span class="keyword">double</span> inputFeatureVectorNorm;</div><div class="line"></div><div class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">ScriptFactory</span> <span class="keyword">implements</span> <span class="title">NativeScriptFactory</span> </span>&#123;</div><div class="line">        <span class="meta">@Override</span></div><div class="line">        <span class="function"><span class="keyword">public</span> ExecutableScript <span class="title">newScript</span><span class="params">(@Nullable Map&lt;String, Object&gt; params)</span> <span class="keyword">throws</span> ScriptException </span>&#123;</div><div class="line">            <span class="keyword">return</span> <span class="keyword">new</span> FeatureVectorScoringSearchScript(params);</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="meta">@Override</span></div><div class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">needsScores</span><span class="params">()</span> </span>&#123;</div><div class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">private</span> <span class="title">FeatureVectorScoringSearchScript</span><span class="params">(Map&lt;String, Object&gt; params)</span> <span class="keyword">throws</span> ScriptException </span>&#123;</div><div class="line">        <span class="keyword">this</span>.field = (String) params.get(<span class="string">"field"</span>);</div><div class="line">        String inputFeatureVectorStr = (String) params.get(<span class="string">"inputFeatureVector"</span>);</div><div class="line">        <span class="keyword">if</span> (<span class="keyword">this</span>.field == <span class="keyword">null</span> || inputFeatureVectorStr == <span class="keyword">null</span> || inputFeatureVectorStr.trim().length() == <span class="number">0</span>) &#123;</div><div class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> ScriptException(<span class="string">"Initialize script "</span> + SCRIPT_NAME + <span class="string">" failed!"</span>);</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="keyword">this</span>.version = (String) params.get(<span class="string">"version"</span>);</div><div class="line">        <span class="keyword">this</span>.baseConstant = params.get(<span class="string">"baseConstant"</span>) != <span class="keyword">null</span> ? Double.parseDouble(params.get(<span class="string">"baseConstant"</span>).toString()) : DEFAULT_BASE_CONSTANT;</div><div class="line">        <span class="keyword">this</span>.factorConstant = params.get(<span class="string">"factorConstant"</span>) != <span class="keyword">null</span> ? Double.parseDouble(params.get(<span class="string">"factorConstant"</span>).toString()) : DEFAULT_FACTOR_CONSTANT;</div><div class="line"></div><div class="line">        String[] inputFeatureVectorArr = inputFeatureVectorStr.split(<span class="string">","</span>);</div><div class="line">        <span class="keyword">int</span> dimension = inputFeatureVectorArr.length;</div><div class="line">        <span class="keyword">double</span> sumOfSquare = <span class="number">0.0</span>D;</div><div class="line">        <span class="keyword">this</span>.inputFeatureVector = <span class="keyword">new</span> <span class="keyword">double</span>[dimension];</div><div class="line">        <span class="keyword">double</span> temp;</div><div class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> index = <span class="number">0</span>; index &lt; dimension; index++) &#123;</div><div class="line">            temp = Double.parseDouble(inputFeatureVectorArr[index].trim());</div><div class="line">            <span class="keyword">this</span>.inputFeatureVector[index] = temp;</div><div class="line">            sumOfSquare += temp * temp;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="keyword">this</span>.inputFeatureVectorNorm = Math.sqrt(sumOfSquare);</div><div class="line">        LOGGER.debug(<span class="string">"FeatureVectorScoringSearchScript.init, version:&#123;&#125;, norm:&#123;&#125;, baseConstant:&#123;&#125;, factorConstant:&#123;&#125;."</span></div><div class="line">                , <span class="keyword">this</span>.version, <span class="keyword">this</span>.inputFeatureVectorNorm, <span class="keyword">this</span>.baseConstant, <span class="keyword">this</span>.factorConstant);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="meta">@Override</span></div><div class="line">    <span class="function"><span class="keyword">public</span> Object <span class="title">run</span><span class="params">()</span> </span>&#123;</div><div class="line">        <span class="keyword">if</span> (<span class="keyword">this</span>.inputFeatureVectorNorm == <span class="number">0</span>) &#123;</div><div class="line">            <span class="keyword">return</span> <span class="keyword">this</span>.baseConstant;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="keyword">if</span> (!doc().containsKey(<span class="keyword">this</span>.field) || doc().get(<span class="keyword">this</span>.field) == <span class="keyword">null</span>) &#123;</div><div class="line">            LOGGER.error(<span class="string">"cannot find field &#123;&#125;."</span>, field);</div><div class="line">            <span class="keyword">return</span> <span class="keyword">this</span>.baseConstant;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        String docFeatureVectorStr = ((ScriptDocValues.Strings) doc().get(<span class="keyword">this</span>.field)).getValue();</div><div class="line">        <span class="keyword">return</span> calculateScore(docFeatureVectorStr);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">double</span> <span class="title">calculateScore</span><span class="params">(String docFeatureVectorStr)</span> </span>&#123;</div><div class="line">        <span class="comment">// 1. check docFeatureVector</span></div><div class="line">        <span class="keyword">if</span> (docFeatureVectorStr == <span class="keyword">null</span>) &#123;</div><div class="line">            <span class="keyword">return</span> <span class="keyword">this</span>.baseConstant;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        docFeatureVectorStr = docFeatureVectorStr.trim();</div><div class="line">        <span class="keyword">if</span> (docFeatureVectorStr.isEmpty()) &#123;</div><div class="line">            <span class="keyword">return</span> <span class="keyword">this</span>.baseConstant;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="comment">// 2. check version and get feature vector array of document</span></div><div class="line">        String[] docFeatureVectorArr;</div><div class="line">        <span class="keyword">if</span> (<span class="keyword">this</span>.version != <span class="keyword">null</span>) &#123;</div><div class="line">            String versionPrefix = version + <span class="string">"|"</span>;</div><div class="line">            <span class="keyword">if</span> (!docFeatureVectorStr.startsWith(versionPrefix)) &#123;</div><div class="line">                <span class="keyword">return</span> <span class="keyword">this</span>.baseConstant;</div><div class="line">            &#125;</div><div class="line"></div><div class="line">            docFeatureVectorArr = docFeatureVectorStr.substring(versionPrefix.length()).split(<span class="string">","</span>);</div><div class="line">        &#125; <span class="keyword">else</span> &#123;</div><div class="line">            docFeatureVectorArr = docFeatureVectorStr.split(<span class="string">","</span>);</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="comment">// 3. check the dimension of input and document</span></div><div class="line">        <span class="keyword">int</span> dimension = <span class="keyword">this</span>.inputFeatureVector.length;</div><div class="line">        <span class="keyword">if</span> (docFeatureVectorArr == <span class="keyword">null</span> || docFeatureVectorArr.length != dimension) &#123;</div><div class="line">            <span class="keyword">return</span> <span class="keyword">this</span>.baseConstant;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="comment">// 4. calculate the relevance score of the two feature vector</span></div><div class="line">        <span class="keyword">double</span> sumOfSquare = <span class="number">0.0</span>D;</div><div class="line">        <span class="keyword">double</span> sumOfProduct = <span class="number">0.0</span>D;</div><div class="line">        <span class="keyword">double</span> tempValueInDouble;</div><div class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; dimension; i++) &#123;</div><div class="line">            tempValueInDouble = Double.parseDouble(docFeatureVectorArr[i].trim());</div><div class="line">            sumOfProduct += tempValueInDouble * <span class="keyword">this</span>.inputFeatureVector[i];</div><div class="line">            sumOfSquare += tempValueInDouble * tempValueInDouble;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="keyword">if</span> (sumOfSquare == <span class="number">0</span>) &#123;</div><div class="line">            <span class="keyword">return</span> <span class="keyword">this</span>.baseConstant;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="keyword">double</span> cosScore = sumOfProduct / (Math.sqrt(sumOfSquare) * inputFeatureVectorNorm);</div><div class="line">        <span class="keyword">return</span> <span class="keyword">this</span>.baseConstant + <span class="keyword">this</span>.factorConstant * cosScore;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h1 id="总结与后续改进"><a href="#总结与后续改进" class="headerlink" title="总结与后续改进"></a>总结与后续改进</h1><ul>
<li>基于word2vec、Elasticsearch和自定义的脚本插件，我们就实现了一个个性化的搜索服务，相对于原有的实现，新版的点击率和转化率都有大幅的提升；</li>
<li>基于word2vec的商品向量还有一个可用之处，就是可以用来实现相似商品的推荐；</li>
<li>但是以我个人的理解，使用word2vec来实现个性化搜索或个性化推荐是有一定局限性的，因为它只能处理用户点击历史这样的时序数据，而无法全面的去考虑用户偏好，这个还是有很大的改进和提升的空间；</li>
<li>后续的话我们会更多的参考业界的做法，更多地更全面地考虑用户的偏好，另外还需要考虑时效性的问题，以优化商品排序和推荐。</li>
</ul>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul>
<li><a href="https://en.wikipedia.org/wiki/Personalized_search" target="_blank" rel="external">Personalized search Wiki</a></li>
<li><a href="http://blog.csdn.net/soso_blog/article/details/6050346" target="_blank" rel="external">搜索下一站：个性化搜索基本方法和简单实验</a></li>
<li><a href="http://www.infoq.com/cn/presentations/jingdong-personalized-search-engine-based-on-big-data-technology" target="_blank" rel="external">京东基于大数据技术的个性化电商搜索引擎</a></li>
<li><a href="https://www.zhihu.com/question/35574888" target="_blank" rel="external">淘宝为什么还不能实现个性化推荐和搜索？</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在&lt;a href=&quot;http://ginobefunny.com/post/learning_word2vec/&quot;&gt;word2vec学习小记&lt;/a&gt;一文中我们曾经学习了word2vec这个工具，它基于神经网络语言模型并在其基础上进行优化，最终能获取词向量和语言模型。在我们的商品搜索系统里，采用了word2vec的方式来计算用户向量和商品向量，并通过Elasticsearch的function_score评分机制和自定义的脚本插件来实现个性化搜索。&lt;/p&gt;
    
    </summary>
    
      <category term="Elasticsearch" scheme="http://ginobefunny.com/categories/Elasticsearch/"/>
    
    
      <category term="Elasticsearch" scheme="http://ginobefunny.com/tags/Elasticsearch/"/>
    
      <category term="word2vec" scheme="http://ginobefunny.com/tags/word2vec/"/>
    
      <category term="个性化" scheme="http://ginobefunny.com/tags/%E4%B8%AA%E6%80%A7%E5%8C%96/"/>
    
      <category term="搜索" scheme="http://ginobefunny.com/tags/%E6%90%9C%E7%B4%A2/"/>
    
      <category term="推荐" scheme="http://ginobefunny.com/tags/%E6%8E%A8%E8%8D%90/"/>
    
  </entry>
  
  <entry>
    <title>基于Elasticsearch实现搜索推荐</title>
    <link href="http://ginobefunny.com/post/search_recommendation_implemention_based_elasticsearch/"/>
    <id>http://ginobefunny.com/post/search_recommendation_implemention_based_elasticsearch/</id>
    <published>2017-03-21T09:18:39.000Z</published>
    <updated>2017-03-24T01:32:41.619Z</updated>
    
    <content type="html"><![CDATA[<p>在<a href="http://ginobefunny.com/post/search_suggestion_implemention_based_elasticsearch/">基于Elasticsearch实现搜索建议</a>一文中我们曾经介绍过如何基于Elasticsearch来实现搜索建议，而本文是在此基础上进一步优化搜索体验，在当搜索无结果或结果过少时提供推荐搜索词给用户。</p>
<a id="more"></a>
<h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h1><p>在根据用户输入和筛选条件进行搜索后，有时返回的是无结果或者结果很少的情况，为了提升用户搜索体验，需要能够给用户推荐一些相关的搜索词，比如用户搜索【迪奥】时没有找到相关的商品，可以推荐搜索【香水】、【眼镜】等关键词。</p>
<h1 id="设计思路"><a href="#设计思路" class="headerlink" title="设计思路"></a>设计思路</h1><p>首先需要分析搜索无结果或者结果过少可能的原因，我总结了一下，主要包括主要可能：</p>
<ol>
<li>搜索的关键词在本网不存在，比如【迪奥】；</li>
<li>搜索的关键词在本网的商品很少，比如【科比】；</li>
<li>搜索的关键词拼写有问题，比如把【阿迪达斯】写成了【阿迪大斯】；</li>
<li>搜索的关键词过多，由于我们采用的是cross_fields，在一个商品内不可能包含所有的Term，导致无结果，比如【阿迪达斯 耐克 卫衣 运动鞋】；</li>
</ol>
<p>那么针对以上情况，可以采用以下方式进行处理：</p>
<ol>
<li>搜索的关键词在本网不存在，可以通过爬虫的方式获取相关知识，然后根据<strong>搜索建议词</strong>去提取，比如去百度百科的<a href="http://baike.baidu.com/item/%E8%BF%AA%E5%A5%A5/291012?sefr=cr" target="_blank" rel="external">迪奥</a>词条里就能提取出【香水】、【香氛】和【眼镜】等关键词；当然基于爬虫的知识可能存在偏差，此时需要能够有人工审核或人工更正的部分；</li>
<li>搜索的关键词在本网的商品很少，有两种解决思路，一种是通过方式1的爬虫去提取关键词，另外一种是通过返回商品的信息去聚合出关键词，如品牌、品类、风格、标签等，这里我们采用的是后者（在测试后发现后者效果更佳）；</li>
<li>搜索的关键词拼写有问题，这就需要<strong>拼写纠错</strong>出场了，先纠错然后根据纠错后的词去提供搜索推荐；</li>
<li>搜索的关键词过多，有两种解决思路，一种是识别关键词的类型，如是品牌、品类、风格还是性别，然后通过一定的组合策略来实现搜索推荐；另外一种则是根据用户的输入到搜索建议词里去匹配，设置最小匹配为一个匹配到一个Term即可，这种方式实现比较简单而且效果也不错，所以我们采用的是后者。</li>
</ol>
<p>所以，我们在实现搜索推荐的核心是之前讲到的搜索建议词，它提供了本网主要的关键词，另外一个很重要的是它本身包含了<strong>关联商品数的属性</strong>，这样就可以保证推荐给用户的关键词是可以搜索出结果的。</p>
<h1 id="实现细节"><a href="#实现细节" class="headerlink" title="实现细节"></a>实现细节</h1><h2 id="整体设计"><a href="#整体设计" class="headerlink" title="整体设计"></a>整体设计</h2><p>整体设计框架如下图所示：</p>
<p><img src="http://oi46mo3on.bkt.clouddn.com/11_es_suggestion/search_recommadation_overview.png" alt="搜索推荐整体设计"></p>
<h2 id="搜索建议词索引"><a href="#搜索建议词索引" class="headerlink" title="搜索建议词索引"></a>搜索建议词索引</h2><p>在<a href="http://ginobefunny.com/post/search_suggestion_implemention_based_elasticsearch/">基于Elasticsearch实现搜索建议</a>一文已有说明，请移步阅读。此次增加了一个keyword.keyword_lowercase的字段用于拼写纠错，这里列取相关字段的索引：</p>
<pre><code>PUT /suggest_index
{
  &quot;mappings&quot;: {
    &quot;suggest&quot;: {
      &quot;properties&quot;: {
        &quot;keyword&quot;: {
          &quot;fields&quot;: {
            &quot;keyword&quot;: {
              &quot;type&quot;: &quot;string&quot;,
              &quot;index&quot;: &quot;not_analyzed&quot;
            },
            &quot;keyword_lowercase&quot;: {
              &quot;type&quot;: &quot;string&quot;,
              &quot;analyzer&quot;: &quot;lowercase_keyword&quot;
            },
            &quot;keyword_ik&quot;: {
              &quot;type&quot;: &quot;string&quot;,
              &quot;analyzer&quot;: &quot;ik_smart&quot;
            },
            &quot;keyword_pinyin&quot;: {
              &quot;type&quot;: &quot;string&quot;,
              &quot;analyzer&quot;: &quot;pinyin_analyzer&quot;
            },
            &quot;keyword_first_py&quot;: {
              &quot;type&quot;: &quot;string&quot;,
              &quot;analyzer&quot;: &quot;pinyin_first_letter_keyword_analyzer&quot;
            }
          },
          &quot;type&quot;: &quot;multi_field&quot;
        },
        &quot;type&quot;: {
          &quot;type&quot;: &quot;long&quot;
        },
        &quot;weight&quot;: {
          &quot;type&quot;: &quot;long&quot;
        },
        &quot;count&quot;: {
          &quot;type&quot;: &quot;long&quot;
        }
      }
    }
  }
}
</code></pre><h2 id="商品数据索引"><a href="#商品数据索引" class="headerlink" title="商品数据索引"></a>商品数据索引</h2><p>这里只列取相关字段的mapping：</p>
<pre><code>PUT /product_index
{
  &quot;mappings&quot;: {
    &quot;product&quot;: {
      &quot;properties&quot;: {
        &quot;productSkn&quot;: {
          &quot;type&quot;: &quot;long&quot;
        },
        &quot;productName&quot;: {
          &quot;type&quot;: &quot;string&quot;,
          &quot;analyzer&quot;: &quot;ik_smart&quot;
        },
        &quot;brandName&quot;: {
          &quot;type&quot;: &quot;string&quot;,
          &quot;analyzer&quot;: &quot;ik_smart&quot;
        },
        &quot;sortName&quot;: {
          &quot;type&quot;: &quot;string&quot;,
          &quot;analyzer&quot;: &quot;ik_smart&quot;
        },
        &quot;style&quot;: {
          &quot;type&quot;: &quot;string&quot;,
          &quot;analyzer&quot;: &quot;ik_smart&quot;
        }
      }
    }
  }
}
</code></pre><h2 id="关键词映射索引"><a href="#关键词映射索引" class="headerlink" title="关键词映射索引"></a>关键词映射索引</h2><p>主要就是source和dest直接的映射关系。</p>
<pre><code>PUT /conversion_index
{
  &quot;mappings&quot;: {
    &quot;conversion&quot;: {
      &quot;properties&quot;: {
        &quot;source&quot;: {
          &quot;type&quot;: &quot;string&quot;,
          &quot;analyzer&quot;: &quot;lowercase_keyword&quot;
        },
        &quot;dest&quot;: {
          &quot;type&quot;: &quot;string&quot;,
          &quot;index&quot;: &quot;not_analyzed&quot;
        }
      }
    }
  }
}
</code></pre><h2 id="爬虫数据爬取"><a href="#爬虫数据爬取" class="headerlink" title="爬虫数据爬取"></a>爬虫数据爬取</h2><p>在实现的时候，我们主要是爬取了百度百科上面的词条，在实际的实现中又分为了全量爬虫和增加爬虫。</p>
<h3 id="全量爬虫"><a href="#全量爬虫" class="headerlink" title="全量爬虫"></a>全量爬虫</h3><p>全量爬虫我这边是从网上下载了一份<a href="https://pan.baidu.com/s/1gfcHXvX" target="_blank" rel="external">他人汇总的词条URL资源</a>，里面根据一级分类包含多个目录，每个目录又根据二级分类包含多个词条，每一行的内容的格式如下：</p>
<pre><code>李宁!http://baike.baidu.com/view/10670.html?fromTaglist
diesel!http://baike.baidu.com/view/394305.html?fromTaglist
ONLY!http://baike.baidu.com/view/92541.html?fromTaglist
lotto!http://baike.baidu.com/view/907709.html?fromTaglist
</code></pre><p>这样在启动的时候我们就可以使用多线程甚至分布式的方式爬虫自己感兴趣的词条内容作为初始化数据保持到爬虫数据表。为了保证幂等性，如果再次全量爬取时就需要排除掉数据库里已有的词条。</p>
<h3 id="增量爬虫"><a href="#增量爬虫" class="headerlink" title="增量爬虫"></a>增量爬虫</h3><ol>
<li>在商品搜索接口中，如果搜索某个关键词关联的商品数为0或小于一定的阈值（如20条），就通过Redis的ZSet进行按天统计；</li>
<li>统计的时候是区分搜索无结果和结果过少两个Key的，因为两种情况实际上是有所区别的，而且后续在搜索推荐查询时也有用到这个统计结果；</li>
<li>增量爬虫是每天凌晨运行，根据前一天统计的关键词进行爬取，爬取前需要排除掉已经爬过的关键词和黑名单中的关键词；</li>
<li>所谓黑名单的数据包含两种：一种是每天增量爬虫失败的关键字（一般会重试几次，确保失败后加入黑名单），一种是人工维护的确定不需要爬虫的关键词；</li>
</ol>
<h2 id="爬虫数据关键词提取"><a href="#爬虫数据关键词提取" class="headerlink" title="爬虫数据关键词提取"></a>爬虫数据关键词提取</h2><ol>
<li>首先需要明确关键词的范围，这里我们采用的是suggest中类型为品牌、品类、风格、款式的词作为关键词；</li>
<li>关键词提取的核心步骤就是对爬虫内容和关键词分别分词，然后进行分词匹配，看该爬虫数据是否包含关键词的所有Term（如果就是一个Term就直接判断包含就好了）；在处理的时候还可以对匹配到关键词的次数进行排序，最终的结果就是一个key-value的映射，如{迪奥 -&gt; [香水,香氛,时装,眼镜], 纪梵希 -&gt; [香水,时装,彩妆,配饰,礼服]}；</li>
</ol>
<h2 id="管理关键词映射"><a href="#管理关键词映射" class="headerlink" title="管理关键词映射"></a>管理关键词映射</h2><ol>
<li>由于爬虫数据提取的关键词是和词条的内容相关联的，因此很有可能提取的关键词效果不大好，因此就需要人工管理；</li>
<li>管理动作主要是包括添加、修改和置失效关键词映射，然后增量地更新到conversion_index索引中；</li>
</ol>
<h2 id="搜索推荐服务的实现"><a href="#搜索推荐服务的实现" class="headerlink" title="搜索推荐服务的实现"></a>搜索推荐服务的实现</h2><ol>
<li>首先如果对搜索推荐的入口进行判断，一些非法的情况不进行推荐（比如关键词太短或太长），另外由于搜索推荐并非核心功能，可以增加一个全局动态参数来控制是否进行搜索推荐；</li>
<li>在<strong>设计思路</strong>里面我们分析过可能有4中场景需要搜索推荐，如何高效、快速地找到具体的场景从而减少不必要的查询判断是推荐服务实现的关键；这个在设计的时候就需要综合权衡，我们通过一段时间的观察后，目前采用的逻辑的伪代码如下：</li>
</ol>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> JSONObject <span class="title">recommend</span><span class="params">(SearchResult searchResult, String queryWord)</span> </span>&#123;</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">        String keywordsToSearch = queryWord;</div><div class="line"></div><div class="line">        <span class="comment">// 搜索推荐分两部分</span></div><div class="line">        <span class="comment">// 1) 第一部分是最常见的情况，包括有结果、根据SKN搜索、关键词未出现在空结果Redis ZSet里</span></div><div class="line">        <span class="keyword">if</span> (containsProductInSearchResult(searchResult)) &#123;</div><div class="line">            <span class="comment">// 1.1） 搜索有结果的 优先从搜索结果聚合出品牌等关键词进行查询</span></div><div class="line">            String aggKeywords = aggKeywordsByProductList(searchResult);</div><div class="line">            keywordsToSearch = queryWord + <span class="string">" "</span> + aggKeywords;</div><div class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (isQuerySkn(queryWord)) &#123;</div><div class="line">            <span class="comment">// 1.2） 如果是查询SKN 没有查询到的 后续的逻辑也无法推荐 所以直接到ES里去获取关键词</span></div><div class="line">            keywordsToSearch = aggKeywordsBySkns(queryWord);</div><div class="line">            <span class="keyword">if</span> (StringUtils.isEmpty(keywordsToSearch)) &#123;</div><div class="line">                <span class="keyword">return</span> defaultSuggestRecommendation();</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        Double count = searchKeyWordService.getKeywordCount(RedisKeys.SEARCH_KEYWORDS_EMPTY, queryWord);</div><div class="line">        <span class="keyword">if</span> (count == <span class="keyword">null</span> || queryWord.length() &gt;= <span class="number">5</span>) &#123;</div><div class="line">            <span class="comment">// 1.3) 如果该关键词一次都没有出现在空结果列表或者长度大于5 则该词很有可能是可以搜索出结果的</span></div><div class="line">            <span class="comment">//      因此优先取suggest_index去搜索一把 减少后面的查询动作</span></div><div class="line">            JSONObject recommendResult = recommendBySuggestIndex(queryWord, keywordsToSearch, <span class="keyword">false</span>);</div><div class="line">            <span class="keyword">if</span> (isNotEmptyResult(recommendResult)) &#123;</div><div class="line">                <span class="keyword">return</span> recommendResult;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="comment">// 2) 第二部分是通过Conversion和拼写纠错去获取关键词 由于很多品牌的拼写可能比较相近 因此先走Conversion然后再拼写检查</span></div><div class="line">        String spellingCorrentWord = <span class="keyword">null</span>, dest = <span class="keyword">null</span>;</div><div class="line">        <span class="keyword">if</span> (allowGetingDest(queryWord) &amp;&amp; StringUtils.isNotEmpty((dest = getSuggestConversionDestBySource(queryWord)))) &#123;</div><div class="line">            <span class="comment">// 2.1) 爬虫和自定义的Conversion处理</span></div><div class="line">            keywordsToSearch = dest;</div><div class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (allowSpellingCorrent(queryWord) </div><div class="line">		         &amp;&amp; StringUtils.isNotEmpty((spellingCorrentWord = suggestService.getSpellingCorrectKeyword(queryWord)))) &#123;</div><div class="line">            <span class="comment">// 2.2) 执行拼写检查 由于在搜索建议的时候会进行拼写检查 所以缓存命中率高</span></div><div class="line">            keywordsToSearch = spellingCorrentWord;</div><div class="line">        &#125; <span class="keyword">else</span> &#123;</div><div class="line">            <span class="comment">// 2.3) 如果两者都没有 则直接返回</span></div><div class="line">            <span class="keyword">return</span> defaultSuggestRecommendation();</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        JSONObject recommendResult = recommendBySuggestIndex(queryWord, keywordsToSearch, dest != <span class="keyword">null</span>);</div><div class="line">        <span class="keyword">return</span> isNotEmptyResult(recommendResult) ? recommendResult : defaultSuggestRecommendation();</div><div class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</div><div class="line">        logger.error(<span class="string">"[func=recommend][queryWord="</span> + queryWord + <span class="string">"]"</span>, e);</div><div class="line">        <span class="keyword">return</span> defaultSuggestRecommendation();</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>其中涉及到的几个函数简单说明下：</p>
<ul>
<li>aggKeywordsByProductList方法用商品列表的结果，聚合出出现次数最多的几个品牌和品类（比如各2个），这样我们就可以得到4个关键词，和原先用户的输入拼接后调用recommendBySuggestIndex获取推荐词；</li>
<li>aggKeywordsBySkns方法是根据用户输入的SKN先到product_index索引获取商品列表，然后再调用aggKeywordsByProductList去获取品牌和品类的关键词列表；</li>
<li>getSuggestConversionDestBySource方法是查询conversion_index索引去获取关键词提取的结果，这里在调用recommendBySuggestIndex时有个参数，该参数主要是用于处理是否限制只能是输入的关键词；</li>
<li>getSpellingCorrectKeyword方法为拼写检查，在调用suggest_index处理时有个地方需要注意一下，拼写检查是基于编辑距离的，大小写不一致的情况会导致Elasticsearch Suggester无法得到正确的拼写建议，因此在处理时需要两边都转换为小写后进行拼写检查；</li>
<li>最终都需要调用recommendBySuggestIndex方法获取搜索推荐，因为通过suggest_index索引可以确保推荐出去的词是有意义的且关联到商品的。该方法核心逻辑的伪代码如下：</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">private</span> JSONObject <span class="title">recommendBySuggestIndex</span><span class="params">(String srcQueryWord, String keywordsToSearch, <span class="keyword">boolean</span> isLimitKeywords)</span> </span>&#123;</div><div class="line">    <span class="comment">// 1) 先对keywordsToSearch进行分词</span></div><div class="line">    List&lt;String&gt; terms = <span class="keyword">null</span>;</div><div class="line">    <span class="keyword">if</span> (isLimitKeywords) &#123;</div><div class="line">        terms = Arrays.stream(keywordsToSearch.split(<span class="string">","</span>)).filter(term -&gt; term != <span class="keyword">null</span> &amp;&amp; term.length() &gt; <span class="number">1</span>)</div><div class="line">                      .distinct().collect(Collectors.toList());</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">        terms = searchAnalyzeService.getAnalyzeTerms(keywordsToSearch, <span class="string">"ik_smart"</span>);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">if</span> (CollectionUtils.isEmpty(terms)) &#123;</div><div class="line">        <span class="keyword">return</span> <span class="keyword">new</span> JSONObject();</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">// 2) 根据terms搜索构造搜索请求</span></div><div class="line">    SearchParam searchParam = <span class="keyword">new</span> SearchParam();</div><div class="line">    searchParam.setPage(<span class="number">1</span>);</div><div class="line">    searchParam.setSize(<span class="number">3</span>);</div><div class="line"></div><div class="line">    <span class="comment">// 2.1) 构建FunctionScoreQueryBuilder</span></div><div class="line">    QueryBuilder queryBuilder = isLimitKeywords ? buildQueryBuilderByLimit(terms)</div><div class="line">                                  : buildQueryBuilder(keywordsToSearch, terms);</div><div class="line">    searchParam.setQuery(queryBuilder);</div><div class="line">    </div><div class="line">    <span class="comment">// 2.2) 设置过滤条件</span></div><div class="line">    BoolQueryBuilder boolFilter = QueryBuilders.boolQuery();</div><div class="line">    boolFilter.must(QueryBuilders.rangeQuery(<span class="string">"count"</span>).gte(<span class="number">20</span>));</div><div class="line">    boolFilter.mustNot(QueryBuilders.termQuery(<span class="string">"keyword.keyword_lowercase"</span>, srcQueryWord.toLowerCase()));</div><div class="line">    <span class="keyword">if</span> (isLimitKeywords) &#123;</div><div class="line">        boolFilter.must(QueryBuilders.termsQuery(<span class="string">"keyword.keyword_lowercase"</span>, terms.stream()</div><div class="line">            .map(String::toLowerCase).collect(Collectors.toList())));</div><div class="line">    &#125;</div><div class="line">    searchParam.setFiter(boolFilter);</div><div class="line"></div><div class="line">    <span class="comment">// 2.3) 按照得分、权重、数量的规则降序排序</span></div><div class="line">    List&lt;SortBuilder&gt; sortBuilders = <span class="keyword">new</span> ArrayList&lt;&gt;(<span class="number">3</span>);</div><div class="line">    sortBuilders.add(SortBuilders.fieldSort(<span class="string">"_score"</span>).order(SortOrder.DESC));</div><div class="line">    sortBuilders.add(SortBuilders.fieldSort(<span class="string">"weight"</span>).order(SortOrder.DESC));</div><div class="line">    sortBuilders.add(SortBuilders.fieldSort(<span class="string">"count"</span>).order(SortOrder.DESC));</div><div class="line">    searchParam.setSortBuilders(sortBuilders);</div><div class="line"></div><div class="line">    <span class="comment">// 4) 先从缓存中获取</span></div><div class="line">    <span class="keyword">final</span> String indexName = SearchConstants.INDEX_NAME_SUGGEST;</div><div class="line">    JSONObject suggestResult = searchCacheService.getJSONObjectFromCache(indexName, searchParam);</div><div class="line">    <span class="keyword">if</span> (suggestResult != <span class="keyword">null</span>) &#123;</div><div class="line">        <span class="keyword">return</span> suggestResult;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">// 5) 调用ES执行搜索</span></div><div class="line">    SearchResult searchResult = searchCommonService.doSearch(indexName, searchParam);</div><div class="line"></div><div class="line">    <span class="comment">// 6) 构建结果加入缓存</span></div><div class="line">    suggestResult = <span class="keyword">new</span> JSONObject();</div><div class="line">    List&lt;String&gt; resultTerms = searchResult.getResultList().stream()</div><div class="line">            .map(map -&gt; (String) map.get(<span class="string">"keyword"</span>)).collect(Collectors.toList());</div><div class="line">    suggestResult.put(<span class="string">"search_recommendation"</span>, resultTerms);</div><div class="line">    searchCacheService.addJSONObjectToCache(indexName, searchParam, suggestResult);</div><div class="line">    <span class="keyword">return</span> suggestResult;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">private</span> QueryBuilder <span class="title">buildQueryBuilderByLimit</span><span class="params">(List&lt;String&gt; terms)</span> </span>&#123;</div><div class="line">    FunctionScoreQueryBuilder functionScoreQueryBuilder</div><div class="line">        = <span class="keyword">new</span> FunctionScoreQueryBuilder(QueryBuilders.matchAllQuery());</div><div class="line"></div><div class="line">    <span class="comment">// 给品类类型的关键词加分</span></div><div class="line">    functionScoreQueryBuilder.add(QueryBuilders.termQuery(<span class="string">"type"</span>, Integer.valueOf(<span class="number">2</span>)),</div><div class="line">        ScoreFunctionBuilders.weightFactorFunction(<span class="number">3</span>));</div><div class="line"></div><div class="line">    <span class="comment">// 按词出现的顺序加分</span></div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; terms.size(); i++) &#123;</div><div class="line">        functionScoreQueryBuilder.add(QueryBuilders.termQuery(<span class="string">"keyword.keyword_lowercase"</span>, </div><div class="line">   terms.get(i).toLowerCase()),</div><div class="line">            ScoreFunctionBuilders.weightFactorFunction(terms.size() - i));</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    functionScoreQueryBuilder.boostMode(CombineFunction.SUM);</div><div class="line">    <span class="keyword">return</span> functionScoreQueryBuilder;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">private</span> QueryBuilder <span class="title">buildQueryBuilder</span><span class="params">(String keywordsToSearch, Set&lt;String&gt; termSet)</span> </span>&#123;</div><div class="line">    <span class="comment">// 1) 对于suggest的multi-fields至少要有一个字段匹配到 匹配得分为常量1</span></div><div class="line">    MultiMatchQueryBuilder queryBuilder = QueryBuilders.multiMatchQuery(keywordsToSearch.toLowerCase(),</div><div class="line">            <span class="string">"keyword.keyword_ik"</span>, <span class="string">"keyword.keyword_pinyin"</span>, </div><div class="line">            <span class="string">"keyword.keyword_first_py"</span>, <span class="string">"keyword.keyword_lowercase"</span>)</div><div class="line">        .analyzer(<span class="string">"ik_smart"</span>)</div><div class="line">        .type(MultiMatchQueryBuilder.Type.BEST_FIELDS)</div><div class="line">        .operator(MatchQueryBuilder.Operator.OR)</div><div class="line">        .minimumShouldMatch(<span class="string">"1"</span>);</div><div class="line"></div><div class="line">    FunctionScoreQueryBuilder functionScoreQueryBuilder</div><div class="line">        = <span class="keyword">new</span> FunctionScoreQueryBuilder(QueryBuilders.constantScoreQuery(queryBuilder));</div><div class="line">			</div><div class="line">    <span class="keyword">for</span> (String term : termSet) &#123;</div><div class="line">        <span class="comment">// 2) 对于完全匹配Term的加1分</span></div><div class="line">        functionScoreQueryBuilder.add(QueryBuilders.termQuery(<span class="string">"keyword.keyword_lowercase"</span>, term.toLowerCase()),</div><div class="line">            ScoreFunctionBuilders.weightFactorFunction(<span class="number">1</span>));</div><div class="line"></div><div class="line">        <span class="comment">// 3) 对于匹配到一个Term的加2分</span></div><div class="line">        functionScoreQueryBuilder.add(QueryBuilders.termQuery(<span class="string">"keyword.keyword_ik"</span>, term),</div><div class="line">            ScoreFunctionBuilders.weightFactorFunction(<span class="number">2</span>));</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    functionScoreQueryBuilder.boostMode(CombineFunction.SUM);</div><div class="line">    <span class="keyword">return</span> functionScoreQueryBuilder;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>最后，从实际运行的统计来看，有90%以上的查询都能在1.3)的情况下返回推荐词，而这一部分还没有进行拼写纠错和conversion_index索引的查询，因此还是比较高效的；剩下的10%在最坏的情况且缓存都没有命中的情况下，最多还需要进行三次ES的查询，性能是比较差的，但是由于有缓存而且大部分的无结果的关键词都比较集中，因此也在可接受的范围，这一块可以考虑再增加一个动态参数，在大促的时候进行关闭处理。</p>
<h1 id="小结与后续改进"><a href="#小结与后续改进" class="headerlink" title="小结与后续改进"></a>小结与后续改进</h1><ul>
<li>通过以上的设计和实现，我们实现了一个效果不错的搜索推荐功能，线上使用效果如下：</li>
</ul>
<pre><code>//搜索【迪奥】，本站无该品牌商品
没有找到 &quot;迪奥&quot; 相关的商品， 为您推荐 &quot;香水&quot; 的搜索结果。或者试试 &quot;香氛&quot;  &quot;眼镜&quot; 

//搜索【puma 运动鞋 上衣】，关键词太多无法匹配
没有找到 &quot;puma 运动鞋 上衣&quot; 相关的商品， 为您推荐 &quot;PUMA 运动鞋&quot; 的搜索结果。或者试试 &quot;PUMA 运动鞋 女&quot;  &quot;PUMA 运动鞋 男&quot;

//搜索【puma 上衣】，结果太少
&quot;puma 上衣&quot; 搜索结果太少了，试试 &quot;上衣&quot;  &quot;PUMA&quot;  &quot;PUMA 休闲&quot; 关键词搜索

//搜索【51489312】特定的SKN，结果太少
&quot;51489312&quot; 搜索结果太少了，试试 &quot;夹克&quot;  &quot;PUMA&quot;  &quot;户外&quot; 关键词搜索

//搜索【blackjauk】，拼写错误
没有找到 &quot;blackjauk&quot; 相关的商品， 为您推荐 &quot;BLACKJACK&quot; 的搜索结果。或者试试 &quot;BLACKJACK T恤&quot;  &quot;BLACKJACK 休闲裤&quot; 
</code></pre><ul>
<li>后续考虑的改进包括：1.继续统计各种无结果或结果太少场景出现的频率和对应推荐词的实现，优化搜索推荐服务的效率；2.爬取更多的语料资源，提升Conversion的准确性；3.考虑增加个性化的功能，给用户推荐Ta最感兴趣的内容。</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在&lt;a href=&quot;http://ginobefunny.com/post/search_suggestion_implemention_based_elasticsearch/&quot;&gt;基于Elasticsearch实现搜索建议&lt;/a&gt;一文中我们曾经介绍过如何基于Elasticsearch来实现搜索建议，而本文是在此基础上进一步优化搜索体验，在当搜索无结果或结果过少时提供推荐搜索词给用户。&lt;/p&gt;
    
    </summary>
    
      <category term="Elasticsearch" scheme="http://ginobefunny.com/categories/Elasticsearch/"/>
    
    
      <category term="Elasticsearch" scheme="http://ginobefunny.com/tags/Elasticsearch/"/>
    
      <category term="搜索" scheme="http://ginobefunny.com/tags/%E6%90%9C%E7%B4%A2/"/>
    
      <category term="推荐" scheme="http://ginobefunny.com/tags/%E6%8E%A8%E8%8D%90/"/>
    
      <category term="拼写纠错" scheme="http://ginobefunny.com/tags/%E6%8B%BC%E5%86%99%E7%BA%A0%E9%94%99/"/>
    
  </entry>
  
  <entry>
    <title>一个简易的Elasticsearch动态同义词插件</title>
    <link href="http://ginobefunny.com/post/elasticsearch_dynamic_synonym_plugin/"/>
    <id>http://ginobefunny.com/post/elasticsearch_dynamic_synonym_plugin/</id>
    <published>2017-03-15T07:07:22.000Z</published>
    <updated>2017-03-16T02:47:30.376Z</updated>
    
    <content type="html"><![CDATA[<p>Elasticsearch自带了一个synonym同义词插件，但是该插件只能使用文件或在分析器中静态地配置同义词，如果需要添加或修改，需要修改配置文件和重启，使用方式不够友好。通过学习Elasticsearch的synonym代码，自研了一个可动态维护同义词的插件，并以运用于生产环境，供大家参考。</p>
<a id="more"></a>
<h1 id="Elasticsearch自带的SynonymTokenFilter"><a href="#Elasticsearch自带的SynonymTokenFilter" class="headerlink" title="Elasticsearch自带的SynonymTokenFilter"></a>Elasticsearch自带的SynonymTokenFilter</h1><p>Elasticsearch自带的同义词过滤器支持在分析器配置（使用synonyms参数）和文件中配置（使用synonyms_path参数）同义词，配置方式如下：</p>
<pre><code>{
    &quot;index&quot; : {
        &quot;analysis&quot; : {
            &quot;analyzer&quot; : {
                &quot;synonym_analyzer&quot; : {
                    &quot;tokenizer&quot; : &quot;whitespace&quot;,
                    &quot;filter&quot; : [&quot;my_synonym&quot;]
                }
            },
            &quot;filter&quot; : {
                &quot;my_synonym&quot; : {
                    &quot;type&quot; : &quot;synonym&quot;,
                    &quot;expand&quot;: true,
                    &quot;ignore_case&quot;: true, 
                    &quot;synonyms_path&quot; : &quot;analysis/synonym.txt&quot;
                    &quot;synonyms&quot; : [&quot;阿迪, 阿迪达斯, adidasi =&gt; Adidas&quot;,&quot;Nike, 耐克, naike&quot;]
                }
            }
        }
    }
}
</code></pre><p>在配置同义词规则时有<a href="https://www.elastic.co/guide/en/elasticsearch/reference/2.3/analysis-synonym-tokenfilter.html#_solr_synonyms" target="_blank" rel="external">Solr synonyms</a>和<a href="https://www.elastic.co/guide/en/elasticsearch/reference/2.3/analysis-synonym-tokenfilter.html#_wordnet_synonyms" target="_blank" rel="external">WordNet synonyms</a>，一般我们使用的都是Solr synonyms。在配置时又存在映射和对等两种方式，区别如下：</p>
<pre><code>// 精确映射同义词，【阿迪】、【阿迪达斯】和【adidasi】的token将会转换为【Adidas】存入倒排索引中
阿迪, 阿迪达斯, adidasi =&gt; Adidas

// 对等同义词
// 当expand为true时，当出现以下任何一个token，三个token都会存入倒排索引中
// 当expand为false时，当出现以下任何一个token，第一个token也就是【Nike】会存入倒排索引中
Nike, 耐克, naike
</code></pre><h1 id="DynamicSynonymTokenFilter"><a href="#DynamicSynonymTokenFilter" class="headerlink" title="DynamicSynonymTokenFilter"></a>DynamicSynonymTokenFilter</h1><h2 id="实现方式"><a href="#实现方式" class="headerlink" title="实现方式"></a>实现方式</h2><ul>
<li>DynamicSynonymTokenFilter参考了SynonymTokenFilter的方式，但又予以简化，使用一个HashMap来保存同义词之间的转换关系；</li>
<li>DynamicSynonymTokenFilter只支持Solr synonyms，同时也支持expand和ignore_case参数的配置；</li>
<li>DynamicSynonymTokenFilter通过数据库来管理同义词的配置，并轮询数据库（通过version字段判断是否存在规则变化）实现同义词的动态管理；</li>
</ul>
<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>1.下载插件源码</p>
<pre><code>git clone git@github.com:ginobefun/elasticsearch-dynamic-synonym.git
</code></pre><p>2.使用maven编译插件</p>
<pre><code>mvn clean install -DskipTests
</code></pre><p>3.在ES_HOME/plugin目录新建dynamic-synonym目录，并将target/releases/elasticsearch-dynamic-synonym-VERSION.zip文件解压到该目录</p>
<p>4.在MySQL中创建Elasticsearch同义词数据库并创建用户</p>
<pre><code>create database elasticsearch;
DROP TABLE IF EXISTS `dynamic_synonym_rule`;
CREATE TABLE `dynamic_synonym_rule` (
  `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `rule` varchar(255) NOT NULL,
  `status` tinyint(1) NOT NULL DEFAULT &apos;1&apos; COMMENT &apos;1: available, 0:unavailable&apos;,
  `version` int(11) NOT NULL,
  PRIMARY KEY (`id`),
  KEY `IDX_DYNAMIC_SYNONYM_VERSION` (`version`),
  KEY `IDX_DYNAMIC_SYNONYM_RULE` (`rule`)
) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=utf8;

-- ----------------------------
-- insert sample records
-- ----------------------------
INSERT INTO `dynamic_synonym_rule` VALUES (&apos;1&apos;, &apos;阿迪, 阿迪达斯, adidasi =&gt; Adidas&apos;, &apos;1&apos;, &apos;1&apos;);
INSERT INTO `dynamic_synonym_rule` VALUES (&apos;2&apos;, &apos;Nike, 耐克, naike&apos;, &apos;1&apos;, &apos;2&apos;);
</code></pre><p>5.重启Elasticsearch</p>
<h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><p>在Elasticsearch的elasticsearch.yml文件或在API创建索引时配置分析器和过滤器：</p>
<pre><code>index:
  analysis:
    filter:
      my_synonym:
        type: dynamic-synonym
        expand: true
        ignore_case: true
        tokenizer: whitespace
        db_url: jdbc:mysql://localhost:3306/elasticsearch?user=test_user&amp;password=test_pwd&amp;useUnicode=true&amp;characterEncoding=UTF8
    analyzer:
      analyzer_with_dynamic_synonym:
        type: custom
        tokenizer: whitespace
        filter: [&quot;my_synonym&quot;]
</code></pre><h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><p>测试分析器效果【阿迪】</p>
<pre><code>http://localhost:9200/test/_analyze?analyzer=analyzer_with_dynamic_synonym&amp;text=阿迪

{
  &quot;tokens&quot;: [
    {
      &quot;token&quot;: &quot;adidas&quot;,
      &quot;start_offset&quot;: 0,
      &quot;end_offset&quot;: 2,
      &quot;type&quot;: &quot;SYNONYM&quot;,
      &quot;position&quot;: 0
    }
  ]
}
</code></pre><p>测试分析器效果【耐克】</p>
<pre><code>http://localhost:9200/test/_analyze?analyzer=analyzer_with_dynamic_synonym&amp;text=耐克

{
  &quot;tokens&quot;: [
    {
      &quot;token&quot;: &quot;nike&quot;,
      &quot;start_offset&quot;: 0,
      &quot;end_offset&quot;: 2,
      &quot;type&quot;: &quot;SYNONYM&quot;,
      &quot;position&quot;: 0
    },
    {
      &quot;token&quot;: &quot;耐克&quot;,
      &quot;start_offset&quot;: 0,
      &quot;end_offset&quot;: 2,
      &quot;type&quot;: &quot;SYNONYM&quot;,
      &quot;position&quot;: 1
    },
    {
      &quot;token&quot;: &quot;naike&quot;,
      &quot;start_offset&quot;: 0,
      &quot;end_offset&quot;: 2,
      &quot;type&quot;: &quot;SYNONYM&quot;,
      &quot;position&quot;: 2
    }
  ]
}
</code></pre><p>往数据库中插入一条同义词，测试【范斯】</p>
<pre><code>INSERT INTO `dynamic_synonym_rule` VALUES (&apos;3&apos;, &apos;Vans, 范斯&apos;, &apos;1&apos;, &apos;3&apos;);

// wait for 2 minutes to reload 
[2017-03-15 15:52:28,895][INFO ][node                     ] [node-local] started
[2017-03-15 15:55:29,645][INFO ][dynamic-synonym          ] Start to reload synonym rule...
[2017-03-15 15:55:29,661][INFO ][dynamic-synonym          ] Succeed to reload 3 synonym rule!

http://localhost:9200/test/_analyze?analyzer=analyzer_with_dynamic_synonym&amp;text=范斯

{
  &quot;tokens&quot;: [
    {
      &quot;token&quot;: &quot;vans&quot;,
      &quot;start_offset&quot;: 0,
      &quot;end_offset&quot;: 2,
      &quot;type&quot;: &quot;SYNONYM&quot;,
      &quot;position&quot;: 0
    },
    {
      &quot;token&quot;: &quot;范斯&quot;,
      &quot;start_offset&quot;: 0,
      &quot;end_offset&quot;: 2,
      &quot;type&quot;: &quot;SYNONYM&quot;,
      &quot;position&quot;: 1
    }
  ]
}
</code></pre><h1 id="总结与后续改进"><a href="#总结与后续改进" class="headerlink" title="总结与后续改进"></a>总结与后续改进</h1><ul>
<li>通过学习Elasticsearch源码自己实现了一个简易版的同义词插件，通过同义词的配置可以实现同义词规则的增删改的动态更新；</li>
<li>需要注意的是，同义词的动态更新存在一个很重要的问题是原本在索引中已存在的数据不受同义词更新动态的影响，因此在使用时需要考虑是否可以容忍该问题，一个通常的做法是在某个时刻集中管理同义词，更新后执行索引重建动作；</li>
<li>另外该插件目前存在一个问题，就是同义词的映射关系在内存中是一个全局数据，因此如果有多个不同的同义词过滤器则会存在问题，代码初始化时以第一个成功初始化的过滤器生成的映射关系为准，这个后续版本考虑改进。</li>
</ul>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/guide/current/using-synonyms.html" target="_blank" rel="external">Using Synonyms</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/2.3/analysis-synonym-tokenfilter.html" target="_blank" rel="external">Synonym Token Filter</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Elasticsearch自带了一个synonym同义词插件，但是该插件只能使用文件或在分析器中静态地配置同义词，如果需要添加或修改，需要修改配置文件和重启，使用方式不够友好。通过学习Elasticsearch的synonym代码，自研了一个可动态维护同义词的插件，并以运用于生产环境，供大家参考。&lt;/p&gt;
    
    </summary>
    
      <category term="Elasticsearch" scheme="http://ginobefunny.com/categories/Elasticsearch/"/>
    
    
      <category term="Elasticsearch" scheme="http://ginobefunny.com/tags/Elasticsearch/"/>
    
      <category term="同义词" scheme="http://ginobefunny.com/tags/%E5%90%8C%E4%B9%89%E8%AF%8D/"/>
    
      <category term="synonym" scheme="http://ginobefunny.com/tags/synonym/"/>
    
      <category term="插件" scheme="http://ginobefunny.com/tags/%E6%8F%92%E4%BB%B6/"/>
    
  </entry>
  
  <entry>
    <title>阅读随手记 201703</title>
    <link href="http://ginobefunny.com/post/reading_record_201703/"/>
    <id>http://ginobefunny.com/post/reading_record_201703/</id>
    <published>2017-03-03T00:33:19.000Z</published>
    <updated>2017-03-29T08:49:59.077Z</updated>
    
    <content type="html"><![CDATA[<p>关键字：微服务, 架构, Event Sourcing, CQRS, Redis, TDD, 消息中间件, 缓存, RPC, 监控, 高性能, 高并发, 高可用, 机器学习, 深度学习, 人工智能。<br><a id="more"></a></p>
<h3 id="Martin-Fowler谈如何理解事件驱动和CQRS-Martin-Fowler-薛命灯"><a href="#Martin-Fowler谈如何理解事件驱动和CQRS-Martin-Fowler-薛命灯" class="headerlink" title="Martin Fowler谈如何理解事件驱动和CQRS Martin Fowler/薛命灯"></a><a href="http://mp.weixin.qq.com/s?__biz=MzA5Nzc4OTA1Mw==&amp;mid=2659599045&amp;idx=1&amp;sn=02aee31a7e947626df37165e3484953f" target="_blank" rel="external">Martin Fowler谈如何理解事件驱动和CQRS</a> Martin Fowler/薛命灯</h3><p>原文地址:<a href="https://martinfowler.com/articles/201701-event-driven.html" target="_blank" rel="external">https://martinfowler.com/articles/201701-event-driven.html</a></p>
<ul>
<li>为了帮助读者理解“事件驱动”的含义，软件大师Martin Fowler在他的博客上总结出了四种基于事件驱动的模型。</li>
<li>事件通知（Event Notification）：事件通知是最基本也是最简单的模型，当一个系统发生了变更，它会通过发送事件消息的形式通知其他系统，发送消息的系统不要求接收消息的系统返回任何响应，即使有响应返回，它也不对其进行任何处理。事件通知的好处在于它的简单性，并且有助于降低系统间的耦合性。不过太多的事件通知可能会带来问题，太多的事件难以跟踪，发生问题难以调试；另外通知事件不会包含太多的数据，额外的请求不仅会造成延迟；</li>
<li>事件传递状态转移（Event-Carried State Transfer）：它比事件通知更进一步，这个模型最大的特点是事件里包含了发生变更的数据。对于接收事件的系统来说，无需再次向源系统发起请求，从而降低了延迟。而且就算源系统宕机，也不会影响到后续的流程。不过，既然把变更数据放在事件里进行传输，那么占用更多的带宽是不可避免的了。</li>
<li>事件溯源（Event-Sourcing）：其核心理念是在对系统的状态做出变更时，把每次变更记录为一个事件，在未来的任何时刻，都可以通过重新处理这些事件来重建系统的状态。事件存储是主要的事件来源，可以从事件存储中重建系统的状态。其好处是存储结构简单，易于存储，不需要用到事务控制从而可以避免使用锁，事件本身还可以充当审计日志的作用。不足之处在于如果事件很多，重放事件是一个耗时的过程，而且在重放过程中可能会涉及与第三方外部系统发生交互，所以需要做一些额外的操作。</li>
<li>CQRS：是Command Query Resposibility Segregation的缩写，它将读操作和写操作进行分离，不仅让逻辑更清晰，而且可以各自进行优化。对于读多写少的系统来说，就特别适合使用CQRS，因为可以针对读性能和写性能进行优化，而且可以进行横向扩展。不过CQRS的概念虽然简单，但是实现起来相对复杂，而且涉及到很多领域驱动设计的概念，最好结合事件溯源一起使用。</li>
</ul>
<h3 id="Event-Sourcing-Martin-Fowler"><a href="#Event-Sourcing-Martin-Fowler" class="headerlink" title="Event Sourcing Martin Fowler"></a><a href="https://martinfowler.com/eaaDev/EventSourcing.html" target="_blank" rel="external">Event Sourcing</a> Martin Fowler</h3><blockquote>
<p>Event Sourcing ensures that all changes to application state are stored as a sequence of events. Not just can we query these events, we can also use the event log to reconstruct past states, and as a foundation to automatically adjust the state to cope with retroactive changes.</p>
<p>The fundamental idea of Event Sourcing is that of ensuring every change to the state of an application is captured in an event object, and that these event objects are themselves stored in the sequence they were applied for the same lifetime as the application state itself.</p>
<p>The most obvious thing we’ve gained by using Event Sourcing is that we now have a log of all the changes. Not just can we see where each ship is, we can see where it’s been. </p>
<p>The key to Event Sourcing is that we guarantee that all changes to the domain objects are initiated by the event objects. This leads to a number of facilities that can be built on top of the event log: Complete Rebuild, Temporal Query, Event Replay.</p>
<p>In many applications it’s more common to request recent application states, if so a faster alternative is to store the current application state and if someone wants the special features that Event Sourcing offers then that additional capability is built on top.</p>
<p>There are a number of choices about where to put the logic for handling events. The primary choice is whether to put the logic in Transaction Scripts or Domain Model. As usual Transaction Scripts are better for simple logic and a Domain Model is better when things get more complicated.</p>
<p>As well as events playing themselves forwards, it’s also often useful for them to be able to reverse themselves.</p>
<p>Many of the advantages of Event Sourcing stem from the ability to replay events at will, but if these events cause update messages to be sent to external systems, then things will go wrong because those external systems don’t know the difference between real processing and replays. To handle this you’ll need to wrap any external systems with a Gateway. This in itself isn’t too onerous since it’s a thoroughly good idea in any case. The gateway has to be a bit more sophisticated so it can deal with any replay processing that the Event Sourcing system is doing.</p>
<p>When to Use It: Packaging up every change to an application as an event is an interface style that not everyone is comfortable with, and many find to be awkward. As a result it’s not a natural choice and to use it means that you expect to get some form of return. One obvious form of return is that it’s easy to serialize the events to make an Audit Log. Another use for this kind of complete Audit Log is to help with debugging. Event Sourcing is the foundation for Parallel Models or Retroactive Events. If you want to use either of those patterns you will need to use Event Sourcing first. </p>
</blockquote>
<h3 id="CQRS-Martin-Fowler"><a href="#CQRS-Martin-Fowler" class="headerlink" title="CQRS Martin Fowler"></a><a href="https://martinfowler.com/bliki/CQRS.html" target="_blank" rel="external">CQRS</a> Martin Fowler</h3><blockquote>
<p>CQRS stands for Command Query Responsibility Segregation. At its heart is the notion that you can use a different model to update information than the model you use to read information. For some situations, this separation can be valuable, but beware that for most systems CQRS adds risky complexity.</p>
<p>The change that CQRS introduces is to split that conceptual model into separate models for update and display, which it refers to as Command and Query respectively following the vocabulary of CommandQuerySeparation. The rationale is that for many problems, particularly in more complicated domains, having the same conceptual model for commands and queries leads to a more complex model that does neither well.</p>
</blockquote>
<p><img src="http://oi46mo3on.bkt.clouddn.com/15_reading_201702/cqrs.png" alt="CQRS"></p>
<blockquote>
<p>The two models might not be separate object models, it could be that the same objects have different interfaces for their command side and their query side, rather like views in relational databases. But usually when I hear of CQRS, they are clearly separate models.</p>
<p>When to use it: CQRS is a significant mental leap for all concerned, so shouldn’t be tackled unless the benefit is worth the jump. In particular CQRS should only be used on specific portions of a system (a BoundedContext in DDD lingo) and not the system as a whole. So far I see benefits in two directions. Firstly is that a few complex domains may be easier to tackle by using CQRS. The other main benefit is in handling high performance applications. </p>
</blockquote>
<h3 id="RxJava2实例解析-Victor-Grazi-薛命灯Rays"><a href="#RxJava2实例解析-Victor-Grazi-薛命灯Rays" class="headerlink" title="RxJava2实例解析  Victor Grazi/薛命灯Rays"></a><a href="http://www.infoq.com/cn/articles/rxjava2-by-example" target="_blank" rel="external">RxJava2实例解析</a>  Victor Grazi/薛命灯Rays</h3><ul>
<li>响应式编程是一种处理异步数据流的规范，它为数据流的转换和聚合以及数据流的控制管理提供了工具支持，它让考量程序整体设计的工作变得简单。但它使用起来并不简单，它的学习曲线也并不平坦。</li>
<li>传统的编程模式以对象为基础，而响应式以事件流为基础。事件可能以多种形式出现，比如对象、数据源、鼠标移动信息或者异常。</li>
<li>首先要记住的是，响应式里所有的东西都是流。Observable封装了流，是最基本的单元。流可以包含零个或多个事件，有未完成和已完成两种状态，可以正常结束也可以发生错误。如果一个流正常完成或者发生错误，说明处理结束了，虽然有些工具可以对错误进行重试或者使用不同的流替换发生错误的流。</li>
<li>一个Observable对象必须要有一个订阅者来处理它所生成的事件。所幸的是，现在Java支持Lambda表达式，我们就可以使用简洁的声明式风格来表示订阅操作：</li>
</ul>
<pre><code>Observable&lt;String&gt; howdy = Observable.just(&quot;Howdy!&quot;);
howdy.subscribe(System.out::println);
</code></pre><ul>
<li>zip操作通过成对的“zip”映射转换把源流的元素跟另一个给定流的元素组合起来，其中的映射可以使用Lambda表达式来表示。只要其中的一个流完成操作，整个zip操作也跟着停止，另一个未完成的流剩下的事件就会被忽略。zip可以支持最多9个源流的zip操作。zipWith操作可以把一个指定流合并到一个已存在的流里。我们可以使用range和zipWith操作加入编号，并用String.format做映射转换：</li>
</ul>
<pre><code>Observable.fromIterable(words)
 .zipWith(Observable.range(1, Integer.MAX_VALUE), (string, count)-&gt;String.format(&quot;%2d. %s&quot;, count, string))
 .subscribe(System.out::println);
</code></pre><ul>
<li>我们从小被告知“quick brown fox”这个全字母短句包含了英语里所有的字母，现在让我们对这些字母进行排序看看：</li>
</ul>
<pre><code>List&lt;String&gt; words = Arrays.asList(&quot;the&quot;, &quot;quick&quot;, &quot;brown&quot;, &quot;fox&quot;, &quot;jumped&quot;, 
    &quot;over&quot;, &quot;the&quot;, &quot;lazy&quot;, &quot;dogs&quot;);

Observable.fromIterable(words)
 .flatMap(word -&gt; Observable.fromArray(word.split(&quot;&quot;)))
 .distinct()
 .sorted()
 .zipWith(Observable.range(1, Integer.MAX_VALUE), (string, count) -&gt; String.format(&quot;%2d. %s&quot;, count, string))
 .subscribe(System.out::println);
</code></pre><ul>
<li>但是到目前为止，所有的代码都跟Java 8里引入的Streams API很相似，不过这种相似只是一种巧合，因为响应式包含的内容远不止这些。响应式引入了执行时间、节流、流量控制等概念，而且它们可以被连接到“永不停止”的处理流程里。响应式产生的结果虽然不是集合，但你可以用任何期望的方式来处理这些结果。</li>
<li>在RxJava的前期版本中，即使对于无需流控制的小型流，Observable也给出了流控制方法。为符合响应式的规范，RxJava2将流控制从Observable类中移除，并引入了新的Flowable类。Flowable可以看作是提供流控制的Observable。</li>
<li>要连接到长期运行的现有数据源上，除非是提供背压控制，我们通常会选择使用Flowable，使用一种Observable的并行语法。a. 调用Flowable的publish方法生成一个新的ConnectableFlowable； b. 调用ConnectableFlowable的connect方法开始生成数据；</li>
<li>要连接到一个已有的数据源上，可以在这个数据源上添加监听器（如果你喜欢这么做），监听器会把事件传播给订阅者，然后在每个事件发生时调用订阅者的onNext方法。在实现监听器的时候要确保每个订阅者仍然处于订阅状态，否则就要停止把事件传播给它，同时要注意回压信号。所幸的是，这些工作可以由Flowabled的create方法来处理。</li>
</ul>
<h3 id="Redis的内存优化-CacheCloud"><a href="#Redis的内存优化-CacheCloud" class="headerlink" title="Redis的内存优化 CacheCloud"></a><a href="https://cachecloud.github.io/2017/02/16/Redis%E5%86%85%E5%AD%98%E4%BC%98%E5%8C%96/" target="_blank" rel="external">Redis的内存优化</a> CacheCloud</h3><ul>
<li>Redis存储的所有值对象在内部定义为redisObject结构体，内部结构如下图所示：</li>
</ul>
<p><img src="http://i1.itc.cn/20170216/3084_2c3a0c00_6cbc_c4d9_01fe_8852cc497653_1.png" alt="RedisObject"></p>
<ul>
<li>Redis存储的数据都使用redisObject来封装，包括string,hash,list,set,zset在内的所有数据类型。理解redisObject对内存优化非常有帮助。type字段表示当前对象使用的数据类型，encoding字段表示Redis内部编码类型，lru字段记录对象最后一次被访问的时间，refcount字段记录当前对象被引用的次数，*ptr字段与对象的数据内容相关，如果是整数直接存储数据，否则表示指向数据的指针。</li>
<li>降低Redis内存使用最直接的方式就是缩减键（key）和值（value）的长度。值对象缩减比较复杂，常见需求是把业务对象序列化成二进制数组放入Redis。首先应该在业务上精简业务对象，去掉不必要的属性避免存储无效数据。其次在序列化工具选择上，应该选择更高效的序列化工具来降低字节数组大小。</li>
<li>对象共享池指Redis内部维护[0-9999]的整数对象池。创建大量的整数类型redisObject存在内存开销，每个redisObject内部结构至少占16字节，甚至超过了整数自身空间消耗。所以Redis内存维护一个[0-9999]的整数对象池，用于节约内存。除了整数值对象，其他类型如list,hash,set,zset内部元素也可以使用整数对象池。因此开发中在满足需求的前提下，尽量使用整数对象以节省内存。</li>
<li>字符串优化：Redis没有采用原生C语言的字符串类型而是自己实现了字符串结构，内部简单动态字符串，简称SDS；因为字符串(SDS)存在预分配机制，日常开发中要小心预分配带来的内存浪费，从测试数据看，同样的数据追加后内存消耗非常严重；字符串之所以采用预分配的方式是防止修改操作需要不断重分配内存和字节数据拷贝，但同样也会造成内存的浪费，字符串预分配每次并不都是翻倍扩容；字符串重构指不一定把每份数据作为字符串整体存储，像json这样的数据可以使用hash结构，使用二级结构存储也能帮我们节省内存，同时可以使用hmget,hmset命令支持字段的部分读取修改，而不用每次整体存取。</li>
<li>编码优化：Redis对外提供了string,list,hash,set,zet等类型，但是Redis内部针对不同类型存在编码的概念。Redis作者想通过不同编码实现效率和空间的平衡，比如当我们的存储只有10个元素的列表，当使用双向链表数据结构时，必然需要维护大量的内部字段如每个元素需要前置指针、后置指针、数据指针等，造成空间浪费，如果采用连续内存结构的压缩列表(ziplist)，将会节省大量内存，而由于数据长度较小，存取操作时间复杂度即使为O(n2)性能也可满足需求。</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/15_reading_201702/type_encoding_relation.png" alt="类型和编码关系"></p>
<ul>
<li>控制key的数量：当使用Redis存储大量数据时，通常会存在大量键，过多的键同样会消耗大量内存。Redis本质是一个数据结构服务器，它为我们提供多种数据结构，如hash，list，set，zset等结构。使用Redis时不要进入一个误区，大量使用get/set这样的API，把Redis当成Memcached使用。对于存储相同的数据内容利用Redis的数据结构降低外层键的数量，也可以节省大量内存。</li>
</ul>
<h3 id="Redis架构之防雪崩设计：网站不宕机背后的兵法-付磊，张益军"><a href="#Redis架构之防雪崩设计：网站不宕机背后的兵法-付磊，张益军" class="headerlink" title="Redis架构之防雪崩设计：网站不宕机背后的兵法 付磊，张益军"></a><a href="https://mp.weixin.qq.com/s/TBCEwLVAXdsTszRVpXhVug" target="_blank" rel="external">Redis架构之防雪崩设计：网站不宕机背后的兵法</a> 付磊，张益军</h3><ul>
<li>缓存穿透预防及优化：缓存穿透是指查询一个根本不存在的数据，缓存层和存储层都不会命中；缓存穿透将导致不存在的数据每次请求都要到存储层去查询，失去了缓存保护后端存储的意义；解决方法：缓存空对象和布隆过滤器拦截；</li>
<li>缓存雪崩问题优化：由于缓存层承载着大量请求，有效的保护了存储层，但是如果缓存层由于某些原因整体不能提供服务，于是所有的请求都会达到存储层，存储层的调用量会暴增，造成存储层也会挂掉的情况；预防和解决缓存雪崩问题，可以从以下三个方面进行着手：保证缓存层服务高可用性、依赖隔离组件为后端限流并降级、提前演练。</li>
<li>缓存热点key重建优化：开发人员使用缓存和过期时间的策略既可以加速数据读写，又保证数据的定期更新，这种模式基本能够满足绝大部分需求。但如果热点Key和重建缓存耗时两个问题同时出现，可能就会对应用造成致命的危害；解决方法：互斥锁（只允许一个线程重建缓存）、永远不过期（唯一不足的就是重构缓存期间会出现数据不一致的情况）。</li>
</ul>
<h3 id="现代化的缓存设计方案-Benjamin-Manes-简直"><a href="#现代化的缓存设计方案-Benjamin-Manes-简直" class="headerlink" title="现代化的缓存设计方案 Benjamin Manes/简直"></a><a href="http://ifeve.com/design-of-a-modern-cache/" target="_blank" rel="external">现代化的缓存设计方案</a> Benjamin Manes/简直</h3><ul>
<li>缓存是提升性能的通用方法，现在大多数的缓存实现都使用了经典的技术。这篇文章中，我们会发掘Caffeine中的现代化的实现方法。Caffeine 是一个开源的Java缓存库，它能提供高命中率和出色的并发能力。期望读者们能被这些想法激发，进而将它们应用到任何你喜欢的编程语言中。</li>
<li>驱逐策略：缓存的驱逐策略是为了预测哪些数据在短期内最可能被再次用到，从而提升缓存的命中率。LRU策略或许是最流行的驱逐策略，但LRU通过历史数据来预测未来是局限的，它会认为最后到来的数据是最可能被再次访问的。现代缓存扩展了对历史数据的使用，结合就近程度和访问频次来更好的预测数据。其中一种保留历史信息的方式是使用popularity sketch（一种压缩、概率性的数据结构）来从一大堆访问事件中定位频繁的访问者。Window TinyLFU（W-TinyLFU）算法将 sketch 作为过滤器，当新来的数据比要驱逐的数据高频时，这个数据才会被缓存接纳。这个许可窗口给予每个数据项积累热度的机会，而不是立即过滤掉。对于长期保留的数据，W-TinyLFU 使用了分段 LRU（Segmented LRU，缩写 SLRU）策略。起初，一个数据项存储被存储在试用段（probationary segment）中，在后续被访问到时，它会被提升到保护段（protected segment）中（保护段占总容量的 80%）。保护段满后，有的数据会被淘汰回试用段，这也可能级联的触发试用段的淘汰。这套机制确保了访问间隔小的热数据被保存下来，而被重复访问少的冷数据则被回收。</li>
<li>过期策略：鉴于大多数场景里不同数据项使用的都是固定的过期时长，Caffien采用了统一过期时间的方式。这个限制让用 O（1）的有序队列组织数据成为可能。针对数据的写后过期，维护了一个写入顺序队列，针对读后过期，维护了一个读取顺序队列。缓存能复用驱逐策略下的队列以及下面将要介绍的并发机制，让过期的数据项在缓存的维护阶段被抛弃掉。</li>
<li>并发：由于在大多数的缓存策略中，数据的读取都会伴随对缓存状态的写操作，并发的缓存读取被视为一个难点问题。在 Caffeine 中，有一组缓冲区被用来记录读写。一次访问首先会被因线程而异的哈希到 stripped ring buffer 上，当检测到竞争时，缓冲区会自动扩容。一个 ring buffer 容量满载后，会触发异步的执行操作，而后续的对该 ring buffer 的写入会被丢弃，直到这个 ring buffer 可被使用。虽然因为 ring buffer 容量满而无法被记录该访问，但缓存值依然会返回给调用方。这种策略信息的丢失不会带来大的影响，因为 W-TinyLFU 能识别出我们希望保存的热点数据。通过使用因线程而异的哈希算法替代在数据项的键上做哈希，缓存避免了瞬时的热点 key 的竞争问题。写数据时，采用更传统的并发队列，每次变更会引起一次立即的执行。</li>
</ul>
<h3 id="大话程序猿眼里的高并发之续篇-SFLYQ"><a href="#大话程序猿眼里的高并发之续篇-SFLYQ" class="headerlink" title="大话程序猿眼里的高并发之续篇 SFLYQ"></a><a href="https://blog.thankbabe.com/2017/02/27/high-concurrency-scheme-xp/" target="_blank" rel="external">大话程序猿眼里的高并发之续篇</a> SFLYQ</h3><ul>
<li>分层（将系统在横向维度上切分成几个部分，每个部门负责一部分相对简单并比较单一的职责，然后通过上层对下层的依赖和调度组成一个完整的系统），分割（在纵向方面对业务进行切分，将一块相对复杂的业务分割成不同的模块单元），分布式（分布式应用和服务，将分层或者分割后的业务分布式部署，独立的应用服务器、数据库和缓存服务器，当业务达到一定用户量的时候，再进行服务器均衡负载，数据库、缓存主从集群）；</li>
<li>集群：对于用户访问集中的业务独立部署服务器，应用服务器，数据库，nosql数据库。核心业务基本上需要搭建集群，即多台服务器部署相同的应用构成一个集群，通过负载均衡设备共同对外提供服务，服务器集群能够为相同的服务提供更多的并发支持；</li>
<li>异步：在高并发业务中如果涉及到数据库操作，主要压力都是在数据库服务器上面，虽然使用主从分离，但是数据库操作都是在主库上操作，单台数据库服务器连接池允许的最大连接数量是有限的，像这种涉及数据库操作的高并发的业务，就要考虑使用异步了，客户端发起接口请求，服务端快速响应，客户端展示结果给用户，数据库操作通过异步同步；</li>
<li>缓存：数据不经常变化，我们可以把数据进行缓存，Cache是直接存储在应用服务器中，读取速度快，内存数据库服务器允许连接数可以支撑到很大，而且数据存储在内存，读取速度快，再加上主从集群，可以支撑很大的并发查询；</li>
<li>面向服务：使用服务化思维，将核心业务或者通用的业务功能抽离成服务独立部署，对外提供接口的方式提供功能。最理想化的设计是可以把一个复杂的系统抽离成多个服务，共同组成系统的业务，优点：松耦合，高可用性，高伸缩性，易维护。通过面向服务化设计，独立服务器部署，均衡负载，数据库集群，可以让服务支撑更高的并发；</li>
<li>冗余，自动化：当高并发业务所在的服务器出现宕机的时候，需要有备用服务器进行快速的替代，在应用服务器压力大的时候可以快速添加机器到集群中，所以我们就需要有备用机器可以随时待命。 最理想的方式是可以通过自动化监控服务器资源消耗来进行报警，自动切换降级方案，自动的进行服务器替换和添加操作等，通过自动化可以减少人工的操作的成本，而且可以快速操作，避免人为操作上面的失误。</li>
</ul>
<h3 id="浅谈机器学习基础-我偏笑-NSNirvana"><a href="#浅谈机器学习基础-我偏笑-NSNirvana" class="headerlink" title="浅谈机器学习基础 我偏笑_NSNirvana"></a><a href="http://www.jianshu.com/p/ed9ae5385b89" target="_blank" rel="external">浅谈机器学习基础</a> 我偏笑_NSNirvana</h3><p>下篇的地址为:<a href="http://www.jianshu.com/p/0359e3b7bb1b" target="_blank" rel="external">http://www.jianshu.com/p/0359e3b7bb1b</a></p>
<h3 id="浅谈深度学习基础-我偏笑-NSNirvana"><a href="#浅谈深度学习基础-我偏笑-NSNirvana" class="headerlink" title="浅谈深度学习基础 我偏笑_NSNirvana"></a><a href="http://www.jianshu.com/p/df9a4473d6d4" target="_blank" rel="external">浅谈深度学习基础</a> 我偏笑_NSNirvana</h3><p>下篇的地址为:<a href="http://www.jianshu.com/p/3d1ddfce1563" target="_blank" rel="external">http://www.jianshu.com/p/3d1ddfce1563</a></p>
<h3 id="机器学习算法实现解析——word2vec源码解析-zhiyong-will"><a href="#机器学习算法实现解析——word2vec源码解析-zhiyong-will" class="headerlink" title=" 机器学习算法实现解析——word2vec源码解析 zhiyong_will"></a><a href="http://blog.csdn.net/google19890102/article/details/51887344#" target="_blank" rel="external"> 机器学习算法实现解析——word2vec源码解析</a> zhiyong_will</h3><h3 id="分布式系列文章——Paxos算法原理与推导-linbingdong"><a href="#分布式系列文章——Paxos算法原理与推导-linbingdong" class="headerlink" title="分布式系列文章——Paxos算法原理与推导 linbingdong"></a><a href="http://linbingdong.com/2017/03/17/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0%E2%80%94%E2%80%94Paxos%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%E4%B8%8E%E6%8E%A8%E5%AF%BC/" target="_blank" rel="external">分布式系列文章——Paxos算法原理与推导</a> linbingdong</h3><h3 id="分布式一致性算法：Raft-算法-linbingdong"><a href="#分布式一致性算法：Raft-算法-linbingdong" class="headerlink" title="分布式一致性算法：Raft 算法 linbingdong"></a><a href="http://linbingdong.com/2017/02/19/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%80%E8%87%B4%E6%80%A7%E7%AE%97%E6%B3%95%EF%BC%9ARaft%20%E7%AE%97%E6%B3%95%EF%BC%88Raft%20%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%EF%BC%89/" target="_blank" rel="external">分布式一致性算法：Raft 算法</a> linbingdong</h3><h3 id="解决业务代码里的分布式事务一致性问题-陶文"><a href="#解决业务代码里的分布式事务一致性问题-陶文" class="headerlink" title="解决业务代码里的分布式事务一致性问题 陶文"></a><a href="https://zhuanlan.zhihu.com/p/25346771" target="_blank" rel="external">解决业务代码里的分布式事务一致性问题</a> 陶文</h3><ul>
<li>微服务架构解决了很多问题，但是同时引入了很多问题，本文要探讨的是如何解决下面这几个问题：有大量的同步RPC依赖，如何保证自身的可靠性？RPC调用失败，降级处理之后如何保证数据可修复？消息队列是一个RPC主流程的旁路流程，怎么保证可靠性？消息队列怎么保持与数据库的事务一致？</li>
<li>同步转异步，解决稳定性问题：在平时的时候，都是RPC同步调用，如果调用失败了，则自动把同步调用降级为异步的，消息此时进入队列，然后异步被重试。</li>
</ul>
<p><img src="http://pic2.zhimg.com/v2-9f5a5418027eae507551518a6aaa1179_b.png" alt="同步转异步"></p>
<ul>
<li>把消息队列放入到主流程：如果要把重要的业务逻辑挂在消息队列后面，必须要保证消息队列里的数据的完整性，不能有丢失的情况，所以不能是把消息队列的写入作为一个旁路的逻辑。如果消息队列写入失败或者超时，都应该直接返回错误，而不是允许继续执行。在无法及时写入的情况，我们需要使用本地文件充当一个缓冲。实际上是通过引入本地文件队列结合远程分布式队列构成一个可用性更高，延迟更低的组合队列方案。这个本地的队列如果能封装到一个 Kafka 的 Agent 作为本地写入的代理，那是最理想的实现方式。</li>
</ul>
<p><img src="http://pic2.zhimg.com/v2-eeccde7413dc2331257eece53558dce1_b.png" alt="把消息队列放入到主流程"></p>
<ul>
<li>保障分布式事务一致性：我们需要一个延迟队列，在业务入口的时候挂一个延迟job，然后执行完了取消它。如果没有执行完，则延迟队列负责去触发这个延迟任务，把整个业务流程重复执行一遍。这样我们就可以保证任意rpc操作流程的最终一致性了。而入kafka消息队列作为RPC操作的一种，自然也是可以得到保证的了。</li>
</ul>
<p><img src="http://pic1.zhimg.com/v2-a8904d61ad34784fd77c32b1932e8f78_b.png" alt="保障分布式事务一致性"></p>
<h3 id="TensorFlow和Caffe、CNTK、MXNet等其他7种深度学习框架的对比-黄文坚-唐源"><a href="#TensorFlow和Caffe、CNTK、MXNet等其他7种深度学习框架的对比-黄文坚-唐源" class="headerlink" title="TensorFlow和Caffe、CNTK、MXNet等其他7种深度学习框架的对比 黄文坚/唐源"></a><a href="http://mp.weixin.qq.com/s?__biz=MzA5NzkxMzg1Nw==&amp;mid=2653162065&amp;idx=1&amp;sn=c0f50fe72cb495dc19b9861d3bd5d67f" target="_blank" rel="external">TensorFlow和Caffe、CNTK、MXNet等其他7种深度学习框架的对比</a> 黄文坚/唐源</h3><h4 id="TensorFlow"><a href="#TensorFlow" class="headerlink" title="TensorFlow"></a>TensorFlow</h4><ul>
<li>TensorFlow是相对高阶的机器学习库，用户可以方便地用它设计神经网络结构，而不必为了追求高效率的实现亲自写C++或CUDA18代码。</li>
<li>它和Theano一样都支持自动求导，用户不需要再通过反向传播求解梯度。</li>
<li>其核心代码和Caffe一样是用C++编写的，使用C++简化了线上部署的复杂度，并让手机这种内存和CPU资源都紧张的设备可以运行复杂模型（Python则会比较消耗资源，并且执行效率不高）。</li>
<li>除了核心代码的C++接口，TensorFlow还有官方的Python、Go和Java接口，是通过SWIG（Simplified Wrapper and Interface Generator）实现的，这样用户就可以在一个硬件配置较好的机器中用Python进行实验，并在资源比较紧张的嵌入式环境或需要低延迟的环境中用C++部署模型。SWIG支持给C/C++代码提供各种语言的接口，因此其他脚本语言的接口未来也可以通过SWIG方便地添加。</li>
<li>TensorFlow也有内置的TF.Learn和TF.Slim等上层组件可以帮助快速地设计新网络，并且兼容Scikit-learn estimator接口，可以方便地实现evaluate、grid search、cross validation等功能。</li>
<li>同时TensorFlow不只局限于神经网络，其数据流式图支持非常自由的算法表达，当然也可以轻松实现深度学习以外的机器学习算法。事实上，只要可以将计算表示成计算图的形式，就可以使用TensorFlow。</li>
<li>用户可以写内层循环代码控制计算图分支的计算，TensorFlow会自动将相关的分支转为子图并执行迭代运算。TensorFlow也可以将计算图中的各个节点分配到不同的设备执行，充分利用硬件资源。</li>
<li>在数据并行模式上，TensorFlow和Parameter Server很像，但TensorFlow有独立的Variable node，不像其他框架有一个全局统一的参数服务器，因此参数同步更自由。</li>
<li>TensorFlow和Spark的核心都是一个数据计算的流式图，Spark面向的是大规模的数据，支持SQL等操作，而TensorFlow主要面向内存足以装载模型参数的环境，这样可以最大化计算效率。</li>
<li>TensorFlow的另外一个重要特点是它灵活的移植性，可以将同一份代码几乎不经过修改就轻松地部署到有任意数量CPU或GPU的PC、服务器或者移动设备上。</li>
<li>TensorBoard是TensorFlow的一组Web应用，用来监控TensorFlow运行过程，或可视化Computation Graph。</li>
<li>TensorFlow拥有产品级的高质量代码，有Google强大的开发、维护能力的加持，整体架构设计也非常优秀。</li>
</ul>
<h4 id="Caffe"><a href="#Caffe" class="headerlink" title="Caffe"></a>Caffe</h4><ul>
<li>Caffe全称为Convolutional Architecture for Fast Feature Embedding，是一个被广泛使用的开源深度学习框架，目前由伯克利视觉学中心进行维护。Caffe的创始人是加州大学伯克利的Ph.D.贾扬清，他同时也是TensorFlow的作者之一。</li>
<li>Caffe的主要优势包括如下几点：容易上手，网络结构都是以配置文件形式定义，不需要用代码设计网络；训练速度快，能够训练state-of-the-art的模型与大规模的数据；组件模块化，可以方便地拓展到新的模型和学习任务上。</li>
<li>Caffe的核心概念是Layer，每一个神经网络的模块都是一个Layer。Layer接收输入数据，同时经过内部计算产生输出数据。设计网络结构时，只需要把各个Layer拼接在一起构成完整的网络（通过写protobuf配置文件定义）。</li>
<li>Caffe最开始设计时的目标只针对于图像，没有考虑文本、语音或者时间序列的数据，因此Caffe对卷积神经网络的支持非常好，但对时间序列RNN、LSTM等支持得不是特别充分。同时，基于Layer的模式也对RNN不是非常友好，定义RNN结构时比较麻烦。</li>
</ul>
<h4 id="Theano"><a href="#Theano" class="headerlink" title="Theano"></a>Theano</h4><ul>
<li>Theano诞生于2008年，由蒙特利尔大学Lisa Lab团队开发并维护，是一个高性能的符号计算及深度学习库。因其出现时间早，可以算是这类库的始祖之一，也一度被认为是深度学习研究和应用的重要标准之一。</li>
<li>Theano的核心是一个数学表达式的编译器，专门为处理大规模神经网络训练的计算而设计。它可以将用户定义的各种计算编译为高效的底层代码，并链接各种可以加速的库，比如BLAS、CUDA等。</li>
</ul>
<h4 id="Torch"><a href="#Torch" class="headerlink" title="Torch"></a>Torch</h4><ul>
<li>Torch给自己的定位是LuaJIT上的一个高效的科学计算库，支持大量的机器学习算法，同时以GPU上的计算优先。</li>
<li>Torch的历史非常悠久，但真正得到发扬光大是在Facebook开源了其深度学习的组件之后，此后包括Google、Twitter、NYU、IDIAP、Purdue等组织都大量使用Torch。</li>
<li>Torch的目标是让设计科学计算算法变得便捷，它包含了大量的机器学习、计算机视觉、信号处理、并行运算、图像、视频、音频、网络处理的库，同时和Caffe类似，Torch拥有大量的训练好的深度学习模型。</li>
<li>它可以支持设计非常复杂的神经网络的拓扑图结构，再并行化到CPU和GPU上，在Torch上设计新的Layer是相对简单的。</li>
</ul>
<h4 id="Keras"><a href="#Keras" class="headerlink" title="Keras"></a>Keras</h4><ul>
<li>Keras是一个崇尚极简、高度模块化的神经网络库，使用Python实现，并可以同时运行在TensorFlow和Theano上。它旨在让用户进行最快速的原型实验，让想法变为结果的这个过程最短。</li>
<li>Theano和TensorFlow的计算图支持更通用的计算，而Keras则专精于深度学习。Theano和TensorFlow更像是深度学习领域的NumPy，而Keras则是这个领域的Scikit-learn。</li>
<li>它提供了目前为止最方便的API，用户只需要将高级的模块拼在一起，就可以设计神经网络，它大大降低了编程开销和阅读别人代码时的理解开销。</li>
</ul>
<h4 id="MXNet"><a href="#MXNet" class="headerlink" title="MXNet"></a>MXNet</h4><ul>
<li>MXNet是DMLC（Distributed Machine Learning Community）开发的一款开源的、轻量级、可移植的、灵活的深度学习库，它让用户可以混合使用符号编程模式和指令式编程模式来最大化效率和灵活性，目前已经是AWS官方推荐的深度学习框架。</li>
<li>它是各个框架中率先支持多GPU和分布式的，同时其分布式性能也非常高。MXNet的核心是一个动态的依赖调度器，支持自动将计算任务并行化到多个GPU或分布式集群。</li>
<li>它上层的计算图优化算法可以让符号计算执行得非常快，而且节约内存，开启mirror模式会更加省内存，甚至可以在某些小内存GPU上训练其他框架因显存不够而训练不了的深度学习模型，也可以在移动设备上运行基于深度学习的图像识别等任务。</li>
</ul>
<h4 id="CNTK"><a href="#CNTK" class="headerlink" title="CNTK"></a>CNTK</h4><ul>
<li>CNTK（Computational Network Toolkit）是微软研究院（MSR）开源的深度学习框架。它最早由start the deep learning craze的演讲人创建，目前已经发展成一个通用的、跨平台的深度学习系统，在语音识别领域的使用尤其广泛。</li>
<li>CNTK通过一个有向图将神经网络描述为一系列的运算操作，这个有向图中子节点代表输入或网络参数，其他节点代表各种矩阵运算。CNTK支持各种前馈网络，包括MLP、CNN、RNN、LSTM、Sequence-to-Sequence模型等，也支持自动求解梯度。</li>
<li>CNTK有丰富的细粒度的神经网络组件，使得用户不需要写底层的C++或CUDA，就能通过组合这些组件设计新的复杂的Layer。</li>
<li>CNTK拥有产品级的代码质量，支持多机、多GPU的分布式训练。</li>
</ul>
<h4 id="Deeplearning4J"><a href="#Deeplearning4J" class="headerlink" title="Deeplearning4J"></a>Deeplearning4J</h4><ul>
<li>Deeplearning4J（简称DL4J）是一个基于Java和Scala的开源的分布式深度学习库，由Skymind于2014年6月发布，其核心目标是创建一个即插即用的解决方案原型。</li>
<li>DL4J拥有一个多用途的n-dimensional array的类，可以方便地对数据进行各种操作；拥有多种后端计算核心，用以支持CPU及GPU加速，在图像识别等训练任务上的性能与Caffe相当。</li>
<li>可以与Hadoop及Spark自动整合，同时可以方便地在现有集群上进行扩展，同时DL4J的并行化是根据集群的节点和连接自动优化，不像其他深度学习库那样可能需要用户手动调整。</li>
</ul>
<h3 id="微服务与RPC-凤凰牌老熊"><a href="#微服务与RPC-凤凰牌老熊" class="headerlink" title="微服务与RPC 凤凰牌老熊"></a><a href="http://mp.weixin.qq.com/s?__biz=MzI4OTQ3MTI2NA==&amp;mid=2247483731&amp;idx=1&amp;sn=5f32d6a9757a48d0dcdb5921c131e2bd" target="_blank" rel="external">微服务与RPC</a> 凤凰牌老熊</h3><ul>
<li>RPC vs Restful：RPC支持多种语言（但不是所有语言），四层通讯协议，性能高，节省带宽，相对Restful协议，使用Thrift RPC，在同等硬件条件下，带宽使用率仅为前者的20%，性能却提升一个数量级，但是这种协议最大的问题在于无法穿透防火墙。而以Spring Cloud为代表所支持的Restful协议，优势在于能够穿透防火墙，使用方便，语言无关，基本上可以使用各种开发语言实现的系统，都可以接受Restful的请求，但性能和带宽占用上有劣势。所以业内对微服务的实现，基本是确定一个组织边界，在该边界内使用RPC，边界外使用Restful。</li>
<li>RPC选型：Apache Thrift是目前最为成熟的框架，优点在于稳定、高性能，缺点在于它仅提供RPC服务，其他的功能，包括限流、熔断、服务治理等，都需要自己实现，或者使用第三方软件。Google Protobuf一直只有数据模型的实现，而2015年才推出的gRPC还缺乏重量级的用户。Thrift 提供多种高性能的传输协议，但在数据定义上，不如Protobuf强大，而Protobuf的劣势在于其RPC服务的实现性能不佳，为此，Apache Thrift + Protobuf的RPC实现，成为不少公司的选择。</li>
<li>服务注册与发现：Spring cloud提供了服务注册和发现功能，如果需要自己实现，可以考虑使用Apache Zookeeper作为注册表，使用Apache Curator 来管理Zookeeper的链接；对服务注册来说，注册表结构需要详细设计，一般注册表结构会按照如下方式组织：机房区域-部门-服务类型-服务名称-服务器地址。</li>
<li>连接池：RPC服务访问和数据库类似，建立链接是一个耗时的过程，连接池是服务调用的标配。目前还没有成熟的开源Apache Thrift链接池，一般互联网公司都会开发内部自用的链接池。自己实现可以基于JDBC链接池做改进，比如参考Apache commons DBCP链接池，使用Apache Pools来管理链接。连接池实现的主要难点在于如何从多个服务器中选举出来为当前调用提供服务的连接。比如目前有10台机器在提供服务，上一次分配的是第4台服务器，本次应该分配哪一台？在实现上，需要收集每台机器的QOS以及当前的负担，分配一个最佳的连接。</li>
<li>API网关：如果有一个应用需要调用多个服务，对这个应用来说，就需要维护和多个服务器之间的链接。服务的重启，都会对连接池以及客户端的访问带来影响。为此，在微服务中，广泛会使用到API网关。API网关可以认为是一系列服务集合的访问入口。从面向对象设计的角度看，它与外观模式类似，实现对所提供服务的封装。</li>
<li>熔断与限流：熔断一般采用电路熔断器模式(Circuit Breaker Patten)，当某个服务发生错误，每秒错误次数达到阈值时，不再响应请求，直接返回服务器忙的错误给调用方，延迟一段时间后，尝试开放50%的访问，如果错误还是高，则继续熔断，否则恢复到正常情况。限流指按照访问方、IP地址或者域名等方式对服务访问进行限制，一旦超过给定额度，则禁止其访问。 除了使用Hystrix，如果要自己实现，可以考虑使用使用Guava RateLimiter。</li>
<li>服务演化：随着服务访问量的增加，服务的实现也会不断演化以提升性能，主要的方法有读写分离、缓存等。</li>
</ul>
<h3 id="CMU论文：一部深度学习发展史，看神经网络兴衰更替-Haohan-Wang-Bhiksha-Raj-张易"><a href="#CMU论文：一部深度学习发展史，看神经网络兴衰更替-Haohan-Wang-Bhiksha-Raj-张易" class="headerlink" title="CMU论文：一部深度学习发展史，看神经网络兴衰更替  Haohan Wang/Bhiksha Raj/张易"></a><a href="http://mp.weixin.qq.com/s/xNERrFKU4tY3lGmwBIJC9w" target="_blank" rel="external">CMU论文：一部深度学习发展史，看神经网络兴衰更替</a>  Haohan Wang/Bhiksha Raj/张易</h3><ul>
<li>论文地址：<a href="https://128.84.21.199/abs/1702.07800" target="_blank" rel="external">https://128.84.21.199/abs/1702.07800</a></li>
<li>从亚里士多德的联想主义心理学到神经网络的优化方法，CMU的这篇最新论文回顾解析了深度学习的演化历史，不仅提供了一个全面的背景知识，而且总结了一座座发展里程碑背后的闪光思想，为未来的深度学习研究提供了方向。</li>
<li>人工智能的发展或许可以追溯到公元前仰望星空的古希腊人，当亚里士多德为了解释人类大脑的运行规律而提出了联想主义心理学的时候，他恐怕不会想到，两千多年后的今天，人们正在利用联想主义心理学衍化而来的人工神经网络，构建超级人工智能，一起又一次地挑战人类大脑认知的极限；</li>
<li>联想主义心理学是一种理论，认为人的意识是一组概念元素，被这些元素之间的关联组织在一起。受柏拉图的启发，亚里士多德审视了记忆和回忆的过程，提出了四种联想法则：邻接（空间或时间上接近的事物或事件倾向于在意识中相关联）、频率（两个事件的发生次数与这两个事件之间的关联强度成正比）、相似性（关于一个事件的思维倾向于触发类似事件的思维）、对比（关于一个事件的思维倾向于触发相反事件的思维）。</li>
<li>1949年，Hebb提出了那条著名的规则：一起发射的神经元连在一起。更具体的表述是：“当神经元A的轴突和神经元B足够接近并反复或持续激发它时，其中一个或两个神经元就会发生增长或新陈代谢的变化，例如激发B的神经元之一——A efficiency——会增加。” </li>
<li>尽管Hebbian学习规则被视为奠定了神经网络的基础，但今天看来它的缺陷是显而易见的：随着共同出现的次数增加，连接的权重不断增加，主信号的权重将呈指数增长。这就是Hebbian学习规则的不稳定性。</li>
<li>将感知器放在一起，就变成了基本的神经网络。通过并列放置感知器，我们能得到一个单层神经网络。通过堆叠一个单层神经网络，我们会得到一个多层神经网络，这通常被称为多层感知器（MLP ）。单层神经网络具有局限性，正是这种局限性导致了相关的研究曾经一度停滞了进二十年，但同时，也正是这种局限性刺激了神经网络向更高层结构进发，渐渐迎来了如今的深度学习时代。</li>
<li>神经网络的一个显著特性，即众所周知的通用逼近属性，可以被粗略描述为MLP可以表示任何函数。可以从以下三方面探讨这一属性：布尔逼近（一个隐藏层的MLP可以准确的表示布尔函数）、连续逼近（一个隐藏层的MLP可以以任意精度逼近任何有界连续函数）、任意逼近（两个隐藏层的MLP可以以任意精度逼近任何函数）。</li>
<li>universal approximation成为如今神经网络与深度学习一片繁荣景象的重要理论基石，universal approximation的相关理论——一个多层神经网络具备表达任何方程的能力——已经成为深度学习的标志性特点。</li>
<li>从八十年代的Self Organizing Map到 Hopfield Network, 再到鼎鼎大名的Boltzmann Machine和Restricted Boltzmann  Machine，直到Hinton塑造的Deep Belief Network。深度学习的研究一路走来，悠长的历史之中，作者带领我们研读了这几个璀璨明星的诞生过程，以及这些作品诞生时的内在联系。下图总结了涉及的模型，水平轴代表这些模型的计算复杂度，而垂直轴代表表达能力，这是六个里程碑式的模型。</li>
</ul>
<p><img src="http://mmbiz.qpic.cn/mmbiz_png/UicQ7HgWiaUb0KmiabiaAYL2leR3beqyIhQyc0z6hq5kRy6wr4g89VHsarib8ALSd1I6okiarOdLrkH5qT7z2nE0icQsw/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1" alt=""></p>
<ul>
<li>卷积神经网络的谱系主要是从对人类视觉皮层的认识演变而来。卷积神经网络的视觉问题的成功原因之一是：复制人类视觉系统的仿生设计。卷积作为一个非常有效的视觉特征提取工具，几乎是深度学习在计算机视觉问题上如此成功的基石。</li>
<li>递归神经网络（RNN）是神经网络的一种，其单位的连接形成了有向循环; 这种性质赋予了其处理时间数据的能力。</li>
<li>优化是深度学习发展历史上不可回避的课题。目前存在的优化方式有：梯度法、剔除法、BatchNormalization。</li>
</ul>
<h3 id="北大AI公开课第1讲：雷鸣评人工智能前沿与行业结合点-新智元"><a href="#北大AI公开课第1讲：雷鸣评人工智能前沿与行业结合点-新智元" class="headerlink" title="北大AI公开课第1讲：雷鸣评人工智能前沿与行业结合点 新智元"></a><a href="http://mp.weixin.qq.com/s/UPsHwCqF4MJ1uTAunfza3g" target="_blank" rel="external">北大AI公开课第1讲：雷鸣评人工智能前沿与行业结合点</a> 新智元</h3><p>视频回放链接：<a href="http://www.iqiyi.com/l_19rrf6l46v.html" target="_blank" rel="external">http://www.iqiyi.com/l_19rrf6l46v.html</a></p>
<h3 id="北大AI公开课第2讲：雷鸣-amp-余凯漫谈嵌入式AI-新智元"><a href="#北大AI公开课第2讲：雷鸣-amp-余凯漫谈嵌入式AI-新智元" class="headerlink" title="北大AI公开课第2讲：雷鸣&amp;余凯漫谈嵌入式AI 新智元"></a><a href="http://mp.weixin.qq.com/s/cl8FMJagC9GZvSIdVAmrJQ" target="_blank" rel="external">北大AI公开课第2讲：雷鸣&amp;余凯漫谈嵌入式AI</a> 新智元</h3><p>视频回放链接：<a href="http://www.iqiyi.com/w_19rtza2dh9.html" target="_blank" rel="external">http://www.iqiyi.com/w_19rtza2dh9.html</a></p>
<h3 id="北大AI公开课第3讲：蚂蚁金服漆远-人工智能驱动的金融生活服务-新智元"><a href="#北大AI公开课第3讲：蚂蚁金服漆远-人工智能驱动的金融生活服务-新智元" class="headerlink" title="北大AI公开课第3讲：蚂蚁金服漆远 人工智能驱动的金融生活服务 新智元"></a><a href="http://mp.weixin.qq.com/s/Sj0jUshXXLCfJsxvoH5eYg" target="_blank" rel="external">北大AI公开课第3讲：蚂蚁金服漆远 人工智能驱动的金融生活服务</a> 新智元</h3><p>视频回放链接：<a href="http://www.iqiyi.com/l_19rrfk4wof.html" target="_blank" rel="external">http://www.iqiyi.com/l_19rrfk4wof.html</a></p>
<h3 id="北大AI公开课第4讲：吴甘沙：智能驾驶，有多少AI可以重来-新智元"><a href="#北大AI公开课第4讲：吴甘沙：智能驾驶，有多少AI可以重来-新智元" class="headerlink" title="北大AI公开课第4讲：吴甘沙：智能驾驶，有多少AI可以重来 新智元"></a><a href="http://mp.weixin.qq.com/s/deWPwaw4IWY4GcC-LK7Ilw" target="_blank" rel="external">北大AI公开课第4讲：吴甘沙：智能驾驶，有多少AI可以重来</a> 新智元</h3><p>视频回放链接：<a href="http://www.iqiyi.com/w_19ru020epp.html" target="_blank" rel="external">http://www.iqiyi.com/w_19ru020epp.html</a></p>
<h3 id="北大AI公开课第5讲：小米黄江吉-产品化引领人工智能硬件发展-新智元"><a href="#北大AI公开课第5讲：小米黄江吉-产品化引领人工智能硬件发展-新智元" class="headerlink" title="北大AI公开课第5讲：小米黄江吉 产品化引领人工智能硬件发展 新智元"></a><a href="http://mp.weixin.qq.com/s/Wdz0wNSpf_5FWkUOJ-XfEw" target="_blank" rel="external">北大AI公开课第5讲：小米黄江吉 产品化引领人工智能硬件发展</a> 新智元</h3><p>视频回放链接：<a href="http://www.iqiyi.com/l_19rrfgd203.html" target="_blank" rel="external">http://www.iqiyi.com/l_19rrfgd203.html</a></p>
<h3 id="实例化DevOps原则-伍斌"><a href="#实例化DevOps原则-伍斌" class="headerlink" title="实例化DevOps原则 伍斌"></a><a href="http://mp.weixin.qq.com/s?__biz=MjM5MjY3OTgwMA==&amp;mid=2652456410&amp;idx=1&amp;sn=e1e54a5b790969e1cd86f70f4d4363db" target="_blank" rel="external">实例化DevOps原则</a> 伍斌</h3><ul>
<li>DevOps的起源可以分为两条线：比利时独立咨询师Patrick Debois思考能否把敏捷的实践引入Ops团队；图片分享网站Flickr的两个开发者于2009年发表了一个引燃DevOps的演讲－－《每天部署10次以上：Flickr公司的Dev与Ops的合作》；</li>
<li>Flickr公司的两位演讲者所表达的“Dev和Ops的共同目标是让业务所要求的那些变化能随时上线可用”这一观点，其实就是DevOps的愿景。而要达到这一点，可以使用一个现成的工具：精益。源自丰田生产方式的“精益”的愿景就是“Shortest lead time”，即用最短的时间来完成从客户下订单到收到货物的全过程。这恰好能帮助实现DevOps的上述愿景。</li>
<li>从上面DevOps的起源中能够看出三点：DevOps源自草根社区，最初并没有什么自上而下设计出来的理论框架；DevOps背后的原则，就是上面两条线中所涉及的敏捷和精益的原则；DevOps的愿景是让业务所要求的那些变化能随时上线可用。 </li>
<li>一些DevOps从业者，纷纷设定自己的DevOps框架。其中比较有名的框架有Damon Edwards所定义并被Jez Humble所修订的CALMS（Culture, Automation, Lean, Metrics, ），和Gene Kim所定义的The Three Ways。</li>
<li></li>
</ul>
<h3 id="TDD-is-dead-Long-live-testing-David-Heinemeier-Hansson"><a href="#TDD-is-dead-Long-live-testing-David-Heinemeier-Hansson" class="headerlink" title="TDD is dead. Long live testing.  David Heinemeier Hansson"></a><a href="http://david.heinemeierhansson.com/2014/tdd-is-dead-long-live-testing.html" target="_blank" rel="external">TDD is dead. Long live testing.</a>  David Heinemeier Hansson</h3><ul>
<li>DHH是Ruby on Rails的创始人，Basecamp公司的创始人和CTO；</li>
<li>DHH先批判了测试先行的教条主义，虽然它能提升开发人员对软件质量的自信，且对于自动回归测试是有帮助的，但它不应该作为每日工作的教条；而业界近些年来对于TDD的推崇以及对未采用TDD开发的嘲讽更让DHH发火；当DHH多次尝试TDD而发现这会伤害程序的设计时，DHH声明不以TDD的方式开发软件了；</li>
<li>DHH的建议：重新平衡单元测试和系统测试，减少对于单元测试的重视，将更多的精力投入到系统测试；但千万不要跳入只做系统测试的极端里。</li>
</ul>
<h3 id="Is-TDD-Dead-Martin-Fowler-David-Heinemeier-Hansson-Kent-Beck"><a href="#Is-TDD-Dead-Martin-Fowler-David-Heinemeier-Hansson-Kent-Beck" class="headerlink" title="Is TDD Dead? Martin Fowler/David Heinemeier Hansson/Kent Beck"></a><a href="https://martinfowler.com/articles/is-tdd-dead/" target="_blank" rel="external">Is TDD Dead?</a> Martin Fowler/David Heinemeier Hansson/Kent Beck</h3><ul>
<li>Part 1: <a href="https://www.youtube.com/watch?v=z9quxZsLcfo" target="_blank" rel="external">https://www.youtube.com/watch?v=z9quxZsLcfo</a></li>
<li>Part 2: <a href="https://www.youtube.com/watch?v=JoTB2mcjU7w" target="_blank" rel="external">https://www.youtube.com/watch?v=JoTB2mcjU7w</a></li>
<li>Part 3: <a href="https://www.youtube.com/watch?v=YNw4baDz6WA" target="_blank" rel="external">https://www.youtube.com/watch?v=YNw4baDz6WA</a></li>
<li>Part 4: <a href="https://www.youtube.com/watch?v=dGtasFJnUxI" target="_blank" rel="external">https://www.youtube.com/watch?v=dGtasFJnUxI</a></li>
<li>Part 5 &amp; Part 6: <a href="https://www.youtube.com/watch?v=gWD6REVeKW4" target="_blank" rel="external">https://www.youtube.com/watch?v=gWD6REVeKW4</a> </li>
</ul>
<h3 id="让我们再聊聊TDD-刘冉"><a href="#让我们再聊聊TDD-刘冉" class="headerlink" title="让我们再聊聊TDD 刘冉"></a><a href="http://mp.weixin.qq.com/s/dmreBAzk2Mz94YfkXN8PHg" target="_blank" rel="external">让我们再聊聊TDD</a> 刘冉</h3><ul>
<li>总结一下，技术人员拒绝TDD的主要原因在于难度大、工作量大、Mock的大量使用导致很难测试业务价值等。这些理解主要是建立在片面的理解和实践之上，而在我的认知中，TDD的核心是：<strong>先写测试，并使用它帮助开发人员来驱动软件开发</strong>。</li>
<li>首先是先写测试，这里的测试并不只是单元测试，也不是说一定要使用mock和stub来做测试。这里的测试就是指软件测试本身，可以是基于代码单元的单元测试，可以是基于业务需求的功能测试，也可以是基于特定验收条件的验收测试。其次是帮助开发人员，主要是帮助开发人员理解软件的功能需求和验收条件，帮助其思考和设计代码，从而达到驱动开发的目的</li>
<li>TDD是包含两部分：ATDD（验收驱动测试开发，首先BA或者QA编写验收测试用例，然后Dev通过验收测试来理解需求和验收条件，并编写实现代码直到验收测试用例通过）与UTDD（单元驱动测试开发，首先Dev编写单元测试用例，然后编写实现代码直到单元测试通过）。</li>
</ul>
<p><img src="http://mmbiz.qpic.cn/mmbiz_png/aaVJqS7LaMLN7tU0IqFick0jH1uSDLQ5fVd3nXpDRicibEZ3JMjw6CDr0KUk3F1jJ1TmdkcLOaPLQaPUrHZicvRJFQ/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1" alt="TDD包括ATDD和UTDD"></p>
<ul>
<li>TDD不是银弹，不要期望它能解决任何问题，无论是UTDD、EDD还是BDD，根据自己项目的实际情况，比如资金、人力资源、时间、组织架构等，合理的选择。  </li>
<li>TDD并没有死，死的是你的持续学习、思考、实践与总结。TDD其实早已融入日常的软件开发工作中，只是很多人还没有意识到。</li>
</ul>
<h3 id="让我们再聊聊TDD-续——人人都在做TDD-刘冉"><a href="#让我们再聊聊TDD-续——人人都在做TDD-刘冉" class="headerlink" title="让我们再聊聊TDD 续——人人都在做TDD 刘冉"></a><a href="https://mp.weixin.qq.com/s/tm70lIohbbhlZ50LoFD5HA" target="_blank" rel="external">让我们再聊聊TDD 续——人人都在做TDD</a> 刘冉</h3><ul>
<li>现实世界中TDD的实施一般分为三个阶段，即无意识的TDD、被动通过技术实现的TDD、以及有意识和主动通过技术实现的TDD。</li>
<li>无意识的TDD：当拿到一个新的软件需求时，首先会思考如何实现，其中包括当前软件架构、业务分解、实现设计、代码分层、代码实现等，然后通过思考和设计所得到的产出物来驱动代码实现，进而在代码实现中会思考如何通过一个或多个函数或者算法来实现业务逻辑，这类思考其实已经是意识思维上的TDD，它帮助开发人员先在大脑里面设计并验证代码实现，甚至帮助其重构代码；其实开发人员在开发前思考测试逻辑和用例的过程就是在做TDD了；只不过这是初级的无意识的TDD，没有明确的产出来协助和规范这个测试驱动开发方式，也缺乏快速反馈、度量、传递和协作等；</li>
<li>被动通过技术实现TDD：由于意识层面上的难易程度和工作量都比技术层面上相对较小，所以前者实施起来相对容易一些，而后者则相对较难，所以如果通过了各种手段强行实施TDD，而没有主动去摆正做TDD的意识，甚至没有足够的技术能力，那么这样的TDD就是一个倒三角，非常容易倒塌；</li>
<li>有意识和主动通过技术实现TDD：首先要突破思维意识的局限，认识到TDD的普遍存在性和适用性，不要害怕和排斥TDD这种思维和开发模式；其次要主动学习，并刻意练习TDD的技术实现，提升自己的技术能力，从而在技术层面能更容易的实现TDD，摆脱被动TDD的困境；只有大量的刻意练习才能让你在真实的代码编写过程中去思考和理解TDD，去运用你通过学习得到的知识，最终才能做到有意识和主动的通过技术去实现TDD，TDD的倒三角才能变成一个稳定的砖块，然后哪里需要往哪里搬。</li>
</ul>
<p><img src="http://mmbiz.qpic.cn/mmbiz_png/aaVJqS7LaMKMVkVm2qvJxgkWcicPgeIgXShj5d81Z98AMXJcfvASJlstibhFHSLDw3tI2XxSMuUSGpDMic3rLTSibg/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1" alt="TDD"></p>
<h3 id="推行TDD的思考-张逸"><a href="#推行TDD的思考-张逸" class="headerlink" title="推行TDD的思考  张逸"></a><a href="http://mp.weixin.qq.com/s/c2xmaqjD9U07PWX4BMOyZA" target="_blank" rel="external">推行TDD的思考</a>  张逸</h3><ul>
<li>开发人员的质量意识：由开发人员编写测试带来的收益，最重要的一点不在于测试本身，而在于它能促进开发、测试以及需求分析人员的交流与沟通；也能让开发者从消费者角度去思考接口的设计；软件质量除了外部质量之外，内部质量同等重要，维护成本的增加主要归咎于内部质量的糟糕；</li>
<li>需求分析与任务分解：TDD要求我们在编写测试之前要做好合理的任务分解。若没有很好地理解需求，任务分解就无法顺利进行；任务分解应该是有层次的，即业务价值——&gt;业务功能——&gt;业务实现；任务分解是TDD的核心，是驱动设计和开发的重要力量；</li>
<li>测试先行的编程习惯：任务分解应该是TDD的起点，多数开发者未能形成任务分解的习惯，因此在改变为测试先行的时候，错以为应该一上来就写测试；测试驱动开发仍可进行事先设计，设计并不仅包含技术层面的设计如对OO思想乃至设计模式的运用，它本身还包括对需求的分析与建模；测试驱动开发提倡的任务分解，实际上就是一种需求的分析，如何寻找职责，以及识别职责的承担者则可以视为建模设计；在开始测试驱动开发之前，做适度的事先设计，还有利于我们仔细思考技术实现的解决方案，它与测试驱动接口的设计并不相悖；</li>
<li>重构能力：TDD的核心是红——绿——重构，没有好的重构能力，TDD就会有缺失，若说代码的内部质量是生命的话，重构就是灵魂；重构手法与代码坏味道一一对应，若有测试保障，重构就变得安全；重要的是要找到重构的节奏感，即小步前行，每次重构必运行测试的良好习惯；在TDD过程中，若能结对自然是上佳选择，当一个人在掌控键盘时，另一个人就可以重点关注代码的可读性，看看代码是否散发出臭味；</li>
<li>单元测试的基础设施：最好能找到一些开源的测试框架，包括生成测试数据，模拟测试行为等，因为你遇到的问题，别人可能早已遇见过。</li>
</ul>
<h3 id="大数据时代的新型数据库-—-图数据库-Neo4j-的应用-张帜"><a href="#大数据时代的新型数据库-—-图数据库-Neo4j-的应用-张帜" class="headerlink" title="大数据时代的新型数据库 — 图数据库 Neo4j 的应用 张帜"></a><a href="http://mp.weixin.qq.com/s?__biz=MzI3MzEzMDI1OQ==&amp;mid=2651816597&amp;idx=1&amp;sn=e061823e2020258a729dcf7498f8aba4" target="_blank" rel="external">大数据时代的新型数据库 — 图数据库 Neo4j 的应用</a> 张帜</h3><ul>
<li>什么是图数据库：是基于数学里的图论的理论和算法而实现的高效处理复杂关系网络的新型数据库系统，它实际上就是处理关系的、处理网络的数据库系统。图数据库是善于处理大量的、复杂的、互联的、多变的数据，它处理这些数据的效率，远远高于关系型数据库。</li>
<li>从数据库的结构来看，它包含的概念非常的简单，他包含的概念只有节点和关系。节点可以带标签，节点和关系也都可以带属性。</li>
</ul>
<p><img src="http://mmbiz.qpic.cn/mmbiz_jpg/3xsFRgx4kHrLeb7AhVmAnDeMrVwXM98asCP41iaOpd0dLez09Cv4aK9yfrbT1IcnbOHm1P7iadtwSaAUTBhzeqpA/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1" alt="节点、关系和属性"></p>
<p><img src="http://mmbiz.qpic.cn/mmbiz_jpg/3xsFRgx4kHrLeb7AhVmAnDeMrVwXM98aeUtYBEyanQich093rOm4o0HjRsyx6IPs5uq0TXWE0XSvjefZO81Yllw/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1" alt="创建节点、关系"></p>
<p><img src="http://mmbiz.qpic.cn/mmbiz_jpg/3xsFRgx4kHrLeb7AhVmAnDeMrVwXM98aPQnqgn8hBNeDNLz2NFBlUz8No2KTn27CgZ1iaiayoxq5cKSN8PSPV9ww/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1" alt="查询"></p>
<ul>
<li>为什么要用图数据库：世界本来就是由各种关系组成的；关系型数据库处理复杂关系的时候，建模难、性能低、查询难、扩展难；图数据库它是专门为处理复杂关系而创建出来的，它具有开发的优势和部署的优势。</li>
<li>Neo4j的关键产品特征：社区版不支持集群，免费；企业版支持集群，是收费的。</li>
</ul>
<p><img src="http://mmbiz.qpic.cn/mmbiz_jpg/3xsFRgx4kHrLeb7AhVmAnDeMrVwXM98ajpQAxkojh6gQpO5NicDiaTgkiaiaYBoboxVGicTN3KPx6Prw3icgBXg1C5NA/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1" alt="Neo4j的关键产品特征"></p>
<h3 id="Apache-Spark和Apache-Storm的区别-fuqingchuan"><a href="#Apache-Spark和Apache-Storm的区别-fuqingchuan" class="headerlink" title="Apache Spark和Apache Storm的区别 fuqingchuan"></a><a href="http://mp.weixin.qq.com/s/GgUhOTTbmliGsvdp6qE3OA" target="_blank" rel="external">Apache Spark和Apache Storm的区别</a> fuqingchuan</h3><ul>
<li>Apache Spark是基于内存的分布式数据分析平台，旨在解决快速批处理分析任务、迭代机器学习任务、交互查询以及图处理任务。其最主要的特点在于，Spark使用了RDD或者说弹性分布式数据集。RDD非常适合用于计算的流水线式并行操作。RDD的不变性(immutable)保证，使其具有很好的容错能力。</li>
</ul>
<p><img src="http://mmbiz.qpic.cn/mmbiz_png/8y6qG4b7IlKAuicCqd4ODeOibt1TS0XFrVRtG19JexY5T9g7IcUibNMtHRIJzjbpEQmwddqvx7GSoYhxic8D6Ex0RQ/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1" alt="Spark架构"> </p>
<ul>
<li>Apache Storm专注于流处理或者一些调用复杂的事件处理。Storm实现了一种容错方法，用于在事件流入系统时执行计算或流水线化多个计算。人们可以使用Storm在非结构化数据流入系统到期望的格式时对其进行转换。</li>
</ul>
<p><img src="http://mmbiz.qpic.cn/mmbiz_png/8y6qG4b7IlKAuicCqd4ODeOibt1TS0XFrVniaOm0DboNBGE137aFpFtCDWkcQtrIn5IXbunp41caucuSXCpECMyDA/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1" alt="Storm架构"></p>
<ul>
<li>Storm和Spark专注于相当不同的应用场景。对比Storm Trident和Spark Streaming，应该是更加公平的比较。由于Spark的RDD本质上是不可变的，Spark Streaming实现了一种方法，用于在用户定义的时间间隔中“批处理”传入的更新，并将其转换为自己的RDD。 然后Spark通用的并行运算符就可以对这些RDD执行计算。这与Storm处理每个事件不同，Storm是真正的流式处理。</li>
<li>总而言之，这两种技术之间的一个主要区别是Spark执行数据并行计算，而Storm执行任务并行计算，这两种设计都是各自领域内的权衡。</li>
</ul>
<h3 id="大数据系统的Lambda架构-张逸"><a href="#大数据系统的Lambda架构-张逸" class="headerlink" title="大数据系统的Lambda架构 张逸"></a><a href="http://mp.weixin.qq.com/s?__biz=MzA4NTkwODkyMQ==&amp;mid=208690964&amp;idx=1&amp;sn=03497b08e36f091c4afdf4b30532ea8f" target="_blank" rel="external">大数据系统的Lambda架构</a> 张逸</h3><ul>
<li>在大数据处理系统中，如何有效地将real time与batch job结合起来，既发挥前者对响应的实时性，又能解决对海量数据的分析与处理？答案就是Lambda架构思想。</li>
<li>传统系统的问题：无法很好地支持系统的可伸缩性；数据库对于分区是不了解的，无法帮助你应对分区、复制与分布式查询；最糟糕的问题是系统并没有为人为错误进行工程设计，仅靠备份是不能治本的；</li>
<li>数据系统的概念：如果数据系统通过查找过去的数据去回答问题，则通常需要访问整个数据集。因此可以给data system的最通用的定义：Query = function(all data)</li>
<li>一个大数据系统必须具备的属性包括：健壮性和容错性（Robustness和Fault Tolerance）、低延迟的读与更新（Low Latency reads and updates）、可伸缩性（Scalability）、通用性（Generalization）、可扩展性（Extensibility）、内置查询（Ad hoc queries）、维护最小（Minimal maintenance）、可调试性（Debuggability）；</li>
<li>Lambda架构：主要思想就是将大数据系统构建为多个层次，如下图所示</li>
</ul>
<p><img src="http://mmbiz.qpic.cn/mmbiz/d3Qa7X5fakCejz6If7PwqE84e6DhKj3LsYAy0WogdNxlZZCVk6RtBdjvWrkYOgLFaIq5GblDUZvD0BF1ksbaWA/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1" alt="Lambda架构分层"></p>
<ul>
<li>理想状态下，任何数据访问都可以从表达式Query = function(all data)开始，但是，若数据达到相当大的一个级别（例如PB），且还需要支持实时查询时，就需要耗费非常庞大的资源。一个解决方式是预运算查询函数（precomputed query funciton）。Mathan Marz将这种预运算查询函数称之为Batch View，当需要执行查询时，可以从Batch View中读取结果。这样一个预先运算好的View是可以建立索引的，因而可以支持随机读取。于是系统就变成：</li>
</ul>
<pre><code>batch view = function(all data)
query = function(batch view)
</code></pre><ul>
<li>Batch Layer：在Lambda架构中，实现batch view = function(all data)的部分被称之为batch layer。它承担了两个职责：<br>存储Master Dataset，这是一个不变的持续增长的数据集；针对这个Master Dataset进行预运算。利用Batch Layer进行预运算的作用实际上就是将大数据变小，从而有效地利用资源，改善实时查询的性能。</li>
<li>Serving Layer：Serving Layer负责对batch view进行操作，从而为最终的实时查询提供支撑。因此Serving Layer的职责包含：对batch view的随机访问；更新batch view。Serving Layer应该是一个专用的分布式数据库，以支持对batch view的加载、随机读取以及更新。注意，它并不支持对batch view的随机写，因为随机写会为数据库引来许多复杂性。</li>
<li>Speed Layer：从对数据的处理来看，speed layer与batch layer非常相似，它们之间最大的区别是前者只处理最近的数据，后者则要处理所有的数据。另一个区别是为了满足最小的延迟，speed layer并不会在同一时间读取所有的新数据，相反，它会在接收到新数据时，更新realtime view，而不会像batch layer那样重新运算整个view。speed layer是一种增量的计算，而非重新运算（recomputation）。因而，Speed Layer的作用包括：对更新到serving layer带来的高延迟的一种补充；快速、增量的算法；最终Batch Layer会覆盖speed layer；</li>
<li>总结下来，Lambda架构就是如下的三个等式：</li>
</ul>
<pre><code>batch view = function(all data)
realtime view = function(realtime view, new data)
query = function(batch view . realtime view)
</code></pre><p>整个Lambda架构如下图所示：</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/d3Qa7X5fakCejz6If7PwqE84e6DhKj3LxUnITFzls4Vzdzo0okkC97nMrOQtg83MBXVwD3BExm2zicd1AlJlcoA/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1" alt="Lambda架构"></p>
<h3 id="Lambda架构与推荐在电商网站实践-王富平"><a href="#Lambda架构与推荐在电商网站实践-王富平" class="headerlink" title="Lambda架构与推荐在电商网站实践 王富平"></a><a href="http://mp.weixin.qq.com/s?__biz=MzAwMDU1MTE1OQ==&amp;mid=401800864&amp;idx=1&amp;sn=e86e31a4aa6279f5b515f9116da47d59" target="_blank" rel="external">Lambda架构与推荐在电商网站实践</a> 王富平</h3><ul>
<li>Lambda架构：Lambda架构由Storm的作者Nathan Marz提出。旨在设计出一个能满足实时大数据系统关键特性的架构，具有高容错、低延时和可扩展等特性。Lambda架构整合离线计算和实时计算，融合不可变性（Immutability）、读写分离和复杂性隔离等一系列架构原则，可集成Hadoop、Kafka、Storm、Spark、HBase等各类大数据组件。</li>
<li>Lambda有两个假设：不可变假设（Lambda架构要求data不可变）、Monoid假设（理想情况下满足Monoid 的function可以转换为 query = function(all data/ 2) + function(all data/ 2)）；</li>
<li>Lambda三层架构：批处理层（批量处理数据，生成离线结果）、实时处理层（实时处理在线数据，生成增量结果）、服务层（结合离线、在线计算结果，推送上层）；</li>
</ul>
<p><img src="http://mmbiz.qpic.cn/mmbiz/8XkvNnTiapONibSkm2GM78fSeyNicjYLiaS4KLn4NnjP1tN9xKmiapauPIKMw9EW4Nu6N18W2DlDVWUQu2RxU7ljWkA/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1" alt="Lambda三层架构"></p>
<ul>
<li>Lambda架构缺点：Lambda需要将所有的算法实现两次，一次是为批处理系统，另一次是为实时系统，还要求查询得到的是两个系统结果的合并。考虑到将复杂算法正确地实现一次都是一个挑战，执行两次这样的任务以及调试不可避免的问题显然是难上加难。除此之外，运维两个分布式多节点的服务肯定比运维一个更难。【参加<a href="http://www.infoq.com/cn/news/2014/09/lambda-architecture-questions】" target="_blank" rel="external">http://www.infoq.com/cn/news/2014/09/lambda-architecture-questions】</a></li>
<li>推荐系统的最终目的是提高转化率，手段是推送用户感兴趣的、需要的产品。1号店会根据你实时浏览、加车、收藏、从购物车删除、下单等行为，计算相关产品的权重，把相应的产品立刻更新到猜你喜欢栏位。</li>
<li>Netflix推荐架构：批处理层（从Hive、pig数据仓库，离线计算推荐模型，生成离线推荐结果）、实时处理层（从消息队列实时拉取用户行为数据与事件，生成在线推荐结果）、服务层（结合离线、在线推荐结果，为用户生成推荐列表）；</li>
</ul>
<p><img src="http://mmbiz.qpic.cn/mmbiz/8XkvNnTiapONibSkm2GM78fSeyNicjYLiaS4uGGpcicBjCDXz90x0gib9Z6A1flicrwFmicyTt3DyvDpS3t3Ub5uu4OjKw/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1" alt="Netflix推荐架构"></p>
<ul>
<li>1号店推荐系统实践：推荐引擎组件包括用户意图、用户画像、千人千面、情境推荐、反向推荐、主题推荐；</li>
</ul>
<p><img src="http://mmbiz.qpic.cn/mmbiz/8XkvNnTiapONibSkm2GM78fSeyNicjYLiaS4FYPdPln0wy1kLaxOKPc6geibsjAkVNgWbvHr5gt1pib07OqvWMF10lBQ/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1" alt="1号店推荐系统实践"></p>
<h3 id="技术的边界-阮一峰"><a href="#技术的边界-阮一峰" class="headerlink" title="技术的边界 阮一峰"></a><a href="http://www.ruanyifeng.com/blog/2017/03/boundary.html" target="_blank" rel="external">技术的边界</a> 阮一峰</h3><ul>
<li>当代社会就像一座机器组成的监狱，学会技术可以摆脱牢房。</li>
<li>我们理应享受技术成果，问题是，技术正变得越来越先进，也越来越难懂，大多数人已经不能够理解技术了。</li>
<li>技术已经到了这样一个地步：我们走一步看一步，谁也不知道十年后，技术会突破到什么程度。</li>
<li>我最近常想，技术有没有边界呢？一旦接近”绝境长城”，我们会自觉停在那里，不再往下发展吗？举例来说，人工智能领域有一个概念，叫做”终极智能”。意思是，当机器的智能达到这种程度时，就不需要人类再做发明创造了，因为机器自己就会发明创造。如果这种”终极智能”真的可能实现，技术要不要去实现它呢？</li>
<li>一个依赖技术的高科技、高度自动化的社会，也是一个非常脆弱的社会。整个人类正坐在一架软件驾驶的飞机里面，只能祈祷软件运行永远不发生错误。一旦发生问题，人类就会坠机。</li>
</ul>
<h3 id="分布式开放消息系统-RocketMQ-的原理与实践-CHEN川"><a href="#分布式开放消息系统-RocketMQ-的原理与实践-CHEN川" class="headerlink" title="分布式开放消息系统(RocketMQ)的原理与实践 CHEN川"></a><a href="http://www.jianshu.com/p/453c6e7ff81c" target="_blank" rel="external">分布式开放消息系统(RocketMQ)的原理与实践</a> CHEN川</h3><ul>
<li>分布式消息系统作为实现分布式系统可扩展、可伸缩性的关键组件，需要具有高吞吐量、高可用等特点。而谈到消息系统的设计，就回避不了两个问题：消息的顺序问题和消息的重复问题；</li>
<li>消息有序指的是可以按照消息的发送顺序来消费。要实现严格的顺序消息，简单且可行的办法就是：保证“生产者 - MQServer - 消费者”是一对一对一的关系，但这样并行度就会成为消息系统的瓶颈，而且还需要更多的异常处理；有些问题，看起来很重要，但实际上我们可以通过合理的设计或者将问题分解来规避。如果硬要把时间花在解决问题本身，实际上不仅效率低下，而且也是一种浪费。从这个角度来看消息的顺序问题，我们可以得出两个结论：不关注乱序的应用实际大量存在、队列无序并不意味着消息无序；RocketMQ通过轮询所有队列的方式来确定消息被发送到哪一个队列，在获取到路由信息以后，会根据MessageQueueSelector实现的算法来选择一个队列，同一个OrderId获取到的肯定是同一个队列。</li>
<li>造成消息重复的根本原因是：网络不可达。只要通过网络交换数据，就无法避免这个问题。所以解决这个问题的办法就是绕过这个问题。那么问题就变成了：如果消费端收到两条一样的消息，应该怎样处理？消费端处理消息的业务逻辑保持幂等性，保证每条消息都有唯一编号且保证消息处理成功与去重表的日志同时出现；RocketMQ不保证消息不重复，如果你的业务需要保证严格的不重复消息，需要你自己在业务端去重。</li>
<li>RocketMQ除了支持普通消息，顺序消息，另外还支持事务消息。将大事务拆分成多个小事务异步执行，这样基本上能够将跨机事务的执行效率优化到与单机一致。</li>
<li>Producer轮询某topic下的所有队列的方式来实现发送方的负载均衡，如果Producer发送消息失败，会自动重试；</li>
<li>RocketMQ的消息存储是由consume queue和commit log配合完成的。</li>
<li>RocketMQ消息订阅有两种模式，一种是Push模式，即MQServer主动向消费端推送；另外一种是Pull模式，即消费端在需要时，主动到MQServer拉取。但在具体实现时，Push和Pull模式都是采用消费端主动拉取的方式。</li>
</ul>
<h3 id="当前服务器配置能承受多大的QPS？如何评估？-Susie-Xia-Anant-Rao-薛命灯"><a href="#当前服务器配置能承受多大的QPS？如何评估？-Susie-Xia-Anant-Rao-薛命灯" class="headerlink" title="当前服务器配置能承受多大的QPS？如何评估？ Susie Xia/Anant Rao/薛命灯"></a><a href="https://mp.weixin.qq.com/s?__biz=MzA5Nzc4OTA1Mw==&amp;mid=2659599084&amp;idx=1&amp;sn=a67802f91645f2a3864b584842676803" target="_blank" rel="external">当前服务器配置能承受多大的QPS？如何评估？</a> Susie Xia/Anant Rao/薛命灯</h3><ul>
<li>LinkedIn的基础设施上运行着数百个应用，它们为4.67亿的LinkedIn会员提供服务。为了能够准确地对服务容量极限进行评估，并有效地识别出容量瓶颈，我们的解决方案需要具备如下特点：利用生产环境来突破实验室的局限、使用真实的流量作为负载、最小化对用户体验造成的影响、低运营成本和开销、自动伸缩；</li>
<li>我们使用生产环境的真实流量，通过Redliner实现自动化的容量评估和准确的余量分析。Redliner在目标服务上运行压力测试，逐步增加流量，直到服务无法处理更多的流量为止，以此来评估服务的吞吐量。</li>
<li>Redliner自动从生产环境引入流量，并确保对用户只有很小的影响。在设计Redliner时，我们遵循了两个原则：影响最小化和完全自动化。</li>
<li>在进行流量重定向时，最主要的问题是如何避免对站点和用户造成影响。Redliner使用以下的策略来缓解对生产环境性能造成的影响。首先，通过增量的方式将流量导向redline实例。其次，Redliner对服务进行实时的监控，并根据实际情况来分发流量。Redliner捕捉实时的性能指标，并基于EKG健康评估规则的计算结果来确定服务的健康状况。</li>
<li>我们借助LinkedIn的技术平台保证Redliner自动化的健壮性和伸缩性。Redliner能够运行调度测试，通过EKG检测性能状态，还能利用A/B测试平台XLNT来动态地调整导向目标服务的流量。在经过几轮的迭代之后，Redliner就可以确定单个服务能够处理的最大QPS。一般整个过程需要不到一个小时的时间。</li>
<li>下图是Redliner的架构图，包含导流组件和容量评估组件。主要组件如下：导流层（代理/负载均衡器）、服务健康状态分析器和服务度量指标收集器。</li>
</ul>
<p><img src="http://mmbiz.qpic.cn/mmbiz_png/LaW7jDBKBg2hgd0MSVb4auiavNkXAR4ich8icBvIq0CqO733atJrBW2ueCWpWpjOhtwkibnJj96Wx9k1BmtLX2Yq1w/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1" alt="Redliner的架构图"></p>
<h3 id="解读GraphQL-王亦凡"><a href="#解读GraphQL-王亦凡" class="headerlink" title="解读GraphQL 王亦凡"></a><a href="https://mp.weixin.qq.com/s/U5WhVHKp4Q5ZW6JUamlufQ" target="_blank" rel="external">解读GraphQL</a> 王亦凡</h3><ul>
<li>GraphQL是Facebook推出的一个查询语言，可能和听起来不同，它并不是一个数据库查询语言，而是你可以用任何其他语言实现的一个用于查询的抽象层。</li>
<li>通常你可以通过GraphQL让你的客户端请求有权决定获取的数据结构，也可以通过GraphQL获得更好的多版本API兼容性。并且与大多数查询语言不同的是，GraphQL是一个静态类型的查询语言，这意味着你可以通过GraphQL获得更强大更安全的开发体验。</li>
<li>为什么要选择GraphQL？ GraphQL的核心目标就是取代RESTful API。</li>
<li>REST有什么问题？ 资源分类导致性能受限、在现代场景中难于维护、缺乏约束、严格，抽象，但并不能解决客户端问题；</li>
<li>GraphQL好在哪？Github宣布他们打算拥抱GraphQL；GraphQL用来构建客户端API，但它并不关心视图，也不关心服务的到底是什么客户端；至于请求什么数据，数据怎么组织，全都是客户端说了算；GraphQL的意思，顾名思义就是图查询语言；静态类型；兼容多版本；</li>
<li>GraphQL潜在的问题？ 可能会存在服务器性能隐患、安全问题、需要重新思考Cache策略。</li>
</ul>
<p><strong>免责声明：相关链接版本为原作者所有，如有侵权，请告知删除。</strong></p>
<p><strong>随手记系列：</strong></p>
<ul>
<li><a href="http://ginobefunny.com/post/reading_record_201702/">阅读随手记 201702</a></li>
<li><a href="http://ginobefunny.com/post/reading_record_201701/">阅读随手记 201701</a></li>
<li><a href="http://ginobefunny.com/post/reading_record_201612/">阅读随手记 201612</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;关键字：微服务, 架构, Event Sourcing, CQRS, Redis, TDD, 消息中间件, 缓存, RPC, 监控, 高性能, 高并发, 高可用, 机器学习, 深度学习, 人工智能。&lt;br&gt;
    
    </summary>
    
      <category term="Reading Record" scheme="http://ginobefunny.com/categories/Reading-Record/"/>
    
    
      <category term="微服务" scheme="http://ginobefunny.com/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"/>
    
      <category term="RPC" scheme="http://ginobefunny.com/tags/RPC/"/>
    
      <category term="机器学习" scheme="http://ginobefunny.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="http://ginobefunny.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="高性能" scheme="http://ginobefunny.com/tags/%E9%AB%98%E6%80%A7%E8%83%BD/"/>
    
      <category term="高并发" scheme="http://ginobefunny.com/tags/%E9%AB%98%E5%B9%B6%E5%8F%91/"/>
    
      <category term="高可用" scheme="http://ginobefunny.com/tags/%E9%AB%98%E5%8F%AF%E7%94%A8/"/>
    
      <category term="架构" scheme="http://ginobefunny.com/tags/%E6%9E%B6%E6%9E%84/"/>
    
      <category term="消息中间件" scheme="http://ginobefunny.com/tags/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
      <category term="缓存" scheme="http://ginobefunny.com/tags/%E7%BC%93%E5%AD%98/"/>
    
      <category term="监控" scheme="http://ginobefunny.com/tags/%E7%9B%91%E6%8E%A7/"/>
    
      <category term="人工智能" scheme="http://ginobefunny.com/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
      <category term="CQRS" scheme="http://ginobefunny.com/tags/CQRS/"/>
    
      <category term="Redis" scheme="http://ginobefunny.com/tags/Redis/"/>
    
      <category term="Event Sourcing" scheme="http://ginobefunny.com/tags/Event-Sourcing/"/>
    
      <category term="TDD" scheme="http://ginobefunny.com/tags/TDD/"/>
    
  </entry>
  
  <entry>
    <title>word2vec学习小记</title>
    <link href="http://ginobefunny.com/post/learning_word2vec/"/>
    <id>http://ginobefunny.com/post/learning_word2vec/</id>
    <published>2017-02-13T06:39:39.000Z</published>
    <updated>2017-02-14T09:18:41.387Z</updated>
    
    <content type="html"><![CDATA[<p>word2vec是Google于2013年开源推出的一个用于获取词向量的工具包，它简单、高效，因此引起了很多人的关注。最近项目组使用word2vec来实现个性化搜索，在阅读资料的过程中做了一些笔记，用于后面进一步学习。<br><a id="more"></a></p>
<p><strong>前注：word2vec涉及的相关理论和推导是非常严(ku)格(zao)的，本文作为一个初学者的学习笔记，希望能从自己的理解中尽量用简单的描述，如有错误或者歧义的地方，欢迎指正。</strong></p>
<h2 id="word2vec是什么？"><a href="#word2vec是什么？" class="headerlink" title="word2vec是什么？"></a>word2vec是什么？</h2><blockquote>
<p>This tool provides an efficient implementation of the continuous bag-of-words and skip-gram architectures for computing vector representations of words. These representations can be subsequently used in many natural language processing applications and for further research.</p>
</blockquote>
<p>从官方的介绍可以看出word2vec是一个将词表示为一个向量的工具，通过该向量表示，可以用来进行更深入的自然语言处理，比如机器翻译等。</p>
<p>为了理解word2vec的设计思想，我们有必要先学习一下自然语言处理的相关发展历程和基础知识。</p>
<h2 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h2><h3 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h3><ul>
<li>语言模型其实就是看一句话是不是正常人说出来的。这玩意很有用，比如机器翻译、语音识别得到若干候选之后，可以利用语言模型挑一个尽量靠谱的结果。在 NLP 的其它任务里也都能用到。</li>
<li>用数学表达的话，就是给定T个词<strong>w1,w2,…wT</strong>，看它是自然语言的概率P，公式如下所示：</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/14_learning_w2v/language_model.png" alt="语言模型形式化表达"></p>
<ul>
<li>举例来说，比如一段语音识别为“我喜欢吃梨”和“我喜欢吃力”，根据分词和上述公式，可以得到两种表述的概率计算分别为下面的公式，而其中每一个子项的概率我们可以事先通过大量的语料统计得到，这样我们就可以得到更好的识别效果。</li>
</ul>
<pre><code>P(&apos;我喜欢吃梨&apos;) = P(&apos;我&apos;) * P(&apos;喜欢&apos;|&apos;我&apos;) * P(&apos;吃&apos;|&apos;我&apos;,&apos;喜欢&apos;) * P(&apos;梨&apos;|&apos;我&apos;,&apos;喜欢&apos;,&apos;吃&apos;)
P(&apos;我喜欢吃力&apos;) = P(&apos;我&apos;) * P(&apos;喜欢&apos;|&apos;我&apos;) * P(&apos;吃力&apos;|&apos;我&apos;,&apos;喜欢&apos;)
</code></pre><h3 id="N-gram模型"><a href="#N-gram模型" class="headerlink" title="N-gram模型"></a>N-gram模型</h3><ul>
<li>通过上面的语言模型计算的例子，大家可以发现，如果一个句子比较长，那么它的计算量会很大；</li>
<li>牛逼的科学家们想出了一个<strong>N-gram模型</strong>来简化计算，在计算某一项的概率时Context不是考虑前面所有的词，而是前<strong>N-1</strong>个词；</li>
<li>当然牛逼的科学家们还在此模型上继续优化，比如<strong>N-pos模型</strong>从语法的角度出发，先对词进行词性标注分类，在此基础上来计算模型的概率；后面还有一些针对性的语言模型改进，这里就不一一介绍。</li>
<li>通过上面简短的语言模型介绍，我们可以看出核心的计算在于P(wi|Contenti)，对于其的计算主要有两种思路：一种是基于统计的思路，另外一种是通过函数拟合的思路；前者比较容易理解但是实际运用的时候有一些问题（比如如果组合在语料里没出现导致对应的条件概率变为0等），而函数拟合的思路就是通过语料的输入训练出一个函数P(wi|Contexti) = f(wi,Contexti;θ)，这样对于测试数据就直接套用函数计算概率即可，这也是机器学习中惯用的思路之一。</li>
</ul>
<h3 id="词向量表示"><a href="#词向量表示" class="headerlink" title="词向量表示"></a>词向量表示</h3><ul>
<li>自然语言理解的问题要转化为机器学习的问题，第一步肯定是要找一种方法把这些符号数学化。</li>
<li>最直观的就是把每个词表示为一个很长的向量。这个向量的维度是词表的大小，其中绝大多数元素为0，只有一个维度的值为1，这个维度就代表了当前的词。这种表示方式被称为<strong>One-hot Representation</strong>。这种方式的优点在于简洁，但是却无法描述词与词之间的关系。</li>
<li>另外一种表示方法是通过一个低维的向量（通常为50维、100维或200维），其基于“<strong>具有相似上下文的词，应该具有相似的语义</strong>”的假说，这种表示方式被称为<strong>Distributed Representation</strong>。它是一个稠密、低维的实数向量，它的每一维表示词语的一个潜在特征，该特征捕获了有用的句法和语义特征。其特点是将词语的不同句法和语义特征分布到它的每一个维度上去表示。这种方式的好处是可以通过空间距离或者余弦夹角来描述词与词之间的相似性。</li>
<li>以下我们来举个例子看看两者的区别：</li>
</ul>
<pre><code>// One-hot Representation 向量的维度是词表的大小，比如有10w个词，该向量的维度就是10w
v(&apos;足球&apos;) = [0 1 0 0 0 0 0 ......]
v(&apos;篮球&apos;) = [0 0 0 0 0 1 0 ......]

// Distributed Representation 向量的维度是某个具体的值如50
v(&apos;足球&apos;) = [0.26 0.49 -0.54 -0.08 0.16 0.76 0.33 ......]
v(&apos;篮球&apos;) = [0.31 0.54 -0.48 -0.01 0.28 0.94 0.38 ......]
</code></pre><ul>
<li>最后需要说明的是一个词的向量表示对于不同的语料和场景结果是不同的。下面就介绍一种最常用的计算词向量的语言模型。</li>
</ul>
<h3 id="神经网络概率语言模型"><a href="#神经网络概率语言模型" class="headerlink" title="神经网络概率语言模型"></a>神经网络概率语言模型</h3><ul>
<li>神经网络概率语言模型（NNLM）把词向量作为输入（初始的词向量是随机值），训练语言模型的同时也在训练词向量，最终可以同时得到语言模型和词向量。</li>
<li>Bengio等牛逼的科学家们用了一个三层的神经网络来构建语言模型，同样也是N-gram 模型。 网络的第一层是输入层，是是上下文的N-1个向量组成的(n-1)m维向量；第二层是隐藏层，使用tanh作为激活函数；第三层是输出层，每个节点表示一个词的未归一化概率，最后使用softmax激活函数将输出值归一化。</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/14_learning_w2v/nnlm.png" alt="神经网络概率语言模型"></p>
<ul>
<li>得到这个模型，然后就可以利用梯度下降法把模型优化出来，最终得到语言模型和词向量表示。</li>
</ul>
<h3 id="word2vec的核心模型"><a href="#word2vec的核心模型" class="headerlink" title="word2vec的核心模型"></a>word2vec的核心模型</h3><ul>
<li>word2vec在NNLM和<a href="http://techblog.youdao.com/?p=915" target="_blank" rel="external">其他语言模型</a>的基础进行了优化，有CBOW模型和Skip-Gram模型，还有Hierarchical Softmax和Negative Sampling两个降低复杂度的近似方法，两两组合出四种实现。</li>
<li>无论是哪种模型，其基本网络结构都是在下图的基础上，省略掉了隐藏层；</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/14_learning_w2v/w2v_model.jpg" alt="word2vec的核心模型"></p>
<p>CBOW和Skip-gram模型<br><img src="http://oi46mo3on.bkt.clouddn.com/14_learning_w2v/w2v_model_2.jpg" alt="word2vec的核心模型"></p>
<ul>
<li>CBOW（Continuous Bag-of-Words Model）是一种根据上下文的词语预测当前词语的出现概率的模型，其图示如上图左。CBOW是已知上下文，估算当前词语的语言模型；</li>
<li>而Skip-gram只是逆转了CBOW的因果关系而已，即已知当前词语，预测上下文，其图示如上图右；</li>
<li>Hierarchical Softmax使用的办法其实是借助了分类的概念。假设我们是把所有的词都作为输出，那么“足球”、“篮球”都是混在一起的。而Hierarchical Softmax则是把这些词按照类别进行区分的，二叉树上的每一个节点可以看作是一个使用哈夫曼编码构造的二分类器。在算法的实现中，模型会赋予这些抽象的中间节点一个合适的向量，真正的词会共用这些向量。这种近似的处理会显著带来性能上的提升同时又不会丢失很大的准确性。</li>
<li>Negative Sampling也是用二分类近似多分类，区别在于它会采样一些负例，调整模型参数使得可以区分正例和负例。换一个角度来看，就是Negative Sampling有点懒，它不想把分母中的所有词都算一次，就稍微选几个算算。</li>
<li>这一部分的模型实现比较复杂，网上也有很多资料可以参考，感兴趣的可以读读这两篇：<a href="http://www.hankcs.com/nlp/word2vec.html" target="_blank" rel="external">word2vec原理推导与代码分析</a>和<a href="http://blog.csdn.net/mytestmy/article/details/26969149" target="_blank" rel="external">深度学习word2vec笔记之算法篇</a>。</li>
</ul>
<h2 id="通过实践了解word2vec"><a href="#通过实践了解word2vec" class="headerlink" title="通过实践了解word2vec"></a>通过实践了解word2vec</h2><p>学习了上面的基础知识之后，我们就通过一个例子来感受一下word2vec的效果。</p>
<h3 id="下载语料"><a href="#下载语料" class="headerlink" title="下载语料"></a>下载语料</h3><p>从搜狗实验室下载<a href="http://www.sogou.com/labs/resource/ca.php" target="_blank" rel="external">全网新闻数据(SogouCA)</a>，该语料来自若干新闻站点2012年6月—7月期间国内、国际、体育、社会、娱乐等18个频道的新闻数据。</p>
<p>下载该文件解压后大约为1.5G，包含120w条以上的新闻，文件的内容格式如下图所示：</p>
<p><img src="http://oi46mo3on.bkt.clouddn.com/14_learning_w2v/corpus_format.png" alt="搜狗CA语料内容格式"></p>
<h3 id="获取新闻内容"><a href="#获取新闻内容" class="headerlink" title="获取新闻内容"></a>获取新闻内容</h3><p>这里我们先对语料进行初步处理，只获取新闻内容部分。可以执行以下命令获取content部分：</p>
<pre><code>cat news_tensite_xml.dat | iconv -f gbk -t utf-8 -c | grep &quot;&lt;content&gt;&quot;  &gt; corpus.txt
</code></pre><h3 id="分词处理"><a href="#分词处理" class="headerlink" title="分词处理"></a>分词处理</h3><p>由于word2vec处理的数据是单词分隔的语句，对于中文来说，需要先进行分词处理。这里采用的是中国自然语言处理开源组织开源的<a href="https://github.com/NLPchina/ansj_seg" target="_blank" rel="external">ansj_seg</a>分词器，核心代码如下所示：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordAnalyzer</span> </span>&#123;</div><div class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String TAG_START_CONTENT = <span class="string">"&lt;content&gt;"</span>;</div><div class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String TAG_END_CONTENT = <span class="string">"&lt;/content&gt;"</span>;</div><div class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String INPUT_FILE = <span class="string">"/home/test/w2v/corpus.txt"</span>;</div><div class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String OUTPUT_FILE = <span class="string">"/home/test/w2v/corpus_out.txt"</span>;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">        BufferedReader reader = <span class="keyword">null</span>;</div><div class="line">        PrintWriter pw = <span class="keyword">null</span>;</div><div class="line">        <span class="keyword">try</span> &#123;</div><div class="line">            System.out.println(<span class="string">"开始处理分词..."</span>);</div><div class="line">            reader = IOUtil.getReader(INPUT_FILE, <span class="string">"UTF-8"</span>);</div><div class="line">            pw = <span class="keyword">new</span> PrintWriter(OUTPUT_FILE);</div><div class="line">            <span class="keyword">long</span> start = System.currentTimeMillis();</div><div class="line">            <span class="keyword">int</span> totalCharactorLength = <span class="number">0</span>;</div><div class="line">            <span class="keyword">int</span> totalTermCount = <span class="number">0</span>;</div><div class="line">            Set&lt;String&gt; set = <span class="keyword">new</span> HashSet&lt;String&gt;();</div><div class="line">            String temp = <span class="keyword">null</span>;</div><div class="line">            <span class="keyword">while</span> ((temp = reader.readLine()) != <span class="keyword">null</span>) &#123;</div><div class="line">                temp = temp.trim();</div><div class="line">                <span class="keyword">if</span> (temp.startsWith(TAG_START_CONTENT)) &#123;</div><div class="line">                    <span class="comment">//System.out.println("处理文本:" + temp);</span></div><div class="line">                    <span class="keyword">int</span> end = temp.indexOf(TAG_END_CONTENT);</div><div class="line">                    String content = temp.substring(TAG_START_CONTENT.length(), end);</div><div class="line">                    totalCharactorLength += content.length();</div><div class="line">                    Result result = ToAnalysis.parse(content);</div><div class="line">                    <span class="keyword">for</span> (Term term : result) &#123;</div><div class="line">                        String item = term.getName().trim();</div><div class="line">                        totalTermCount++;</div><div class="line">                        pw.print(item + <span class="string">" "</span>);</div><div class="line">                        set.add(item);</div><div class="line">                    &#125;</div><div class="line">                    pw.println();</div><div class="line">                &#125;</div><div class="line">            &#125;</div><div class="line"></div><div class="line">            <span class="keyword">long</span> end = System.currentTimeMillis();</div><div class="line">            System.out.println(<span class="string">"共"</span> + totalTermCount + <span class="string">"个Term，共"</span> </div><div class="line">                + set.size() + <span class="string">"个不同的Term，共 "</span> </div><div class="line">                + totalCharactorLength + <span class="string">"个字符，每秒处理字符数:"</span> </div><div class="line">                + (totalCharactorLength * <span class="number">1000.0</span> / (end - start)));</div><div class="line">        &#125; <span class="keyword">finally</span> &#123;</div><div class="line">            <span class="comment">// close reader and pw</span></div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>分词处理之后的文件内容如下所示：<br><img src="http://oi46mo3on.bkt.clouddn.com/14_learning_w2v/corpus_ansj_analyze.png" alt="分词之后的语料"></p>
<h3 id="下载word2vec源码并编译"><a href="#下载word2vec源码并编译" class="headerlink" title="下载word2vec源码并编译"></a>下载word2vec源码并编译</h3><p>这里我没有从官网下载而是从github上的<a href="https://github.com/svn2github/word2vec" target="_blank" rel="external">svn2github/word2vec</a>项目下载源码，下载之后执行make命令编译，这个过程很快就可以结束。</p>
<h3 id="开始word2vec处理"><a href="#开始word2vec处理" class="headerlink" title="开始word2vec处理"></a>开始word2vec处理</h3><p>编译成功后开始处理。我这里用的是CentOS 64位的虚拟机，八核CPU，32G内存，整个处理过程耗时大约4个小时。</p>
<pre><code>./word2vec -train ../corpus_out.txt -output vectors.bin -cbow 0 -size 200 -window 5 -negative 0 -hs 1 -sample 1e-3 -threads 12 -binary 1

// 参数解释
-train 训练数据 
-output 结果输入文件，即每个词的向量 
-cbow 是否使用cbow模型，0表示使用skip-gram模型，1表示使用cbow模型，默认情况下是skip-gram模型，cbow模型快一些，skip-gram模型效果好一些 
-size 表示输出的词向量维数 
-window 为训练的窗口大小，5表示每个词考虑前5个词与后5个词（实际代码中还有一个随机选窗口的过程，窗口大小&lt;=5) 
-negative 表示是否使用负例采样方法0表示不使用，其它的值目前还不是很清楚 
-hs 是否使用Hierarchical Softmax方法，0表示不使用，1表示使用 
-sample 表示采样的阈值，如果一个词在训练样本中出现的频率越大，那么就越会被采样
-binary 表示输出的结果文件是否采用二进制存储，0表示不使用（即普通的文本存储，可以打开查看），1表示使用，即vectors.bin的存储类型
</code></pre><h3 id="测试处理结果"><a href="#测试处理结果" class="headerlink" title="测试处理结果"></a>测试处理结果</h3><p>处理结束之后，使用distance命令可以测试处理结果，以下是分别测试【足球】和【改革】的效果：</p>
<p><img src="http://oi46mo3on.bkt.clouddn.com/14_learning_w2v/w2v_test_football.png" alt="测试足球一词的词向量"></p>
<p><img src="http://oi46mo3on.bkt.clouddn.com/14_learning_w2v/w2v_test_reform.png" alt="测试改革一词的词向量"></p>
<h2 id="学习小结"><a href="#学习小结" class="headerlink" title="学习小结"></a>学习小结</h2><ul>
<li>word2vec的模型是基于神经网络来训练词向量的工具；</li>
<li>word2vec通过一系列的模型和框架对原有的NNLM进行优化，简化了计算但准确度还是保持得很好；</li>
<li>word2vec的主要的应用还是自然语言的处理，通过训练出来的词向量，可以进行聚类等处理，或者作为其他深入学习的输入。另外，word2vec还适用于一些时序数据的挖掘，比如用户商品的浏览分析、用户APP的下载等，通过这些数据的分析，可以得到商品或者APP的向量表示，从而用于个性化搜索和推荐。</li>
</ul>
<h2 id="参考材料"><a href="#参考材料" class="headerlink" title="参考材料"></a>参考材料</h2><ul>
<li><a href="https://code.google.com/archive/p/word2vec/" target="_blank" rel="external">word2vec HomePage</a></li>
<li><a href="https://arxiv.org/pdf/1301.3781.pdf" target="_blank" rel="external">Efficient Estimation of Word Representations in Vector Space</a></li>
<li><a href="http://licstar.net/archives/328" target="_blank" rel="external">Deep Learning in NLP （一）词向量和语言模型</a></li>
<li><a href="http://blog.csdn.net/itplus/article/details/37969635" target="_blank" rel="external">word2vec 中的数学原理详解</a></li>
<li><a href="http://blog.csdn.net/zhaoxinfan/article/details/11069485" target="_blank" rel="external">利用word2vec对关键词进行聚类</a></li>
<li><a href="http://blog.csdn.net/eastmount/article/details/50637476" target="_blank" rel="external">word2vec词向量训练及中文文本相似度计算</a></li>
<li><a href="http://www.cnblogs.com/Determined22/p/5780305.html" target="_blank" rel="external">词表示模型（一）：表示学习；syntagmatic与paradigmatic两类模型；基于矩阵的LSA和GloVe</a></li>
<li><a href="http://www.cnblogs.com/Determined22/p/5804455.html" target="_blank" rel="external">词表示模型（二）：基于神经网络的模型：NPLM；word2vec（CBOW/Skip-gram）</a></li>
<li><a href="http://www.cnblogs.com/Determined22/p/5807362.html" target="_blank" rel="external">词表示模型（三）：word2vec（CBOW/Skip-gram）的加速：Hierarchical Softmax与Negative Sampling</a></li>
<li><a href="http://blog.csdn.net/mytestmy/article/details/26961315" target="_blank" rel="external">深度学习word2vec笔记之基础篇</a></li>
<li><a href="https://www.zhihu.com/question/21661274/answer/19331979" target="_blank" rel="external">Google 开源项目 word2vec 的分析？_杨超的回答</a></li>
<li><a href="https://www.zybuluo.com/Dounm/note/591752" target="_blank" rel="external">Word2Vec-知其然知其所以然</a></li>
<li><a href="http://www.nustm.cn/blog/index.php/archives/842" target="_blank" rel="external">word2vec 原理篇</a></li>
<li><a href="http://www.hankcs.com/nlp/word2vec.html" target="_blank" rel="external">word2vec原理推导与代码分析</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;word2vec是Google于2013年开源推出的一个用于获取词向量的工具包，它简单、高效，因此引起了很多人的关注。最近项目组使用word2vec来实现个性化搜索，在阅读资料的过程中做了一些笔记，用于后面进一步学习。&lt;br&gt;
    
    </summary>
    
      <category term="NLP" scheme="http://ginobefunny.com/categories/NLP/"/>
    
    
      <category term="Google" scheme="http://ginobefunny.com/tags/Google/"/>
    
      <category term="入门" scheme="http://ginobefunny.com/tags/%E5%85%A5%E9%97%A8/"/>
    
      <category term="教程" scheme="http://ginobefunny.com/tags/%E6%95%99%E7%A8%8B/"/>
    
      <category term="实例" scheme="http://ginobefunny.com/tags/%E5%AE%9E%E4%BE%8B/"/>
    
      <category term="word2vec" scheme="http://ginobefunny.com/tags/word2vec/"/>
    
      <category term="NLP" scheme="http://ginobefunny.com/tags/NLP/"/>
    
      <category term="词向量" scheme="http://ginobefunny.com/tags/%E8%AF%8D%E5%90%91%E9%87%8F/"/>
    
      <category term="自然语言处理" scheme="http://ginobefunny.com/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>阅读随手记 201702</title>
    <link href="http://ginobefunny.com/post/reading_record_201702/"/>
    <id>http://ginobefunny.com/post/reading_record_201702/</id>
    <published>2017-02-08T04:31:23.000Z</published>
    <updated>2017-03-29T03:18:45.777Z</updated>
    
    <content type="html"><![CDATA[<p>关键字：微服务, 分布式, 配置中心, Java编程, 推荐系统, 运维, 高并发, 高可用, 机器学习, 深度学习。<br><a id="more"></a></p>
<h3 id="Microservices-A-definition-of-this-new-architectural-term-Martin-Fowler"><a href="#Microservices-A-definition-of-this-new-architectural-term-Martin-Fowler" class="headerlink" title="Microservices: A definition of this new architectural term Martin Fowler"></a><a href="https://martinfowler.com/articles/microservices.html" target="_blank" rel="external">Microservices: A definition of this new architectural term</a> Martin Fowler</h3><h4 id="微服务架构风格"><a href="#微服务架构风格" class="headerlink" title="微服务架构风格"></a>微服务架构风格</h4><ul>
<li>一组微型的服务/独立开发/通过轻量级机制通信/自动化的独立部署/各服务之间可采用不同语言和技术方案；</li>
<li>很难给微服务下一个具体的定义，Martin Fowler通过九大特性来阐述微服务；</li>
</ul>
<blockquote>
<p>the microservice architectural style is an approach to developing a single application as a suite of small services, each running in its own process and communicating with lightweight mechanisms, often an HTTP resource API. These services are built around business capabilities and independently deployable by fully automated deployment machinery. There is a bare minimum of centralized management of these services, which may be written in different programming languages and use different data storage technologies.</p>
</blockquote>
<h4 id="微服务的九大特性"><a href="#微服务的九大特性" class="headerlink" title="微服务的九大特性"></a>微服务的九大特性</h4><h5 id="1-组件化与服务"><a href="#1-组件化与服务" class="headerlink" title="1. 组件化与服务"></a>1. 组件化与服务</h5><ul>
<li><strong>组件</strong>（component）是一个可以独立更换和升级的软件单元；</li>
<li><strong>软件库）</strong>（libraries是能被链接到程序且通过方法调用的组件；</li>
<li><strong>服务</strong>（services）是进程外的组件，通过web service或rpc的机制来通信；</li>
<li>使用服务而非软件库的方式来组件化的主要原因是服务可被独立部署（避免一个组件修改导致需要重新部署整个应用），另外一个好处是能获得更加显式的组件接口（想想单体应用中的API依赖）；</li>
<li>当然也有不足之处，比如远程调用比进程内的调用要更加昂贵；</li>
</ul>
<h5 id="2-围绕业务功能组织团队"><a href="#2-围绕业务功能组织团队" class="headerlink" title="2. 围绕业务功能组织团队"></a>2. 围绕业务功能组织团队</h5><ul>
<li>对一个大型的应用进行分解时，通常是按照技术层面来进行划分，比如分为前端开发、后端开发和数据库开发（当然实际情况比这更细），但是这样会带来一个问题就是一个很小的需求都需要跨团队的项目合作和进度安排；这就是<strong>康威定律</strong>（任何设计系统的组织，都会产生这样一个设计，即该设计的结构与该组织的沟通结构相一致）起作用的例子；</li>
<li>而微服务是通过<strong>业务功能</strong>将系统分解为若干服务，这些服务针对该业务领域提供多层次广泛的软件实现；因此团队是<strong>跨职能</strong>的，它拥有软件开发所需的全方位的技能（如用户体验、数据库和项目管理）；</li>
<li>大型单体应用系统也可以根据业务功能进行模块化设计，但是通常的问题是一个团队会包含太多的功能，而团队之间的边界也不够清晰；</li>
</ul>
<h5 id="3-做产品而非做项目"><a href="#3-做产品而非做项目" class="headerlink" title="3. 做产品而非做项目"></a>3. 做产品而非做项目</h5><ul>
<li>大部分的应用开发都使用这样一个产品模型：一旦某项软件功能已交付，就会将软件移交给维护团队，而开发团队随之被解散；</li>
<li>而微服务主张<strong>“一个团队应该拥有该产品的整个生命周期”</strong>（原子亚马逊的“you build, you run it”）这样的理念，即一个开发团队对一个生产环境下运行的软件负全责；</li>
<li>这样的产品理念是与业务功能相绑定的，它不会把软件开成是一系列待完成功能的集合，而是一种持续提升客户业务功能的关系；</li>
<li>当然，单体应用也可以采用上述的产品理念，但是更细粒度的服务能使服务的开发者和它的用户更近；</li>
</ul>
<h5 id="4-智能终端和傻瓜管道"><a href="#4-智能终端和傻瓜管道" class="headerlink" title="4. 智能终端和傻瓜管道"></a>4. 智能终端和傻瓜管道</h5><ul>
<li>在不同的进程进行通信时，多数的产品或方法会在其中加入大量的智能特性，有个典型的例子就是ESB（企业服务总线），它通常包括消息路由、编制、转换和业务规则应用；</li>
<li>而微服务主张采用另一种做法：<strong>智能终端</strong>（smart endpoints）和<strong>傻瓜管道</strong>（dumb pipes）。这里的理解是应该尽可能地简化进程间的通信，将一些需要智能处理的逻辑（比如路由、重试等）交给服务处理；</li>
<li>微服务最常用的两种协议是：带有资源API的HTTP请求-响应协议和轻量级的消息发送协议（如RabbitMQ、ZeroMQ）；</li>
<li>将一个单体应用拆分为微服务的最大挑战就是改变原有的通信模式，如果直接将原先的进程内方法调用改为RPC会导致微服务直接产生繁琐的通信，因此应该考虑更粗粒度的方式调用；</li>
</ul>
<h5 id="5-去中心化的治理"><a href="#5-去中心化的治理" class="headerlink" title="5. 去中心化的治理"></a>5. 去中心化的治理</h5><ul>
<li>集中治理的一个问题是会趋向于在单一技术平台上制定标准从而带来局限性；</li>
<li>微服务中的<strong>分权治理</strong>（去中心化的）使得每个服务可以选择不同的技术，比如选择不同的语言和数据库；</li>
<li>相比于选用一组已定义好的标准，微服务的开发者更喜欢自己编写一些有用的工具，这些工具通常源于他们的微服务实施过程并分享给更多的团队；比如Nteflix就是一个很好的例子，这家提供网络视频点播的公司开源了一系列的实施微服务的工具；</li>
<li>对微服务社区来说，像<a href="https://martinfowler.com/bliki/TolerantReader.html" target="_blank" rel="external"><strong>容错读取</strong></a>（Tolerant Reader）和<a href="https://martinfowler.com/articles/consumerDrivenContracts.html" target="_blank" rel="external"><strong>消费者驱动的契约</strong></a>（Consumer-Driven Contracts）的模式已经被用于日常管理；</li>
<li>像Netflix这样的公司已经开始推行分权治理，这样可以令程序员更加注重质量（谁都不想半夜被电话叫去修复问题对吧），而这些与之前传统的集中治理有着天壤之别；</li>
</ul>
<h5 id="6-去中心化的数据管理"><a href="#6-去中心化的数据管理" class="headerlink" title="6. 去中心化的数据管理"></a>6. 去中心化的数据管理</h5><ul>
<li><strong>去中心化的数据管理</strong>从最抽象的层面看，意味着各个系统对客观世界所构建的概念模型将彼此不同，比如客户的概念对于销售和支撑团队就有所不同；</li>
<li>思考这类问题的一个有用的方法就是使用<strong>领域驱动设计</strong>（Domain-Driven Design, DDD）中的<a href="http://martinfowler.com/bliki/BoundedContext.html" target="_blank" rel="external"><strong>上下文边界</strong></a>（Bounded Context）的概念；DDD将一个复杂的领域划分为多个上下文边界，并且将它们的相互关系用图表示出来；</li>
<li>不同于单体应用喜欢共用一个单独的数据库（也许是被商业数据库的license逼出来的），微服务更喜欢让每一个服务管理其自有的数据库，可以采用相同数据库技术的不同实例，也可以采用完全不同的数据库系统；</li>
<li>但是去中心化的数据管理会带来数据一致性的问题，对此微服务强调的是<strong>无事务的协调</strong>（transactionless coordination between services），这源自微服务社区明确地认识到以下两点：即数据一致性可能只要求数据最终一致性，并且一致性问题能够通过补偿操作来进行处理；</li>
</ul>
<h5 id="7-基础设施自动化"><a href="#7-基础设施自动化" class="headerlink" title="7. 基础设施自动化"></a>7. 基础设施自动化</h5><ul>
<li>云的发展已经很大程度上降低了构建、部署和运维微服务的复杂性了；</li>
<li>许多使用微服务构建的团队都具备<strong>持续交付</strong>（Continuous Delivery）的经验，这样的团队广泛采用了基础设施自动化的技术，如下图的构建流水线所示：</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/15_reading_201702/basic_pipeline.png" alt="构建流水线"></p>
<ul>
<li>持续交付需要大量的<strong>自动化测试</strong>以及<strong>自动化部署的能力</strong>，通过这两个关键特点使得我们能更有信心、更愉快地部署功能到生成环境；另外，微服务的独立性也使得部署更加容易（这里的容易是指只需要部署修改的服务而不需要部署整个应用），当然也会带来困难（原来只需要部署一个系统，而现在需要部署更多的服务），此时就需要在部署工具上投入精力改进；</li>
</ul>
<h5 id="8-容错设计"><a href="#8-容错设计" class="headerlink" title="8. 容错设计"></a>8. 容错设计</h5><ul>
<li>使用微服务作为组件，需要设计成能容忍这些服务所出现的故障；一旦某个服务出现故障，其他任何对该服务的调用都会出现故障，客户端需要尽可能优雅地处理这种情况；与单体应用相比，这是微服务引入的额外的复杂性；</li>
<li>Netflix公司开源的<a href="https://github.com/Netflix/SimianArmy" target="_blank" rel="external"><strong>Simian Army</strong></a>能够诱导服务发生故障来测试应用的弹性和监控能力；</li>
<li><a href="http://martinfowler.com/bliki/CircuitBreaker.html" target="_blank" rel="external"><strong>断路器</strong></a>（Circuit Breaker）是一种用于隔离故障的模式，Netflix公司的这篇很精彩的<a href="http://techblog.netflix.com/2012/02/fault-tolerance-in-high-volume.html" target="_blank" rel="external">博客</a>解释了这些模式是如何应用的；</li>
<li><strong>容错设计</strong>要求能够快速地检测出故障，而且在可能的情况下自动恢复服务；此时<strong>实时监控</strong>就变得非常重要，它可用来检查架构元素指标（比如数据库每秒接收到多少请求）和业务相关指标（如系统每分钟收到多少订单）以便预测故障的发生；这对于微服务来说尤为重要，因为微服务对于服务编排和事件协作的偏好更易导致突发行为；</li>
<li>采用微服务的团队通常希望使用仪表盘或者日志记录装置来监控服务的运行和各项指标；</li>
</ul>
<h5 id="9-演进式设计"><a href="#9-演进式设计" class="headerlink" title="9. 演进式设计"></a>9. 演进式设计</h5><ul>
<li>每当试图将软件系统拆分为各个组件时，都会面对一个棘手的问题，即如何拆分，拆分的原则是什么？</li>
<li>一个组件的关键属性，是具有<strong>独立可替换性和可升级性</strong>（independent replacement and upgradeability），这使得我们在重写一个组件时更多的是聚焦于功能而非与不用担心与其他的关联组件；</li>
<li>而事实上，许多做微服务的团队会更进一步，他们明确地预期许多服务将来会报废而不是长期演进；比如英国卫报网站，他们依然使用原先的单体应用作为网站核心，而对于一些新的功能，比如增加报道一个体育赛事的页面，就会采用微服务的方式来添加，一旦赛事结束了，这个服务就可以被废除；</li>
<li>这种强调可更换性的特点是模块化设计一般性原则的一个特例，通过<strong>变化模式</strong>（the pattern of change）来驱动进行模块化的实现；将那些能在同时发生变化的东西放到相同的模块中，如果发现需要同时反复变更两个服务是，这就是它们两个需要被合并的一个信号；</li>
<li>将一个个组件放入一个个服务中增加了做出更精细化版本发布的机会，对于单体应用，任何变化都需要做一次整个应用系统的全量构建和部署，而对微服务来说，只需要部署修改的服务即可；</li>
</ul>
<h4 id="微服务是未来的方向吗？"><a href="#微服务是未来的方向吗？" class="headerlink" title="微服务是未来的方向吗？"></a>微服务是未来的方向吗？</h4><ul>
<li>作者通过该文阐述了微服务的主要思路和原则，在当时已经有一些公司如亚马逊和Netflix提供了正面的经验，收到了不少正面的评价；</li>
<li>架构决策所产生的真正效果通常需要若干年后才能真正显现，当时考虑的限制微服务的因素主要有组件拆分的难度、组件直接的复杂关联关系以及团队技能，但从目前的发展来看，已经有越来越多的企业采用了微服务的架构；</li>
</ul>
<h3 id="Netflix-Conductor：一个微服务编制引擎-Abel-Avram-杨雷"><a href="#Netflix-Conductor：一个微服务编制引擎-Abel-Avram-杨雷" class="headerlink" title="Netflix Conductor：一个微服务编制引擎 Abel Avram/杨雷"></a><a href="http://www.infoq.com/cn/news/2016/12/netflix-conductor" target="_blank" rel="external">Netflix Conductor：一个微服务编制引擎</a> Abel Avram/杨雷</h3><ul>
<li>Netflix开发了一个叫<a href="https://netflix.github.io/conductor/" target="_blank" rel="external">Conductor</a>的编制引擎，已经在内部生产环境中使用了一年了。在这段时间里，Netflix已经运行了大约260万个处理工作流，包括简单的线性工作流，以及运行数天的动态工作流。</li>
<li>主要特性：能够构建复杂工作流；能够通过微服务执行任务；使用JSON DSL描述的工作流蓝图；执行过程可见、可跟踪；能够暂停、恢复、重启、停止任务；任务执行通常是异步的，也可以强制同步执行；处理工作流能够扩展到百万级别；</li>
<li>Conductor的架构图如下所示；其中API和存储层都是可插拔的，允许使用不同的队列和存储引擎；工作流中的任务分为两种类型：Worker（运行在远端机器上的用户任务和System（运行在引擎的JVM上的任务），后者是用来对Worker执行任务进行branch、fork、join；Worker任务通过HTTP或者gRPC和Conductor通信；</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/15_reading_201702/conductor-architecture.png" alt="Conductor的架构图"></p>
<h3 id="深度学习并不是在“模拟人脑”-周志华"><a href="#深度学习并不是在“模拟人脑”-周志华" class="headerlink" title="深度学习并不是在“模拟人脑” 周志华"></a><a href="https://mp.weixin.qq.com/s?__biz=MzI5NTIxNTg0OA==&amp;mid=2247485059&amp;idx=1&amp;sn=e516921ae4beda15c7ddf5cb4c93bf78" target="_blank" rel="external">深度学习并不是在“模拟人脑”</a> 周志华</h3><ul>
<li>当<strong>特征信息和样本信息</strong>不充分时，机器学习可能就帮不上忙；</li>
<li>用机器学习解决问题更多的时候像一个裁缝，一定要量体裁衣，针对某个问题专门设计有效的方法，这样才能得到一个更好的结果；<strong>按需设计、度身定制</strong>，是在做机器学习应用的时候特别重要的一点；</li>
<li>机器学习有着深厚的理论基础，其中最基本的理论模型叫做<strong>概率近似正确模型</strong>；机器学习做的事情，是你给我数据之后，希望能够以很高的概率给出一个好模型；</li>
<li>从生物机理来说的话，一个神经元收到很多其它神经元发来的电位信号，信号经过放大到达它这里，如果这个累积信号比它自己的电位高了，那这个神经元就被激活了；其实神经网络本质上，是一个简单函数通过多层嵌套叠加形成的一个数学模型，背后其实是数学和工程在做支撑；而神经生理学起的作用，可以说是给了一点点启发，但是远远不像现在很多人说的神经网络研究受到神经生理学的“指导”，或者是“模拟脑”；</li>
<li>深度学习火起来的3个因素：有了大量的训练数据、有很强的计算设备、使用大量的“窍门”（Trick）；</li>
</ul>
<h3 id="分布式配置管理平台的设计与实现-架构文摘"><a href="#分布式配置管理平台的设计与实现-架构文摘" class="headerlink" title="分布式配置管理平台的设计与实现 架构文摘"></a><a href="http://mp.weixin.qq.com/s?__biz=MzIyNjE4NjI2Nw==&amp;mid=2652558155&amp;idx=1&amp;sn=351b10f4ecb80756bc91a25487d482be" target="_blank" rel="external">分布式配置管理平台的设计与实现</a> 架构文摘</h3><ul>
<li>分布式配置平台的一些应用场景：对某些配置的更新，不想要重启应用，并且能近似实时生效；希望将配置进行统一管理，而非放入各应用的配置文件中；</li>
<li>分布式配置平台需要满足的一些基本特性：高可用性（服务器集群应该无单点故障）、容错性（主要针对客户端，应保证即便在配置平台不可用时，也不影响客户端的正常运行）、高性能、可靠的存储、近似实时生效、负载均衡、扩展性；</li>
<li>分布式配置平台Diablo的设计与实现：对等的服务器集群（Server被视为是对等的，没有主从关系）、高性能处理（客户端应用获取配置时，仅会从本地缓存中获取，开发人员在控制台更改配置后，会通知客户端刷新缓冲）、使用Redis作存储、重试等待（diablo会通过重试等待等机制保证，在服务端集群不可用时，也不会影响客户端应用的正常运行，而是等待集群恢复）、请求负载（使用一致性哈希分配客户端连接是Server）、配置更新实时生效（diablo使用了特殊Pull模式，即长轮询）；</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/15_reading_201702/diablo.jpg" alt="Diablo的架构设计"></p>
<h3 id="一篇好TM长的关于配置中心的文章-坤宇"><a href="#一篇好TM长的关于配置中心的文章-坤宇" class="headerlink" title="一篇好TM长的关于配置中心的文章 坤宇"></a><a href="http://jm.taobao.org/2016/09/28/an-article-about-config-center/" target="_blank" rel="external">一篇好TM长的关于配置中心的文章</a> 坤宇</h3><ul>
<li>每一个分布式系统都应该有一个动态配置管理系统</li>
<li>配置与环境：某个配置项，其具体的值域的定义往往跟具体的环境相关联，现实中相当一部分配置在不同的环境必须设定不同的值，但是也有相当的另一部分配置在不同的环境要设定为完全一致的值。配置管理系统应该做的是提供方便的交互方式保证这两种不同的一致性诉求同时得到很好的满足，这种诉求分为3个方面，如下示意图:</li>
</ul>
<p><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1NFSpNpXXXXajapXXXXXXXXXX" alt="配置与环境"></p>
<ul>
<li>另外，一个配置中心也应该具备的能力是配置集的导出\导入功能，可以让应用将A环境中的配置集方便的导出和导入到环境B中的能力；</li>
<li>业界最新动态：Apache Commons Configuration（太繁琐）、<a href="http://owner.aeonbits.org" target="_blank" rel="external">owner</a>（简单易上手，特性看起来很多，但是在很多关键常用的特性反倒是没有）、<a href="http://www.cfg4j.org/" target="_blank" rel="external">cfg4j</a>（简单易上手，cfg4j 支持跟多种后端集成，做配置中心的解决方案，api设计也非常的不错）、Spring Framework、Spring Cloud Config Server；</li>
<li>配置(configuration)与元数据(metadata)：配置的修改基本上都是由人来驱动，并且在ops上实现变更；而元数据的本质是一小段程序元数据，它很多时候是程序产生，程序消费，由程序通过调用Diamond的客户端api来实现变更，中间不会有ops 或者人的介入。Diamond 不光是应用配置存储，其目前存储的数据，很大一部分是metadata，所以Diamond 其实也是一个元数据存储中心。</li>
</ul>
<h3 id="Reddit是如何使用Memcached来存储3TB缓存数据的？-薛命灯"><a href="#Reddit是如何使用Memcached来存储3TB缓存数据的？-薛命灯" class="headerlink" title="Reddit是如何使用Memcached来存储3TB缓存数据的？ 薛命灯"></a><a href="http://mp.weixin.qq.com/s?__biz=MzA5Nzc4OTA1Mw==&amp;mid=2659598891&amp;idx=1&amp;sn=6ae9c213ce167b3a8295b7d7d4804048" target="_blank" rel="external">Reddit是如何使用Memcached来存储3TB缓存数据的？</a> 薛命灯</h3><h4 id="Reddit的缓存规模和基本策略"><a href="#Reddit的缓存规模和基本策略" class="headerlink" title="Reddit的缓存规模和基本策略"></a>Reddit的缓存规模和基本策略</h4><p>Reddit目前使用了54个规格为r3.2xlarge的AWS EC2实例，每个实例拥有61GB内存，也就是说总的缓存大小差不多是3.3TB。Reddit的缓存包含了多种类型的数据，包括数据库对象、查询结果集、函数调用，还有一些看起来不太像缓存的东西，比如限定速率、分布式锁等等。</p>
<p><img src="http://oi46mo3on.bkt.clouddn.com/15_reading_201702/reddit_cache_1.png" alt="Reddit的缓存规模"></p>
<p>如何管理这么大规模的缓存是一件很挑战性的事情，Reddit采用的是“不要把所有鸡蛋放在同一个篮子里”的基本策略。也就是说，他们并不是把3.3TB的内存看成一个总的大缓存池，而是按照负载类型对缓存进行分类，每种类型占用一定数量的缓存空间。</p>
<h4 id="Reddit的缓存类型"><a href="#Reddit的缓存类型" class="headerlink" title="Reddit的缓存类型"></a>Reddit的缓存类型</h4><ul>
<li>数据库对象缓存（thing-cache）：Reddit最大的缓存池。这些对象是无schema的，开发人员可以很容易地对这些对象添加新属性，而无需对数据库schema进行变更。这些对象包括用户评论、链接和账户等等。该类型缓存是Reddit最繁忙也最有用的缓存，命中率高达99%。</li>
<li>主缓存（cache-main）：主缓存是Reddit第二大缓存池。这个缓存是一般性的缓存，里面存放的所有用来展示/r/all的结果集。</li>
<li>渲染缓存（cache-render）：第三大缓存用来存放渲染过的页面模板或页面片段。这个缓存相对安全，就算发生失效，也不会对系统造成太大影响。它的命中率只有大概50%左右，毕竟页面信息需要不断更新，所以渲染过的页面模板或片段也需要更新。</li>
<li>持久缓存（cache-perma）：它的命中率超过了99%。这个缓存用来存放数据库的查询结果，还有用户评论和链接。为什么管这个缓存叫持久缓存，因为他们使用了读-改-写（read-modify-write）的模式。例如，在用户新增一个评论时，他们会同时更新缓存和后端的数据库（Cassandra），而不是简单地让缓存失效，这样就避免了需要再次从数据库加载数据。</li>
<li>非缓存对象池：除了上述的几种缓存，Reddit还使用了速率限定和分布式锁。</li>
</ul>
<h4 id="mcrouter"><a href="#mcrouter" class="headerlink" title="mcrouter"></a>mcrouter</h4><ul>
<li>mcrouter是由Facebook开源的Memcached连接池。就像访问数据库要使用数据库连接池一样，使用连接池可以对连接进行重用和管理，避免了重复创建和销毁连接的开销。</li>
<li>mcrouter提供了多种路由类型，比如PrefixSelectorRoute，它通过匹配key的前缀来决定应该到哪个缓存上获取数据。这样就可以把特定功能的操作路由到特定的缓存上。</li>
<li>如果要往缓存集群里增加新的缓存实例，那么可以使用WarmUpRoute。WarmUpRoute的工作原理是说，把所有写操作路由到“冷”缓存上，而把未命中的读操作路由到“热”缓存上，然后把在“热”缓存上命中的缓存结果异步地更新到“冷”缓存上，那么下次同样的读操作就也可以在“冷”缓存上命中。</li>
<li>mcrouter还提供了FailoverRoute，顾名思义，这个特性可以避免缓存的单点故障；</li>
<li>Reddit还使用了影子缓存，不同于WarmUpRoute，它会把读操作和写操作都拷贝一份到新的实例上，但前提是不改变数据源。</li>
</ul>
<h4 id="自定义监控"><a href="#自定义监控" class="headerlink" title="自定义监控"></a>自定义监控</h4><ul>
<li>基于Memcached的“stats slabs”命令自己写了一个追踪板块度量指标的工具，他们还开发了一个简陋的可视化仪表盘；</li>
<li>Reddit团队还开发了另外一个工具，叫作mcsauna。这个工具被部署在每个缓存服务器上，它可以检测网络流量，并根据配置规则把不同的key保存在不同的bucket里，然后把结果输出到文件上。FilesCollector会收集这些文件，分析里面的key，并以图形化的方式呈现出来。从这些图形上可以看出那些热点的key。</li>
</ul>
<h4 id="展望"><a href="#展望" class="headerlink" title="展望"></a>展望</h4><p>缓存为提升网站的响应速度做出了不可磨灭的贡献。而在如何使用缓存方面，Reddit还有很长的路要走。接下来，他们可能要想着如何通过服务发现来对配置进行自动化，从而实现缓存的自动扩展，而不需要人工的介入。而随着Memcached版本的不断改进，他们也要针对现有系统进行调整，从而最大化缓存的性能。</p>
<h3 id="微信高并发资金交易系统设计方案-方乐明"><a href="#微信高并发资金交易系统设计方案-方乐明" class="headerlink" title="微信高并发资金交易系统设计方案 方乐明"></a><a href="http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2650995437&amp;idx=1&amp;sn=fefff4bff3e183d656a2d242e4c0a382" target="_blank" rel="external">微信高并发资金交易系统设计方案</a> 方乐明</h3><ul>
<li>微信红包的两大业务特点：微信红包业务比普通商品“秒杀”有更海量的并发要求、微信红包业务要求更严格的安全级别；</li>
<li>微信红包系统的技术难点：事务级操作量级大、事务性要求严格；</li>
<li>解决高并发问题常用方案：使用内存操作替代实时的DB事务操作（用内存操作替代磁盘操作，提高了并发性能，但是DB持久化可能会丢数据）、使用乐观锁替代悲观锁（可以提高DB的并发处理能力，但是回滚失败带来很差的用户体验）；</li>
</ul>
<h4 id="微信红包系统的高并发解决方案"><a href="#微信红包系统的高并发解决方案" class="headerlink" title="微信红包系统的高并发解决方案"></a>微信红包系统的高并发解决方案</h4><h5 id="系统垂直SET化，分而治之"><a href="#系统垂直SET化，分而治之" class="headerlink" title="系统垂直SET化，分而治之"></a>系统垂直SET化，分而治之</h5><p>红包系统根据微信红包ID，按一定的规则（如按ID尾号取模等），垂直上下切分。切分后，一个垂直链条上的逻辑Server服务器、DB统称为一个SET。各个SET之间相互独立，互相解耦。并且同一个红包ID的所有请求，包括发红包、抢红包、拆红包、查详情详情等，垂直stick到同一个SET内处理，高度内聚。通过这样的方式，系统将所有红包请求这个巨大的洪流分散为多股小流，互不影响，分而治之。</p>
<h5 id="逻辑Server层将请求排队，解决DB并发问题。"><a href="#逻辑Server层将请求排队，解决DB并发问题。" class="headerlink" title="逻辑Server层将请求排队，解决DB并发问题。"></a>逻辑Server层将请求排队，解决DB并发问题。</h5><p>如果到达DB的事务操作不是并发的，而是串行的，就不会存在“并发抢锁”的问题了。按这个思路，为了使拆红包的事务操作串行地进入DB，只需要将请求在Server层以FIFO的方式排队，就可以达到这个效果。从而问题就集中到Server的FIFO队列设计上。</p>
<p>微信红包系统设计了分布式的、轻巧的、灵活的FIFO队列方案。其具体实现如下：</p>
<ul>
<li>将同一个红包ID的所有请求stick到同一台Server；</li>
<li>设计单机请求排队方案：将stick到同一台Server上的所有请求在被接收进程接收后，按红包ID进行排队。然后串行地进入worker进程（执行业务逻辑）进行处理，从而达到排队的效果；</li>
<li>增加memcached控制并发：利用memcached的CAS原子累增操作，控制同时进入DB执行拆红包事务的请求数，超过预先设定数值则直接拒绝服务。用于DB负载升高时的降级体验；</li>
</ul>
<h5 id="双维度库表设计，保障系统性能稳定"><a href="#双维度库表设计，保障系统性能稳定" class="headerlink" title="双维度库表设计，保障系统性能稳定"></a>双维度库表设计，保障系统性能稳定</h5><p>处理微信红包数据的冷热分离时，系统在以红包ID维度分库表的基础上，增加了以循环天分表的维度，形成了双维度分库表的特色。具体来说，就是分库表规则像db_xx.t_y_dd设计，其中，xx/y是红包ID的hash值后三位，dd的取值范围在01~31，代表一个月天数最多31天。通过这种双维度分库表方式，解决了DB单表数据量膨胀导致性能下降的问题，保障了系统性能的稳定性。同时，在热冷分离的问题上，又使得数据搬迁变得简单而优雅。</p>
<h3 id="分布式系统理论基础-一致性、2PC和3PC-bangerlee"><a href="#分布式系统理论基础-一致性、2PC和3PC-bangerlee" class="headerlink" title="分布式系统理论基础 - 一致性、2PC和3PC bangerlee"></a><a href="http://www.cnblogs.com/bangerlee/p/5268485.html" target="_blank" rel="external">分布式系统理论基础 - 一致性、2PC和3PC</a> bangerlee</h3><ul>
<li>狭义的分布式系统指由网络连接的计算机系统，每个节点独立地承担计算或存储任务，节点间通过网络协同工作。广义的分布式系统是一个相对的概念，正如Leslie Lamport所说：</li>
</ul>
<blockquote>
<p>What is a distributed systeme. Distribution is in the eye of the beholder.<br>To the user sitting at the keyboard, his IBM personal computer is a nondistributed system.<br>To a flea crawling around on the circuit board, or to the engineer who designed it, it’s very much a distributed system.</p>
</blockquote>
<ul>
<li>一致性是分布式理论中的根本性问题；何为一致性问题？简单而言，一致性问题就是相互独立的节点之间如何达成一项决议的问题。分布式系统中，进行数据库事务提交(commit transaction)、Leader选举、序列号生成等都会遇到一致性问题。</li>
<li>假设一个具有N个节点的分布式系统，当其满足以下条件时，我们说这个系统满足一致性：全认同(agreement)、值合法(validity)、可结束(termination)；</li>
<li>分布式系统实现起来并不轻松，因为它面临着这些问题：消息传递异步无序(asynchronous)、节点宕机(fail-stop)、节点宕机恢复(fail-recover)、网络分化(network partition)、拜占庭将军问题(byzantine failure)；</li>
<li>一致性还具备两个属性，一个是强一致(safety)，它要求所有节点状态一致、共进退；一个是可用(liveness)，它要求分布式系统24*7无间断对外服务。FLP定理(FLP impossibility)已经证明在一个收窄的模型中(异步环境并只存在节点宕机)，不能同时满足safety和liveness。FLP定理是分布式系统理论中的基础理论，正如物理学中的能量守恒定律彻底否定了永动机的存在，FLP定理否定了同时满足safety和liveness的一致性协议的存在。</li>
</ul>
<h4 id="2PC"><a href="#2PC" class="headerlink" title="2PC"></a>2PC</h4><p>2PC（tow phase commit，两阶段提交）顾名思义它分成两个阶段，先由一方进行提议(propose)并收集其他节点的反馈(vote)，再根据反馈决定提交(commit)或中止(abort)事务。我们将提议的节点称为协调者(coordinator)，其他参与决议节点称为参与者(participants, 或cohorts)。</p>
<p>在阶段一中，coordinator发起一个提议，分别问询各participant是否接受。<br><img src="http://images2015.cnblogs.com/blog/116770/201603/116770-20160313202532507-1396598167.png" alt="2PC阶段一"></p>
<p>在阶段二中，coordinator根据participant的反馈，提交或中止事务，如果participant全部同意则提交，只要有一个participant不同意就中止。<br><img src="http://images2015.cnblogs.com/blog/116770/201603/116770-20160313203429600-179395429.png" alt="2PC阶段二"></p>
<p>在异步环境(asynchronous)并且没有节点宕机(fail-stop)的模型下，2PC可以满足全认同、值合法、可结束，是解决一致性问题的一种协议。但如果再加上节点宕机(fail-recover)的考虑，就要求 coordinator/participant 记录历史状态，以备coordinator宕机后watchdog对participant查询、coordinator宕机恢复后重新找回状态；</p>
<h4 id="3PC"><a href="#3PC" class="headerlink" title="3PC"></a>3PC</h4><p>在2PC中一个participant的状态只有它自己和coordinator知晓，假如coordinator提议后自身宕机，在watchdog启用前一个participant又宕机，其他participant就会进入既不能回滚、又不能强制commit的阻塞状态，直到participant宕机恢复。这引出两个疑问：</p>
<ul>
<li>能不能去掉阻塞，使系统可以在commit/abort前回滚(rollback)到决议发起前的初始状态；</li>
<li>当次决议中，participant间能不能相互知道对方的状态，又或者participant间根本不依赖对方的状态；</li>
</ul>
<p>相比2PC，3PC增加了一个准备提交(prepare to commit)阶段来解决以上问题。coordinator接收完participant的反馈(vote)之后，进入阶段2，给各个participant发送准备提交(prepare to commit)指令。participant接到准备提交指令后可以锁资源，但要求相关操作必须可回滚。coordinator接收完确认(ACK)后进入阶段3、进行commit/abort，3PC的阶段3与2PC的阶段2无异。协调者备份(coordinator watchdog)、状态记录(logging)同样应用在3PC。</p>
<p><img src="http://images2015.cnblogs.com/blog/116770/201603/116770-20160314002734304-489496391.png" alt="3PC"></p>
<p>因为有了准备提交(prepare to commit)阶段，3PC的事务处理延时也增加了1个RTT，变为3个RTT(propose+precommit+commit)，但是它防止participant宕机后整个系统进入阻塞态，增强了系统的可用性，对一些现实业务场景是非常值得的。</p>
<h3 id="分布式系统理论基础-选举、多数派和租约-bangerlee"><a href="#分布式系统理论基础-选举、多数派和租约-bangerlee" class="headerlink" title="分布式系统理论基础 - 选举、多数派和租约 bangerlee"></a><a href="http://www.cnblogs.com/bangerlee/p/5767845.html" target="_blank" rel="external">分布式系统理论基础 - 选举、多数派和租约</a> bangerlee</h3><ul>
<li>选举(election)是分布式系统实践中常见的问题，通过打破节点间的对等关系，选得的leader(或叫master、coordinator)有助于实现事务原子性、提升决议效率。多数派(quorum)的思路帮助我们在网络分化的情况下达成决议一致性，在leader选举的场景下帮助我们选出唯一leader。租约(lease)在一定期限内给予节点特定权利，也可以用于实现leader选举。</li>
<li>选举(electioin)：一致性问题(consistency)是独立的节点间如何达成决议的问题，选出大家都认可的leader本质上也是一致性问题；Bully算法是最常见的选举算法，其要求每个节点对应一个序号，序号最高的节点为leader，leader宕机后次高序号的节点被重选为leader；Bully算法中有2PC的身影，都具有提议(propose)和收集反馈(vote)的过程；在一致性算法Paxos、ZAB、Raft中，为提升决议效率均有节点充当leader的角色；</li>
<li>多数派(quorum)：在网络分化的场景下以上Bully算法会遇到一个问题，被分隔的节点都认为自己具有最大的序号、将产生多个leader；多数派的思路在分布式系统中很常见，其确保网络分化情况下决议唯一；多数派的原理说起来很简单，假如节点总数为2f+1，则一项决议得到多于 f 节点赞成则获得通过。leader选举中，网络分化场景下只有具备多数派节点的部分才可能选出leader，这避免了多leader的产生；</li>
<li>租约(lease)：选举中很重要的一个问题，怎么判断leader不可用、什么时候应该发起重新选举？最先可能想到会通过心跳(heart beat)判别leader状态是否正常，但在网络拥塞或瞬断的情况下，这容易导致出现双主；租约(lease)是解决该问题的常用方法，其最初提出时用于解决分布式缓存一致性问题，后面在分布式锁等很多方面都有应用；租约的原理同样不复杂，中心思想是每次租约时长内只有一个节点获得租约、到期后必须重新颁发租约；租约机制确保了一个时刻最多只有一个leader，避免只使用心跳机制产生双主的问题，在实践应用中，zookeeper、ectd可用于租约颁发。</li>
</ul>
<h3 id="分布式系统理论基础-时间、时钟和事件顺序-bangerlee"><a href="#分布式系统理论基础-时间、时钟和事件顺序-bangerlee" class="headerlink" title="分布式系统理论基础 - 时间、时钟和事件顺序 bangerlee"></a><a href="http://www.cnblogs.com/bangerlee/p/5448766.html" target="_blank" rel="external">分布式系统理论基础 - 时间、时钟和事件顺序</a> bangerlee</h3><ul>
<li>现实生活中时间是很重要的概念，时间可以记录事情发生的时刻、比较事情发生的先后顺序。分布式系统的一些场景也需要记录和比较不同节点间事件发生的顺序，但不同于日常生活使用物理时钟记录时间，分布式系统使用逻辑时钟记录事件顺序关系；</li>
<li>在1978年提出逻辑时钟的概念，并描述了一种逻辑时钟的表示方法，这个方法被称为Lamport时间戳(Lamport timestamps)。分布式系统中按是否存在节点交互可分为三类事件，一类发生于节点内部，二是发送事件，三是接收事件。Lamport时间戳原理如下：</li>
</ul>
<pre><code>每个事件对应一个Lamport时间戳，初始值为0
如果事件在节点内发生，时间戳加1
如果事件属于发送事件，时间戳加1并在消息中带上该时间戳
如果事件属于接收事件，时间戳 = Max(本地时间戳，消息中的时间戳) + 1
</code></pre><p><img src="http://images2015.cnblogs.com/blog/116770/201605/116770-20160501174922566-1686627384.png" alt="Lamport timestamps"></p>
<ul>
<li>Lamport时间戳帮助我们得到事件顺序关系，但还有一种顺序关系不能用Lamport时间戳很好地表示出来，那就是同时发生关系(concurrent)。Vector clock是在Lamport时间戳基础上演进的另一种逻辑时钟方法，它通过vector结构不但记录本节点的Lamport时间戳，同时也记录了其他节点的Lamport时间戳。Vector clock的原理与Lamport时间戳类似，使用图例如下：</li>
</ul>
<p><img src="http://images2015.cnblogs.com/blog/116770/201605/116770-20160502134654404-1109556515.png" alt="Vector clock"></p>
<ul>
<li>基于Vector clock我们可以获得任意两个事件的顺序关系，结果或为先后顺序或为同时发生，识别事件顺序在工程实践中有很重要的引申应用，最常见的应用是发现数据冲突(detect conflict)。分布式系统中数据一般存在多个副本(replication)，多个副本可能被同时更新，这会引起副本间数据不一致，Version vector的实现与Vector clock非常类似，目的用于发现数据冲突。下面通过一个例子说明Version vector的用法：</li>
</ul>
<p><img src="http://images2015.cnblogs.com/blog/116770/201605/116770-20160502183034013-800335383.png" alt="Version vector"></p>
<ul>
<li>Vector clock只用于发现数据冲突，不能解决数据冲突。如何解决数据冲突因场景而异，具体方法有以最后更新为准(last write win)，或将冲突的数据交给client由client端决定如何处理，或通过quorum决议事先避免数据冲突的情况发生。</li>
<li>由于记录了所有数据在所有节点上的逻辑时钟信息，Vector clock和Version vector在实际应用中可能面临的一个问题是vector过大，用于数据管理的元数据(meta data)甚至大于数据本身。解决该问题的方法是使用server id取代client id创建vector (因为server的数量相对client稳定)，或设定最大的size、如果超过该size值则淘汰最旧的vector信息。</li>
<li>小结：以上介绍了分布式系统里逻辑时钟的表示方法，通过Lamport timestamps可以建立事件的全序关系，通过Vector clock可以比较任意两个事件的顺序关系并且能表示无因果关系的事件，将Vector clock的方法用于发现数据版本冲突，于是有了Version vector。</li>
</ul>
<h3 id="分布式系统理论基础-CAP-bangerlee"><a href="#分布式系统理论基础-CAP-bangerlee" class="headerlink" title="分布式系统理论基础 - CAP bangerlee"></a><a href="http://www.cnblogs.com/bangerlee/p/5328888.html" target="_blank" rel="external">分布式系统理论基础 - CAP</a> bangerlee</h3><ul>
<li>CAP由Eric Brewer在2000年PODC会议上提出，是Eric Brewer在Inktomi期间研发搜索引擎、分布式web缓存时得出的关于数据一致性(consistency)、服务可用性(availability)、分区容错性(partition-tolerance)的猜想：</li>
</ul>
<blockquote>
<p>It is impossible for a web service to provide the three following guarantees : Consistency, Availability and Partition-tolerance.</p>
</blockquote>
<ul>
<li>该猜想在提出两年后被证明成立[4]，成为我们熟知的CAP定理：</li>
</ul>
<blockquote>
<p>数据一致性(consistency)：如果系统对一个写操作返回成功，那么之后的读请求都必须读到这个新数据；如果返回失败，那么所有读操作都不能读到这个数据，对调用者而言数据具有强一致性(strong consistency) ；</p>
<p>服务可用性(availability)：所有读写请求在一定时间内得到响应，可终止、不会一直等待；</p>
<p>分区容错性(partition-tolerance)：在网络分区的情况下，被分隔的节点仍能正常对外服务；</p>
</blockquote>
<ul>
<li>在某时刻如果满足AP，分隔的节点同时对外服务但不能相互通信，将导致状态不一致，即不能满足C；如果满足CP，网络分区的情况下为达成C，请求只能一直等待，即不满足A；如果要满足CA，在一定时间内要达到节点状态一致，要求不能出现网络分区，则不能满足P。C、A、P三者最多只能满足其中两个，和FLP定理一样，CAP定理也指示了一个不可达的结果(impossibility result)。</li>
<li>要理解P，我们看回CAP证明中P的定义：</li>
</ul>
<blockquote>
<p>In order to model partition tolerance, the network will be allowed to lose arbitrarily many messages sent from one node to another.</p>
</blockquote>
<p>网络分区的情况符合该定义，网络丢包的情况也符合以上定义，另外节点宕机，其他节点发往宕机节点的包也将丢失，这种情况同样符合定义。现实情况下我们面对的是一个不可靠的网络、有一定概率宕机的设备，这两个因素都会导致Partition，因而分布式系统实现中P是一个必须项，而不是可选项。对于分布式系统工程实践，CAP理论更合适的描述是：在满足分区容错的前提下，没有算法能同时满足数据一致性和服务可用性：</p>
<blockquote>
<p>In a network subject to communication failures, it is impossible for any web service to implement an atomic read/write shared memory that guarantees a response to every request.</p>
</blockquote>
<ul>
<li>CAP定理证明中的一致性指强一致性，强一致性要求多节点组成的被调要能像单节点一样运作、操作具备原子性，数据在时间、时序上都有要求。如果放宽这些要求，还有序列一致性(sequential consistency)和最终一致性(eventual consistency)。工程实践中，较常见的做法是通过异步拷贝副本(asynchronous replication)、quorum/NRW，实现在调用端看来数据强一致、被调端最终一致，在调用端看来服务可用、被调端允许部分节点不可用的效果。</li>
</ul>
<h3 id="分布式系统理论进阶-Paxos-bangerlee"><a href="#分布式系统理论进阶-Paxos-bangerlee" class="headerlink" title="分布式系统理论进阶 - Paxos bangerlee"></a><a href="http://www.cnblogs.com/bangerlee/p/5655754.html" target="_blank" rel="external">分布式系统理论进阶 - Paxos</a> bangerlee</h3><ul>
<li>Paxos协议在节点宕机恢复、消息无序或丢失、网络分化的场景下能保证决议的一致性，是被讨论最广泛的一致性协议。</li>
<li>一致性问题是在节点宕机、消息无序等场景可能出现的情况下，相互独立的节点之间如何达成决议的问题，作为解决一致性问题的协议，Paxos的核心是节点间如何确定并只确定一个值(value)。</li>
<li>和2PC类似，Paxos先把节点分成两类，发起提议(proposal)的一方为proposer，参与决议的一方为acceptor。</li>
</ul>
<pre><code>P1.  一个acceptor接受它收到的第一项提议 //假如只有一个proposer发起提议，并且节点不宕机、消息不丢包
P2.  如果一项值为v的提议被确定，那么后续只确定值为v的提议
P2a. 如果一项值为v的提议被确定，那么acceptor后续只接受值为v的提议
P2b. 如果一项值为v的提议被确定，那么proposer后续只发起值为v的提议
P2c. 对于提议(n,v)，acceptor的多数派S中，如果存在acceptor最近一次(即ID值最大)接受的提议的值为v&apos;，那么要求v = v&apos;；否则v可为任意值
</code></pre><ul>
<li>以上提到的各项约束条件可以归纳为3点，如果proposer/acceptor满足下面3点，那么在少数节点宕机、网络分化隔离的情况下，在“确定并只确定一个值”这件事情上可以保证一致性(consistency)：</li>
</ul>
<pre><code>B1(ß): ß中每一轮决议都有唯一的ID标识
B2(ß): 如果决议B被acceptor多数派接受，则确定决议B
B3(ß): 对于ß中的任意提议B(n,v)，acceptor的多数派中如果存在acceptor最近一次(即ID值最大)接受的提议的值为v&apos;，那么要求v = v&apos;；否则v可为任意值
</code></pre><ul>
<li>至此，proposer/acceptor完成一轮决议可归纳为prepare和accept两个阶段。prepare阶段proposer发起提议问询提议值、acceptor回应问询并进行promise；accept阶段完成决议，图示如下：</li>
</ul>
<p><img src="http://images2015.cnblogs.com/blog/116770/201607/116770-20160712125617045-527200085.png" alt="Paxos"></p>
<h3 id="分布式系统理论进阶-Raft、Zab-bangerlee"><a href="#分布式系统理论进阶-Raft、Zab-bangerlee" class="headerlink" title="分布式系统理论进阶 - Raft、Zab bangerlee"></a><a href="http://www.cnblogs.com/bangerlee/p/5991417.html" target="_blank" rel="external">分布式系统理论进阶 - Raft、Zab</a> bangerlee</h3><ul>
<li>Paxos偏向于理论、对如何应用到工程实践提及较少。理解的难度加上现实的骨感，在生产环境中基于Paxos实现一个正确的分布式系统非常难；Raft在2013年提出，提出的时间虽然不长，但已经有很多系统基于Raft实现。相比Paxos，Raft的买点就是更利于理解、更易于实行。</li>
<li>为达到更容易理解和实行的目的，Raft将问题分解和具体化：Leader统一处理变更操作请求，一致性协议的作用具化为保证节点间操作日志副本(log replication)一致，以term作为逻辑时钟(logical clock)保证时序，节点运行相同状态机(state machine)得到一致结果。Raft协议具体过程如下：</li>
</ul>
<p><img src="http://images2015.cnblogs.com/blog/116770/201610/116770-20161024005549560-244386650.png" alt="Raft"></p>
<pre><code>①Client发起请求，每一条请求包含操作指令
②请求交由Leader处理，Leader将操作指令(entry)追加(append)至操作日志，紧接着对Follower发起AppendEntries请求、尝试让操作日志副本在Follower落地
③如果Follower多数派(quorum)同意AppendEntries请求，Leader进行commit操作、把指令交由状态机处理
④状态机处理完成后将结果返回给Client
</code></pre><ul>
<li>Paxos中Leader的存在是为了提升决议效率，Leader的有无和数目并不影响决议一致性，Raft要求具备唯一Leader，并把一致性问题具体化为保持日志副本的一致性，以此实现相较Paxos而言更容易理解、更容易实现的目标。</li>
<li>Zab的全称是Zookeeper atomic broadcast protocol，是Zookeeper内部用到的一致性协议。相比Paxos，Zab最大的特点是保证强一致性(strong consistency)。和Raft一样，Zab要求唯一Leader参与决议，Zab可以分解成discovery、sync、broadcast三个阶段：</li>
</ul>
<p><img src="http://images2015.cnblogs.com/blog/116770/201610/116770-20161025133734734-658183229.jpg" alt="Zab"></p>
<pre><code>discovery: 选举产生PL(prospective leader)，PL收集Follower epoch(cepoch)，根据Follower的反馈PL产生newepoch(每次选举产生新Leader的同时产生新epoch，类似Raft的term)
sync: PL补齐相比Follower多数派缺失的状态、之后各Follower再补齐相比PL缺失的状态，PL和Follower完成状态同步后PL变为正式Leader(established leader)
broadcast: Leader处理Client的写操作，并将状态变更广播至Follower，Follower多数派通过之后Leader发起将状态变更落地(deliver/commit)
</code></pre><ul>
<li>了解完Zab的基本原理，我们再来看Zab怎样保证强一致性，Zab通过约束事务先后顺序达到强一致性，先广播的事务先commit、FIFO，Zab称之为primary order(以下简称PO)。实现PO的核心是zxid。</li>
<li>Paxos、Raft、Zab和VR都是解决一致性问题的协议，Paxos协议原文倾向于理论，Raft、Zab、VR倾向于实践，一致性保证程度等的不同也导致这些协议间存在差异。</li>
</ul>
<h3 id="亿级规模的Elasticsearch优化实战-王卫华"><a href="#亿级规模的Elasticsearch优化实战-王卫华" class="headerlink" title="亿级规模的Elasticsearch优化实战 王卫华"></a><a href="http://mp.weixin.qq.com/s?__biz=MzAwMDU1MTE1OQ==&amp;mid=209488723&amp;idx=1&amp;sn=d60c0637d7a9f4a4b981a69f10c6b90a" target="_blank" rel="external">亿级规模的Elasticsearch优化实战</a> 王卫华</h3><ul>
<li>索引优化：SSD是经济压力能承受情况下的不二选择。减少碎片也可以提高索引速度，每天进行优化还是很有必要的。在初次索引的时候，把replica设置为0，也能提高索引速度。</li>
<li>索引优化相关参数：threadpool.index.queue_size、indices.memory.index_buffer_size、index.translog.flush_threshold_ops和refresh_interval。</li>
<li>查询优化：可以使用多个集群，每个集群使用不同的routing，比如用户是一个routing维度。在实践中，这个routing非常重要。</li>
<li>索引越来越大，单个shard也很巨大，查询速度也越来越慢。这时候，是选择分索引还是更多的shards？在实践过程中，更多的shards会带来额外的索引压力，即IO 力。我们选择了分索引，比如按照每个大分类一个索引，或者主要的大城市一个索引。然后将他们进行合并查询。如：<a href="http://cluster1:9200/shanghai,beijing/_search?routing=fang；" target="_blank" rel="external">http://cluster1:9200/shanghai,beijing/_search?routing=fang；</a></li>
<li>线程池我们默认使用fixed，使用cached有可能控制不好。主要是比较大的分片relocation时，会导致分片自动下线，集群可能处于危险状态。</li>
<li>128G内存的机器配置一个JVM，然后是巨大的heapsize（如64G）还是配多个JVM instance，较小的 heapsize（如32G）？我的建议是后者。实际使用中，后者也能帮助我们节省不少资源，并提供不错的性能。具体请参阅 “Don’t Cross 32 GB!” （<a href="https://www.elastic.co/guide/en/elasticsearch/guide/current/heap-sizing.html#compressed_oops）" target="_blank" rel="external">https://www.elastic.co/guide/en/elasticsearch/guide/current/heap-sizing.html#compressed_oops）</a></li>
</ul>
<h3 id="怎样读一本书V5-0-ljinkai"><a href="#怎样读一本书V5-0-ljinkai" class="headerlink" title="怎样读一本书V5.0 ljinkai"></a><a href="https://ljinkai.github.io/2017/02/08/how-to-read-a-book" target="_blank" rel="external">怎样读一本书V5.0</a> ljinkai</h3><p><img src="http://oi46mo3on.bkt.clouddn.com/15_reading_201702/how_to_read_a_book.png" alt="怎样读一本书"></p>
<h3 id="JVM为什么需要GC？-周明耀"><a href="#JVM为什么需要GC？-周明耀" class="headerlink" title="JVM为什么需要GC？ 周明耀"></a><a href="http://mp.weixin.qq.com/s/KVj5NwYyXyJdaqKIDu-Avg" target="_blank" rel="external">JVM为什么需要GC？</a> 周明耀</h3><ul>
<li>HotSpot的垃圾回收器总结：如果你想要最小化地使用内存和并行开销，请选Serial GC；如果你想要最大化应用程序的吞吐量，请选Parallel GC；如果你想要最小化GC的中断或停顿时间，请选CMS GC。</li>
<li>G1 GC基本思想：G1 GC是一个压缩收集器，它基于回收最大量的垃圾原理进行设计。G1 GC利用递增、并行、独占暂停这些属性，通过拷贝方式完成压缩目标。此外，它也借助并行、多阶段并行标记这些方式来帮助减少标记、重标记、清除暂停的停顿时间，让停顿时间最小化是它的设计目标之一。</li>
<li>G1 GC的垃圾回收循环组成：年轻代循环、多步骤并行标记循环、混合收集循环、Full GC；</li>
<li>G1的区间设计：在G1中，堆被平均分成若干个大小相等的区域（Region）。每个Region都有一个关联的Remembered Set（简称RS），RS的数据结构是Hash表，里面的数据是Card Table （堆中每512byte映射在card table 1byte）。简单的说RS里面存在的是Region中存活对象的指针。当Region中数据发生变化时，首先反映到Card Table中的一个或多个Card上，RS通过扫描内部的Card Table得知Region中内存使用情况和存活对象。在使用Region过程中，如果Region被填满了，分配内存的线程会重新选择一个新的Region，空闲Region被组织到一个基于链表的数据结构（LinkedList）里面，这样可以快速找到新的Region。</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/15_reading_201702/design_of_g1.png" alt="G1的区间设计"></p>
<h3 id="让机器读懂用户–大数据中的用户画像-杨杰"><a href="#让机器读懂用户–大数据中的用户画像-杨杰" class="headerlink" title="让机器读懂用户–大数据中的用户画像 杨杰"></a><a href="https://mp.weixin.qq.com/s?__biz=MzI2MzM3MzkyMg==&amp;mid=2247484433&amp;idx=1&amp;sn=f30a6a3585becc1a500772aaa78fd937" target="_blank" rel="external">让机器读懂用户–大数据中的用户画像</a> 杨杰</h3><ul>
<li>用户画像（persona）的概念最早由交互设计之父Alan Cooper提出:“Personas are a concrete representation of target users.” 是指真实用户的虚拟代表，是建立在一系列属性数据之上的目标用户模型。随着互联网的发展，现在我们说的用户画像又包含了新的内涵——通常用户画像是根据用户人口学特征、网络浏览内容、网络社交活动和消费行为等信息而抽象出的一个标签化的用户模型。构建用户画像的核心工作，<strong>主要是利用存储在服务器上的海量日志和数据库里的大量数据进行分析和挖掘</strong>，给用户贴“标签”，而“标签”是能表示用户某一维度特征的标识。具体的标签形式可以参考下图某网站给其中一个用户打的标签。</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/15_reading_201702/persona.jpg" alt="用户画像例子"></p>
<ul>
<li>用户画像的作用：精准营销、用户研究、个性服务、业务决策；</li>
<li>用户画像的内容：对于大部分互联网公司，用户画像都会包含<strong>人口属性和行为特征</strong>。人口属性主要指用户的年龄、性别、所在的省份和城市、教育程度、婚姻情况、生育情况、工作所在的行业和职业等。行为特征主要包含活跃度、忠诚度等指标。另外，电商购物网站的用户画像，一般会提取用户的<strong>网购兴趣和消费能力</strong>等指标。</li>
<li>用户画像的生产，大致可以分为以下几步：</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/15_reading_201702/netease_persona.jpg" alt="网易用户画像"></p>
<blockquote>
<ol>
<li>用户建模，指确定提取的用户特征维度，和需要使用到的数据源。</li>
<li>数据收集，通过数据收集工具，如Flume或自己写的脚本程序，把需要使用的数据统一存放到Hadoop集群。</li>
<li>数据清理，数据清理的过程通常位于Hadoop集群，也有可能与数据收集同时进行，这一步的主要工作，是把收集到各种来源、杂乱无章的数据进行字段提取，得到关注的目标特征。</li>
<li>模型训练，有些特征可能无法直接从数据清理得到，比如用户感兴趣的内容或用户的消费水平，那么可以通过收集到的已知特征进行学习和预测。</li>
<li>属性预测，利用训练得到的模型和用户的已知特征，预测用户的未知特征。</li>
<li>数据合并，把用户通过各种数据源提取的特征进行合并，并给出一定的可信度。</li>
<li>数据分发，对于合并后的结果数据，分发到精准营销、个性化推荐、CRM等各个平台，提供数据支持。</li>
</ol>
</blockquote>
<ul>
<li>应用示例之个性化推荐：很多推荐场景都会用到基于商品的协同过滤，而基于商品协同过滤的核心是一个商品相关性矩阵W，假设有n个商品，那么W就是一个n * n的矩阵，矩阵的元素wij代表商品Ii和Ij之间的相关系数。而根据用户访问和购买商品的行为特征，可以把用户表示成一个n维的特征向量U=[ i1, i2, …, in ]。于是U * W可以看成用户对每个商品的感兴趣程度V=[ v1, v2, …, vn ]，这里v1即是用户对商品I1的感兴趣程度，v1= i1*w11 + i2*w12 + in*w1n。如果把相关系数w11, w12, …, w1n 看成要求的变量，那么就可以用LR模型，代入训练集用户的行为向量U，进行求解。这样一个初步的LR模型就训练出来了，效果和基于商品的协同过滤类似。</li>
</ul>
<h3 id="推荐系统本质与网易严选实践-沈燕"><a href="#推荐系统本质与网易严选实践-沈燕" class="headerlink" title="推荐系统本质与网易严选实践  沈燕"></a><a href="http://mp.weixin.qq.com/s?__biz=MzA4Mzc0NjkwNA==&amp;mid=2650782153&amp;idx=1&amp;sn=d90906f57d45d8991a9be1269627315d" target="_blank" rel="external">推荐系统本质与网易严选实践</a>  沈燕</h3><ul>
<li>推荐系统作用本质：有资料称亚马逊的推荐系统带来的GMV占其全站总量的20%-30%；推荐的本质就是提升用户体验，为此它们最主要的方式就是帮助用户快速的找到它需要的产商品，其他的方式还包括给用户新颖感等。</li>
<li>推荐系统工作原理本质：所谓embedding，数学上的意义就是映射。如word2vec通过语料训练把词变成一个数百维的向量，向量的每一维没有明确的物理意义（或者说我们无法理解）。推荐系统如果可以把人很精确地映射成一个向量，把物品也映射成一个同维度同意义的向量，那么推荐就是可以按规则处理的精确的事情了。</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/15_reading_201702/recommendation.png" alt="最佳的推荐形式"></p>
<ul>
<li>电商推荐系统的特点：商品种类数巨大，不同的商品需要不同的embedding；单种商品深度不够，难以有效embedding；人对商品的兴趣大都建立在短期或者瞬时需求之上；大量耐消品的影响；用户理论上对所有商品都会有兴趣。基于以上的原因，在电商领域难以找到完美的embedding方式来实现推荐。其实我们在看各大电商的个性化推荐时，无论宣称背后用怎样复杂的模型融合，从结果看，用户近期行为的权重是非常大的，使得结果非常像itemCF推荐出来的。</li>
</ul>
<h4 id="网易严选推荐实践"><a href="#网易严选推荐实践" class="headerlink" title="网易严选推荐实践"></a>网易严选推荐实践</h4><p>网易严选推荐的基础模型采用的是CTR模型，基于LR（逻辑回归）。</p>
<p><img src="http://oi46mo3on.bkt.clouddn.com/15_reading_201702/netease_recommendation_1.jpg" alt="网易严选推荐模型"></p>
<p>在核心的特征工程方面，网易严选推荐团队将用户的具体属性（性别、收入水平、地域等）、用户在网易严选的行为属性（短期，长期）、及时间上下文（季节、上次购买时间间隔等）作为属性空间，从1层迪卡尔积开始往上构造N层迪卡尔积形成复杂属性空间P，挖掘属性空间与商品的相关，对有明显相关（正相关或负相关）的（属性、物品）对构造特征。</p>
<p>用户属性空间</p>
<p><img src="http://oi46mo3on.bkt.clouddn.com/15_reading_201702/netease_recommendation_2.jpg" alt="用户属性空间"></p>
<p>具体属性应用</p>
<p><img src="http://oi46mo3on.bkt.clouddn.com/15_reading_201702/netease_recommendation_3.jpg" alt="具体属性应用"></p>
<p>行为属性作为抽象属性与具体属性置以相同的地位</p>
<p><img src="http://oi46mo3on.bkt.clouddn.com/15_reading_201702/netease_recommendation_4.jpg" alt="行为属性"></p>
<p>二阶属性（属性的2重迪卡尔积）</p>
<p><img src="http://oi46mo3on.bkt.clouddn.com/15_reading_201702/netease_recommendation_5.jpg" alt="二阶属性"></p>
<p>从结果来看，这一套特征工程方法可以挖出比较全的特征集，在鲁棒性与效果上都有不错的效果，自上线以来各项指标均在稳步提升。</p>
<h3 id="深入探索Java-8-Lambda表达式-Richard-Warburton-Raoul-Urma-Mario-Fusco-段建华"><a href="#深入探索Java-8-Lambda表达式-Richard-Warburton-Raoul-Urma-Mario-Fusco-段建华" class="headerlink" title="深入探索Java 8 Lambda表达式  Richard Warburton/Raoul Urma/Mario Fusco/段建华"></a><a href="http://www.infoq.com/cn/articles/Java-8-Lambdas-A-Peek-Under-the-Hood" target="_blank" rel="external">深入探索Java 8 Lambda表达式</a>  Richard Warburton/Raoul Urma/Mario Fusco/段建华</h3><ul>
<li>为什么匿名内部类不好？编译器会为每一个匿名内部类创建一个类文件，而类在使用之前需要加载类文件并进行验证，这个过程则会影响应用的启动性能，另外类文件的加载很有可能是一个耗时的操作，这其中包含了磁盘IO和解压JAR文件。最重要的，一旦Lambda表达式使用了匿名内部类实现，就会限制了后续Lambda表达式实现的更改，降低了其随着JVM改进而改进的能力。</li>
<li>Lambdas表达式和invokedynamic：将Lambda表达式转化成字节码只需要如下两步：1.生成一个invokedynamic调用点，也叫做Lambda工厂，当调用时返回一个Lambda表达式转化成的函数式接口实例；2.将Lambda表达式的方法体转换成方法供invokedynamic指令调用。需要注意的是编译器对于Lambda表达式的翻译策略并非固定的，因为这样invokedynamic可以使编译器在后期使用不同的翻译实现策略。</li>
<li>性能分析：Lambda工厂的预热准备需要消耗时间，但Lambda工厂方式也会比匿名内部类加载要快，最高可达100倍；如果是不进行捕获变量，这一步会自动进行优化，避免在基于Lambda工厂实现下额外创建对象；对于真实方法的调用，匿名内部类和Lambda表达式执行的操作相同，没有性能上的差别；</li>
<li>对于大多数情况来说，Lambda表达式要比匿名内部类性能更优。然而现状并非完美，基于测量驱动优化，我们仍然有很大的提升空间。</li>
</ul>
<h3 id="红黑树-Hosee"><a href="#红黑树-Hosee" class="headerlink" title="红黑树 Hosee"></a><a href="https://my.oschina.net/hosee/blog/618828" target="_blank" rel="external">红黑树</a> Hosee</h3><ul>
<li>先来看下算法导论对R-B Tree的介绍：</li>
</ul>
<blockquote>
<p>红黑树，一种二叉查找树，但在每个结点上增加一个存储位表示结点的颜色，可以是Red或Black。<br>通过对任何一条从根到叶子的路径上各个结点着色方式的限制，红黑树确保没有一条路径会比其他路径长出俩倍，因而是接近平衡的。</p>
</blockquote>
<ul>
<li>二叉查找树，也称有序二叉树（ordered binary tree），是指一棵空树或者具有下列性质的二叉树：</li>
</ul>
<pre><code>1.若任意节点的左子树不空，则左子树上所有结点的值均小于它的根结点的值；
2.若任意节点的右子树不空，则右子树上所有结点的值均大于它的根结点的值；
3.任意节点的左、右子树也分别为二叉查找树。
4.没有键值相等的节点（no duplicate nodes）。
</code></pre><ul>
<li>红黑树虽然本质上是一棵二叉查找树，但它在二叉查找树的基础上增加了着色和相关的性质使得红黑树相对平衡，从而保证了红黑树的查找、插入、删除的时间复杂度最坏为O(log n)。但它是如何保证一棵n个结点的红黑树的高度始终保持在logn的呢？这就引出了红黑树的5个性质：</li>
</ul>
<pre><code>1.每个结点要么是红的要么是黑的。  
2.根结点是黑的。  
3.每个叶结点（叶结点即指树尾端NIL指针或NULL结点）都是黑的。  
4.如果一个结点是红的，那么它的两个儿子都是黑的。  
5.对于任意结点而言，其到叶结点树尾端NIL指针的每条路径都包含相同数目的黑结点。
</code></pre><p><img src="http://static.oschina.net/uploads/space/2016/0220/152640_Hxex_2243330.png" alt="红黑树"></p>
<ul>
<li>当在对红黑树进行插入和删除等操作时，对树做了修改可能会破坏红黑树的性质。为了继续保持红黑树的性质，可以通过对结点进行重新着色，以及对树进行相关的旋转操作，即通过修改树中某些结点的颜色及指针结构，来达到对红黑树进行插入或删除结点等操作后继续保持它的性质或平衡的目的。</li>
<li>红黑树的插入：首先，将红黑树当作一颗二叉查找树，将节点插入；然后，将节点着色为红色；最后，通过旋转和重新着色等方法来修正该树，使之重新成为一颗红黑树。</li>
<li>红黑树与AVL树的区别：红黑树旋转操作非常局部化，而且次数极少（插入最多两次旋转，删除最多三次旋转），而改变颜色的操作不会影响到用户对树的query操作（即不要lock），另外很多树，如AVL树，2-3树,2-4树都可以转化成红黑树，红黑树能达到O(logn)高度，但是不像AVL树那样严格要求左右子树高度差必需相差不超过1。可以说RB树是目前为止高度要求最灵活的准平衡BST。准平衡是相对完全二叉树来说的，AVL树(比如Fibonacci树)也不是完美平衡的。</li>
</ul>
<h3 id="业界难题-“跨库分页”的四种方案-58沈剑"><a href="#业界难题-“跨库分页”的四种方案-58沈剑" class="headerlink" title="业界难题-“跨库分页”的四种方案 58沈剑"></a><a href="http://mp.weixin.qq.com/s?__biz=MjM5ODYxMDA5OQ==&amp;mid=2651959942&amp;idx=1&amp;sn=e9d3fe111b8a1d44335f798bbb6b9eea" target="_blank" rel="external">业界难题-“跨库分页”的四种方案</a> 58沈剑</h3><p><strong>方法一：全局视野法</strong></p>
<ol>
<li>将order by time offset X limit Y，改写成order by time offset 0 limit X+Y</li>
<li>服务层对得到的N*(X+Y)条数据进行内存排序，内存排序后再取偏移量X后的Y条记录这种方法随着翻页的进行，性能越来越低。</li>
</ol>
<p><strong>方法二：业务折衷法-禁止跳页查询</strong></p>
<ol>
<li>用正常的方法取得第一页数据，并得到第一页记录的time_max</li>
<li>每次翻页，将order by time offset X limit Y，改写成order by time where time&gt;$time_max limit Y 以保证每次只返回一页数据，性能为常量。</li>
</ol>
<p><strong>方法三：业务折衷法-允许模糊数据</strong><br>将order by time offset X limit Y，改写成order by time offset X/N limit Y/N</p>
<p><strong>方法四：二次查询法</strong></p>
<ol>
<li>将order by time offset X limit Y，改写成order by time offset X/N limit Y</li>
<li>找到最小值time_min</li>
<li>between二次查询，order by time between $time_min and $time_i_max</li>
<li>设置虚拟time_min，找到time_min在各个分库的offset，从而得到time_min在全局的offset</li>
<li>得到了time_min在全局的offset，自然得到了全局的offset X limit Y</li>
</ol>
<h3 id="从GITLAB误删除数据库想到的-陈皓"><a href="#从GITLAB误删除数据库想到的-陈皓" class="headerlink" title="从GITLAB误删除数据库想到的 陈皓"></a><a href="http://coolshell.cn/articles/17680.html" target="_blank" rel="external">从GITLAB误删除数据库想到的</a> 陈皓</h3><ul>
<li>事件回顾：Gitlab某员工在做负载均衡工作时需要解决突发情况，误将删除命令敲到生产环境的窗口上导致线上数据库被删除，然后视图通过多种备份机制都无法恢复，最终只能从6小时前的数据库中拷贝回来，导致在这6个小时期间的数据丢失；</li>
<li>人肉运维：一个公司的运维能力的强弱和你上线上环境敲命令是有关的，你越是喜欢上线敲命令你的运维能力就越弱，越是通过自动化来处理问题，你的运维能力就越强。</li>
<li>数据丢失有各种各样的情况，不单单只是人员的误操作，比如，掉电、磁盘损坏、中病毒等等，在这些情况下，你设计的那些想流程、规则、人肉检查、权限系统、checklist等等统统都不管用了，这个时候，你觉得应该怎么做呢？是的，你会发现，你不得不用更好的技术去设计出一个高可用的系统！别无它法。</li>
<li>关于备份：如果你要让你的备份系统随时都可以用，那么你就要让它随时都Live着，而随时都Live着的多结点系统，基本上就是一个分布式的高可用的系统。</li>
<li>非技术方面：故障反思（5 whys分析）、工程师文化（如果你是一个技术公司，你就会更多的相信技术而不是管理）、事件公开（公开所有的细节，会让大众少很多猜测的空间，有利于抵制流言和黑公关，同时，还会赢得大众的理解和支持。）；</li>
</ul>
<h3 id="AWS-的-S3-故障回顾和思考-陈皓"><a href="#AWS-的-S3-故障回顾和思考-陈皓" class="headerlink" title="AWS 的 S3 故障回顾和思考 陈皓"></a><a href="http://coolshell.cn/articles/17737.html" target="_blank" rel="external">AWS 的 S3 故障回顾和思考</a> 陈皓</h3><ul>
<li>故障原因：AWS某员工在修复账务系统问题，需要移除某些子系统时有一条命令搞错了，移除了大量S3的控制系统，包括对象索引服务和位置服务系统。而这两个系统重启花费了非常长时间（由于该系统非常稳定，以及很长时间没有重启过，而数据量级却一直在增长），最终导致服务挂了4个小时；</li>
<li>AWS后续改进措施：改进运维操作工具（让删除服务这个操作变慢一些、任何服务在运行时都应该有一个最小资源数、Review所有和其它的运维工具）；改进恢复过程（分解现有厚重的重要服务成更小的单元、今年内完成对 Index 索引服务的分区计划）；</li>
<li>一个系统的高可用的因素很多，不仅仅只是系统架构，更重要的是——高可用运维。对于高可用的运维，平时的故障演习是很重要的。</li>
</ul>
<h3 id="关于高可用的系统-陈皓"><a href="#关于高可用的系统-陈皓" class="headerlink" title="关于高可用的系统 陈皓"></a><a href="http://coolshell.cn/articles/17459.html" target="_blank" rel="external">关于高可用的系统</a> 陈皓</h3><ul>
<li>理解高可用系统：要做到数据不丢，就必需要持久化；要做到服务高可用，就必需要有备用（复本），无论是应用结点还是数据结点；要做到复制，就会有数据一致性的问题；我们不可能做到100%的高可用，也就是说，我们能做到几个9个的SLA；</li>
<li>高可用系统的技术解决方案：下图基本上来说是目前高可用系统中能看得到的所有的解决方案的基础了。M/S、MM实现起来不难，但是会有很多问题，2PC的问题就是性能不行，而Paxos的问题就是太复杂，实现难度太大。</li>
</ul>
<p><img src="http://coolshell.cn//wp-content/uploads/2014/01/Transaction-Across-DataCenter.jpg" alt="Transaction Across DataCenter"></p>
<ul>
<li>高可用技术方案的示例：MySQL的高可用的方案的SLA（下图下面红色的标识表示了这个方案有几个9）：</li>
</ul>
<p><img src="http://coolshell.cn//wp-content/uploads/2016/08/mysql-high-availability-solutions-feb-2015-webinar-9-638.jpg" alt="MySQL的高可用的方案的SLA"></p>
<pre><code>1.MySQL Repleaction就是传统的异步数据同步或是半同步Semi-Sync这个方式本质上不到2个9；
2.MySQL Fabric简单来说就是数据分片下的M/S的读写分离模式。这个方案的的可用性可以达到99%；
3.DRBD通过底层的磁盘同步技术来解决数据同步的问题，就是RAID 1——把两台以上的主机的硬盘镜像成一个。这个方案不到3个9；
4.Solaris Clustering/Oracle VM ，这个机制监控了包括硬件、操作系统、网络和数据库。这个方案一般会伴随着节点间的“心跳机制”，
而且还会动用到SAN（Storage Area Network）或是本地的分布式存储系统，还会动用虚拟化技术来做虚拟机的迁移以降低宕机时间的概率。这个解决方案完全就是一个“全栈式的解决方案”。这个方案接近4个9；
5.MySQL Cluster是官方的一个开源方案，其把MySQL的集群分成SQL Node 和Data Node，
Data Node是一个自动化sharing和复制的集群NDB，为了更高的可用性，MySQL Cluster采用了“完全同步”的数据复制的机制来冗余数据结点。这个方案接近5个9；
</code></pre><ul>
<li>影响高可用的因素：无计划的宕机原因（系统级的故障、数据和中介的故障、自然灾害、人为破坏等）；有计划的宕机原因（日常任务、运维相关、升级相关）；</li>
<li>要干出高可用的系统，其中包括但不限于：软件的设计、编码、测试、上线和软件配置管理的水平；工程师的人员技能水平；运维的管理和技术水平；数据中心的运营管理水平；依赖于第三方服务的管理水平；</li>
<li>深层交的东西则是——对工程这门科学的尊重：对待技术的态度、一个公司的工程文化、领导者对工程的尊重；</li>
</ul>
<h3 id="专访RocketMQ联合创始人：项目思路、技术细节和未来规划-王小瑞-冯嘉"><a href="#专访RocketMQ联合创始人：项目思路、技术细节和未来规划-王小瑞-冯嘉" class="headerlink" title="专访RocketMQ联合创始人：项目思路、技术细节和未来规划 王小瑞/冯嘉"></a><a href="http://jm.taobao.org/2017/03/03/RocketMQ-future-idea/" target="_blank" rel="external">专访RocketMQ联合创始人：项目思路、技术细节和未来规划</a> 王小瑞/冯嘉</h3><ul>
<li>RocketMQ的由来：第一代，推模式，数据存储采用关系型数据库，典型代表包括Notify、Napoli；第二代，拉模式，自研的专有消息存储，典型代表MetaQ；第三代，以拉模式为主，兼有推模式的高性能、低延迟消息引擎RocketMQ，在二代功能特性的基础上，为电商金融领域添加了可靠重试、基于文件存储的分布式事务等特性，并做了大量优化。</li>
<li>RocketMQ的技术概览：在我们看来，它最大的创新点在于能够通过精巧的横向、纵向扩展，不断满足与日俱增的海量消息在高吞吐、高可靠、低延迟方面的要求。目前RocketMQ主要由NameServer、Broker、Producer以及Consumer四部分构成，所有的集群都具有水平扩展能力，如下图所示：</li>
</ul>
<p><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1FUR8PVXXXXbbXpXXXXXXXXXX" alt="RocketMQ的技术概览"></p>
<ul>
<li>与其他消息中间件比较：RabbitMQ是AMQP规范的参考实现，AMQP是一个线路层协议，面面俱到，很系统，也稍显复杂；ActiveMQ是JMS规范的参考实现，JMS虽说是一个API级别的协议，但其内部还是定义了一些实现约束，不过缺少多语言支撑；而Kafka最初被设计用来做日志处理，是一个不折不扣的大数据通道，追求高吞吐，存在丢消息的可能；RocketMQ天生为金融互联网领域而生，追求高可靠、高可用、高并发、低延迟，是一个阿里巴巴由内而外成功孕育的典范。</li>
</ul>
<p><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1trKoPVXXXXXpXXXXXXXXXXXX" alt="与其他消息中间件比较"></p>
<ul>
<li>三项技术发力点：消息的顺序（全局保序）、消息的去重（目前的版本是不支持去重的，建议用户通过外置全局存储自己做判重处理，后续版本内置解决方案）、分布式的挑战（基于Zab一致性协议，利用分布式锁和通知机制保障多副本数据的一致性）；</li>
<li>新一代RocketMQ：期望构建一套厂商无关的集线路层、API层于一体的规范，这也是第四代消息引擎最大的亮点。</li>
</ul>
<h3 id="分布式事务原理与实践-沈询"><a href="#分布式事务原理与实践-沈询" class="headerlink" title="分布式事务原理与实践 沈询"></a><a href="http://jm.taobao.org/2017/02/09/20170209/" target="_blank" rel="external">分布式事务原理与实践</a> 沈询</h3><ul>
<li>事务的四大特性分别是：原子型、一致性、隔离性和持久性。</li>
<li>事务单元是通过Begin-Traction，然后Commit（Begin-Traction、Commit和Rollback之间所有针对数据的写入、读取的操作都应该添加同步访问），Begin和Commit之间就是一个同步的事务单元。</li>
<li>Two Phase Lock（2PL）是数据库中非常重要的一个概念。数据库操作Insert、Update、Delete都是先读再写的操作，例如Insert操作是先读取数据，读取之后判读数据是否存在，如果不存在，则写入该数据，如果数据存在，则返回错误。</li>
<li>处理事务的常见方法有排队法、排他锁、读写锁、MVCC等方式。事务处理中最重要也是最简单的方案是排队法，单线程地处理一堆数据；有些场景不适合用单线程操作，可以利用排他锁的方式来快速隔离并发读写事务；读写锁的核心是在多次读的操作中，同时允许多个读者来访问共享资源，提高并发性；MVCC本质是Copy On Write，也就是每次写都是以重新开始一个新的版本的方式写入数据，因此，数据库中也就包含了之前的所有版本，在数据读的过程中，先申请一个版本号，如果该版本号小于正在写入的版本号，则数据一定可以查询到，无需等到新版本完全写完即可返回查询结果。</li>
<li>事务的调优原则：尽可能减少锁的覆盖范围、增加锁上可并行的线程数、选择正确锁类型（比如悲观锁适合并发争抢比较严重的场景，乐观锁适合并发争抢不太严重的场景）；</li>
</ul>
<h3 id="对比了解Grafana与Kibana的关键差异-Asaf-Yigal-冬雨"><a href="#对比了解Grafana与Kibana的关键差异-Asaf-Yigal-冬雨" class="headerlink" title="对比了解Grafana与Kibana的关键差异 Asaf Yigal/冬雨"></a><a href="http://www.infoq.com/cn/articles/grafana-vs-kibana-the-key-differences-to-know" target="_blank" rel="external">对比了解Grafana与Kibana的关键差异</a> Asaf Yigal/冬雨</h3><ul>
<li>Kibana是一个分析和可视化平台，它可以让你浏览、可视化存储在Elasticsearch集群上排名靠前的日志数据，并构建仪表盘。你可以执行深入的数据分析并以多种图表、表格和地图方式可视化这些数据。Kibana的仪表盘非常简单易用，任何人都可以使用它，甚至IT技能和知识很少的业务人员也可以使用。</li>
<li>Grafana是一个开源仪表盘工具，它可用于Graphite、InfluxDB与 OpenTSDB一起使用。最新的版本还可以用于其他的数据源，比如Elasticsearch。它包含一个独一无二的Graphite目标解析器，从而可以简化度量和函数的编辑。Grafana快速的客户端渲染默认使用的是 Flot ，即使很长的时间范围也可应对。</li>
<li>日志与度量：Grafana专注于根据CPU和IO利用率之类的特定指标提供时间序列图表，而Kibana能创建一个复杂的日志分析仪表盘；</li>
<li>基于角色的访问：默认情况下Kibana的仪表盘是公开的，Grafana内置的RBA允许你维护用户和团队访问仪表盘的权限；</li>
<li>仪表盘灵活性：虽然Kibana有大量内置的图表类型，但它们之上的控制仍是最初的限制，Grafana包括更多的选择，可以更灵活地浏览和使用图表；</li>
<li>数据源的集成：Grafana支持许多不同的存储后端，它是针对数据源所具备的特性和能力特别定制的，而Kibana原生集成进了ELK栈，这使安装极为简单，对用户非常友好；</li>
<li>开源社区：ELK仍保持着快速的增长，并有潜力在不久的将来保持领先；</li>
<li>共同协作：Kibana和Grafana都是强大的可视化工具。然而，Grafana和InfluxDB组合是用于度量数据的，反之，Kibana是流行的ELK栈的一部分，它可以更为灵活地浏览日志数据。这两个平台都是好的选择，甚至有时还可以互补。首先，用Kibana去分析你的日志。然后，把数据导入到Grafana作为可视化层。这些的前提是需要同一个Elasticsearch库。</li>
</ul>
<h3 id="百度宣布将在Kubernetes上运行其深度学习平台PaddlePaddle-木环"><a href="#百度宣布将在Kubernetes上运行其深度学习平台PaddlePaddle-木环" class="headerlink" title="百度宣布将在Kubernetes上运行其深度学习平台PaddlePaddle 木环"></a><a href="http://www.infoq.com/cn/news/2017/02/baidu-Kubernetes-PaddlePaddle" target="_blank" rel="external">百度宣布将在Kubernetes上运行其深度学习平台PaddlePaddle</a> 木环</h3><ul>
<li>本月初，Kubernetes在其官网上宣布了百度的PaddlePaddle成为目前唯一官方支持Kubernetes的深度学习框架。PaddlePaddle是百度于2016年9月开源的一款深度学习平台，具有易用，高效，灵活和可伸缩等特点，为百度内部多项产品提供深度学习算法支持。</li>
<li>Kubernetes 把很多分散的物理计算资源抽象成一个巨大的资源池，它利用这些资源来帮助用户执行计算任务。对于用户来说，操作一个分散的集群资源可以像使用一台计算机一样简单。对于这个项目，Kubernetes 主要负责将学习任务分配到集群的物理节点上进行运算；如果遇到任务失败的情况，Kubernetes 会自动重启任务。</li>
<li>能不能将框架的作业和任务模式，同“容器”这个全新的部署概念匹配起来，才是现阶段最重要的。毕竟，如果框架连正常运行起来都很困难，再好的资源利用率提升机制也没有用武之地。在这一点上，Kubernetes应该说是现有的容器管理项目中做的最好的。</li>
<li>容器化实施深度学习的优点：轻量级、更高的资源利用率、基于容器的设计模式、高度的可扩展性和容错能力；</li>
</ul>
<h3 id="建设DevOps统一运维监控平台，先从日志监控说起-王海龙"><a href="#建设DevOps统一运维监控平台，先从日志监控说起-王海龙" class="headerlink" title="建设DevOps统一运维监控平台，先从日志监控说起 王海龙"></a><a href="http://mp.weixin.qq.com/s/QqoyLhCdy85gD9ixOdCYqg" target="_blank" rel="external">建设DevOps统一运维监控平台，先从日志监控说起</a> 王海龙</h3><ul>
<li>DevOps浪潮下带来的监控挑战：监控源的多样化挑战、海量数据的分析处理挑战、软硬件数据资源的管理分析挑战；</li>
<li>一个好的统一监控平台，应当具备：高度抽象模型，扩展监控指标、多种监控视图、强大的数据加工能力、多种数据采集技术、多种报警机制、全路径问题跟踪；</li>
<li>统一监控平台由七大角色构成：监控源、数据采集、数据存储、数据分析、数据展现、预警中心、CMDB(企业软硬件资产管理)。</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/15_reading_201702/monitor_system.jpg" alt="统一监控平台"></p>
<ul>
<li>日志监控的技术栈</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/15_reading_201702/monitor_system_tach.jpg" alt="日志监控的技术栈"></p>
<ul>
<li>ELK-日志监控经典方案</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/15_reading_201702/monitor_system_elk.jpg" alt="ELK-日志监控经典方案"> </p>
<ul>
<li>微服务+容器云背景下的日志监控实践：跑在容器中的应用、数据库等软件都会把日志落到容器日志（docker日志），然后在docker系统服务上进行配置，将docker容器日志输出到系统日志服务journald中。这样，容器中的日志就统一到了系统日志中。针对于运行在虚拟机上的系统软件，如kubernetes、etcd等，配置成系统服务service，使用systemd管理，自然也就做到了将其日志输入到journald中。再往上就比较简单了，自实现一个agent，读取journald中的日志，通过tcp协议发送到fluentd中，考虑到现在的日志量并不会太大，所以没有再使用kafka进行缓冲，而是直接经过fluentd的拦截和过滤，将日志发送到Elasticsearch中。</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/15_reading_201702/monitor_system_inuse.jpg" alt="日志监控实践"> </p>
<ul>
<li>如何选择适合自己的日志监控方案：工具能力是否满足、性能对比、看技术能力是否能cover住、监控平台日志量评估；</li>
</ul>
<h3 id="函数式编程入门教程-阮一峰"><a href="#函数式编程入门教程-阮一峰" class="headerlink" title="函数式编程入门教程 阮一峰"></a><a href="http://www.ruanyifeng.com/blog/2017/02/fp-tutorial.html" target="_blank" rel="external">函数式编程入门教程</a> 阮一峰</h3><ul>
<li>函数式编程的起源，是一门叫做范畴论（Category Theory）的数学分支。理解函数式编程的关键，就是理解范畴论。它是一门很复杂的数学，认为世界上所有的概念体系，都可以抽象成一个个的”范畴”（category）。</li>
<li>范畴就是使用箭头连接的物体。也就是说，彼此之间存在某种关系的概念、事物、对象等等，都构成”范畴”。箭头表示范畴成员之间的关系，正式的名称叫做”态射”（morphism）。范畴论认为，同一个范畴的所有成员，就是不同状态的”变形”（transformation）。通过”态射”，一个成员可以变形成另一个成员。</li>
<li>我们可以把”范畴”想象成是一个容器，里面包含两样东西：值（value）和值的变形关系，也就是函数。</li>
<li>本质上，函数式编程只是范畴论的运算方法，跟数理逻辑、微积分、行列式是同一类东西，都是数学方法，只是碰巧它能用来写程序。</li>
<li>如果一个值要经过多个函数，才能变成另外一个值，就可以把所有中间步骤合并成一个函数，这叫做”函数的合成”（compose）。</li>
<li>f(x)和g(x)合成为f(g(x))，有一个隐藏的前提，就是f和g都只能接受一个参数。如果可以接受多个参数就需要函数柯里化了。所谓”柯里化”，就是把一个多参数的函数，转化为单参数函数。</li>
<li>函子是函数式编程里面最重要的数据类型，也是基本的运算单位和功能单位。它首先是一种范畴，也就是说，是一个容器，包含了值和变形关系。比较特殊的是，它的变形关系可以依次作用于每一个值，将当前容器变形成另一个容器。</li>
<li>学习函数式编程，实际上就是学习函子的各种运算。</li>
</ul>
<p><strong>免责声明：相关链接版本为原作者所有，如有侵权，请告知删除。</strong></p>
<p><strong>随手记系列：</strong></p>
<ul>
<li><a href="http://ginobefunny.com/post/reading_record_201701/">阅读随手记 201701</a></li>
<li><a href="http://ginobefunny.com/post/reading_record_201612/">阅读随手记 201612</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;关键字：微服务, 分布式, 配置中心, Java编程, 推荐系统, 运维, 高并发, 高可用, 机器学习, 深度学习。&lt;br&gt;
    
    </summary>
    
      <category term="Reading Record" scheme="http://ginobefunny.com/categories/Reading-Record/"/>
    
    
      <category term="推荐系统" scheme="http://ginobefunny.com/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
      <category term="微服务" scheme="http://ginobefunny.com/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"/>
    
      <category term="机器学习" scheme="http://ginobefunny.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="http://ginobefunny.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="高并发" scheme="http://ginobefunny.com/tags/%E9%AB%98%E5%B9%B6%E5%8F%91/"/>
    
      <category term="高可用" scheme="http://ginobefunny.com/tags/%E9%AB%98%E5%8F%AF%E7%94%A8/"/>
    
      <category term="分布式" scheme="http://ginobefunny.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
      <category term="配置中心" scheme="http://ginobefunny.com/tags/%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83/"/>
    
      <category term="Java编程" scheme="http://ginobefunny.com/tags/Java%E7%BC%96%E7%A8%8B/"/>
    
      <category term="运维" scheme="http://ginobefunny.com/tags/%E8%BF%90%E7%BB%B4/"/>
    
  </entry>
  
  <entry>
    <title>Protocol Buffers简明教程</title>
    <link href="http://ginobefunny.com/post/learning_protobuf/"/>
    <id>http://ginobefunny.com/post/learning_protobuf/</id>
    <published>2017-02-07T11:32:47.000Z</published>
    <updated>2017-02-10T05:12:25.760Z</updated>
    
    <content type="html"><![CDATA[<p>随着微服务架构的流行，RPC框架渐渐地成为服务框架的一个重要部分。在很多RPC的设计中，都采用了高性能的编解码技术，Protocol Buffers就属于其中的佼佼者。<a href="https://github.com/google/protobuf" target="_blank" rel="external">Protocol Buffers</a>是Google开源的一个语言无关、平台无关的通信协议，其小巧、高效和友好的兼容性设计，使其被广泛使用。<br><a id="more"></a></p>
<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><h3 id="protobuf是什么？"><a href="#protobuf是什么？" class="headerlink" title="protobuf是什么？"></a>protobuf是什么？</h3><blockquote>
<p>Protocol buffers are Google’s language-neutral, platform-neutral, extensible mechanism for serializing structured data – think XML, but smaller, faster, and simpler. You define how you want your data to be structured once, then you can use special generated source code to easily write and read your structured data to and from a variety of data streams and using a variety of languages.</p>
</blockquote>
<ul>
<li>Google良心企业出厂的；</li>
<li>是一种序列化对象框架（或者说是编解码框架），其他功能相似的有Java自带的序列化、Facebook的Thrift和JBoss Marshalling等；</li>
<li>通过proto文件定义结构化数据，其他功能相似的比如XML、JSON等；</li>
<li>自带代码生成器，支持多种语言；</li>
</ul>
<h3 id="为什么叫“Protocol-Buffers”？"><a href="#为什么叫“Protocol-Buffers”？" class="headerlink" title="为什么叫“Protocol Buffers”？"></a>为什么叫“Protocol Buffers”？</h3><p>官方如是说：</p>
<blockquote>
<p>The name originates from the early days of the format, before we had the protocol buffer compiler to generate classes for us. At the time, there was a class called ProtocolBuffer which actually acted as a buffer for an individual method. Users would add tag/value pairs to this buffer individually by calling methods like AddValue(tag, value). The raw bytes were stored in a buffer which could then be written out once the message had been constructed.</p>
<p>Since that time, the “buffers” part of the name has lost its meaning, but it is still the name we use. Today, people usually use the term “protocol message” to refer to a message in an abstract sense, “protocol buffer” to refer to a serialized copy of a message, and “protocol message object” to refer to an in-memory object representing the parsed message.</p>
</blockquote>
<h3 id="核心特点"><a href="#核心特点" class="headerlink" title="核心特点"></a>核心特点</h3><ul>
<li>语言无关、平台无关</li>
<li>简洁</li>
<li>高性能</li>
<li>良好的兼容性</li>
</ul>
<h3 id="“变态的”性能表现"><a href="#“变态的”性能表现" class="headerlink" title="“变态的”性能表现"></a>“变态的”性能表现</h3><p>有位网友曾经做过<a href="http://agapple.iteye.com/blog/859052" target="_blank" rel="external">各种通用序列化协议技术的对比</a>，我这里直接拿来给大家感受一下：</p>
<p><strong>序列化响应时间对比</strong></p>
<p><img src="http://oi46mo3on.bkt.clouddn.com/13_learning_protobuf/protobuf_comparation_time.png" alt="序列化响应时间对比"></p>
<p><strong>序列化bytes对比</strong></p>
<p><img src="http://oi46mo3on.bkt.clouddn.com/13_learning_protobuf/protobuf_comparation_bytes.png" alt="序列化bytes对比"></p>
<p><strong>具体的数字</strong></p>
<p><img src="http://oi46mo3on.bkt.clouddn.com/13_learning_protobuf/protobuf_comparation_result.png" alt="具体的数字"></p>
<h2 id="快速开始"><a href="#快速开始" class="headerlink" title="快速开始"></a>快速开始</h2><p>以下示例源码已上传至github：<a href="https://github.com/ginobefun/learning_projects/tree/master/learning-protobuf" target="_blank" rel="external">https://github.com/ginobefun/learning_projects/tree/master/learning-protobuf</a></p>
<h3 id="新建一个maven项目并添加依赖"><a href="#新建一个maven项目并添加依赖" class="headerlink" title="新建一个maven项目并添加依赖"></a>新建一个maven项目并添加依赖</h3><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;
         xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

    &lt;groupId&gt;com.ginobefunny.learning&lt;/groupId&gt;
    &lt;artifactId&gt;leanring-protobuf&lt;/artifactId&gt;
    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;

    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.google.protobuf&lt;/groupId&gt;
            &lt;artifactId&gt;protobuf-java&lt;/artifactId&gt;
            &lt;version&gt;3.2.0&lt;/version&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;
&lt;/project&gt;
</code></pre><h3 id="新建protobuf的消息定义文件addressbook-proto"><a href="#新建protobuf的消息定义文件addressbook-proto" class="headerlink" title="新建protobuf的消息定义文件addressbook.proto"></a>新建protobuf的消息定义文件addressbook.proto</h3><pre><code>syntax = &quot;proto3&quot;; // 声明为protobuf 3定义文件
package tutorial;

option java_package = &quot;com.ginobefunny.learning.protobuf.message&quot;; // 声明生成消息类的java包路径
option java_outer_classname = &quot;AddressBookProtos&quot;;  // 声明生成消息类的类名

message Person {
  string name = 1;
  int32 id = 2;
  string email = 3;

  enum PhoneType {
    MOBILE = 0;
    HOME = 1;
    WORK = 2;
  }

  message PhoneNumber {
    string number = 1;
    PhoneType type = 2;
  }

  repeated PhoneNumber phones = 4;
}

message AddressBook {
  repeated Person people = 1;
}
</code></pre><h3 id="使用protoc工具生成消息对应的Java类"><a href="#使用protoc工具生成消息对应的Java类" class="headerlink" title="使用protoc工具生成消息对应的Java类"></a>使用protoc工具生成消息对应的Java类</h3><ul>
<li>从<a href="https://github.com/google/protobuf/releases/" target="_blank" rel="external">已发布版本</a>中下载protoc工具，比如protoc-3.2.0-win32；</li>
<li>解压后将bin目录添加到path路径；</li>
<li>执行以下protoc命令生成Java类：</li>
</ul>
<pre><code>protoc -I=. --java_out=src/main/java addressbook.proto
</code></pre><h3 id="编写测试类写入和读取序列化文件"><a href="#编写测试类写入和读取序列化文件" class="headerlink" title="编写测试类写入和读取序列化文件"></a>编写测试类写入和读取序列化文件</h3><ul>
<li>AddPerson类通过用户每次添加一个联系人，并序列化保存到指定文件中。</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">AddPerson</span> </span>&#123;</div><div class="line"></div><div class="line">    <span class="comment">// 通过用户输入构建一个Person对象</span></div><div class="line">    <span class="keyword">static</span> AddressBookProtos.<span class="function">Person <span class="title">promptForAddress</span><span class="params">(BufferedReader stdin,</span></span></div><div class="line">                                                     PrintStream stdout) <span class="keyword">throws</span> IOException &#123;</div><div class="line">        AddressBookProtos.Person.Builder person = AddressBookProtos.Person.newBuilder();</div><div class="line"></div><div class="line">        stdout.print(<span class="string">"Enter person ID: "</span>);</div><div class="line">        person.setId(Integer.valueOf(stdin.readLine()));</div><div class="line"></div><div class="line">        stdout.print(<span class="string">"Enter name: "</span>);</div><div class="line">        person.setName(stdin.readLine());</div><div class="line"></div><div class="line">        stdout.print(<span class="string">"Enter email address (blank for none): "</span>);</div><div class="line">        String email = stdin.readLine();</div><div class="line">        <span class="keyword">if</span> (email.length() &gt; <span class="number">0</span>) &#123;</div><div class="line">            person.setEmail(email);</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</div><div class="line">            stdout.print(<span class="string">"Enter a phone number (or leave blank to finish): "</span>);</div><div class="line">            String number = stdin.readLine();</div><div class="line">            <span class="keyword">if</span> (number.length() == <span class="number">0</span>) &#123;</div><div class="line">                <span class="keyword">break</span>;</div><div class="line">            &#125;</div><div class="line"></div><div class="line">            AddressBookProtos.Person.PhoneNumber.Builder phoneNumber =</div><div class="line">                    AddressBookProtos.Person.PhoneNumber.newBuilder().setNumber(number);</div><div class="line"></div><div class="line">            stdout.print(<span class="string">"Is this a mobile, home, or work phone? "</span>);</div><div class="line">            String type = stdin.readLine();</div><div class="line">            <span class="keyword">if</span> (type.equals(<span class="string">"mobile"</span>)) &#123;</div><div class="line">                phoneNumber.setType(AddressBookProtos.Person.PhoneType.MOBILE);</div><div class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (type.equals(<span class="string">"home"</span>)) &#123;</div><div class="line">                phoneNumber.setType(AddressBookProtos.Person.PhoneType.HOME);</div><div class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (type.equals(<span class="string">"work"</span>)) &#123;</div><div class="line">                phoneNumber.setType(AddressBookProtos.Person.PhoneType.WORK);</div><div class="line">            &#125; <span class="keyword">else</span> &#123;</div><div class="line">                stdout.println(<span class="string">"Unknown phone type.  Using default."</span>);</div><div class="line">            &#125;</div><div class="line"></div><div class="line">            person.addPhones(phoneNumber);</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="keyword">return</span> person.build();</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">// 加载指定的序列化文件（如不存在则创建一个新的），再通过用户输入增加一个新的联系人到地址簿，最后序列化到文件中</span></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">        <span class="keyword">if</span> (args.length != <span class="number">1</span>) &#123;</div><div class="line">            System.err.println(<span class="string">"Usage:  AddPerson ADDRESS_BOOK_FILE"</span>);</div><div class="line">            System.exit(-<span class="number">1</span>);</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        AddressBookProtos.AddressBook.Builder addressBook = AddressBookProtos.AddressBook.newBuilder();</div><div class="line"></div><div class="line">        <span class="comment">// Read the existing address book.</span></div><div class="line">        <span class="keyword">try</span> &#123;</div><div class="line">            addressBook.mergeFrom(<span class="keyword">new</span> FileInputStream(args[<span class="number">0</span>]));</div><div class="line">        &#125; <span class="keyword">catch</span> (FileNotFoundException e) &#123;</div><div class="line">            System.out.println(args[<span class="number">0</span>] + <span class="string">": File not found.  Creating a new file."</span>);</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="comment">// Add an address.</span></div><div class="line">        addressBook.addPeople(promptForAddress(<span class="keyword">new</span> BufferedReader(<span class="keyword">new</span> InputStreamReader(System.in)),</div><div class="line">                        System.out));</div><div class="line"></div><div class="line">        <span class="comment">// Write the new address book back to disk.</span></div><div class="line">        FileOutputStream output = <span class="keyword">new</span> FileOutputStream(args[<span class="number">0</span>]);</div><div class="line">        addressBook.build().writeTo(output);</div><div class="line">        output.close();</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<ul>
<li>ListPeople类读取序列化文件并输出所有联系人信息。</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ListPeople</span> </span>&#123;</div><div class="line"></div><div class="line">    <span class="comment">// 打印地址簿中所有联系人信息</span></div><div class="line">    <span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">print</span><span class="params">(AddressBookProtos.AddressBook addressBook)</span> </span>&#123;</div><div class="line">        <span class="keyword">for</span> (AddressBookProtos.Person person: addressBook.getPeopleList()) &#123;</div><div class="line">            System.out.println(<span class="string">"Person ID: "</span> + person.getId());</div><div class="line">            System.out.println(<span class="string">"  Name: "</span> + person.getName());</div><div class="line">            <span class="keyword">if</span> (!person.getPhonesList().isEmpty()) &#123;</div><div class="line">                System.out.println(<span class="string">"  E-mail address: "</span> + person.getEmail());</div><div class="line">            &#125;</div><div class="line"></div><div class="line">            <span class="keyword">for</span> (AddressBookProtos.Person.PhoneNumber phoneNumber : person.getPhonesList()) &#123;</div><div class="line">                <span class="keyword">switch</span> (phoneNumber.getType()) &#123;</div><div class="line">                    <span class="keyword">case</span> MOBILE:</div><div class="line">                        System.out.print(<span class="string">"  Mobile phone #: "</span>);</div><div class="line">                        <span class="keyword">break</span>;</div><div class="line">                    <span class="keyword">case</span> HOME:</div><div class="line">                        System.out.print(<span class="string">"  Home phone #: "</span>);</div><div class="line">                        <span class="keyword">break</span>;</div><div class="line">                    <span class="keyword">case</span> WORK:</div><div class="line">                        System.out.print(<span class="string">"  Work phone #: "</span>);</div><div class="line">                        <span class="keyword">break</span>;</div><div class="line">                &#125;</div><div class="line">                System.out.println(phoneNumber.getNumber());</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">// 加载指定的序列化文件，并输出所有联系人信息</span></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">        <span class="keyword">if</span> (args.length != <span class="number">1</span>) &#123;</div><div class="line">            System.err.println(<span class="string">"Usage:  ListPeople ADDRESS_BOOK_FILE"</span>);</div><div class="line">            System.exit(-<span class="number">1</span>);</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="comment">// Read the existing address book.</span></div><div class="line">        AddressBookProtos.AddressBook addressBook =</div><div class="line">                AddressBookProtos.AddressBook.parseFrom(<span class="keyword">new</span> FileInputStream(args[<span class="number">0</span>]));</div><div class="line"></div><div class="line">        print(addressBook);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="验证效果"><a href="#验证效果" class="headerlink" title="验证效果"></a>验证效果</h3><p>先添加一个联系人Gino</p>
<p><img src="http://oi46mo3on.bkt.clouddn.com/13_learning_protobuf/AddPerson1.png" alt="添加一个联系人Gino"></p>
<p>再添加一个联系人Slightly</p>
<p><img src="http://oi46mo3on.bkt.clouddn.com/13_learning_protobuf/AddPerson2.png" alt="添加一个联系人Gino"></p>
<p>最后显示所有联系人信息</p>
<p><img src="http://oi46mo3on.bkt.clouddn.com/13_learning_protobuf/ListPerson.png" alt="添加一个联系人Gino"></p>
<h3 id="实例小结"><a href="#实例小结" class="headerlink" title="实例小结"></a>实例小结</h3><ul>
<li>通过以上的例子我们能大概感受到开发protobuf序列化的大致步骤：定义proto文件、生成对应的Java类文件、通过消息类的构造器构造对象并通过writeTo序列化、通过parseFrom反序列化对象；</li>
<li>如果查看中间序列化的文件，我们可以发现protobuf序列化的二进制文件非常紧凑，因此文件更小，传输性能更好。</li>
</ul>
<h2 id="深入学习"><a href="#深入学习" class="headerlink" title="深入学习"></a>深入学习</h2><h3 id="关于proto文件"><a href="#关于proto文件" class="headerlink" title="关于proto文件"></a>关于proto文件</h3><h4 id="protobuf版本"><a href="#protobuf版本" class="headerlink" title="protobuf版本"></a>protobuf版本</h4><ul>
<li>protobuf现在主流的有2.X和3.X版本，两者之间相差比较大，对于刚采用的建议使用3.X版本；</li>
<li>如果采用3.X版本，需要再proto文件第一个非注释行声明（就像我们上面的例子那样），因为protobuf默认认为是2.X版本；</li>
</ul>
<h4 id="message结构"><a href="#message结构" class="headerlink" title="message结构"></a>message结构</h4><ul>
<li>在一个proto文件中可以包含多个message定义，message之间可以互相引用，message还可以嵌套message和枚举类；</li>
<li>一个message通常包含一至多个字段；</li>
<li>每个字段包含以下几个部分：字段描述符（可选）、字段类型、字段名称和字段对应的Tag；</li>
</ul>
<h4 id="字段描述符"><a href="#字段描述符" class="headerlink" title="字段描述符"></a>字段描述符</h4><p>字段描述符用于描述字段出现的频率，有以下两个可选值：</p>
<ul>
<li>singular：表示出现0次或1次；如果没有声明描述符，默认为singular；</li>
<li>repeated：表示出现0次或多次；</li>
</ul>
<h4 id="字段类型"><a href="#字段类型" class="headerlink" title="字段类型"></a>字段类型</h4><ul>
<li>基本数据类型：包括double、float、bool、string、bytes、int32、int64、uint32、uint64、sint32、sint64、fixed32、fixed64、sfixed32、sfixed64；</li>
<li>引用其他message类型：这个就有点像我们Java里面的对象引用的方式；</li>
<li>枚举类型：对于枚举类型，protobuf有个约束：枚举的第一项对应的值必须为0；下面是一个包含枚举类型的消息定义：</li>
</ul>
<pre><code>message SearchRequest {
  string query = 1;
  int32 page_number = 2;
  int32 result_per_page = 3;
  enum Corpus {
    UNIVERSAL = 0;
    WEB = 1;
    IMAGES = 2;
    LOCAL = 3;
    NEWS = 4;
    PRODUCTS = 5;
    VIDEO = 6;
  }
  Corpus corpus = 4;
}
</code></pre><h4 id="字段对应的Tag"><a href="#字段对应的Tag" class="headerlink" title="字段对应的Tag"></a>字段对应的Tag</h4><ul>
<li>对应同一个message里面的字段，每个字段的Tag是必须唯一数字；</li>
<li>Tag主要用于说明字段在二进制文件的对应关系，一旦指定字段为对应的Tag，不应该在后续进行变更；</li>
<li>对于Tag的分配，1~15只用一个byte进行编码（因此应该留给那些常用的字段），16~2047用两个byte进行编码，最大支持到536870911，但是中间有一段（19000~19999）是protobuf内部使用的；</li>
<li>可以通过reserved关键字来预留Tag和字段名，还有一种场景是如果某个字段已经被废弃了不希望后续被采用，也可以用reserved关键字声明；</li>
</ul>
<h4 id="字段的默认值"><a href="#字段的默认值" class="headerlink" title="字段的默认值"></a>字段的默认值</h4><p>protobuf 2.X版本是支持在字段中声明默认值的，但是在3.X版本中去掉了默认值的定义，主要是为了区别用户是否设置了一个和默认值一样的值的情况。对于3.X版本，protobuf采用以下规则处理默认值：</p>
<ul>
<li>对应string类型，默认值为一个空字符串；</li>
<li>对于bytes类型，默认值为一个空的byte数组；</li>
<li>对于bool类型，默认值为false；</li>
<li>对于数值类型，默认值为0；</li>
<li>对于枚举类型，默认值为第一项，也即值为0的那个枚举值；</li>
<li>对于引用其他message类型：其默认值和对应的语言是相关的；</li>
</ul>
<h3 id="Map字段类型"><a href="#Map字段类型" class="headerlink" title="Map字段类型"></a>Map字段类型</h3><ul>
<li>protobuf也支持定义Map类型的字段，但是对于Map的key的类型只能是整数型（包括各种int32和int64）和string类型；</li>
<li>Map类型不能定义为repeated；</li>
<li>Map类型的数据是无序的；</li>
<li>以下是一个Map类型的字段定义示例：</li>
</ul>
<pre><code>map&lt;string, Project&gt; projects = 3;
</code></pre><h3 id="导入其他proto文件"><a href="#导入其他proto文件" class="headerlink" title="导入其他proto文件"></a>导入其他proto文件</h3><ul>
<li>可以通过import关键字导入其他proto文件，从而重用message类型；下面是一个import的示例：</li>
</ul>
<pre><code>import &quot;myproject/other_protos.proto&quot;;
</code></pre><h3 id="如果proto中的message要扩展怎么办？"><a href="#如果proto中的message要扩展怎么办？" class="headerlink" title="如果proto中的message要扩展怎么办？"></a>如果proto中的message要扩展怎么办？</h3><p>proto具有很好的扩展性，但是也要遵循以下原则：</p>
<ul>
<li>不能修改原有字段的Tag；</li>
<li>如果新增一个字段，对于老的二进制序列化文件处理时会给这个字段增加默认值；如果是升级了proto文件而没有升级对应的代码，则新的字段会被忽略；</li>
<li>可以删除字段，但是对应的Tag不应该再被使用，否则对于之前的二进制序列化消息处理时对应关系出现问题；</li>
<li>int32、uint32、int64、uint64和bool类型是相互兼容的，这意味着你可以在他们之间修改类型而不会有兼容性问题；</li>
</ul>
<h3 id="Any消息类型"><a href="#Any消息类型" class="headerlink" title="Any消息类型"></a>Any消息类型</h3><ul>
<li>protobuf内置了一些通用的消息类型，Any就是其他的一种，通过查看它的proto文件可以看到它包含了一个URL标识符和一个byte数组；</li>
<li>在使用Any消息类型之前，需要通过<strong>import “google/protobuf/any.proto”;</strong>导入proto文件定义；</li>
</ul>
<h3 id="Oneof关键字"><a href="#Oneof关键字" class="headerlink" title="Oneof关键字"></a>Oneof关键字</h3><ul>
<li>oneof关键字用于声明一组字段中，必须要有一个字段被赋值；通常比如我们在登陆的时候，可以用手机号、邮箱和用户名登陆，这种时候就可以使用oneof来定义；</li>
<li>当我们对oneof其中一个字段赋值时，其他字段的值将会被清空；所以只有最后一次赋值是有效的；</li>
<li>下面是一个oneof的示例：</li>
</ul>
<pre><code>message LoginMessage {
  oneof user_identifier {
    string user_name = 4;
    string phone_num = 5;
    string user_email = 6;
  }

  string password = 10;
}
</code></pre><h3 id="定义服务"><a href="#定义服务" class="headerlink" title="定义服务"></a>定义服务</h3><ul>
<li>在proto文件中还允许定义RPC服务，以下是一个示例：</li>
</ul>
<pre><code>service SearchService {
  rpc Search (SearchRequest) returns (SearchResponse);
}
</code></pre><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><ul>
<li>随着微服务架构的流行，RPC框架渐渐地成为服务框架的一个重要部分。在很多RPC的设计中，都采用了高性能的编解码技术，protobuf就属于其中的佼佼者；</li>
<li>protobuf相对于其他编解码框架，有着非常惊人的性能表现；</li>
<li>通过一个简单的实例，我们了解如果使用protobuf进行序列化和数据交互；</li>
<li>最后，我们列举了一些重要的特性和配置说明，这些在我们使用protobuf中都会给频繁使用；</li>
<li>后续学习：后面我会根据所学的Netty和protobuf知识，开发一个简单的RPC框架。</li>
</ul>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a href="https://developers.google.com/protocol-buffers/docs/proto3" target="_blank" rel="external">Language Guide (proto3)</a></li>
<li><a href="https://developers.google.com/protocol-buffers/docs/javatutorial" target="_blank" rel="external">Protocol Buffer Basics: Java</a></li>
<li><a href="https://developers.google.com/protocol-buffers/docs/reference/java-generated" target="_blank" rel="external">Java Generated Code</a></li>
<li><a href="https://solicomo.com/network-dev/protobuf-proto3-vs-proto2.html" target="_blank" rel="external">Protobuf 的 proto3 与 proto2 的区别</a></li>
<li><a href="http://agapple.iteye.com/blog/859052" target="_blank" rel="external">几种序列化协议(protobuf,xstream,jackjson,jdk,hessian)相关数据对比</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;随着微服务架构的流行，RPC框架渐渐地成为服务框架的一个重要部分。在很多RPC的设计中，都采用了高性能的编解码技术，Protocol Buffers就属于其中的佼佼者。&lt;a href=&quot;https://github.com/google/protobuf&quot;&gt;Protocol Buffers&lt;/a&gt;是Google开源的一个语言无关、平台无关的通信协议，其小巧、高效和友好的兼容性设计，使其被广泛使用。&lt;br&gt;
    
    </summary>
    
      <category term="OpenSource" scheme="http://ginobefunny.com/categories/OpenSource/"/>
    
    
      <category term="Google" scheme="http://ginobefunny.com/tags/Google/"/>
    
      <category term="入门" scheme="http://ginobefunny.com/tags/%E5%85%A5%E9%97%A8/"/>
    
      <category term="教程" scheme="http://ginobefunny.com/tags/%E6%95%99%E7%A8%8B/"/>
    
      <category term="实例" scheme="http://ginobefunny.com/tags/%E5%AE%9E%E4%BE%8B/"/>
    
      <category term="Protocol Buffers" scheme="http://ginobefunny.com/tags/Protocol-Buffers/"/>
    
      <category term="protobuf" scheme="http://ginobefunny.com/tags/protobuf/"/>
    
  </entry>
  
  <entry>
    <title>代码之外的生存指南</title>
    <link href="http://ginobefunny.com/post/soft_skills/"/>
    <id>http://ginobefunny.com/post/soft_skills/</id>
    <published>2017-02-04T11:54:18.000Z</published>
    <updated>2017-02-04T06:02:02.051Z</updated>
    
    <content type="html"><![CDATA[<p>阅读<a href="https://book.douban.com/subject/26835090/" target="_blank" rel="external">《软技能》</a>一书的笔记和随想。<br><a id="more"></a></p>
<h1 id="笔记整理"><a href="#笔记整理" class="headerlink" title="笔记整理"></a>笔记整理</h1><h2 id="职业篇"><a href="#职业篇" class="headerlink" title="职业篇"></a>职业篇</h2><ul>
<li>你所能犯的最大错误就是相信自己是在为别人工作。这样一来你对工作的安全感依然尽失。职业发展的驱动力一定是来自个体本身。记住：工作是属于公司的，而职业生涯却是属于你自己的。</li>
<li>大多数软件开发人员从职业生涯一开始就犯了几个严重的错误，最大的错误就是没有把自己的软件开发事业当作一桩生意来看待。你应该把自己当作一个企业去思考，把雇主当作是你的软件开发企业的一个客户。</li>
<li>通常软件开发人员售卖的就是他们把一个想法变成一个数字化的现实产品的能力。因此你需要做到：专注于你正在提供怎么样的服务以及如何营销；想法设法提升你的服务；集中精力称为一位专家；</li>
<li>如何设定目标：起步阶段最简单的就是在心中树立一个大目标，然后再建立能帮你达成这个大目标的小目标。较小的目标可以让你航行在自己的轨道上，激励你保持航向朝着更大的目标前进。</li>
<li>学会与人打交道：每个人都希望感到自己重要；永远不要批评；换位思考；避免争吵；</li>
<li>关于面试：让面试官对你怀有好感（比如阅读过你的博客，比如有员工推荐你）；集中精力证明自己无需督促也能自动自发做好事情以及在技术上你确实胜任工作；坚持阅读技术书籍和博客文章，提升自己的技能；扩展自己的社交网络；</li>
<li>专业化很重要：虽然会把你关在一些机会的大门之外，但与此同时它将打开的机会大门要比你用其他方式打开的多得多。专业化的规则是程序越深，潜在的机会越少，但获得这些机会的可能性越大。</li>
<li>不同规模的公司选择：小公司（承担更多职责但稳定性差）、中等规模公司（工作稳定但变化很慢）、大公司（完备的流程和规范但负责一小部分且可能充斥着官僚作风）；</li>
<li>关于晋升：承担责任（负责不受重视的项目、帮助新人成长、负责文档更新等）；引入注目（记录活动日志、提供演讲、发表意见）；自学（提升技能并分享）；成为问题的解决者；</li>
<li>成为专业人士是一种心态。如果我们总是与恐惧。自毁。拖延和自我怀疑作斗争，那么问题就是我们正在像外行那么思考问题。外行毫不起眼，外行人废话连篇，外行屈从于逆境。专业人士可不这么想。不管怎样，他引人注目，他恪尽职守，他始终如一。</li>
<li>对技术虔诚的一大问题是，我们中的大多数崇拜某项特定的技术，只是因为自己熟悉这种技术，我们很自然地会相信自己选择的是最好的，然而这会让我们经常忽略任何反对意见。</li>
</ul>
<h2 id="自我营销篇"><a href="#自我营销篇" class="headerlink" title="自我营销篇"></a>自我营销篇</h2><ul>
<li>自我营销的关键在于：如果想让别人喜欢你，想和你一起工作，你必须要为他们提供价值。自我营销无非就是学习如何控制好自己要传达的信息，塑造好自己的形象，扩展信息送达的人群；</li>
<li>尽管有多种媒介可供你使用，但对于软件开发人员，最突出也是我个人推荐的还是博客。我认为博客就是你在互联网上的大本营，这是一个你完全能够控制信息的地方。</li>
<li>自我营销的基本机制是，要想让人们追随你、倾听你，你就要带给他们价值：你能为他们的问题提供答案，甚至是给他们带去欢乐。</li>
<li>打造成功博客的最大秘诀有且仅有一个 – 持之以恒。定好计划，然后坚持不懈，另外还需要重视博客内容品质。</li>
</ul>
<h2 id="学习篇"><a href="#学习篇" class="headerlink" title="学习篇"></a>学习篇</h2><ul>
<li>通过动手实践和教会他人，我们能学得更好。与其他的学习方式相比，主动学习是效率更高的方式。</li>
<li>十步学习法：要对自己要学的内容有个基本的了解然后利用这些信息勾勒出学习的范围，依靠这些知识找出各种资源来帮助自己学习，最后创建自己的学习计划、列出要学习的相关课程、筛选学习材料，再通过“学习-实践-掌握-教授”的过程假设理解。下面是十步学习法的示意图，第1步到第6步只做一次，集中精力完成足够多的前期调研，确保自己明确知道要学哪些内容，以及如何确认自己已达成目标，另外还需要挑选最好的资源、制定学习计划；第7步到第10步通过LDLT的方式真正领会知识。</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/12_soft_skills/learning_method.png" alt="十步学习法"></p>
<ul>
<li>寻找导师与做一名导师；</li>
<li>发现自己的知识短板：消除短板的关键就是定位短板，然后通过十步学习法用心掌握它。</li>
</ul>
<h2 id="生产力篇"><a href="#生产力篇" class="headerlink" title="生产力篇"></a>生产力篇</h2><ul>
<li>如何专注：挑选短时间片专注于单一任务（番茄工作法）、克服集中于单一任务的痛感（学会享受任务和奖励）、屏蔽打扰；</li>
<li>生产力提升计划：季度计划 -&gt; 月计划 -&gt; 周计划 -&gt; 日计划及执行，使用看板实时关注进展；</li>
<li>番茄工作法：每25分钟一个番茄，专注于当前优先级最高的任务，拥抱变化并诚实地记录中断；</li>
<li>定额工作法：给自己在确定的期限确立一个明确的目标；挑选一些需要重复去做的事情，设定一个定额，如每周写一篇博客；</li>
<li>批量处理生产效率更高，比如处理电子邮件、开短会，避免多任务同时处理；</li>
<li>职业倦怠：穿多那堵墙（很多倦怠是自然而然产生，但是如果咬牙坚持或许就是不一样的风景）；</li>
<li>追踪你的时间：了解自己每天时间的使用情况（比如RescueTime工具），避免浪费；</li>
<li>习惯主要由三个要素构成：暗示、惯例和奖励。找出坏习惯，改掉！养成好习惯。</li>
<li>任何行动往往都比没有行动好，特别是当你一直停滞在不愉快的情势下很长时间的时候。如果这是一个错误，至少你学到了一些东西。这样一来，它就不再是一个错误。如果你仍然选择停滞不前，那么你就学不到任何东西。</li>
</ul>
<h2 id="理财篇"><a href="#理财篇" class="headerlink" title="理财篇"></a>理财篇</h2><ul>
<li>金钱只是一种工具，它会带你去往任何你想去的地方，但不会取代你成为司机；</li>
<li>是成为百万富翁还是一生都靠薪水过活，选择权在你自己，而且在很大程度上取决于你在财务管理方面的知识，以及世界金额系统运行方面的知识。</li>
<li>怎样支配你的薪水：拒绝短期思维（更长远地看待薪水的分配而不仅仅是当前）；资产与负债（通过成本和价值来考虑的理财思维）；</li>
<li>怎样进行薪酬谈判：薪酬水平受声望的影响（自我营销）；先出价者输（先出价的人处于明显的劣势）；被要求先出价怎么办（先要求了解预算范围、不透露当前薪酬、了解自己值什么价钱）；</li>
<li>期权：赋予你再未来某个日期之前以固定价格购买一定数量股票的选择权。</li>
<li>规划退休计划的关键就是利用逆向思维，计算退休目标。</li>
</ul>
<h2 id="健身篇"><a href="#健身篇" class="headerlink" title="健身篇"></a>健身篇</h2><ul>
<li>人的身体就是人的灵魂的最好写照。</li>
<li>健身不仅是保持健康体魄的关键要素之一，也是灵活的、具有创造性的脑力活动的基础。健身可以增强自信心、提高创造力、减少对疾病的恐惧。</li>
<li>设置你的健身标准：挑选一个具体的目标（比如增长肌肉）、创建里程碑、对进展进行可视化。最后保持健康的生活方式。</li>
</ul>
<h2 id="精神篇"><a href="#精神篇" class="headerlink" title="精神篇"></a>精神篇</h2><ul>
<li>信念决定思想，思想决定言语，言语决定行动，行动决定习惯，习惯决定价值，价值决定命运。（by 甘地）</li>
<li>拥有正确的心态：重新启动。积极思考问题的根源是这样一种信念 – 你比你所处的环境更伟大。这种信念让你总能先看到事物好的一面，因为无论身处何处，你都有能力改变自己的未来。这是人类成就的最高信念，是世界上最强大的力量。</li>
</ul>
<h1 id="随想"><a href="#随想" class="headerlink" title="随想"></a>随想</h1><ul>
<li>转变心态，从被卖身契束缚的工人转变为一个自主管理的商人；</li>
<li>设定一个大目标，比如称为一个卓越的高效的工程师；但是这样不够清晰，那么我希望在十年后能成为一个软件开发的自由工作者，让自己依然能高效而简洁地解决编程问题，但是不受企业低效的管理束缚且有能满足高质量生活的收入水平；</li>
<li>对于2017年，希望自己能巩固好编程基础、Java语言核心特性，同时学习微服务的关键设计和实现，搭建自己熟悉的快速开发框架。每个月定时的跟踪和调整这个小目标，激励自己前行；</li>
<li>关于专业化，我想目前给自己比较好的定位还是一个企业级基础平台或中间件的工程师，因为比较喜欢深研技术而不大喜欢具体的业务实现。那么我就应该用很多时间投入到这一块的学习中，掌握常见的中间件技术，并深入理解其中的设计、原理和实现；</li>
<li>对于公司的选择，我最关注的几点：1.团队或导师是否优秀（是否能得到成长）；2.开放的技术氛围（是否高效）；3.开放的时间安排（是否能自主选择和安排）；4.项目有挑战（有挑战才有成长）；</li>
<li>关于博客，我想第一个目的还是写给自己看的（总结和记录，并经常回看和更新）；当然如果自己用心写，肯定能吸引到志同道合者一起讨论学习，从而扩展自己的社交圈和影响力。关于博客的建立，尝试过很多平台之后，我选择了Github Pages + Hexo的方式，这种方式即最大可能地减少对服务器的依赖和搭建过程的繁琐，又不缺少灵活性，使得我们能将更多精力放在写作上而不是博客的维护上；关于博客写作的频率，我希望自己一年下来最少能有60篇博文，每个月至少6篇，当然因为我经常写一些读书笔记，因此这个数量不会成为很大的挑战。另外，对于原创类的博文也要更加注重质量、结构和行文。</li>
<li>关于学习，我现在比较喜欢的就是完整的阅读书籍，相比于作者提供的十步学习法，存在几个地方存在不足：一个是关于目标的设定，在学习的过程中没有给自己明确的学习目标，这样在学习之后无法验证，所以在后续的月度计划时我会增加这一部分的内容；二是缺少全局的掌握，比如学习Java网络，我应该先了解下关于Java网络的知识以及我自己所欠缺的，然后有针对性地去学习；三是关于LDLT，现在主要的就是通过笔记来记录和分享学习的过程，后面会要求自己每次阅读结束都需要写书评或读书笔记。</li>
<li>对于生产力的提升，我现在是通过“年度计划 -&gt; 月计划 -&gt; 周计划 -&gt; 日计划”来规划的，对于年度计划和月计划，通过<a href="http://ginobefunny.com/my2017/">“小目标”</a>定期维护，而对于周计划和日计划，通过任务清单和番茄钟来管理和展示，这样可以确保自己沿着自己的目标方向前进。</li>
<li>关于习惯，现在我上班第一件事就是打开任务清单和番茄计时器，找到今天优先级最高的任务录入番茄计时器，这相当于给自己一个暗示，我今天有这么多的任务需要完成。慢慢地这就成为了一个惯例，我每次休息后第一件事就是看看番茄计时器的完全情况，如果今天完成的不错，会给自己一些奖励，比如下楼喝个奶茶、允许查看网页或者看半个小时手机。</li>
<li>关于理财，我现在比较关注的就是ETF投资，一个是因为本身积累的财富有限，而ETF的定投对于投资的额度限制比较小，另外自己对这一块也比较感兴趣。</li>
<li>关于健身，目前我对自己熬夜方面控制得还算满意。在阅读本书的过程中，我忽然有个想法，我应该增强一些力量、增长一些肌肉，好吧，加一个番茄钟，这个周末研究研究。</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;阅读&lt;a href=&quot;https://book.douban.com/subject/26835090/&quot;&gt;《软技能》&lt;/a&gt;一书的笔记和随想。&lt;br&gt;
    
    </summary>
    
      <category term="CodingLife" scheme="http://ginobefunny.com/categories/CodingLife/"/>
    
    
      <category term="自我完善" scheme="http://ginobefunny.com/tags/%E8%87%AA%E6%88%91%E5%AE%8C%E5%96%84/"/>
    
      <category term="读书笔记" scheme="http://ginobefunny.com/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
      <category term="职场" scheme="http://ginobefunny.com/tags/%E8%81%8C%E5%9C%BA/"/>
    
  </entry>
  
  <entry>
    <title>Elasticsearch Reference阅读笔记</title>
    <link href="http://ginobefunny.com/post/elasticsearch_reference_notes/"/>
    <id>http://ginobefunny.com/post/elasticsearch_reference_notes/</id>
    <published>2017-02-03T03:28:01.000Z</published>
    <updated>2017-02-04T09:38:26.258Z</updated>
    
    <content type="html"><![CDATA[<p>花了几天把<a href="https://www.elastic.co/guide/en/elasticsearch/reference/2.3/index.html" target="_blank" rel="external">Elasticsearch的官方文档</a>读了一遍，随手记一些关键的笔记。</p>
<a id="more"></a>
<h1 id="1-Getting-Started"><a href="#1-Getting-Started" class="headerlink" title="1. Getting Started"></a>1. Getting Started</h1><h2 id="1-1-Elasticsearch"><a href="#1-1-Elasticsearch" class="headerlink" title="1.1 Elasticsearch"></a>1.1 Elasticsearch</h2><p>Elasticsearch is a highly scalable open-source full-text search and analytics engine. It allows you to store, search, and analyze big volumes of data quickly and in near real time. It is generally used as the underlying engine/technology that powers applications that have complex search features and requirements.</p>
<h2 id="1-2-Sharding-is-important-for-two-primary-reasons"><a href="#1-2-Sharding-is-important-for-two-primary-reasons" class="headerlink" title="1.2 Sharding is important for two primary reasons:"></a>1.2 Sharding is important for two primary reasons:</h2><p>It allows you to horizontally split/scale your content volume<br>It allows you to distribute and parallelize operations across shards (potentially on multiple nodes) thus increasing performance/throughput </p>
<h2 id="1-3-Replication-is-important-for-two-primary-reasons"><a href="#1-3-Replication-is-important-for-two-primary-reasons" class="headerlink" title="1.3 Replication is important for two primary reasons:"></a>1.3 Replication is important for two primary reasons:</h2><p>It provides high availability in case a shard/node fails. For this reason, it is important to note that a replica shard is never allocated on the same node as the original/primary shard that it was copied from.<br>It allows you to scale out your search volume/throughput since searches can be executed on all replicas in parallel.</p>
<h2 id="1-4-Cluster-Health"><a href="#1-4-Cluster-Health" class="headerlink" title="1.4 Cluster Health"></a>1.4 Cluster Health</h2><pre><code>curl &apos;localhost:9200/_cat/health?v&apos;

curl &apos;localhost:9200/_cat/nodes?v&apos;
</code></pre><h2 id="1-5-List-All-Indices"><a href="#1-5-List-All-Indices" class="headerlink" title="1.5 List All Indices"></a>1.5 List All Indices</h2><pre><code>curl &apos;localhost:9200/_cat/indices?v&apos;
</code></pre><h2 id="1-6-Create-an-Index"><a href="#1-6-Create-an-Index" class="headerlink" title="1.6 Create an Index"></a>1.6 Create an Index</h2><pre><code>curl -XPUT &apos;localhost:9200/customer?pretty&apos;
</code></pre><h2 id="1-7-Index-and-Query-a-Document"><a href="#1-7-Index-and-Query-a-Document" class="headerlink" title="1.7 Index and Query a Document"></a>1.7 Index and Query a Document</h2><pre><code>curl -XPUT &apos;localhost:9200/customer/external/1?pretty&apos; -d &apos;
{
  &quot;name&quot;: &quot;John Doe&quot;
}&apos;

curl -XGET &apos;localhost:9200/customer/external/1?pretty&apos;
</code></pre><h2 id="1-8-Delete-an-Index"><a href="#1-8-Delete-an-Index" class="headerlink" title="1.8 Delete an Index"></a>1.8 Delete an Index</h2><pre><code>curl -XDELETE &apos;localhost:9200/customer?pretty&apos;
</code></pre><h2 id="1-9-Updating-Documents"><a href="#1-9-Updating-Documents" class="headerlink" title="1.9 Updating Documents"></a>1.9 Updating Documents</h2><p>Note though that Elasticsearch does not actually do in-place updates under the hood. Whenever we do an update, Elasticsearch deletes the old document and then indexes a new document with the update applied to it in one shot.</p>
<h2 id="1-10-Deleting-Documents"><a href="#1-10-Deleting-Documents" class="headerlink" title="1.10 Deleting Documents"></a>1.10 Deleting Documents</h2><pre><code>curl -XDELETE &apos;localhost:9200/customer/external/2?pretty&apos;
</code></pre><h2 id="1-11-Batch-Processing"><a href="#1-11-Batch-Processing" class="headerlink" title="1.11 Batch Processing"></a>1.11 Batch Processing</h2><p>In addition to being able to index, update, and delete individual documents, Elasticsearch also provides the ability to perform any of the above operations in batches using the _bulk API. This functionality is important in that it provides a very efficient mechanism to do multiple operations as fast as possible with as little network roundtrips as possible.</p>
<p>The bulk API executes all the actions sequentially and in order. If a single action fails for whatever reason, it will continue to process the remainder of the actions after it. When the bulk API returns, it will provide a status for each action (in the same order it was sent in) so that you can check if a specific action failed or not.</p>
<h2 id="1-12-The-Search-API"><a href="#1-12-The-Search-API" class="headerlink" title="1.12 The Search API"></a>1.12 The Search API</h2><p>It is important to understand that once you get your search results back, Elasticsearch is completely done with the request and does not maintain any kind of server-side resources or open cursors into your results. This is in stark contrast to many other platforms such as SQL wherein you may initially get a partial subset of your query results up-front and then you have to continuously go back to the server if you want to fetch (or page through) the rest of the results using some kind of stateful server-side cursor.</p>
<h2 id="1-13-Executing-Filters"><a href="#1-13-Executing-Filters" class="headerlink" title="1.13 Executing Filters"></a>1.13 Executing Filters</h2><p>In the previous section, we skipped over a little detail called the document score (_score field in the search results). The score is a numeric value that is a relative measure of how well the document matches the search query that we specified. The higher the score, the more relevant the document is, the lower the score, the less relevant the document is.</p>
<p>But queries do not always need to produce scores, in particular when they are only used for “filtering” the document set. Elasticsearch detects these situations and automatically optimizes query execution in order not to compute useless scores.</p>
<h2 id="1-14-Executing-Aggregations"><a href="#1-14-Executing-Aggregations" class="headerlink" title="1.14 Executing Aggregations"></a>1.14 Executing Aggregations</h2><p>Aggregations provide the ability to group and extract statistics from your data. The easiest way to think about aggregations is by roughly equating it to the SQL GROUP BY and the SQL aggregate functions. In Elasticsearch, you have the ability to execute searches returning hits and at the same time return aggregated results separate from the hits all in one response. This is very powerful and efficient in the sense that you can run queries and multiple aggregations and get the results back of both (or either) operations in one shot avoiding network roundtrips using a concise and simplified API.</p>
<h1 id="2-Setup"><a href="#2-Setup" class="headerlink" title="2. Setup"></a>2. Setup</h1><h2 id="2-1-Environment-Variables"><a href="#2-1-Environment-Variables" class="headerlink" title="2.1 Environment Variables"></a>2.1 Environment Variables</h2><p>Most times it is better to leave the default JAVA_OPTS as they are, and use the ES_JAVA_OPTS environment variable in order to set / change JVM settings or arguments.</p>
<p>The ES_HEAP_SIZE environment variable allows to set the heap memory that will be allocated to elasticsearch java process. It will allocate the same value to both min and max values, though those can be set explicitly (not recommended) by setting ES_MIN_MEM (defaults to 256m), and ES_MAX_MEM (defaults to 1g).</p>
<p>It is recommended to set the min and max memory to the same value, and enable mlockall.</p>
<h2 id="2-2-File-Descriptors"><a href="#2-2-File-Descriptors" class="headerlink" title="2.2 File Descriptors"></a>2.2 File Descriptors</h2><p>Make sure to increase the number of open files descriptors on the machine (or for the user running elasticsearch). Setting it to 32k or even 64k is recommended.</p>
<p>In order to test how many open files the process can open, start it with -Des.max-open-files set to true. This will print the number of open files the process can open on startup.</p>
<p>Alternatively, you can retrieve the max_file_descriptors for each node using the Nodes Info API, with:</p>
<pre><code>curl localhost:9200/_nodes/stats/process?pretty
</code></pre><h2 id="2-3-Virtual-memory"><a href="#2-3-Virtual-memory" class="headerlink" title="2.3 Virtual memory"></a>2.3 Virtual memory</h2><p>Elasticsearch uses a hybrid mmapfs / niofs directory by default to store its indices. The default operating system limits on mmap counts is likely to be too low, which may result in out of memory exceptions. On Linux, you can increase the limits by running the following command as root:</p>
<pre><code>sysctl -w vm.max_map_count=262144
</code></pre><p>To set this value permanently, update the vm.max_map_count setting in /etc/sysctl.conf.</p>
<h2 id="2-4-Memory-Settings"><a href="#2-4-Memory-Settings" class="headerlink" title="2.4 Memory Settings"></a>2.4 Memory Settings</h2><p>Most operating systems try to use as much memory as possible for file system caches and eagerly swap out unused application memory, possibly resulting in the elasticsearch process being swapped. Swapping is very bad for performance and for node stability, so it should be avoided at all costs.</p>
<p>There are three options: Disable swap、Configure swappiness、mlockall</p>
<h2 id="2-5-Elasticsearch-Settings"><a href="#2-5-Elasticsearch-Settings" class="headerlink" title="2.5 Elasticsearch Settings"></a>2.5 Elasticsearch Settings</h2><p>elasticsearch configuration files can be found under ES_HOME/config folder. The folder comes with two files, the elasticsearch.yml for configuring Elasticsearch different modules, and logging.yml for configuring the Elasticsearch logging.</p>
<p>The configuration format is YAML.</p>
<h2 id="2-6-Directory-Layout"><a href="#2-6-Directory-Layout" class="headerlink" title="2.6 Directory Layout"></a>2.6 Directory Layout</h2><p>zip and tar.gz<br>|Type |    Description| Location |<br>|:— |:———–|:———|<br>home  |Home of elasticsearch installation|{extract.path}<br>bin   |Binary scripts including elasticsearch to start a node|{extract.path}/bin<br>conf  |Configuration files elasticsearch.yml and logging.yml|{extract.path}/config<br>data  |The location of the data files of each index / shard allocated on the node|{extract.path}/data<br>logs  |Log files location|{extract.path}/logs<br>plugins|Plugin files location. Each plugin will be contained in a subdirectory|{extract.path}/plugins<br>repo  |Shared file system repository locations.|Not configured<br>script|Location of script files.|{extract.path}/config/scripts</p>
<h1 id="3-Breaking-changes-skipped"><a href="#3-Breaking-changes-skipped" class="headerlink" title="3 Breaking changes (skipped)"></a>3 Breaking changes (skipped)</h1><h1 id="4-API-Conventions-skipped"><a href="#4-API-Conventions-skipped" class="headerlink" title="4 API Conventions (skipped)"></a>4 API Conventions (skipped)</h1><h1 id="5-Document-APIs"><a href="#5-Document-APIs" class="headerlink" title="5 Document APIs"></a>5 Document APIs</h1><h2 id="5-1-Index-API"><a href="#5-1-Index-API" class="headerlink" title="5.1 Index API"></a>5.1 Index API</h2><p>The index API adds or updates a typed JSON document in a specific index, making it searchable. The following example inserts the JSON document into the “twitter” index, under a type called “tweet” with an id of 1:</p>
<pre><code>curl -XPUT &apos;http://localhost:9200/twitter/tweet/1&apos; -d &apos;{
    &quot;user&quot; : &quot;kimchy&quot;,
    &quot;post_date&quot; : &quot;2009-11-15T14:12:12&quot;,
    &quot;message&quot; : &quot;trying out Elasticsearch&quot;
}&apos;
</code></pre><h2 id="5-2-Automatic-Index-Creation"><a href="#5-2-Automatic-Index-Creation" class="headerlink" title="5.2 Automatic Index Creation"></a>5.2 Automatic Index Creation</h2><p>The index operation automatically creates an index if it has not been created before (check out the create index API for manually creating an index), and also automatically creates a dynamic type mapping for the specific type if one has not yet been created (check out the put mapping API for manually creating a type mapping).</p>
<h2 id="5-3-Versioning"><a href="#5-3-Versioning" class="headerlink" title="5.3 Versioning"></a>5.3 Versioning</h2><p>Each indexed document is given a version number. The associated version number is returned as part of the response to the index API request. The index API optionally allows for optimistic concurrency control when the version parameter is specified. This will control the version of the document the operation is intended to be executed against. A good example of a use case for versioning is performing a transactional read-then-update. Specifying a version from the document initially read ensures no changes have happened in the meantime (when reading in order to update, it is recommended to set preference to _primary). For example:</p>
<pre><code>curl -XPUT &apos;localhost:9200/twitter/tweet/1?version=2&apos; -d &apos;{
    &quot;message&quot; : &quot;elasticsearch now has versioning support, double cool!&quot;
}&apos;
</code></pre><h2 id="5-4-Operation-Type"><a href="#5-4-Operation-Type" class="headerlink" title="5.4 Operation Type"></a>5.4 Operation Type</h2><p>The index operation also accepts an op_type that can be used to force a create operation, allowing for “put-if-absent” behavior. When create is used, the index operation will fail if a document by that id already exists in the index.</p>
<p>Here is an example of using the op_type parameter:</p>
<pre><code>curl -XPUT &apos;http://localhost:9200/twitter/tweet/1?op_type=create&apos; -d &apos;{
    &quot;user&quot; : &quot;kimchy&quot;,
    &quot;post_date&quot; : &quot;2009-11-15T14:12:12&quot;,
    &quot;message&quot; : &quot;trying out Elasticsearch&quot;
}&apos;
</code></pre><h2 id="5-5-Routing"><a href="#5-5-Routing" class="headerlink" title="5.5 Routing"></a>5.5 Routing</h2><p>By default, shard placement — or routing — is controlled by using a hash of the document’s id value. For more explicit control, the value fed into the hash function used by the router can be directly specified on a per-operation basis using the routing parameter. For example:</p>
<pre><code>curl -XPOST &apos;http://localhost:9200/twitter/tweet?routing=kimchy&apos; -d &apos;{
    &quot;user&quot; : &quot;kimchy&quot;,
    &quot;post_date&quot; : &quot;2009-11-15T14:12:12&quot;,
    &quot;message&quot; : &quot;trying out Elasticsearch&quot;
}&apos;
</code></pre><h2 id="5-6-Parents-amp-Children-（-适合什么场景-）"><a href="#5-6-Parents-amp-Children-（-适合什么场景-）" class="headerlink" title="5.6 Parents &amp; Children （**适合什么场景?）"></a>5.6 Parents &amp; Children （<strong>**</strong>适合什么场景?）</h2><p>A child document can be indexed by specifying its parent when indexing. For example:</p>
<pre><code>curl -XPUT localhost:9200/blogs/blog_tag/1122?parent=1111 -d &apos;{
    &quot;tag&quot; : &quot;something&quot;
}&apos;
</code></pre><p>When indexing a child document, the routing value is automatically set to be the same as its parent, unless the routing value is explicitly specified using the routing parameter.</p>
<h2 id="5-7-Distributed"><a href="#5-7-Distributed" class="headerlink" title="5.7 Distributed"></a>5.7 Distributed</h2><p>The index operation is directed to the primary shard based on its route (see the Routing section above) and performed on the actual node containing this shard. After the primary shard completes the operation, if needed, the update is distributed to applicable replicas.</p>
<h2 id="5-8-Write-Consistency"><a href="#5-8-Write-Consistency" class="headerlink" title="5.8 Write Consistency"></a>5.8 Write Consistency</h2><p>To prevent writes from taking place on the “wrong” side of a network partition, by default, index operations only succeed if a quorum (&gt;replicas/2+1) of active shards are available. </p>
<h2 id="5-9-Write-Consistency-如果index设置为1个主分片，两个复制分片。当两个复制分片都不可用的时候index在主分片是否成功？复制分片可用了是否能复制？"><a href="#5-9-Write-Consistency-如果index设置为1个主分片，两个复制分片。当两个复制分片都不可用的时候index在主分片是否成功？复制分片可用了是否能复制？" class="headerlink" title="5.9 Write Consistency (**如果index设置为1个主分片，两个复制分片。当两个复制分片都不可用的时候index在主分片是否成功？复制分片可用了是否能复制？)"></a>5.9 Write Consistency (<strong>**</strong>如果index设置为1个主分片，两个复制分片。当两个复制分片都不可用的时候index在主分片是否成功？复制分片可用了是否能复制？)</h2><p>To prevent writes from taking place on the “wrong” side of a network partition, by default, index operations only succeed if a quorum (&gt;replicas/2+1) of active shards are available. </p>
<p>The index operation only returns after all active shards within the replication group have indexed the document (sync replication).</p>
<h2 id="5-10-Refresh"><a href="#5-10-Refresh" class="headerlink" title="5.10 Refresh"></a>5.10 Refresh</h2><p>To refresh the shard (not the whole index) immediately after the operation occurs, so that the document appears in search results immediately, the refresh parameter can be set to true. Setting this option to true should ONLY be done after careful thought and verification that it does not lead to poor performance, both from an indexing and a search standpoint. Note, getting a document using the get API is completely realtime and doesn’t require a refresh.</p>
<h2 id="5-11-Get-API"><a href="#5-11-Get-API" class="headerlink" title="5.11 Get API"></a>5.11 Get API</h2><p>The get API allows to get a typed JSON document from the index based on its id. The following example gets a JSON document from an index called twitter, under a type called tweet, with id valued 1:</p>
<pre><code>curl -XGET &apos;http://localhost:9200/twitter/tweet/1&apos;
</code></pre><h2 id="5-12-Preference"><a href="#5-12-Preference" class="headerlink" title="5.12 Preference"></a>5.12 Preference</h2><p>Controls a preference of which shard replicas to execute the get request on. By default, the operation is randomized between the shard replicas.</p>
<p>The preference can be set to:</p>
<ul>
<li>_primary: The operation will go and be executed only on the primary shards.</li>
<li>_local: The operation will prefer to be executed on a local allocated shard if possible.</li>
<li>Custom (string) value: A custom value will be used to guarantee that the same shards will be used for the same custom value. This can help with “jumping values” when hitting different shards in different refresh states. A sample value can be something like the web session id, or the user name.</li>
</ul>
<h2 id="5-13-Delete-API"><a href="#5-13-Delete-API" class="headerlink" title="5.13 Delete API"></a>5.13 Delete API</h2><p>The delete API allows to delete a typed JSON document from a specific index based on its id. The following example deletes the JSON document from an index called twitter, under a type called tweet, with id valued 1:</p>
<pre><code>curl -XDELETE &apos;http://localhost:9200/twitter/tweet/1&apos;
</code></pre><p>The delete operation gets hashed into a specific shard id. It then gets redirected into the primary shard within that id group, and replicated (if needed) to shard replicas within that id group.</p>
<h2 id="5-14-Update-API"><a href="#5-14-Update-API" class="headerlink" title="5.14 Update API"></a>5.14 Update API</h2><p>The update API allows to update a document based on a script provided. The operation gets the document (collocated with the shard) from the index, runs the script (with optional script language and parameters), and index back the result (also allows to delete, or ignore the operation). It uses versioning to make sure no updates have happened during the “get” and “reindex”.</p>
<p>Note, this operation still means full reindex of the document, it just removes some network roundtrips and reduces chances of version conflicts between the get and the index. The _source field needs to be enabled for this feature to work.</p>
<h2 id="5-15-Update-By-Query-API-new-and-should-still-be-considered-experimental"><a href="#5-15-Update-By-Query-API-new-and-should-still-be-considered-experimental" class="headerlink" title="5.15 Update By Query API (new and should still be considered experimental)"></a>5.15 Update By Query API (new and should still be considered experimental)</h2><p>The simplest usage of _update_by_query just performs an update on every document in the index without changing the source. This is useful to pick up a new property or some other online mapping change. Here is the API:</p>
<pre><code>curl -XPOST &apos;localhost:9200/twitter/_update_by_query?conflicts=proceed&apos;
</code></pre><p>All update and query failures cause the _update_by_query to abort and are returned in the failures of the response. The updates that have been performed still stick. In other words, the process is not rolled back, only aborted.</p>
<h2 id="5-16-Multi-Get-API"><a href="#5-16-Multi-Get-API" class="headerlink" title="5.16 Multi Get API"></a>5.16 Multi Get API</h2><p>Multi GET API allows to get multiple documents based on an index, type (optional) and id (and possibly routing). The response includes a docs array with all the fetched documents, each element similar in structure to a document provided by the get API. Here is an example:</p>
<pre><code>curl &apos;localhost:9200/_mget&apos; -d &apos;{
    &quot;docs&quot; : [
        {
            &quot;_index&quot; : &quot;test&quot;,
            &quot;_type&quot; : &quot;type&quot;,
            &quot;_id&quot; : &quot;1&quot;
        },
        {
            &quot;_index&quot; : &quot;test&quot;,
            &quot;_type&quot; : &quot;type&quot;,
            &quot;_id&quot; : &quot;2&quot;
        }
    ]
}&apos;
</code></pre><h2 id="5-17-Bulk-API"><a href="#5-17-Bulk-API" class="headerlink" title="5.17 Bulk API"></a>5.17 Bulk API</h2><p>The bulk API makes it possible to perform many index/delete operations in a single API call. This can greatly increase the indexing speed.</p>
<p>The REST API endpoint is /_bulk, and it expects the following JSON structure:</p>
<pre><code>action_and_meta_data\n
optional_source\n
action_and_meta_data\n
optional_source\n
....
action_and_meta_data\n
optional_source\n
</code></pre><p>NOTE: the final line of data must end with a newline character \n.</p>
<p>The possible actions are index, create, delete and update. index and create expect a source on the next line, and have the same semantics as the op_type parameter to the standard index API (i.e. create will fail if a document with the same index and type exists already, whereas index will add or replace a document as necessary). delete does not expect a source on the following line, and has the same semantics as the standard delete API. update expects that the partial doc, upsert and script and its options are specified on the next line.</p>
<p>Here is an example of a correct sequence of bulk commands:</p>
<pre><code>{ &quot;index&quot; : { &quot;_index&quot; : &quot;test&quot;, &quot;_type&quot; : &quot;type1&quot;, &quot;_id&quot; : &quot;1&quot; } }
{ &quot;field1&quot; : &quot;value1&quot; }
{ &quot;delete&quot; : { &quot;_index&quot; : &quot;test&quot;, &quot;_type&quot; : &quot;type1&quot;, &quot;_id&quot; : &quot;2&quot; } }
{ &quot;create&quot; : { &quot;_index&quot; : &quot;test&quot;, &quot;_type&quot; : &quot;type1&quot;, &quot;_id&quot; : &quot;3&quot; } }
{ &quot;field1&quot; : &quot;value3&quot; }
{ &quot;update&quot; : {&quot;_id&quot; : &quot;1&quot;, &quot;_type&quot; : &quot;type1&quot;, &quot;_index&quot; : &quot;index1&quot;} }
{ &quot;doc&quot; : {&quot;field2&quot; : &quot;value2&quot;} }
</code></pre><p>The endpoints are /_bulk, /{index}/_bulk, and {index}/{type}/_bulk. When the index or the index/type are provided, they will be used by default on bulk items that don’t provide them explicitly.</p>
<p>A note on the format. The idea here is to make processing of this as fast as possible. As some of the actions will be redirected to other shards on other nodes, only action_meta_data is parsed on the receiving node side.</p>
<h2 id="5-18-Reindex-API-new-and-should-still-be-considered-experimental"><a href="#5-18-Reindex-API-new-and-should-still-be-considered-experimental" class="headerlink" title="5.18 Reindex API (new and should still be considered experimental)"></a>5.18 Reindex API (new and should still be considered experimental)</h2><p>The most basic form of _reindex just copies documents from one index to another. This will copy documents from the twitter index into the new_twitter index:</p>
<pre><code>POST /_reindex
{
  &quot;source&quot;: {
    &quot;index&quot;: &quot;twitter&quot;
  },
  &quot;dest&quot;: {
    &quot;index&quot;: &quot;new_twitter&quot;
  }
}
</code></pre><p>You can limit the documents by adding a type to the source or by adding a query. This will only copy tweet’s made by kimchy into new_twitter:</p>
<pre><code>POST /_reindex
{
  &quot;source&quot;: {
    &quot;index&quot;: &quot;twitter&quot;,
    &quot;type&quot;: &quot;tweet&quot;,
    &quot;query&quot;: {
      &quot;term&quot;: {
        &quot;user&quot;: &quot;kimchy&quot;
      }
    }
  },
  &quot;dest&quot;: {
    &quot;index&quot;: &quot;new_twitter&quot;
  }
}
</code></pre><h2 id="5-19-Term-Vectors"><a href="#5-19-Term-Vectors" class="headerlink" title="5.19 Term Vectors"></a>5.19 Term Vectors</h2><p>Returns information and statistics on terms in the fields of a particular document. The document could be stored in the index or artificially provided by the user. Term vectors are realtime by default, not near realtime. This can be changed by setting realtime parameter to false.</p>
<pre><code>curl -XGET &apos;http://localhost:9200/twitter/tweet/1/_termvectors?pretty=true&apos;
</code></pre><p>Three types of values can be requested: term information, term statistics and field statistics. By default, all term information and field statistics are returned for all fields but no term statistics.</p>
<p>Term information</p>
<ul>
<li>term frequency in the field (always returned)</li>
<li>term positions (positions : true)</li>
<li>start and end offsets (offsets : true)</li>
<li>term payloads (payloads : true), as base64 encoded bytes</li>
</ul>
<p>Term statistics</p>
<ul>
<li>total term frequency (how often a term occurs in all documents)</li>
<li>document frequency (the number of documents containing the current term)</li>
</ul>
<blockquote>
<p>Setting term_statistics to true (default is false) will return term statistics. By default these values are not returned since term statistics can have a serious performance impact.</p>
</blockquote>
<p>Field statistics</p>
<ul>
<li>document count (how many documents contain this field)</li>
<li>sum of document frequencies (the sum of document frequencies for all terms in this field)</li>
<li>sum of total term frequencies (the sum of total term frequencies of each term in this field)</li>
</ul>
<p>The term and field statistics are not accurate. Deleted documents are not taken into account. The information is only retrieved for the shard the requested document resides in, unless dfs is set to true. The term and field statistics are therefore only useful as relative measures whereas the absolute numbers have no meaning in this context. By default, when requesting term vectors of artificial documents, a shard to get the statistics from is randomly selected.</p>
<p>See more examples: <a href="https://www.elastic.co/guide/en/elasticsearch/reference/2.3/docs-termvectors.html#_behaviour" target="_blank" rel="external">https://www.elastic.co/guide/en/elasticsearch/reference/2.3/docs-termvectors.html#_behaviour</a></p>
<h1 id="6-Search-APIs"><a href="#6-Search-APIs" class="headerlink" title="6 Search APIs"></a>6 Search APIs</h1><h2 id="6-1-Search"><a href="#6-1-Search" class="headerlink" title="6.1 Search"></a>6.1 Search</h2><p>The search API allows you to execute a search query and get back search hits that match the query. The query can either be provided using a simple query string as a parameter, or using a request body.</p>
<p>All search APIs can be applied across multiple types within an index, and across multiple indices with support for the multi index syntax. </p>
<h2 id="6-2-URI-Search"><a href="#6-2-URI-Search" class="headerlink" title="6.2 URI Search"></a>6.2 URI Search</h2><p>A search request can be executed purely using a URI by providing request parameters. Not all search options are exposed when executing a search using this mode, but it can be handy for quick “curl tests”. Here is an example:</p>
<pre><code>curl -XGET &apos;http://localhost:9200/twitter/tweet/_search?q=user:kimchy&apos;
</code></pre><h2 id="6-3-Request-Body-Search"><a href="#6-3-Request-Body-Search" class="headerlink" title="6.3 Request Body Search"></a>6.3 Request Body Search</h2><p>The search request can be executed with a search DSL, which includes the Query DSL, within its body. Here is an example:</p>
<pre><code>curl -XGET &apos;http://localhost:9200/twitter/tweet/_search&apos; -d &apos;{
    &quot;query&quot; : {
        &quot;term&quot; : { &quot;user&quot; : &quot;kimchy&quot; }
    }
}&apos;
</code></pre><h2 id="6-4-Query"><a href="#6-4-Query" class="headerlink" title="6.4 Query"></a>6.4 Query</h2><p>The query element within the search request body allows to define a query using the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/2.3/query-dsl.html" target="_blank" rel="external">Query DSL</a>.</p>
<pre><code>{
    &quot;query&quot; : {
        &quot;term&quot; : { &quot;user&quot; : &quot;kimchy&quot; }
    }
}
</code></pre><h2 id="6-5-From-Size"><a href="#6-5-From-Size" class="headerlink" title="6.5 From / Size"></a>6.5 From / Size</h2><p>Pagination of results can be done by using the from and size parameters.</p>
<pre><code>{
    &quot;from&quot; : 0, &quot;size&quot; : 10,
    &quot;query&quot; : {
        &quot;term&quot; : { &quot;user&quot; : &quot;kimchy&quot; }
    }
}
</code></pre><p>Note that from + size can not be more than the index.max_result_window index setting which defaults to 10,000. See the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/2.3/search-request-scroll.html" target="_blank" rel="external">Scroll</a> API for more efficient ways to do deep scrolling.</p>
<h2 id="6-6-Sort-多个字段的排序规则是怎么样的？"><a href="#6-6-Sort-多个字段的排序规则是怎么样的？" class="headerlink" title="6.6 Sort (**多个字段的排序规则是怎么样的？)"></a>6.6 Sort (<strong>**</strong>多个字段的排序规则是怎么样的？)</h2><p>Allows to add one or more sort on specific fields. Each sort can be reversed as well. The sort is defined on a per field level, with special field name for _score to sort by score, and _doc to sort by index order.</p>
<pre><code>{
    &quot;sort&quot; : [
        { &quot;post_date&quot; : {&quot;order&quot; : &quot;asc&quot;}},
        &quot;user&quot;,
        { &quot;name&quot; : &quot;desc&quot; },
        { &quot;age&quot; : &quot;desc&quot; },
        &quot;_score&quot;
    ],
    &quot;query&quot; : {
        &quot;term&quot; : { &quot;user&quot; : &quot;kimchy&quot; }
    }
}
</code></pre><p>The sort values for each document returned are also returned as part of the response.<br>The order option can have the following values:</p>
<ul>
<li>asc: Sort in ascending order</li>
<li>desc: Sort in descending order</li>
</ul>
<p>The order defaults to desc when sorting on the _score, and defaults to asc when sorting on anything else.</p>
<h2 id="6-7-Sort-mode-option"><a href="#6-7-Sort-mode-option" class="headerlink" title="6.7 Sort mode option"></a>6.7 Sort mode option</h2><p>Elasticsearch supports sorting by array or multi-valued fields. The mode option controls what array value is picked for sorting the document it belongs to. The mode option can have the following values:</p>
<ul>
<li>min: Pick the lowest value.</li>
<li>max: Pick the highest value.</li>
<li>sum: Use the sum of all values as sort value. Only applicable for number based array fields.</li>
<li>avg: Use the average of all values as sort value. Only applicable for number based array fields.</li>
<li>median: Use the median of all values as sort value. Only applicable for number based array fields.</li>
</ul>
<h2 id="6-8-Missing-Values"><a href="#6-8-Missing-Values" class="headerlink" title="6.8 Missing Values"></a>6.8 Missing Values</h2><p>The missing parameter specifies how docs which are missing the field should be treated: The missing value can be set to _last, _first, or a custom value (that will be used for missing docs as the sort value). For example:</p>
<pre><code>{
    &quot;sort&quot; : [
        { &quot;price&quot; : {&quot;missing&quot; : &quot;_last&quot;} },
    ],
    &quot;query&quot; : {
        &quot;term&quot; : { &quot;user&quot; : &quot;kimchy&quot; }
    }
}
</code></pre><h2 id="6-9-Script-Based-Sorting"><a href="#6-9-Script-Based-Sorting" class="headerlink" title="6.9 Script Based Sorting"></a>6.9 Script Based Sorting</h2><p>Allow to sort based on custom scripts, here is an example:</p>
<pre><code>{
    &quot;query&quot; : {
        ....
    },
    &quot;sort&quot; : {
        &quot;_script&quot; : {
            &quot;type&quot; : &quot;number&quot;,
            &quot;script&quot; : {
                &quot;inline&quot;: &quot;doc[&apos;field_name&apos;].value * factor&quot;,
                &quot;params&quot; : {
                    &quot;factor&quot; : 1.1
                }
            },
            &quot;order&quot; : &quot;asc&quot;
        }
    }
}
</code></pre><h2 id="6-10-Memory-Considerations"><a href="#6-10-Memory-Considerations" class="headerlink" title="6.10 Memory Considerations"></a>6.10 Memory Considerations</h2><p>When sorting, the relevant sorted field values are loaded into memory. This means that per shard, there should be enough memory to contain them. For string based types, the field sorted on should not be analyzed / tokenized. For numeric types, if possible, it is recommended to explicitly set the type to narrower types (like short, integer and float).</p>
<h2 id="6-11-Source-filtering"><a href="#6-11-Source-filtering" class="headerlink" title="6.11 Source filtering"></a>6.11 Source filtering</h2><p>Allows to control how the _source field is returned with every hit.</p>
<p>By default operations return the contents of the _source field unless you have used the fields parameter or if the _source field is disabled.</p>
<ul>
<li>To disable _source retrieval set to false.</li>
<li>The _source also accepts one or more wildcard patterns to control what parts of the _source should be returned.</li>
<li>Finally, for complete control, you can specify both include and exclude patterns.</li>
</ul>
<h2 id="6-12-Fields"><a href="#6-12-Fields" class="headerlink" title="6.12 Fields"></a>6.12 Fields</h2><blockquote>
<p>The fields parameter is about fields that are explicitly marked as stored in the mapping, which is off by default and generally not recommended. Use source filtering instead to select subsets of the original source document to be returned.</p>
</blockquote>
<h2 id="6-13-Script-Fields"><a href="#6-13-Script-Fields" class="headerlink" title="6.13 Script Fields"></a>6.13 Script Fields</h2><p>Allows to return a script evaluation (based on different fields) for each hit, for example:</p>
<pre><code>{
    &quot;query&quot; : {
        ...
    },
    &quot;script_fields&quot; : {
        &quot;test1&quot; : {
            &quot;script&quot; : &quot;_source.obj1.obj2&quot;
        },
        &quot;test2&quot; : {
            &quot;script&quot; : {
                &quot;inline&quot;: &quot;doc[&apos;my_field_name&apos;].value * factor&quot;,
                &quot;params&quot; : {
                    &quot;factor&quot;  : 2.0
                }
            }
        }
    }
}
</code></pre><p>Note the _source keyword here to navigate the json-like model.</p>
<p>It’s important to understand the difference between doc[‘my_field’].value and _source.my_field. The first, using the doc keyword, will cause the terms for that field to be loaded to memory (cached), which will result in faster execution, but more memory consumption. Also, the doc[…] notation only allows for simple valued fields (can’t return a json object from it) and make sense only on non-analyzed or single term based fields.</p>
<p>The _source on the other hand causes the source to be loaded, parsed, and then only the relevant part of the json is returned.</p>
<h2 id="6-14-Field-Data-Fields-需要了解stored和fielddata的概念"><a href="#6-14-Field-Data-Fields-需要了解stored和fielddata的概念" class="headerlink" title="6.14 Field Data Fields (**需要了解stored和fielddata的概念)"></a>6.14 Field Data Fields (<strong>**</strong>需要了解stored和fielddata的概念)</h2><p>Allows to return the field data representation of a field for each hit, for example:</p>
<pre><code>{
    &quot;query&quot; : {
        ...
    },
    &quot;fielddata_fields&quot; : [&quot;test1&quot;, &quot;test2&quot;]
}
</code></pre><p>Field data fields can work on fields that are not stored.</p>
<p>It’s important to understand that using the fielddata_fields parameter will cause the terms for that field to be loaded to memory (cached), which will result in more memory consumption.</p>
<h2 id="6-15-Post-filter"><a href="#6-15-Post-filter" class="headerlink" title="6.15 Post filter"></a>6.15 Post filter</h2><p>The post_filter is applied to the search hits at the very end of a search request, after aggregations have already been calculated.</p>
<pre><code>{
  &quot;query&quot;: {
    &quot;bool&quot;: {
      &quot;filter&quot;: {
        &quot;term&quot;: {
          &quot;brandName&quot;: &quot;vans&quot;
        }
      }
    }
  },
  &quot;aggs&quot;: {
    &quot;colors&quot;: {
      &quot;terms&quot;: {
        &quot;field&quot;: &quot;colorNames&quot;
      }
    },
    &quot;color_red&quot;: {
      &quot;filter&quot;: {
        &quot;term&quot;: {
          &quot;colorNames&quot;: &quot;红色&quot;
        }
      },
      &quot;aggs&quot;: {
        &quot;smallSorts&quot;: {
          &quot;terms&quot;: {
            &quot;field&quot;: &quot;smallSort&quot;
          }
        }
      }
    }
  },
  &quot;post_filter&quot;: {
    &quot;term&quot;: {
      &quot;colorNames&quot;: &quot;红色&quot;
    }
  }
}
</code></pre><ul>
<li>The main query now finds all products by vans, regardless of color.</li>
<li>The colors agg returns popular colors by vans.</li>
<li>The color_red agg limits the small sort sub-aggregation to red vans products.</li>
<li>Finally, the post_filter removes colors other than red from the search hits.</li>
</ul>
<h2 id="6-16-Highlighting"><a href="#6-16-Highlighting" class="headerlink" title="6.16 Highlighting"></a>6.16 Highlighting</h2><p>Allows to highlight search results on one or more fields. The implementation uses either the lucene highlighter, fast-vector-highlighter or postings-highlighter. The following is an example of the search request body:</p>
<pre><code>{
    &quot;query&quot; : {...},
    &quot;highlight&quot; : {
        &quot;fields&quot; : {
            &quot;content&quot; : {}
        }
    }
}
</code></pre><h3 id="6-16-1-Plain-highlighter"><a href="#6-16-1-Plain-highlighter" class="headerlink" title="6.16.1 Plain highlighter"></a>6.16.1 Plain highlighter</h3><p>The default choice of highlighter is of type plain and uses the Lucene highlighter. It tries hard to reflect the query matching logic in terms of understanding word importance and any word positioning criteria in phrase queries.</p>
<h3 id="6-16-2-Postings-highlighter"><a href="#6-16-2-Postings-highlighter" class="headerlink" title="6.16.2 Postings highlighter"></a>6.16.2 Postings highlighter</h3><p>If index_options is set to offsets in the mapping the postings highlighter will be used instead of the plain highlighter. The postings highlighter:</p>
<ul>
<li>Is faster since it doesn’t require to reanalyze the text to be highlighted: the larger the documents the better the performance gain should be</li>
<li>Requires less disk space than term_vectors, needed for the fast vector highlighter</li>
<li>Breaks the text into sentences and highlights them. Plays really well with natural languages, not as well with - fields containing for instance html markup</li>
<li>Treats the document as the whole corpus, and scores individual sentences as if they were documents in this corpus, using the BM25 algorithm</li>
</ul>
<h3 id="6-16-3-Fast-vector-highlighter"><a href="#6-16-3-Fast-vector-highlighter" class="headerlink" title="6.16.3 Fast vector highlighter"></a>6.16.3 Fast vector highlighter</h3><p>If term_vector information is provided by setting term_vector to with_positions_offsets in the mapping then the fast vector highlighter will be used instead of the plain highlighter. The fast vector highlighter:</p>
<ul>
<li>Is faster especially for large fields (&gt; 1MB)</li>
<li>Can be customized with boundary_chars, boundary_max_scan, and fragment_offset (see below)</li>
<li>Requires setting term_vector to with_positions_offsets which increases the size of the index</li>
<li>Can combine matches from multiple fields into one result. See matched_fields</li>
<li>Can assign different weights to matches at different positions allowing for things like phrase matches being - sorted above term matches when highlighting a Boosting Query that boosts phrase matches over term matches</li>
</ul>
<h2 id="6-17-Rescoring"><a href="#6-17-Rescoring" class="headerlink" title="6.17 Rescoring"></a>6.17 Rescoring</h2><p>Rescoring can help to improve precision by reordering just the top (eg 100 - 500) documents returned by the query and post_filter phases, using a secondary (usually more costly) algorithm, instead of applying the costly algorithm to all documents in the index.</p>
<p>A rescore request is executed on each shard before it returns its results to be sorted by the node handling the overall search request.</p>
<p>Currently the rescore API has only one implementation: the query rescorer, which uses a query to tweak the scoring. In the future, alternative rescorers may be made available, for example, a pair-wise rescorer.</p>
<p>By default the scores from the original query and the rescore query are combined linearly to produce the final _score for each document. The relative importance of the original query and of the rescore query can be controlled with the query_weight and rescore_query_weight respectively. Both default to 1.</p>
<pre><code>{
  &quot;query&quot;: {
    &quot;match&quot;: {
      &quot;productName.productName_ansj&quot;: {
        &quot;operator&quot;: &quot;or&quot;,
        &quot;query&quot;: &quot;连帽 套装&quot;,
        &quot;type&quot;: &quot;boolean&quot;
      }
    }
  },
  &quot;_source&quot;: [
    &quot;productName&quot;
  ],
  &quot;rescore&quot;: {
    &quot;window_size&quot;: 50,
    &quot;query&quot;: {
      &quot;rescore_query&quot;: {
        &quot;match&quot;: {
          &quot;productName.productName_ansj&quot;: {
            &quot;query&quot;: &quot;连帽 套装&quot;,
            &quot;type&quot;: &quot;phrase&quot;,
            &quot;slop&quot;: 2
          }
        }
      },
      &quot;query_weight&quot;: 0.7,
      &quot;rescore_query_weight&quot;: 1.2
    }
  }
}
</code></pre><p>Score Mode</p>
<ul>
<li>total:Add the original score and the rescore query score. The default.</li>
<li>multiply: Multiply the original score by the rescore query score. Useful for function query rescores.</li>
<li>avg: Average the original score and the rescore query score.</li>
<li>max: Take the max of original score and the rescore query score.</li>
<li>min: Take the min of the original score and the rescore query score.</li>
</ul>
<h2 id="6-18-Search-Type"><a href="#6-18-Search-Type" class="headerlink" title="6.18 Search Type"></a>6.18 Search Type</h2><p>There are different execution paths that can be done when executing a distributed search. The distributed search operation needs to be scattered to all the relevant shards and then all the results are gathered back. When doing scatter/gather type execution, there are several ways to do that, specifically with search engines.</p>
<p>One of the questions when executing a distributed search is how many results to retrieve from each shard. For example, if we have 10 shards, the 1st shard might hold the most relevant results from 0 till 10, with other shards results ranking below it. For this reason, when executing a request, we will need to get results from 0 till 10 from all shards, sort them, and then return the results if we want to ensure correct results.</p>
<p>Another question, which relates to the search engine, is the fact that each shard stands on its own. When a query is executed on a specific shard, it does not take into account term frequencies and other search engine information from the other shards. If we want to support accurate ranking, we would need to first gather the term frequencies from all shards to calculate global term frequencies, then execute the query on each shard using these global frequencies.</p>
<p>Also, because of the need to sort the results, getting back a large document set, or even scrolling it, while maintaining the correct sorting behavior can be a very expensive operation. For large result set scrolling, it is best to sort by _doc if the order in which documents are returned is not important.</p>
<p>Elasticsearch is very flexible and allows to control the type of search to execute on a per search request basis. The type can be configured by setting the search_type parameter in the query string. The types are:</p>
<h3 id="6-18-1-Query-Then-Fetch-query-then-fetch"><a href="#6-18-1-Query-Then-Fetch-query-then-fetch" class="headerlink" title="6.18.1 Query Then Fetch(query_then_fetch)"></a>6.18.1 Query Then Fetch(query_then_fetch)</h3><p>The request is processed in two phases. In the first phase, the query is forwarded to all involved shards. Each shard executes the search and generates a sorted list of results, local to that shard. Each shard returns just enough information to the coordinating node to allow it merge and re-sort the shard level results into a globally sorted set of results, of maximum length size.</p>
<p>During the second phase, the coordinating node requests the document content (and highlighted snippets, if any) from only the relevant shards.</p>
<p>Note: This is the default setting, if you do not specify a search_type in your request.</p>
<h3 id="6-18-2-Dfs-Query-Then-Fetch-dfs-query-then-fetch"><a href="#6-18-2-Dfs-Query-Then-Fetch-dfs-query-then-fetch" class="headerlink" title="6.18.2 Dfs, Query Then Fetch(dfs_query_then_fetch)"></a>6.18.2 Dfs, Query Then Fetch(dfs_query_then_fetch)</h3><p>Same as “Query Then Fetch”, except for an initial scatter phase which goes and computes the distributed term frequencies for more accurate scoring.</p>
<h3 id="6-18-3-Count-Deprecated-in-2-0-0-beta1"><a href="#6-18-3-Count-Deprecated-in-2-0-0-beta1" class="headerlink" title="6.18.3 Count (Deprecated in 2.0.0-beta1)"></a>6.18.3 Count (Deprecated in 2.0.0-beta1)</h3><h3 id="6-18-4-Scan-Deprecated-in-2-1-0"><a href="#6-18-4-Scan-Deprecated-in-2-1-0" class="headerlink" title="6.18.4 Scan (Deprecated in 2.1.0)"></a>6.18.4 Scan (Deprecated in 2.1.0)</h3><h2 id="6-19-Scroll"><a href="#6-19-Scroll" class="headerlink" title="6.19 Scroll"></a>6.19 Scroll</h2><p>While a search request returns a single “page” of results, the scroll API can be used to retrieve large numbers of results (or even all results) from a single search request, in much the same way as you would use a cursor on a traditional database.</p>
<p>Scrolling is not intended for real time user requests, but rather for processing large amounts of data, e.g. in order to reindex the contents of one index into a new index with a different configuration.</p>
<h2 id="6-20-Preference"><a href="#6-20-Preference" class="headerlink" title="6.20 Preference"></a>6.20 Preference</h2><p>Controls a preference of which shard replicas to execute the search request on. By default, the operation is randomized between the shard replicas.</p>
<p>The preference is a query string parameter which can be set to:</p>
<ul>
<li>_primary: The operation will go and be executed only on the primary shards.</li>
<li>_primary_first: The operation will go and be executed on the primary shard, and if not available (failover), will execute on other shards.</li>
<li>_replica: The operation will go and be executed only on a replica shard.</li>
<li>_replica_first: The operation will go and be executed only on a replica shard, and if not available (failover), will execute on other shards.</li>
<li>_local: The operation will prefer to be executed on a local allocated shard if possible.</li>
<li>_only_node:xyz: Restricts the search to execute only on a node with the provided node id (xyz in this case).</li>
<li>_prefer_node:xyz: Prefers execution on the node with the provided node id (xyz in this case) if applicable.</li>
<li>_shards:2,3: Restricts the operation to the specified shards. (2 and 3 in this case). This preference can be combined with other preferences but it has to appear first: _shards:2,3;_primary</li>
<li>_only_nodes: Restricts the operation to nodes specified in node specification <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster.html" target="_blank" rel="external">https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster.html</a></li>
<li>Custom (string) value: A custom value will be used to guarantee that the same shards will be used for the same custom value. This can help with “jumping values” when hitting different shards in different refresh states. A sample value can be something like the web session id, or the user name.</li>
</ul>
<h2 id="6-21-Explain"><a href="#6-21-Explain" class="headerlink" title="6.21 Explain"></a>6.21 Explain</h2><p>Enables explanation for each hit on how its score was computed.</p>
<pre><code>{
    &quot;explain&quot;: true,
    &quot;query&quot; : {
        &quot;term&quot; : { &quot;user&quot; : &quot;kimchy&quot; }
    }
}
</code></pre><h2 id="6-22-Version"><a href="#6-22-Version" class="headerlink" title="6.22 Version"></a>6.22 Version</h2><p>Returns a version for each search hit.</p>
<pre><code>{
    &quot;version&quot;: true,
    &quot;query&quot; : {
        &quot;term&quot; : { &quot;user&quot; : &quot;kimchy&quot; }
    }
}
</code></pre><h2 id="6-23-Index-Boost"><a href="#6-23-Index-Boost" class="headerlink" title="6.23 Index Boost"></a>6.23 Index Boost</h2><p>Allows to configure different boost level per index when searching across more than one indices. This is very handy when hits coming from one index matter more than hits coming from another index (think social graph where each user has an index).</p>
<pre><code>{
    &quot;indices_boost&quot; : {
        &quot;index1&quot; : 1.4,
        &quot;index2&quot; : 1.3
    }
}
</code></pre><h2 id="6-24-Inner-hits-Skipped-和parent-child有关系，后续一起学习"><a href="#6-24-Inner-hits-Skipped-和parent-child有关系，后续一起学习" class="headerlink" title="6.24 Inner hits (**Skipped: 和parent/child有关系，后续一起学习)"></a>6.24 Inner hits (<strong>**</strong>Skipped: 和parent/child有关系，后续一起学习)</h2><h2 id="6-25-Search-Template"><a href="#6-25-Search-Template" class="headerlink" title="6.25 Search Template"></a>6.25 Search Template</h2><p>The /_search/template endpoint allows to use the mustache language to pre render search requests, before they are executed and fill existing templates with template parameters.</p>
<pre><code>GET /_search/template
{
    &quot;inline&quot; : {
      &quot;query&quot;: { &quot;match&quot; : { &quot;{{my_field}}&quot; : &quot;{{my_value}}&quot; } },
      &quot;size&quot; : &quot;{{my_size}}&quot;
    },
    &quot;params&quot; : {
        &quot;my_field&quot; : &quot;foo&quot;,
        &quot;my_value&quot; : &quot;bar&quot;,
        &quot;my_size&quot; : 5
    }
}
</code></pre><h2 id="6-26-Search-Shards-API"><a href="#6-26-Search-Shards-API" class="headerlink" title="6.26 Search Shards API"></a>6.26 Search Shards API</h2><p>The search shards api returns the indices and shards that a search request would be executed against. This can give useful feedback for working out issues or planning optimizations with routing and shard preferences.</p>
<pre><code>curl -XGET &apos;localhost:9200/twitter/_search_shards&apos;
</code></pre><h2 id="6-27-Suggesters-Skipped"><a href="#6-27-Suggesters-Skipped" class="headerlink" title="6.27 Suggesters (Skipped)"></a>6.27 Suggesters (Skipped)</h2><p>The suggest feature suggests similar looking terms based on a provided text by using a suggester. Parts of the suggest feature are still under development.</p>
<h2 id="6-28-Count-API"><a href="#6-28-Count-API" class="headerlink" title="6.28 Count API"></a>6.28 Count API</h2><p>The count API allows to easily execute a query and get the number of matches for that query. It can be executed across one or more indices and across one or more types. The query can either be provided using a simple query string as a parameter, or using the Query DSL defined within the request body. Here is an example:</p>
<pre><code>curl -XGET &apos;http://localhost:9200/twitter/tweet/_count?q=user:kimchy&apos;

curl -XGET &apos;http://localhost:9200/twitter/tweet/_count&apos; -d &apos;
{
    &quot;query&quot; : {
        &quot;term&quot; : { &quot;user&quot; : &quot;kimchy&quot; }
    }
}&apos;
</code></pre><h2 id="6-29-Validate-API"><a href="#6-29-Validate-API" class="headerlink" title="6.29 Validate API"></a>6.29 Validate API</h2><p>The validate API allows a user to validate a potentially expensive query without executing it. The following example shows how it can be used:</p>
<pre><code>curl -XPUT &apos;http://localhost:9200/twitter/tweet/1&apos; -d &apos;{
    &quot;user&quot; : &quot;kimchy&quot;,
    &quot;post_date&quot; : &quot;2009-11-15T14:12:12&quot;,
    &quot;message&quot; : &quot;trying out Elasticsearch&quot;
}&apos;
</code></pre><p>When the query is valid, the response contains valid:true:</p>
<pre><code>curl -XGET &apos;http://localhost:9200/twitter/_validate/query?q=user:foo&apos;

{&quot;valid&quot;:true,&quot;_shards&quot;:{&quot;total&quot;:1,&quot;successful&quot;:1,&quot;failed&quot;:0}}
</code></pre><h2 id="6-30-Explain-API"><a href="#6-30-Explain-API" class="headerlink" title="6.30 Explain API"></a>6.30 Explain API</h2><p>The explain api computes a score explanation for a query and a specific document. This can give useful feedback whether a document matches or didn’t match a specific query.</p>
<pre><code>curl -XGET &apos;localhost:9200/twitter/tweet/1/_explain&apos; -d &apos;{
      &quot;query&quot; : {
        &quot;term&quot; : { &quot;message&quot; : &quot;search&quot; }
      }
}&apos;
</code></pre><h2 id="6-30-Profile-API-experimental-and-may-be-changed-or-removed"><a href="#6-30-Profile-API-experimental-and-may-be-changed-or-removed" class="headerlink" title="6.30 Profile API (experimental and may be changed or removed)"></a>6.30 Profile API (experimental and may be changed or removed)</h2><p>The Profile API provides detailed timing information about the execution of individual components in a query. It gives the user insight into how queries are executed at a low level so that the user can understand why certain queries are slow, and take steps to improve their slow queries.</p>
<p>The output from the Profile API is very verbose, especially for complicated queries executed across many shards. Pretty-printing the response is recommended to help understand the output.</p>
<pre><code>curl -XGET &apos;localhost:9200/_search&apos; -d &apos;{
  &quot;profile&quot;: true,
  &quot;query&quot; : {
    &quot;match&quot; : { &quot;message&quot; : &quot;search test&quot; }
  }
}
</code></pre><h2 id="6-31-Field-stats-API-experimental-and-may-be-changed-or-removed"><a href="#6-31-Field-stats-API-experimental-and-may-be-changed-or-removed" class="headerlink" title="6.31 Field stats API (experimental and may be changed or removed)"></a>6.31 Field stats API (experimental and may be changed or removed)</h2><p>The field stats api allows one to find statistical properties of a field without executing a search, but looking up measurements that are natively available in the Lucene index. This can be useful to explore a dataset which you don’t know much about. For example, this allows creating a histogram aggregation with meaningful intervals based on the min/max range of values.</p>
<p>The field stats api by defaults executes on all indices, but can execute on specific indices too.</p>
<p>All indices:</p>
<pre><code>curl -XGET &quot;http://localhost:9200/_field_stats?fields=rating&quot;
</code></pre><p>Specific indices:</p>
<pre><code>curl -XGET &quot;http://localhost:9200/index1,index2/_field_stats?fields=rating&quot;
</code></pre><h1 id="7-Aggregations"><a href="#7-Aggregations" class="headerlink" title="7 Aggregations"></a>7 Aggregations</h1><h2 id="7-1-Aggregations"><a href="#7-1-Aggregations" class="headerlink" title="7.1 Aggregations"></a>7.1 Aggregations</h2><p>The aggregations framework helps provide aggregated data based on a search query. It is based on simple building blocks called aggregations, that can be composed in order to build complex summaries of the data.</p>
<p>An aggregation can be seen as a unit-of-work that builds analytic information over a set of documents. The context of the execution defines what this document set is (e.g. a top-level aggregation executes within the context of the executed query/filters of the search request).</p>
<p>There are many different types of aggregations, each with its own purpose and output. To better understand these types, it is often easier to break them into three main families:</p>
<ul>
<li>Bucketing: A family of aggregations that build buckets, where each bucket is associated with a key and a document criterion. When the aggregation is executed, all the buckets criteria are evaluated on every document in the context and when a criterion matches, the document is considered to “fall in” the relevant bucket. By the end of the aggregation process, we’ll end up with a list of buckets - each one with a set of documents that “belong” to it.</li>
<li>Metric: Aggregations that keep track and compute metrics over a set of documents.</li>
<li>Pipeline: Aggregations that aggregate the output of other aggregations and their associated metrics</li>
</ul>
<p>The interesting part comes next. Since each bucket effectively defines a document set (all documents belonging to the bucket), one can potentially associate aggregations on the bucket level, and those will execute within the context of that bucket. This is where the real power of aggregations kicks in: aggregations can be nested!</p>
<h2 id="7-2-Structuring-Aggregations"><a href="#7-2-Structuring-Aggregations" class="headerlink" title="7.2 Structuring Aggregations"></a>7.2 Structuring Aggregations</h2><p>The following snippet captures the basic structure of aggregations:</p>
<pre><code>&quot;aggregations&quot; : {
    &quot;&lt;aggregation_name&gt;&quot; : {
        &quot;&lt;aggregation_type&gt;&quot; : {
            &lt;aggregation_body&gt;
        }
        [,&quot;meta&quot; : {  [&lt;meta_data_body&gt;] } ]?
        [,&quot;aggregations&quot; : { [&lt;sub_aggregation&gt;]+ } ]?
    }
    [,&quot;&lt;aggregation_name_2&gt;&quot; : { ... } ]*
}
</code></pre><h2 id="7-3-Metrics-Aggregations"><a href="#7-3-Metrics-Aggregations" class="headerlink" title="7.3 Metrics Aggregations"></a>7.3 Metrics Aggregations</h2><p>The aggregations in this family compute metrics based on values extracted in one way or another from the documents that are being aggregated. The values are typically extracted from the fields of the document (using the field data), but can also be generated using scripts.</p>
<p>Numeric metrics aggregations are a special type of metrics aggregation which output numeric values. Some aggregations output a single numeric metric (e.g. avg) and are called single-value numeric metrics aggregation, others generate multiple metrics (e.g. stats) and are called multi-value numeric metrics aggregation. The distinction between single-value and multi-value numeric metrics aggregations plays a role when these aggregations serve as direct sub-aggregations of some bucket aggregations (some bucket aggregations enable you to sort the returned buckets based on the numeric metrics in each bucket).</p>
<h3 id="7-3-1-Avg-Aggregation"><a href="#7-3-1-Avg-Aggregation" class="headerlink" title="7.3.1 Avg Aggregation"></a>7.3.1 Avg Aggregation</h3><p>A single-value metrics aggregation that computes the average of numeric values that are extracted from the aggregated documents. These values can be extracted either from specific numeric fields in the documents, or be generated by a provided script.</p>
<pre><code>{
  &quot;size&quot;: 0,
  &quot;query&quot;: {
    &quot;match_all&quot;: {}
  },
  &quot;aggs&quot;: {
    &quot;avg_price&quot;: {
      &quot;avg&quot;: {
        &quot;field&quot;: &quot;salesPrice&quot;
      }
    }
  }
}

&quot;aggregations&quot;: {
    &quot;avg_price&quot;: {
        &quot;value&quot;: 428.51063644785825
    }
}
</code></pre><h3 id="7-3-2-Cardinality-Aggregation"><a href="#7-3-2-Cardinality-Aggregation" class="headerlink" title="7.3.2 Cardinality Aggregation"></a>7.3.2 Cardinality Aggregation</h3><p>A single-value metrics aggregation that calculates an approximate count of distinct values. Values can be extracted either from specific fields in the document or generated by a script.</p>
<pre><code>{
  &quot;size&quot;: 0,
  &quot;query&quot;: {
    &quot;match_all&quot;: {}
  },
  &quot;aggs&quot;: {
    &quot;brand_count&quot;: {
      &quot;cardinality&quot;: {
        &quot;field&quot;: &quot;brandId&quot;
      }
    }
  }
}

&quot;aggregations&quot;: {
    &quot;brand_count&quot;: {
        &quot;value&quot;: 1186
    }
}
</code></pre><h3 id="7-3-3-Stats-Aggregation"><a href="#7-3-3-Stats-Aggregation" class="headerlink" title="7.3.3 Stats Aggregation"></a>7.3.3 Stats Aggregation</h3><p>A multi-value metrics aggregation that computes stats over numeric values extracted from the aggregated documents. These values can be extracted either from specific numeric fields in the documents, or be generated by a provided script.</p>
<p>The stats that are returned consist of: min, max, sum, count and avg.</p>
<pre><code>{
  &quot;size&quot;: 0,
  &quot;query&quot;: {
    &quot;match_all&quot;: {}
  },
  &quot;aggs&quot;: {
    &quot;price_stat&quot;: {
      &quot;stats&quot;: {
        &quot;field&quot;: &quot;salesPrice&quot;
      }
    }
  }
}

&quot;aggregations&quot;: {
    &quot;price_stat&quot;: {
        &quot;count&quot;: 221275,
        &quot;min&quot;: 0,
        &quot;max&quot;: 131231,
        &quot;avg&quot;: 428.51063644785825,
        &quot;sum&quot;: 94818691.07999983
    }
}
</code></pre><h3 id="7-3-4-Extended-Stats-Aggregation"><a href="#7-3-4-Extended-Stats-Aggregation" class="headerlink" title="7.3.4 Extended Stats Aggregation"></a>7.3.4 Extended Stats Aggregation</h3><p>A multi-value metrics aggregation that computes stats over numeric values extracted from the aggregated documents. These values can be extracted either from specific numeric fields in the documents, or be generated by a provided script.</p>
<p>The extended_stats aggregations is an extended version of the stats aggregation, where additional metrics are added such as sum_of_squares, variance, std_deviation and std_deviation_bounds.</p>
<pre><code>{
  &quot;size&quot;: 0,
  &quot;query&quot;: {
    &quot;match_all&quot;: {}
  },
  &quot;aggs&quot;: {
    &quot;price_stat&quot;: {
      &quot;extended_stats&quot;: {
        &quot;field&quot;: &quot;salesPrice&quot;
      }
    }
  }
}

&quot;aggregations&quot;: {
    &quot;price_stat&quot;: {
        &quot;count&quot;: 221275,
        &quot;min&quot;: 0,
        &quot;max&quot;: 131231,
        &quot;avg&quot;: 428.51063644785825,
        &quot;sum&quot;: 94818691.07999983,
        &quot;sum_of_squares&quot;: 118950750156.63016,
        &quot;variance&quot;: 353948.4012870255,
        &quot;std_deviation&quot;: 594.9356278514723,
        &quot;std_deviation_bounds&quot;: {
        &quot;upper&quot;: 1618.3818921508027,
        &quot;lower&quot;: -761.3606192550864
        }
    }
}
</code></pre><h3 id="7-3-5-Geo-Bounds-Aggregation-Skipped"><a href="#7-3-5-Geo-Bounds-Aggregation-Skipped" class="headerlink" title="7.3.5 Geo Bounds Aggregation (Skipped)"></a>7.3.5 Geo Bounds Aggregation (Skipped)</h3><h3 id="7-3-6-Geo-Centroid-Aggregation-Skipped"><a href="#7-3-6-Geo-Centroid-Aggregation-Skipped" class="headerlink" title="7.3.6 Geo Centroid Aggregation (Skipped)"></a>7.3.6 Geo Centroid Aggregation (Skipped)</h3><h3 id="7-3-7-Max-Aggregation"><a href="#7-3-7-Max-Aggregation" class="headerlink" title="7.3.7 Max Aggregation"></a>7.3.7 Max Aggregation</h3><p>A single-value metrics aggregation that keeps track and returns the maximum value among the numeric values extracted from the aggregated documents.</p>
<pre><code>&quot;aggs&quot;: {
    &quot;max_price&quot;: {
        &quot;max&quot;: {
            &quot;field&quot;: &quot;salesPrice&quot;
        }
    }
}
</code></pre><h3 id="7-3-8-Min-Aggregation"><a href="#7-3-8-Min-Aggregation" class="headerlink" title="7.3.8 Min Aggregation"></a>7.3.8 Min Aggregation</h3><p>A single-value metrics aggregation that keeps track and returns the minimum value among numeric values extracted from the aggregated documents.</p>
<pre><code>&quot;aggs&quot;: {
    &quot;min_price&quot;: {
      &quot;min&quot;: {
        &quot;field&quot;: &quot;salesPrice&quot;
      }
    }
  }
</code></pre><h3 id="7-3-9-Percentiles-Aggregation"><a href="#7-3-9-Percentiles-Aggregation" class="headerlink" title="7.3.9 Percentiles Aggregation"></a>7.3.9 Percentiles Aggregation</h3><p>A multi-value metrics aggregation that calculates one or more percentiles over numeric values extracted from the aggregated documents. </p>
<p>Percentiles show the point at which a certain percentage of observed values occur. For example, the 95th percentile is the value which is greater than 95% of the observed values.</p>
<p>When a range of percentiles are retrieved, they can be used to estimate the data distribution and determine if the data is skewed, bimodal, etc.</p>
<pre><code>&quot;aggs&quot;: {
    &quot;price_outlier&quot;: {
      &quot;percentiles&quot;: {
        &quot;field&quot;: &quot;salesPrice&quot;
      }
    }
  }

&quot;aggregations&quot;: {
    &quot;price_outlier&quot;: {
        &quot;values&quot;: {
            &quot;1.0&quot;: 19,
            &quot;5.0&quot;: 49.049088235742005,
            &quot;25.0&quot;: 148.8903318997934,
            &quot;50.0&quot;: 288.33201291736634,
            &quot;75.0&quot;: 521.2972145384141,
            &quot;95.0&quot;: 1286.9096656603726,
            &quot;99.0&quot;: 2497.931283641535
        }
    }
}
</code></pre><h3 id="7-3-10-Percentile-Ranks-Aggregation"><a href="#7-3-10-Percentile-Ranks-Aggregation" class="headerlink" title="7.3.10 Percentile Ranks Aggregation"></a>7.3.10 Percentile Ranks Aggregation</h3><p>A multi-value metrics aggregation that calculates one or more percentile ranks over numeric values extracted from the aggregated documents.</p>
<p>Percentile rank show the percentage of observed values which are below certain value. For example, if a value is greater than or equal to 95% of the observed values it is said to be at the 95th percentile rank.</p>
<pre><code>&quot;aggs&quot;: {
    &quot;price_outlier&quot;: {
      &quot;percentile_ranks&quot;: {
        &quot;field&quot;: &quot;salesPrice&quot;,
        &quot;values&quot;: [
          200,
          500
        ]
      }
    }
  }

&quot;aggregations&quot;: {
    &quot;price_outlier&quot;: {
        &quot;values&quot;: {
            &quot;200.0&quot;: 37.906112721751086,
            &quot;500.0&quot;: 74.407593883831
        }
    }
}
</code></pre><h3 id="7-3-11-Scripted-Metric-Aggregation-experimental-and-may-be-changed-or-removed"><a href="#7-3-11-Scripted-Metric-Aggregation-experimental-and-may-be-changed-or-removed" class="headerlink" title="7.3.11 Scripted Metric Aggregation (experimental and may be changed or removed)"></a>7.3.11 Scripted Metric Aggregation (experimental and may be changed or removed)</h3><p>A metric aggregation that executes using scripts to provide a metric output.</p>
<p>See a detailed example: <a href="https://www.elastic.co/guide/en/elasticsearch/reference/2.3/search-aggregations-metrics-scripted-metric-aggregation.html" target="_blank" rel="external">https://www.elastic.co/guide/en/elasticsearch/reference/2.3/search-aggregations-metrics-scripted-metric-aggregation.html</a></p>
<h3 id="7-3-12-Sum-Aggregation"><a href="#7-3-12-Sum-Aggregation" class="headerlink" title="7.3.12 Sum Aggregation"></a>7.3.12 Sum Aggregation</h3><p>A single-value metrics aggregation that sums up numeric values that are extracted from the aggregated documents.</p>
<pre><code>{
  &quot;query&quot;: {
    &quot;term&quot;: {
      &quot;brandName&quot;: &quot;vans&quot;
    }
  },
  &quot;aggs&quot;: {
    &quot;salesNum_total&quot;: {
      &quot;sum&quot;: {
        &quot;field&quot;: &quot;salesNum&quot;
      }
    }
  }
}

&quot;aggregations&quot;: {
    &quot;salesNum_total&quot;: {
        &quot;value&quot;: 253365
    }
}
</code></pre><h3 id="7-3-13-Top-hits-Aggregation"><a href="#7-3-13-Top-hits-Aggregation" class="headerlink" title="7.3.13 Top hits Aggregation"></a>7.3.13 Top hits Aggregation</h3><p>A top_hits metric aggregator keeps track of the most relevant document being aggregated. This aggregator is intended to be used as a sub aggregator, so that the top matching documents can be aggregated per bucket.</p>
<p>The top_hits aggregator can effectively be used to group result sets by certain fields via a bucket aggregator. One or more bucket aggregators determines by which properties a result set get sliced into.</p>
<p>Options:</p>
<ul>
<li>from: The offset from the first result you want to fetch.</li>
<li>size: The maximum number of top matching hits to return per bucket. By default the top three matching hits are returned.</li>
<li>sort: How the top matching hits should be sorted. By default the hits are sorted by the score of the main query.   </li>
</ul>
<h3 id="7-3-14-Value-Count-Aggregation"><a href="#7-3-14-Value-Count-Aggregation" class="headerlink" title="7.3.14 Value Count Aggregation"></a>7.3.14 Value Count Aggregation</h3><p>A single-value metrics aggregation that counts the number of values that are extracted from the aggregated documents. Typically, this aggregator will be used in conjunction with other single-value aggregations. For example, when computing the avg one might be interested in the number of values the average is computed over.</p>
<h2 id="7-4-Bucket-Aggregations"><a href="#7-4-Bucket-Aggregations" class="headerlink" title="7.4 Bucket Aggregations"></a>7.4 Bucket Aggregations</h2><p>Bucket aggregations don’t calculate metrics over fields like the metrics aggregations do, but instead, they create buckets of documents. Each bucket is associated with a criterion (depending on the aggregation type) which determines whether or not a document in the current context “falls” into it. In other words, the buckets effectively define document sets. In addition to the buckets themselves, the bucket aggregations also compute and return the number of documents that “fell into” each bucket.</p>
<p>Bucket aggregations, as opposed to metrics aggregations, can hold sub-aggregations. These sub-aggregations will be aggregated for the buckets created by their “parent” bucket aggregation.</p>
<p>There are different bucket aggregators, each with a different “bucketing” strategy. Some define a single bucket, some define fixed number of multiple buckets, and others dynamically create the buckets during the aggregation process.</p>
<h3 id="7-4-1-Children-Aggregation"><a href="#7-4-1-Children-Aggregation" class="headerlink" title="7.4.1 Children Aggregation"></a>7.4.1 Children Aggregation</h3><p>A special single bucket aggregation that enables aggregating from buckets on parent document types to buckets on child documents.</p>
<h3 id="7-4-2-Histogram-Aggregation"><a href="#7-4-2-Histogram-Aggregation" class="headerlink" title="7.4.2 Histogram Aggregation"></a>7.4.2 Histogram Aggregation</h3><p>A multi-bucket values source based aggregation that can be applied on numeric values extracted from the documents. It dynamically builds fixed size (a.k.a. interval) buckets over the values. For example, if the documents have a field that holds a price (numeric), we can configure this aggregation to dynamically build buckets with interval 5 (in case of price it may represent $5). When the aggregation executes, the price field of every document will be evaluated and will be rounded down to its closest bucket - for example, if the price is 32 and the bucket size is 5 then the rounding will yield 30 and thus the document will “fall” into the bucket that is associated with the key 30.</p>
<p>From the rounding function above it can be seen that the intervals themselves must be integers.</p>
<pre><code>{
  &quot;size&quot;: 0,
  &quot;query&quot;: {
    &quot;term&quot;: {
      &quot;brandName&quot;: &quot;vans&quot;
    }
  },
  &quot;aggs&quot;: {
    &quot;prices&quot;: {
      &quot;histogram&quot;: {
        &quot;field&quot;: &quot;salesPrice&quot;,
        &quot;interval&quot;: 200
      }
    }
  }
}

&quot;aggregations&quot;: {
    &quot;prices&quot;: {
    &quot;buckets&quot;: [
        {
        &quot;key&quot;: 0,
        &quot;doc_count&quot;: 838
        }
        ,
        {
        &quot;key&quot;: 200,
        &quot;doc_count&quot;: 1123
        }
        ,
        {
        &quot;key&quot;: 400,
        &quot;doc_count&quot;: 804
        }
        ,
        {
        &quot;key&quot;: 600,
        &quot;doc_count&quot;: 283
        }
        ,
        {
        &quot;key&quot;: 800,
        &quot;doc_count&quot;: 64
        }
        ,
        {
        &quot;key&quot;: 1000,
        &quot;doc_count&quot;: 16
        }
        ,
        {
        &quot;key&quot;: 1200,
        &quot;doc_count&quot;: 18
        }
        ,
        {
        &quot;key&quot;: 1400,
        &quot;doc_count&quot;: 8
        }
        ,
        {
        &quot;key&quot;: 1600,
        &quot;doc_count&quot;: 7
        }
        ]
    }
}
</code></pre><h3 id="7-4-3-Date-Histogram-Aggregation"><a href="#7-4-3-Date-Histogram-Aggregation" class="headerlink" title="7.4.3 Date Histogram Aggregation"></a>7.4.3 Date Histogram Aggregation</h3><p>A multi-bucket aggregation similar to the histogram except it can only be applied on date values. Since dates are represented in elasticsearch internally as long values, it is possible to use the normal histogram on dates as well, though accuracy will be compromised. The reason for this is in the fact that time based intervals are not fixed (think of leap years and on the number of days in a month). For this reason, we need special support for time based data. From a functionality perspective, this histogram supports the same features as the normal histogram. The main difference is that the interval can be specified by date/time expressions.</p>
<p>Requesting bucket intervals of a month.</p>
<pre><code>{
    &quot;aggs&quot; : {
        &quot;articles_over_time&quot; : {
            &quot;date_histogram&quot; : {
                &quot;field&quot; : &quot;date&quot;,
                &quot;interval&quot; : &quot;month&quot;
            }
        }
    }
}
</code></pre><h3 id="7-4-4-Range-Aggregation"><a href="#7-4-4-Range-Aggregation" class="headerlink" title="7.4.4 Range Aggregation"></a>7.4.4 Range Aggregation</h3><p>A multi-bucket value source based aggregation that enables the user to define a set of ranges - each representing a bucket. During the aggregation process, the values extracted from each document will be checked against each bucket range and “bucket” the relevant/matching document. Note that this aggregation includes the from value and excludes the to value for each range.</p>
<pre><code>{
  &quot;size&quot;: 0,
  &quot;query&quot;: {
    &quot;term&quot;: {
      &quot;brandName&quot;: &quot;vans&quot;
    }
  },
  &quot;aggs&quot;: {
    &quot;price_ranges&quot;: {
      &quot;range&quot;: {
        &quot;field&quot;: &quot;salesPrice&quot;,
        &quot;ranges&quot;: [
          {
            &quot;to&quot;: 200
          },
          {
            &quot;from&quot;: 200,
            &quot;to&quot;: 500
          },
          {
            &quot;from&quot;: 500
          }
        ]
      }
    }
  }
}

&quot;aggregations&quot;: {
    &quot;price_ranges&quot;: {
        &quot;buckets&quot;: [
            {
                &quot;key&quot;: &quot;*-200.0&quot;,
                &quot;to&quot;: 200,
                &quot;to_as_string&quot;: &quot;200.0&quot;,
                &quot;doc_count&quot;: 838
            }
            ,
            {
                &quot;key&quot;: &quot;200.0-500.0&quot;,
                &quot;from&quot;: 200,
                &quot;from_as_string&quot;: &quot;200.0&quot;,
                &quot;to&quot;: 500,
                &quot;to_as_string&quot;: &quot;500.0&quot;,
                &quot;doc_count&quot;: 1594
            }
            ,
            {
                &quot;key&quot;: &quot;500.0-*&quot;,
                &quot;from&quot;: 500,
                &quot;from_as_string&quot;: &quot;500.0&quot;,
                &quot;doc_count&quot;: 729
            }
        ]
    }
}
</code></pre><h3 id="7-4-5-Date-Range-Aggregation"><a href="#7-4-5-Date-Range-Aggregation" class="headerlink" title="7.4.5 Date Range Aggregation"></a>7.4.5 Date Range Aggregation</h3><p>A range aggregation that is dedicated for date values. The main difference between this aggregation and the normal range aggregation is that the from and to values can be expressed in Date Math expressions, and it is also possible to specify a date format by which the from and to response fields will be returned. Note that this aggregation includes the from value and excludes the to value for each range.</p>
<pre><code>{
    &quot;aggs&quot;: {
        &quot;range&quot;: {
            &quot;date_range&quot;: {
                &quot;field&quot;: &quot;date&quot;,
                &quot;format&quot;: &quot;MM-yyy&quot;,
                &quot;ranges&quot;: [
                    { &quot;to&quot;: &quot;now-10M/M&quot; }, 
                    { &quot;from&quot;: &quot;now-10M/M&quot; } 
                ]
            }
        }
    }
}
</code></pre><h3 id="7-4-6-Filter-Aggregation"><a href="#7-4-6-Filter-Aggregation" class="headerlink" title="7.4.6 Filter Aggregation"></a>7.4.6 Filter Aggregation</h3><p>Defines a single bucket of all the documents in the current document set context that match a specified filter. Often this will be used to narrow down the current aggregation context to a specific set of documents.</p>
<pre><code>{
  &quot;size&quot;: 0,
  &quot;query&quot;: {
    &quot;term&quot;: {
      &quot;brandName&quot;: &quot;vans&quot;
    }
  },
  &quot;aggs&quot;: {
    &quot;red_products&quot;: {
      &quot;filter&quot;: {
        &quot;term&quot;: {
          &quot;colorNames&quot;: &quot;红色&quot;
        }
      },
      &quot;aggs&quot;: {
        &quot;avg_price&quot;: {
          &quot;avg&quot;: {
            &quot;field&quot;: &quot;salesPrice&quot;
          }
        }
      }
    }
  }
}
</code></pre><h3 id="7-4-7-Filters-Aggregation"><a href="#7-4-7-Filters-Aggregation" class="headerlink" title="7.4.7 Filters Aggregation"></a>7.4.7 Filters Aggregation</h3><p>Defines a multi bucket aggregation where each bucket is associated with a filter. Each bucket will collect all documents that match its associated filter.</p>
<pre><code>{
  &quot;aggs&quot; : {
    &quot;messages&quot; : {
      &quot;filters&quot; : {
        &quot;filters&quot; : {
          &quot;errors&quot; :   { &quot;term&quot; : { &quot;body&quot; : &quot;error&quot;   }},
          &quot;warnings&quot; : { &quot;term&quot; : { &quot;body&quot; : &quot;warning&quot; }}
        }
      },
      &quot;aggs&quot; : {
        &quot;monthly&quot; : {
          &quot;histogram&quot; : {
            &quot;field&quot; : &quot;timestamp&quot;,
            &quot;interval&quot; : &quot;1M&quot;
          }
        }
      }
    }
  }
}
</code></pre><p>In the above example, we analyze log messages. The aggregation will build two collection (buckets) of log messages - one for all those containing an error, and another for all those containing a warning. And for each of these buckets it will break them down by month.Response:</p>
<pre><code>&quot;aggs&quot; : {
  &quot;messages&quot; : {
    &quot;buckets&quot; : {
      &quot;errors&quot; : {
        &quot;doc_count&quot; : 34,
        &quot;monthly&quot; : {
          &quot;buckets&quot; : [
            ... // the histogram monthly breakdown
          ]
        }
      },
      &quot;warnings&quot; : {
        &quot;doc_count&quot; : 439,
        &quot;monthly&quot; : {
          &quot;buckets&quot; : [
             ... // the histogram monthly breakdown
          ]
        }
      }
    }
  }
}
</code></pre><h3 id="7-4-8-Geo-Distance-Aggregation-Skipped"><a href="#7-4-8-Geo-Distance-Aggregation-Skipped" class="headerlink" title="7.4.8 Geo Distance Aggregation (Skipped)"></a>7.4.8 Geo Distance Aggregation (Skipped)</h3><h3 id="7-4-9-GeoHash-grid-Aggregation-Skipped"><a href="#7-4-9-GeoHash-grid-Aggregation-Skipped" class="headerlink" title="7.4.9 GeoHash grid Aggregation (Skipped)"></a>7.4.9 GeoHash grid Aggregation (Skipped)</h3><h3 id="7-4-10-Global-Aggregation"><a href="#7-4-10-Global-Aggregation" class="headerlink" title="7.4.10 Global Aggregation"></a>7.4.10 Global Aggregation</h3><p>Defines a single bucket of all the documents within the search execution context. This context is defined by the indices and the document types you’re searching on, but is not influenced by the search query itself.</p>
<h3 id="7-4-11-IPv4-Range-Aggregation"><a href="#7-4-11-IPv4-Range-Aggregation" class="headerlink" title="7.4.11 IPv4 Range Aggregation"></a>7.4.11 IPv4 Range Aggregation</h3><p>Just like the dedicated date range aggregation, there is also a dedicated range aggregation for IPv4 typed fields:</p>
<h3 id="7-4-12-Missing-Aggregation"><a href="#7-4-12-Missing-Aggregation" class="headerlink" title="7.4.12 Missing Aggregation"></a>7.4.12 Missing Aggregation</h3><p>A field data based single bucket aggregation, that creates a bucket of all documents in the current document set context that are missing a field value (effectively, missing a field or having the configured NULL value set). This aggregator will often be used in conjunction with other field data bucket aggregators (such as ranges) to return information for all the documents that could not be placed in any of the other buckets due to missing field data values.</p>
<h3 id="7-4-13-Nested-Aggregation"><a href="#7-4-13-Nested-Aggregation" class="headerlink" title="7.4.13 Nested Aggregation"></a>7.4.13 Nested Aggregation</h3><p>A special single bucket aggregation that enables aggregating nested documents.</p>
<h3 id="7-4-14-Reverse-nested-Aggregation"><a href="#7-4-14-Reverse-nested-Aggregation" class="headerlink" title="7.4.14 Reverse nested Aggregation"></a>7.4.14 Reverse nested Aggregation</h3><p>A special single bucket aggregation that enables aggregating on parent docs from nested documents. Effectively this aggregation can break out of the nested block structure and link to other nested structures or the root document, which allows nesting other aggregations that aren’t part of the nested object in a nested aggregation.</p>
<h3 id="7-4-15-Significant-Terms-Aggregation"><a href="#7-4-15-Significant-Terms-Aggregation" class="headerlink" title="7.4.15 Significant Terms Aggregation"></a>7.4.15 Significant Terms Aggregation</h3><p>An aggregation that returns interesting or unusual occurrences of terms in a set.</p>
<blockquote>
<p>Warning: The significant_terms aggregation can be very heavy when run on large indices. Work is in progress to provide more lightweight sampling techniques. As a result, the API for this feature may change in backwards incompatible ways.</p>
</blockquote>
<p>Example use cases:</p>
<ul>
<li>Suggesting “H5N1” when users search for “bird flu” in text</li>
<li>Identifying the merchant that is the “common point of compromise” from the transaction history of credit card owners reporting loss</li>
<li>Suggesting keywords relating to stock symbol $ATI for an automated news classifier</li>
<li>Spotting the fraudulent doctor who is diagnosing more than his fair share of whiplash injuries</li>
<li><p>Spotting the tire manufacturer who has a disproportionate number of blow-outs</p>
<p>  {</p>
<pre><code>&quot;query&quot;: {
  &quot;terms&quot;: {
    &quot;smallSort&quot;: [
      &quot;牛仔裤&quot;
    ]
  }
},
&quot;aggregations&quot;: {
  &quot;significantColors&quot;: {
    &quot;significant_terms&quot;: {
      &quot;field&quot;: &quot;colorNames&quot;
    }
  }
}
</code></pre><p>  }</p>
<p>  “aggregations”: {</p>
<pre><code>&quot;significantColors&quot;: {
    &quot;doc_count&quot;: 7365,
    &quot;buckets&quot;: [
        {
            &quot;key&quot;: &quot;蓝色&quot;,
            &quot;doc_count&quot;: 4750,
            &quot;score&quot;: 1.9775784118031037,
            &quot;bg_count&quot;: 35656
        }
        ,
        {
            &quot;key&quot;: &quot;蓝&quot;,
            &quot;doc_count&quot;: 1287,
            &quot;score&quot;: 0.35538801144606,
            &quot;bg_count&quot;: 12949
        }
        ,
        {
            &quot;key&quot;: &quot;原色&quot;,
            &quot;doc_count&quot;: 39,
            &quot;score&quot;: 0.09168423890725523,
            &quot;bg_count&quot;: 65
        }
        ,
        {
            &quot;key&quot;: &quot;浅蓝色&quot;,
            &quot;doc_count&quot;: 79,
            &quot;score&quot;: 0.031059308568117887,
            &quot;bg_count&quot;: 619
        }
        ,
        {
            &quot;key&quot;: &quot;水洗&quot;,
            &quot;doc_count&quot;: 10,
            &quot;score&quot;: 0.030522422208204166,
            &quot;bg_count&quot;: 13
        }
        ,
        {
            &quot;key&quot;: &quot;深蓝色&quot;,
            &quot;doc_count&quot;: 131,
            &quot;score&quot;: 0.024955048079471253,
            &quot;bg_count&quot;: 1664
        }
    ]
}
</code></pre><p>  }</p>
</li>
</ul>
<p>GINO: 为什么深蓝色排在最后？<br>在所有的商品(总数为224778)中，共有1664件商品为深蓝色；但是对于牛仔裤(总数为7365)，只有131件牛仔裤为深蓝色，因此认为他们之间的关联度很低，就不是很推荐深蓝色的牛仔裤。再试试看品牌推荐的效果：</p>
<pre><code>{
  &quot;query&quot;: {
    &quot;terms&quot;: {
      &quot;smallSort&quot;: [
        &quot;牛仔裤&quot;
      ]
    }
  },
  &quot;aggregations&quot;: {
    &quot;significantBrands&quot;: {
      &quot;significant_terms&quot;: {
        &quot;field&quot;: &quot;brandNameEn&quot;
      }
    }
  }
}

&quot;aggregations&quot;: {
    &quot;significantBrands&quot;: {
        &quot;doc_count&quot;: 7365,
        &quot;buckets&quot;: [
        {
            &quot;key&quot;: &quot;xinfeiyang&quot;,
            &quot;doc_count&quot;: 1061,
            &quot;score&quot;: 4.179762739096525,
            &quot;bg_count&quot;: 1079
        }
        ,
        {
            &quot;key&quot;: &quot;lee&quot;,
            &quot;doc_count&quot;: 432,
            &quot;score&quot;: 0.5581216486055066,
            &quot;bg_count&quot;: 1254
        }
        ,
        {
            &quot;key&quot;: &quot;levi&apos;s&quot;,
            &quot;doc_count&quot;: 473,
            &quot;score&quot;: 0.5004641576199044,
            &quot;bg_count&quot;: 1642
        }
        ,
        {
            &quot;key&quot;: &quot;jasonwood&quot;,
            &quot;doc_count&quot;: 495,
            &quot;score&quot;: 0.3789564299098067,
            &quot;bg_count&quot;: 2276
        }
        ,
        {
            &quot;key&quot;: &quot;able&quot;,
            &quot;doc_count&quot;: 304,
            &quot;score&quot;: 0.36273857444325336,
            &quot;bg_count&quot;: 948
        }
        ,
        {
            &quot;key&quot;: &quot;jeans&quot;,
            &quot;doc_count&quot;: 307,
            &quot;score&quot;: 0.3581144552023549,
            &quot;bg_count&quot;: 977
        }
        ,
        {
            &quot;key&quot;: &quot;agamemnon&quot;,
            &quot;doc_count&quot;: 119,
            &quot;score&quot;: 0.3311112854845905,
            &quot;bg_count&quot;: 169
        }
        ,
        {
            &quot;key&quot;: &quot;krbl/korakublue&quot;,
            &quot;doc_count&quot;: 183,
            &quot;score&quot;: 0.31616211163218183,
            &quot;bg_count&quot;: 407
        }
        ,
        {
            &quot;key&quot;: &quot;wrangler&quot;,
            &quot;doc_count&quot;: 198,
            &quot;score&quot;: 0.264814268306479,
            &quot;bg_count&quot;: 557
        }
        ,
        {
            &quot;key&quot;: &quot;evisu&quot;,
            &quot;doc_count&quot;: 143,
            &quot;score&quot;: 0.19733121908058618,
            &quot;bg_count&quot;: 391
        }
        ]
    }
}
</code></pre><h3 id="7-4-16-Sampler-Aggregation-experimental-and-may-be-changed-or-removed"><a href="#7-4-16-Sampler-Aggregation-experimental-and-may-be-changed-or-removed" class="headerlink" title="7.4.16 Sampler Aggregation (experimental and may be changed or removed)"></a>7.4.16 Sampler Aggregation (experimental and may be changed or removed)</h3><p>A filtering aggregation used to limit any sub aggregations’ processing to a sample of the top-scoring documents. Optionally, diversity settings can be used to limit the number of matches that share a common value such as an “author”.</p>
<p>Example use cases:</p>
<ul>
<li>Tightening the focus of analytics to high-relevance matches rather than the potentially very long tail of low-quality matches</li>
<li>Removing bias from analytics by ensuring fair representation of content from different sources</li>
<li>Reducing the running cost of aggregations that can produce useful results using only samples e.g. significant_terms</li>
</ul>
<h3 id="7-4-17-Terms-Aggregation"><a href="#7-4-17-Terms-Aggregation" class="headerlink" title="7.4.17 Terms Aggregation"></a>7.4.17 Terms Aggregation</h3><p>A multi-bucket value source based aggregation where buckets are dynamically built - one per unique value.</p>
<pre><code>{
  &quot;query&quot;: {
    &quot;terms&quot;: {
      &quot;smallSort&quot;: [
        &quot;牛仔裤&quot;
      ]
    }
  },
  &quot;aggs&quot;: {
    &quot;genders&quot;: {
      &quot;terms&quot;: {
        &quot;field&quot;: &quot;genderS&quot;
      }
    }
  }
}

&quot;aggregations&quot;: {
    &quot;genders&quot;: {
        &quot;doc_count_error_upper_bound&quot;: 0,
        &quot;sum_other_doc_count&quot;: 0,
        &quot;buckets&quot;: [
        {
            &quot;key&quot;: &quot;男&quot;,
            &quot;doc_count&quot;: 5271
        }
        ,
        {
            &quot;key&quot;: &quot;女&quot;,
            &quot;doc_count&quot;: 2255
        }
        ]
    }
}
</code></pre><p>The size parameter can be set to define how many term buckets should be returned out of the overall terms list. By default, the node coordinating the search process will request each shard to provide its own top size term buckets and once all shards respond, it will reduce the results to the final list that will then be returned to the client. This means that if the number of unique terms is greater than size, the returned list is slightly off and not accurate (it could be that the term counts are slightly off and it could even be that a term that should have been in the top size buckets was not returned). If set to 0, the size will be set to Integer.MAX_VALUE.</p>
<h2 id="7-5-Pipeline-Aggregations-experimental-and-may-be-changed-or-removed-Skipped）"><a href="#7-5-Pipeline-Aggregations-experimental-and-may-be-changed-or-removed-Skipped）" class="headerlink" title="7.5 Pipeline Aggregations (experimental and may be changed or removed, Skipped）"></a>7.5 Pipeline Aggregations (experimental and may be changed or removed, Skipped）</h2><h2 id="7-6-Caching-heavy-aggregations"><a href="#7-6-Caching-heavy-aggregations" class="headerlink" title="7.6 Caching heavy aggregations"></a>7.6 Caching heavy aggregations</h2><p>Frequently used aggregations (e.g. for display on the home page of a website) can be cached for faster responses. These cached results are the same results that would be returned by an uncached aggregation – you will never get stale results.</p>
<p>See <a href="https://www.elastic.co/guide/en/elasticsearch/reference/2.3/shard-request-cache.html" target="_blank" rel="external">Shard request cache</a> for more details.</p>
<h2 id="7-7-Returning-only-aggregation-results"><a href="#7-7-Returning-only-aggregation-results" class="headerlink" title="7.7 Returning only aggregation results"></a>7.7 Returning only aggregation results</h2><p>There are many occasions when aggregations are required but search hits are not. For these cases the hits can be ignored by setting size=0. For example:</p>
<pre><code>curl -XGET &apos;http://localhost:9200/twitter/tweet/_search&apos; -d &apos;{
  &quot;size&quot;: 0,
  &quot;aggregations&quot;: {
    &quot;my_agg&quot;: {
      &quot;terms&quot;: {
        &quot;field&quot;: &quot;text&quot;
      }
    }
  }
}
&apos;
</code></pre><p>Setting size to 0 avoids executing the fetch phase of the search making the request more efficient.</p>
<h1 id="8-Indices-APIs"><a href="#8-Indices-APIs" class="headerlink" title="8 Indices APIs"></a>8 Indices APIs</h1><p>The indices APIs are used to manage individual indices, index settings, aliases, mappings, index templates and warmers.</p>
<h2 id="8-1-Create-Index"><a href="#8-1-Create-Index" class="headerlink" title="8.1 Create Index"></a>8.1 Create Index</h2><p>The create index API allows to instantiate an index. Elasticsearch provides support for multiple indices, including executing operations across several indices.</p>
<pre><code>curl -XPUT &apos;http://localhost:9200/twitter/&apos; -d &apos;{
    &quot;settings&quot; : {
        &quot;index&quot; : {
            &quot;number_of_shards&quot; : 3, 
            &quot;number_of_replicas&quot; : 2 
        }
    }
}&apos;
</code></pre><p>The create index API allows to provide a set of one or more mappings:</p>
<pre><code>curl -XPOST localhost:9200/test -d &apos;{
    &quot;settings&quot; : {
        &quot;number_of_shards&quot; : 1
    },
    &quot;mappings&quot; : {
        &quot;type1&quot; : {
            &quot;properties&quot; : {
                &quot;field1&quot; : { &quot;type&quot; : &quot;string&quot;, &quot;index&quot; : &quot;not_analyzed&quot; }
            }
        }
    }
}&apos;
</code></pre><h2 id="8-2-Delete-Index"><a href="#8-2-Delete-Index" class="headerlink" title="8.2 Delete Index"></a>8.2 Delete Index</h2><p>The delete index API allows to delete an existing index.</p>
<pre><code>curl -XDELETE &apos;http://localhost:9200/twitter/&apos;
</code></pre><h2 id="8-3-Get-Index"><a href="#8-3-Get-Index" class="headerlink" title="8.3 Get Index"></a>8.3 Get Index</h2><p>The get index API allows to retrieve information about one or more indexes.</p>
<pre><code>curl -XGET &apos;http://localhost:9200/twitter/&apos;
</code></pre><h2 id="8-4-Indices-Exists"><a href="#8-4-Indices-Exists" class="headerlink" title="8.4 Indices Exists"></a>8.4 Indices Exists</h2><p>Used to check if the index (indices) exists or not. For example:</p>
<pre><code>curl -XHEAD -i &apos;http://localhost:9200/twitter&apos;
</code></pre><p>The HTTP status code indicates if the index exists or not. A 404 means it does not exist, and 200 means it does.</p>
<h2 id="8-5-Open-Close-Index-API"><a href="#8-5-Open-Close-Index-API" class="headerlink" title="8.5 Open / Close Index API"></a>8.5 Open / Close Index API</h2><p>The open and close index APIs allow to close an index, and later on opening it. A closed index has almost no overhead on the cluster (except for maintaining its metadata), and is blocked for read/write operations. A closed index can be opened which will then go through the normal recovery process.</p>
<p>The REST endpoint is /{index}/_close and /{index}/_open. For example:</p>
<pre><code>curl -XPOST &apos;localhost:9200/my_index/_close&apos;
curl -XPOST &apos;localhost:9200/my_index/_open&apos;
</code></pre><h2 id="8-6-Put-Mapping"><a href="#8-6-Put-Mapping" class="headerlink" title="8.6 Put Mapping"></a>8.6 Put Mapping</h2><p>The PUT mapping API allows you to provide type mappings while creating a new index, add a new type to an existing index, or add new fields to an existing type:</p>
<pre><code>PUT twitter 
{
  &quot;mappings&quot;: {
    &quot;tweet&quot;: {
      &quot;properties&quot;: {
        &quot;message&quot;: {
          &quot;type&quot;: &quot;string&quot;
        }
      }
    }
  }
}

PUT twitter/_mapping/user 
{
  &quot;properties&quot;: {
    &quot;name&quot;: {
      &quot;type&quot;: &quot;string&quot;
    }
  }
}

PUT twitter/_mapping/tweet 
{
  &quot;properties&quot;: {
    &quot;user_name&quot;: {
      &quot;type&quot;: &quot;string&quot;
    }
  }
}
</code></pre><ul>
<li>Creates an index called twitter with the message field in the tweet mapping type.</li>
<li>Uses the PUT mapping API to add a new mapping type called user.</li>
<li>Uses the PUT mapping API to add a new field called user_name to the tweet mapping type.</li>
</ul>
<h2 id="8-7-Get-Mapping"><a href="#8-7-Get-Mapping" class="headerlink" title="8.7 Get Mapping"></a>8.7 Get Mapping</h2><p>The get mapping API allows to retrieve mapping definitions for an index or index/type.</p>
<pre><code>curl -XGET &apos;http://localhost:9200/twitter/_mapping/tweet&apos;
</code></pre><h2 id="8-8-Get-Field-Mapping"><a href="#8-8-Get-Field-Mapping" class="headerlink" title="8.8 Get Field Mapping"></a>8.8 Get Field Mapping</h2><p>The get field mapping API allows you to retrieve mapping definitions for one or more fields. This is useful when you do not need the complete type mapping returned by the Get Mapping API.</p>
<p>The following returns the mapping of the field text only:</p>
<pre><code>curl -XGET &apos;http://localhost:9200/twitter/_mapping/tweet/field/text&apos;
</code></pre><h2 id="8-9-Types-Exists"><a href="#8-9-Types-Exists" class="headerlink" title="8.9 Types Exists"></a>8.9 Types Exists</h2><p>Used to check if a type/types exists in an index/indices.</p>
<pre><code>curl -XHEAD -i &apos;http://localhost:9200/twitter/tweet&apos;
</code></pre><h2 id="8-10-Index-Aliases"><a href="#8-10-Index-Aliases" class="headerlink" title="8.10 Index Aliases"></a>8.10 Index Aliases</h2><p>APIs in elasticsearch accept an index name when working against a specific index, and several indices when applicable. The index aliases API allow to alias an index with a name, with all APIs automatically converting the alias name to the actual index name. An alias can also be mapped to more than one index, and when specifying it, the alias will automatically expand to the aliases indices. An alias can also be associated with a filter that will automatically be applied when searching, and routing values. An alias cannot have the same name as an index.</p>
<p>Here is a sample of associating the alias alias1 with index test1:</p>
<pre><code>curl -XPOST &apos;http://localhost:9200/_aliases&apos; -d &apos;
{
    &quot;actions&quot; : [
        { &quot;add&quot; : { &quot;index&quot; : &quot;test1&quot;, &quot;alias&quot; : &quot;alias1&quot; } }
    ]
}&apos;
</code></pre><h3 id="Filtered-Aliases"><a href="#Filtered-Aliases" class="headerlink" title="Filtered Aliases"></a>Filtered Aliases</h3><p>Aliases with filters provide an easy way to create different “views” of the same index. The filter can be defined using Query DSL and is applied to all Search, Count, Delete By Query and More Like This operations with this alias.</p>
<p>To create a filtered alias, first we need to ensure that the fields already exist in the mapping:</p>
<pre><code>curl -XPUT &apos;http://localhost:9200/test1&apos; -d &apos;{
  &quot;mappings&quot;: {
    &quot;type1&quot;: {
      &quot;properties&quot;: {
        &quot;user&quot; : {
          &quot;type&quot;: &quot;string&quot;,
          &quot;index&quot;: &quot;not_analyzed&quot;
        }
      }
    }
  }
}&apos;
</code></pre><p>Now we can create an alias that uses a filter on field user:</p>
<pre><code>curl -XPOST &apos;http://localhost:9200/_aliases&apos; -d &apos;{
    &quot;actions&quot; : [
        {
            &quot;add&quot; : {
                 &quot;index&quot; : &quot;test1&quot;,
                 &quot;alias&quot; : &quot;alias2&quot;,
                 &quot;filter&quot; : { &quot;term&quot; : { &quot;user&quot; : &quot;kimchy&quot; } }
            }
        }
    ]
}&apos;
</code></pre><p>An alias can also be added or deleted with the endpoint</p>
<pre><code>curl -XPUT &apos;localhost:9200/logs_201305/_alias/2013&apos;
curl -XDELETE &apos;localhost:9200/logs_201305/_alias/2013&apos;
curl -XGET &apos;localhost:9200/_alias/2013&apos;
curl -XHEAD -i &apos;localhost:9200/_alias/2013&apos;
</code></pre><h2 id="8-11-Update-Indices-Settings"><a href="#8-11-Update-Indices-Settings" class="headerlink" title="8.11 Update Indices Settings"></a>8.11 Update Indices Settings</h2><p>Change specific index level settings in real time.</p>
<p>The REST endpoint is /_settings (to update all indices) or {index}/_settings to update one (or more) indices settings.</p>
<pre><code>curl -XPUT &apos;localhost:9200/my_index/_settings&apos; -d &apos;
{
    &quot;index&quot; : {
        &quot;number_of_replicas&quot; : 4
    }
}&apos;
</code></pre><h3 id="Bulk-Indexing-Usage"><a href="#Bulk-Indexing-Usage" class="headerlink" title="Bulk Indexing Usage"></a>Bulk Indexing Usage</h3><p>For example, the update settings API can be used to dynamically change the index from being more performant for bulk indexing, and then move it to more real time indexing state. Before the bulk indexing is started, use:</p>
<pre><code>curl -XPUT localhost:9200/test/_settings -d &apos;{
    &quot;index&quot; : {
        &quot;refresh_interval&quot; : &quot;-1&quot;
    } }&apos;
</code></pre><p>(Another optimization option is to start the index without any replicas, and only later adding them, but that really depends on the use case).</p>
<p>Then, once bulk indexing is done, the settings can be updated (back to the defaults for example):</p>
<pre><code>curl -XPUT localhost:9200/test/_settings -d &apos;{
    &quot;index&quot; : {
        &quot;refresh_interval&quot; : &quot;1s&quot;
    } }&apos;
</code></pre><p>And, a force merge should be called:</p>
<pre><code>curl -XPOST &apos;http://localhost:9200/test/_forcemerge?max_num_segments=5&apos;
</code></pre><h2 id="8-12-Get-Settings"><a href="#8-12-Get-Settings" class="headerlink" title="8.12 Get Settings"></a>8.12 Get Settings</h2><p>The get settings API allows to retrieve settings of index/indices:</p>
<pre><code>curl -XGET &apos;http://localhost:9200/twitter/_settings&apos;
</code></pre><h2 id="8-13-Analyze"><a href="#8-13-Analyze" class="headerlink" title="8.13 Analyze"></a>8.13 Analyze</h2><p>Performs the analysis process on a text and return the tokens breakdown of the text.</p>
<p>Can be used without specifying an index against one of the many built in analyzers:</p>
<pre><code>curl -XGET &apos;localhost:9200/twitter/_analyze&apos; -d &apos;
{
  &quot;analyzer&quot; : &quot;standard&quot;,
  &quot;text&quot; : &quot;this is a test&quot;
}&apos;
</code></pre><p>All parameters can also supplied as request parameters. For example:</p>
<pre><code>curl -XGET &apos;localhost:9200/_analyze?tokenizer=keyword&amp;filter=lowercase&amp;text=this+is+a+test&apos;
</code></pre><h3 id="Explain-Analyze"><a href="#Explain-Analyze" class="headerlink" title="Explain Analyze"></a>Explain Analyze</h3><p>If you want to get more advanced details, set explain to true (defaults to false). It will output all token attributes for each token. You can filter token attributes you want to output by setting attributes option.</p>
<pre><code>POST productindex/_analyze
{
  &quot;field&quot;: &quot;productName.productName_ansj&quot;,
  &quot;text&quot;: &quot;S.T.A.M.P.S./诗坦表 时尚PU皮表带&quot;,
  &quot;explain&quot;: true
}
</code></pre><h2 id="8-14-Index-Templates"><a href="#8-14-Index-Templates" class="headerlink" title="8.14 Index Templates"></a>8.14 Index Templates</h2><p>Index templates allow you to define templates that will automatically be applied when new indices are created. The templates include both settings and mappings, and a simple pattern template that controls whether the template should be applied to the new index.</p>
<h2 id="8-15-Shadow-replica-indices-experimental-and-may-be-changed-or-removed"><a href="#8-15-Shadow-replica-indices-experimental-and-may-be-changed-or-removed" class="headerlink" title="8.15 Shadow replica indices(experimental and may be changed or removed)"></a>8.15 Shadow replica indices(experimental and may be changed or removed)</h2><p>If you would like to use a shared filesystem, you can use the shadow replicas settings to choose where on disk the data for an index should be kept, as well as how Elasticsearch should replay operations on all the replica shards of an index.</p>
<h2 id="8-16-Indices-Stats"><a href="#8-16-Indices-Stats" class="headerlink" title="8.16 Indices Stats"></a>8.16 Indices Stats</h2><p>Indices level stats provide statistics on different operations happening on an index. The API provides statistics on the index level scope (though most stats can also be retrieved using node level scope).</p>
<p>The following returns high level aggregation and index level stats for all indices:</p>
<pre><code>curl localhost:9200/_stats
</code></pre><p>Specific index stats can be retrieved using:</p>
<pre><code>curl localhost:9200/index1,index2/_stats
</code></pre><p>By default, all stats are returned, returning only specific stats can be specified as well in the URI. Those stats can be any of:</p>
<ul>
<li>docs:The number of docs / deleted docs (docs not yet merged out). Note, affected by refreshing the index.</li>
<li>store: The size of the index.</li>
<li>indexing: Indexing statistics, can be combined with a comma separated list of types to provide document type level stats.</li>
<li>get: Get statistics, including missing stats.</li>
<li>search: Search statistics. You can include statistics for custom groups by adding an extra groups parameter (search operations can be associated with one or more groups). The groups parameter accepts a comma separated list of group names. Use _all to return statistics for all groups.</li>
<li>completion: Completion suggest statistics.</li>
<li>fielddata: Fielddata statistics.</li>
<li>flush:Flush statistics.</li>
<li>merge:Merge statistics.</li>
<li>request_cache: Shard request cache statistics.</li>
<li>refresh: Refresh statistics.</li>
<li>suggest: Suggest statistics.</li>
<li>warmer: Warmer statistics.</li>
<li>translog: Translog statistics.</li>
</ul>
<h2 id="8-17-Indices-Segments"><a href="#8-17-Indices-Segments" class="headerlink" title="8.17 Indices Segments"></a>8.17 Indices Segments</h2><p>Provide low level segments information that a Lucene index (shard level) is built with. Allows to be used to provide more information on the state of a shard and an index, possibly optimization information, data “wasted” on deletes, and so on.</p>
<p>Endpoints include segments for a specific index, several indices, or all:</p>
<pre><code>curl -XGET &apos;http://localhost:9200/test/_segments&apos;
curl -XGET &apos;http://localhost:9200/test1,test2/_segments&apos;
curl -XGET &apos;http://localhost:9200/_segments&apos;
</code></pre><ul>
<li>_0: The key of the JSON document is the name of the segment. This name is used to generate file names: all files starting with this segment name in the directory of the shard belong to this segment.</li>
<li>generation: A generation number that is basically incremented when needing to write a new segment. The segment name is derived from this generation number.</li>
<li>num_docs: The number of non-deleted documents that are stored in this segment.</li>
<li>deleted_docs: The number of deleted documents that are stored in this segment. It is perfectly fine if this number is greater than 0, space is going to be reclaimed when this segment gets merged.</li>
<li>size_in_bytes: The amount of disk space that this segment uses, in bytes.</li>
<li>memory_in_bytes: Segments need to store some data into memory in order to be searchable efficiently. This number returns the number of bytes that are used for that purpose. A value of -1 indicates that Elasticsearch was not able to compute this number.</li>
<li>committed: Whether the segment has been sync’ed on disk. Segments that are committed would survive a hard reboot. No need to worry in case of false, the data from uncommitted segments is also stored in the transaction log so that Elasticsearch is able to replay changes on the next start.</li>
<li>search: Whether the segment is searchable. A value of false would most likely mean that the segment has been written to disk but no refresh occurred since then to make it searchable.</li>
<li>version: The version of Lucene that has been used to write this segment.</li>
<li>compound: Whether the segment is stored in a compound file. When true, this means that Lucene merged all files from the segment in a single one in order to save file descriptors.</li>
</ul>
<h2 id="8-18-Indices-Recovery-Advanced-Topic"><a href="#8-18-Indices-Recovery-Advanced-Topic" class="headerlink" title="8.18 Indices Recovery(Advanced Topic)"></a>8.18 Indices Recovery(Advanced Topic)</h2><p>The indices recovery API provides insight into on-going index shard recoveries. Recovery status may be reported for specific indices, or cluster-wide.</p>
<h2 id="8-19-Indices-Shard-Stores"><a href="#8-19-Indices-Shard-Stores" class="headerlink" title="8.19 Indices Shard Stores"></a>8.19 Indices Shard Stores</h2><p>Provides store information for shard copies of indices. Store information reports on which nodes shard copies exist, the shard copy version, indicating how recent they are, and any exceptions encountered while opening the shard index or from earlier engine failure.</p>
<p>By default, only lists store information for shards that have at least one unallocated copy. When the cluster health status is yellow, this will list store information for shards that have at least one unassigned replica. When the cluster health status is red, this will list store information for shards, which has unassigned primaries.</p>
<p>Endpoints include shard stores information for a specific index, several indices, or all:</p>
<pre><code>curl -XGET &apos;http://localhost:9200/test/_shard_stores&apos;
curl -XGET &apos;http://localhost:9200/test1,test2/_shard_stores&apos;
curl -XGET &apos;http://localhost:9200/_shard_stores&apos;
</code></pre><h2 id="8-20-Clear-Cache"><a href="#8-20-Clear-Cache" class="headerlink" title="8.20 Clear Cache"></a>8.20 Clear Cache</h2><p>The clear cache API allows to clear either all caches or specific cached associated with one or more indices.</p>
<pre><code>curl -XPOST &apos;http://localhost:9200/twitter/_cache/clear&apos;
</code></pre><p>The API, by default, will clear all caches. Specific caches can be cleaned explicitly by setting query, fielddata or request.</p>
<p>All caches relating to a specific field(s) can also be cleared by specifying fields parameter with a comma delimited list of the relevant fields.</p>
<h2 id="8-21-Flush"><a href="#8-21-Flush" class="headerlink" title="8.21 Flush"></a>8.21 Flush</h2><p>The flush API allows to flush one or more indices through an API. The flush process of an index basically frees memory from the index by flushing data to the index storage and clearing the internal transaction log. By default, Elasticsearch uses memory heuristics in order to automatically trigger flush operations as required in order to clear memory.</p>
<pre><code>POST /twitter/_flush
</code></pre><p>The flush API accepts the following request parameters:</p>
<ul>
<li>wait_if_ongoing: If set to true the flush operation will block until the flush can be executed if another flush operation is already executing. The default is false and will cause an exception to be thrown on the shard level if another flush operation is already running.</li>
<li>force: Whether a flush should be forced even if it is not necessarily needed ie. if no changes will be committed to the index. This is useful if transaction log IDs should be incremented even if no uncommitted changes are present. (This setting can be considered as internal)</li>
</ul>
<h2 id="8-22-Synced-Flush-Advanced-Topic"><a href="#8-22-Synced-Flush-Advanced-Topic" class="headerlink" title="8.22 Synced Flush(Advanced Topic)"></a>8.22 Synced Flush(Advanced Topic)</h2><p>Elasticsearch tracks the indexing activity of each shard. Shards that have not received any indexing operations for 5 minutes are automatically marked as inactive. This presents an opportunity for Elasticsearch to reduce shard resources and also perform a special kind of flush, called synced flush. A synced flush performs a normal flush, then adds a generated unique marker (sync_id) to all shards.</p>
<h2 id="8-23-Refresh"><a href="#8-23-Refresh" class="headerlink" title="8.23 Refresh"></a>8.23 Refresh</h2><p>The refresh API allows to explicitly refresh one or more index, making all operations performed since the last refresh available for search. The (near) real-time capabilities depend on the index engine used. For example, the internal one requires refresh to be called, but by default a refresh is scheduled periodically.</p>
<pre><code>curl -XPOST &apos;http://localhost:9200/twitter/_refresh&apos;
</code></pre><h2 id="8-24-Force-Merge"><a href="#8-24-Force-Merge" class="headerlink" title="8.24 Force Merge"></a>8.24 Force Merge</h2><p>The force merge API allows to force merging of one or more indices through an API. The merge relates to the number of segments a Lucene index holds within each shard. The force merge operation allows to reduce the number of segments by merging them.</p>
<p>This call will block until the merge is complete. If the http connection is lost, the request will continue in the background, and any new requests will block until the previous force merge is complete.</p>
<pre><code>curl -XPOST &apos;http://localhost:9200/twitter/_forcemerge&apos;
</code></pre><p>The force merge API accepts the following request parameters:</p>
<ul>
<li>max_num_segments: The number of segments to merge to. To fully merge the index, set it to 1. Defaults to simply checking if a merge needs to execute, and if so, executes it.</li>
<li>only_expunge_deletes: Should the merge process only expunge segments with deletes in it. In Lucene, a document is not deleted from a segment, just marked as deleted. During a merge process of segments, a new segment is created that does not have those deletes. This flag allows to only merge segments that have deletes. Defaults to false. Note that this won’t override the index.merge.policy.expunge_deletes_allowed threshold.</li>
<li>flush: Should a flush be performed after the forced merge. Defaults to true.</li>
</ul>
<h1 id="9-cat-APIs"><a href="#9-cat-APIs" class="headerlink" title="9 cat APIs"></a>9 cat APIs</h1><p>JSON is great… for computers. Even if it’s pretty-printed, trying to find relationships in the data is tedious. Human eyes, especially when looking at an ssh terminal, need compact and aligned text. The cat API aims to meet this need.</p>
<p>All the cat commands accept a query string parameter help to see all the headers and info they provide, and the /_cat command alone lists all the available commands.</p>
<h2 id="9-1-cat-aliases"><a href="#9-1-cat-aliases" class="headerlink" title="9.1 cat aliases"></a>9.1 cat aliases</h2><p>aliases shows information about currently configured aliases to indices including filter and routing infos.</p>
<pre><code>curl &apos;localhost:9200/_cat/aliases?v&apos;
</code></pre><h2 id="9-2-cat-allocation"><a href="#9-2-cat-allocation" class="headerlink" title="9.2 cat allocation"></a>9.2 cat allocation</h2><p>allocation provides a snapshot of how many shards are allocated to each data node and how much disk space they are using.</p>
<pre><code>curl &apos;localhost:9200/_cat/allocation?v&apos;
</code></pre><h2 id="9-3-cat-count"><a href="#9-3-cat-count" class="headerlink" title="9.3 cat count"></a>9.3 cat count</h2><p>count provides quick access to the document count of the entire cluster, or individual indices.</p>
<pre><code>curl &apos;localhost:9200/_cat/indices?v&apos;
green wiki1 3 0 10000 331 168.5mb 168.5mb
green wiki2 3 0   428   0     8mb     8mb
curl &apos;localhost:9200/_cat/count?v&apos;
1384314124582 19:42:04 10428
curl &apos;localhost:9200/_cat/count/wiki2?v&apos;
1384314139815 19:42:19 428
</code></pre><h2 id="9-4-cat-fielddata"><a href="#9-4-cat-fielddata" class="headerlink" title="9.4 cat fielddata"></a>9.4 cat fielddata</h2><p>fielddata shows how much heap memory is currently being used by fielddata on every data node in the cluster.</p>
<pre><code>curl &apos;localhost:9200/_cat/fielddata?v&apos;
id                     host    ip            node          total   body    text
c223lARiSGeezlbrcugAYQ myhost1 10.20.100.200 Jessica Jones 385.6kb 159.8kb 225.7kb
waPCbitNQaCL6xC8VxjAwg myhost2 10.20.100.201 Adversary     435.2kb 159.8kb 275.3kb
yaDkp-G3R0q1AJ-HUEvkSQ myhost3 10.20.100.202 Microchip     284.6kb 109.2kb 175.3kb
</code></pre><p>Fields can be specified either as a query parameter, or in the URL path.</p>
<h2 id="9-5-cat-health"><a href="#9-5-cat-health" class="headerlink" title="9.5 cat health"></a>9.5 cat health</h2><p>health is a terse, one-line representation of the same information from /_cluster/health. It has one option ts to disable the timestamping.</p>
<pre><code>curl &apos;localhost:9200/_cat/health?v&amp;ts=0&apos;
cluster status nodeTotal nodeData shards pri relo init unassign tasks
foo     green          3        3      3   3    0    0        0     0
</code></pre><h2 id="9-6-cat-indices"><a href="#9-6-cat-indices" class="headerlink" title="9.6 cat indices"></a>9.6 cat indices</h2><p>The indices command provides a cross-section of each index. This information spans nodes.</p>
<pre><code>curl &apos;localhost:9200/_cat/indices?v&apos;
</code></pre><h2 id="9-7-cat-master"><a href="#9-7-cat-master" class="headerlink" title="9.7 cat master"></a>9.7 cat master</h2><p>master doesn’t have any extra options. It simply displays the master’s node ID, bound IP address, and node name.</p>
<pre><code>curl &apos;localhost:9200/_cat/master?v&apos;
id                     ip            node
Ntgn2DcuTjGuXlhKDUD4vA 192.168.56.30 Solarr
</code></pre><h2 id="9-8-cat-nodeattrs"><a href="#9-8-cat-nodeattrs" class="headerlink" title="9.8 cat nodeattrs"></a>9.8 cat nodeattrs</h2><p>The nodeattrs command shows custom node attributes.</p>
<pre><code>curl &apos;localhost:9200/_cat/nodeattrs?v&apos;
node       host    ip          attr  value
Black Bolt epsilon 192.168.1.8 rack  rack314
Black Bolt epsilon 192.168.1.8 azone us-east-1    
</code></pre><h2 id="9-9-cat-nodes"><a href="#9-9-cat-nodes" class="headerlink" title="9.9 cat nodes"></a>9.9 cat nodes</h2><p>The nodes command shows the cluster topology.</p>
<pre><code>curl &apos;localhost:9200/_cat/nodes?v&apos;
SP4H 4727 192.168.56.30 9300 2.3.4 1.8.0_73 72.1gb 35.4 93.9mb 79 239.1mb 0.45 3.4h d m Boneyard
_uhJ 5134 192.168.56.10 9300 2.3.4 1.8.0_73 72.1gb 33.3 93.9mb 85 239.1mb 0.06 3.4h d * Athena
HfDp 4562 192.168.56.20 9300 2.3.4 1.8.0_73 72.2gb 74.5 93.9mb 83 239.1mb 0.12 3.4h d m Zarek
</code></pre><h2 id="9-10-cat-pending-tasks"><a href="#9-10-cat-pending-tasks" class="headerlink" title="9.10 cat pending tasks"></a>9.10 cat pending tasks</h2><p>pending_tasks provides the same information as the /_cluster/pending_tasks API in a convenient tabular format.</p>
<pre><code>curl &apos;localhost:9200/_cat/pending_tasks?v&apos;
</code></pre><h2 id="9-11-cat-plugins"><a href="#9-11-cat-plugins" class="headerlink" title="9.11 cat plugins"></a>9.11 cat plugins</h2><p>The plugins command provides a view per node of running plugins. This information spans nodes.</p>
<pre><code>curl &apos;localhost:9200/_cat/plugins?v&apos;
</code></pre><h2 id="9-12-cat-recovery"><a href="#9-12-cat-recovery" class="headerlink" title="9.12 cat recovery"></a>9.12 cat recovery</h2><p>The recovery command is a view of index shard recoveries, both on-going and previously completed. It is a more compact view of the JSON recovery API.</p>
<pre><code>curl &apos;localhost:9200/_cat/recovery?v&apos;
</code></pre><h2 id="9-13-cat-repositories"><a href="#9-13-cat-repositories" class="headerlink" title="9.13 cat repositories"></a>9.13 cat repositories</h2><p>The repositories command shows the snapshot repositories registered in the cluster.</p>
<pre><code>curl &apos;localhost:9200/_cat/repositories?v&apos;
</code></pre><h2 id="9-14-cat-thread-pool"><a href="#9-14-cat-thread-pool" class="headerlink" title="9.14 cat thread pool"></a>9.14 cat thread pool</h2><p>The thread_pool command shows cluster wide thread pool statistics per node. By default the active, queue and rejected statistics are returned for the bulk, index and search thread pools.</p>
<pre><code>curl &apos;localhost:9200/_cat/thread_pool?v&apos;
</code></pre><h2 id="9-15-cat-shards"><a href="#9-15-cat-shards" class="headerlink" title="9.15 cat shards"></a>9.15 cat shards</h2><p>The shards command is the detailed view of what nodes contain which shards. It will tell you if it’s a primary or replica, the number of docs, the bytes it takes on disk, and the node where it’s located.</p>
<p>Here we see a single index, with three primary shards and no replicas:</p>
<pre><code>curl &apos;localhost:9200/_cat/shards?v&apos;
</code></pre><h2 id="9-16-cat-segments"><a href="#9-16-cat-segments" class="headerlink" title="9.16 cat segments"></a>9.16 cat segments</h2><p>The segments command provides low level information about the segments in the shards of an index. It provides information similar to the _segments endpoint.</p>
<pre><code>curl &apos;http://localhost:9200/_cat/segments?v&apos;
</code></pre><h2 id="9-17-cat-snapshots"><a href="#9-17-cat-snapshots" class="headerlink" title="9.17 cat snapshots"></a>9.17 cat snapshots</h2><p>The snapshots command shows all snapshots that belong to a specific repository. To find a list of available repositories to query, the command /_cat/repositories can be used. Querying the snapshots of a repository named repo1 then looks as follows.</p>
<pre><code>curl &apos;localhost:9200/_cat/snapshots/repo1?v&apos;
</code></pre><h1 id="10-Cluster-APIs"><a href="#10-Cluster-APIs" class="headerlink" title="10. Cluster APIs"></a>10. Cluster APIs</h1><h2 id="10-1-Cluster-Health"><a href="#10-1-Cluster-Health" class="headerlink" title="10.1 Cluster Health"></a>10.1 Cluster Health</h2><p>The cluster health API allows to get a very simple status on the health of the cluster.</p>
<pre><code>curl &apos;http://localhost:9200/_cluster/health?pretty=true&apos;
{
  &quot;cluster_name&quot; : &quot;testcluster&quot;,
  &quot;status&quot; : &quot;green&quot;,
  &quot;timed_out&quot; : false,
  &quot;number_of_nodes&quot; : 2,
  &quot;number_of_data_nodes&quot; : 2,
  &quot;active_primary_shards&quot; : 5,
  &quot;active_shards&quot; : 10,
  &quot;relocating_shards&quot; : 0,
  &quot;initializing_shards&quot; : 0,
  &quot;unassigned_shards&quot; : 0,
  &quot;delayed_unassigned_shards&quot;: 0,
  &quot;number_of_pending_tasks&quot; : 0,
  &quot;number_of_in_flight_fetch&quot;: 0,
  &quot;task_max_waiting_in_queue_millis&quot;: 0,
  &quot;active_shards_percent_as_number&quot;: 100
}
</code></pre><h2 id="10-2-Cluster-State"><a href="#10-2-Cluster-State" class="headerlink" title="10.2 Cluster State"></a>10.2 Cluster State</h2><p>The cluster state API allows to get a comprehensive state information of the whole cluster.</p>
<pre><code>curl &apos;http://localhost:9200/_cluster/state&apos;
</code></pre><h2 id="10-3-Cluster-Stats"><a href="#10-3-Cluster-Stats" class="headerlink" title="10.3 Cluster Stats"></a>10.3 Cluster Stats</h2><p>The Cluster Stats API allows to retrieve statistics from a cluster wide perspective. The API returns basic index metrics (shard numbers, store size, memory usage) and information about the current nodes that form the cluster (number, roles, os, jvm versions, memory usage, cpu and installed plugins).</p>
<pre><code>curl -XGET &apos;http://localhost:9200/_cluster/stats?human&amp;pretty&apos;
</code></pre><h2 id="10-4-Pending-cluster-tasks"><a href="#10-4-Pending-cluster-tasks" class="headerlink" title="10.4 Pending cluster tasks"></a>10.4 Pending cluster tasks</h2><p>The pending cluster tasks API returns a list of any cluster-level changes (e.g. create index, update mapping, allocate or fail shard) which have not yet been executed.</p>
<pre><code>curl -XGET &apos;http://localhost:9200/_cluster/pending_tasks&apos;
</code></pre><h2 id="10-5-Cluster-Reroute-Advanced-Topic"><a href="#10-5-Cluster-Reroute-Advanced-Topic" class="headerlink" title="10.5 Cluster Reroute(Advanced Topic)"></a>10.5 Cluster Reroute(Advanced Topic)</h2><p>The reroute command allows to explicitly execute a cluster reroute allocation command including specific commands. For example, a shard can be moved from one node to another explicitly, an allocation can be canceled, or an unassigned shard can be explicitly allocated on a specific node.</p>
<p>Here is a short example of how a simple reroute API call:</p>
<pre><code>curl -XPOST &apos;localhost:9200/_cluster/reroute&apos; -d &apos;{
    &quot;commands&quot; : [ {
        &quot;move&quot; :
            {
              &quot;index&quot; : &quot;test&quot;, &quot;shard&quot; : 0,
              &quot;from_node&quot; : &quot;node1&quot;, &quot;to_node&quot; : &quot;node2&quot;
            }
        },
        {
          &quot;allocate&quot; : {
              &quot;index&quot; : &quot;test&quot;, &quot;shard&quot; : 1, &quot;node&quot; : &quot;node3&quot;
          }
        }
    ]
}&apos;
</code></pre><h2 id="10-6-Cluster-Update-Settings"><a href="#10-6-Cluster-Update-Settings" class="headerlink" title="10.6 Cluster Update Settings"></a>10.6 Cluster Update Settings</h2><p>Allows to update cluster wide specific settings. Settings updated can either be persistent (applied cross restarts) or transient (will not survive a full cluster restart). Here is an example:</p>
<pre><code>curl -XPUT localhost:9200/_cluster/settings -d &apos;{
    &quot;persistent&quot; : {
        &quot;discovery.zen.minimum_master_nodes&quot; : 2
    }
}&apos;
</code></pre><p>Or:</p>
<pre><code>curl -XPUT localhost:9200/_cluster/settings -d &apos;{
    &quot;transient&quot; : {
        &quot;discovery.zen.minimum_master_nodes&quot; : 2
    }
}&apos;
</code></pre><p>The cluster responds with the settings updated. So the response for the last example will be:</p>
<pre><code>{
    &quot;persistent&quot; : {},
    &quot;transient&quot; : {
        &quot;discovery.zen.minimum_master_nodes&quot; : &quot;2&quot;
    }
}
</code></pre><p>Cluster wide settings can be returned using:</p>
<pre><code>curl -XGET localhost:9200/_cluster/settings 
</code></pre><h2 id="10-7-Nodes-statistics"><a href="#10-7-Nodes-statistics" class="headerlink" title="10.7 Nodes statistics"></a>10.7 Nodes statistics</h2><p>The cluster nodes stats API allows to retrieve one or more (or all) of the cluster nodes statistics.</p>
<pre><code>curl -XGET &apos;http://localhost:9200/_nodes/stats&apos;
curl -XGET &apos;http://localhost:9200/_nodes/nodeId1,nodeId2/stats&apos;
</code></pre><p>By default, all stats are returned. You can limit this by combining any of indices, os, process, jvm, transport, http, fs, breaker and thread_pool. For example:</p>
<ul>
<li>indices: Indices stats about size, document count, indexing and deletion times, search times, field cache size, merges and flushes</li>
<li>fs: File system information, data path, free disk space, read/write stats (see FS information)</li>
<li>http: HTTP connection information</li>
<li>jvm: JVM stats, memory pool information, garbage collection, buffer pools, number of loaded/unloaded classes</li>
<li>os: Operating system stats, load average, mem, swap (see OS statistics)</li>
<li>process: Process statistics, memory consumption, cpu usage, open file descriptors (see Process statistics)</li>
<li>thread_pool: Statistics about each thread pool, including current size, queue and rejected tasks</li>
<li>transport: Transport statistics about sent and received bytes in cluster communication</li>
<li>breaker: Statistics about the field data circuit breaker</li>
</ul>
<h2 id="10-8-Nodes-Info"><a href="#10-8-Nodes-Info" class="headerlink" title="10.8 Nodes Info"></a>10.8 Nodes Info</h2><p>The cluster nodes info API allows to retrieve one or more (or all) of the cluster nodes information.</p>
<pre><code>curl -XGET &apos;http://localhost:9200/_nodes&apos;
curl -XGET &apos;http://localhost:9200/_nodes/nodeId1,nodeId2&apos;
</code></pre><p>By default, it just returns all attributes and core settings for a node. It also allows to get only information on settings, os, process, jvm, thread_pool, transport, http and plugins.</p>
<h2 id="10-9-Nodes-hot-threads"><a href="#10-9-Nodes-hot-threads" class="headerlink" title="10.9 Nodes hot_threads"></a>10.9 Nodes hot_threads</h2><p>An API allowing to get the current hot threads on each node in the cluster. Endpoints are /_nodes/hot_threads, and /_nodes/{nodesIds}/hot_threads.</p>
<pre><code>curl -XGET &apos;http://localhost:9200/_nodes/hot_threads&apos;
</code></pre><p>The output is plain text with a breakdown of each node’s top hot threads. Parameters allowed are:</p>
<ul>
<li>threads: number of hot threads to provide, defaults to 3.</li>
<li>interval: the interval to do the second sampling of threads. Defaults to 500ms.</li>
<li>type: The type to sample, defaults to cpu, but supports wait and block to see hot threads that are in wait or block state.</li>
<li>ignore_idle_threads: If true, known idle threads (e.g. waiting in a socket select, or to get a task from an empty queue) are filtered out. Defaults to true.</li>
</ul>
<h1 id="11-Query-DSL"><a href="#11-Query-DSL" class="headerlink" title="11. Query DSL"></a>11. Query DSL</h1><p>Elasticsearch provides a full Query DSL based on JSON to define queries. Think of the Query DSL as an AST of queries, consisting of two types of clauses:</p>
<ul>
<li>Leaf query clauses look for a particular value in a particular field, such as the match, term or range queries. These queries can be used by themselves.</li>
<li>Compound query clauses wrap other leaf or compound queries and are used to combine multiple queries in a logical fashion (such as the bool or dis_max query), or to alter their behaviour (such as the not or constant_score query).</li>
</ul>
<p>Query clauses behave differently depending on whether they are used in query context or filter context.</p>
<p><strong>Query context</strong><br>A query clause used in query context answers the question “How well does this document match this query clause?” Besides deciding whether or not the document matches, the query clause also calculates a _score representing how well the document matches, relative to other documents.</p>
<p>Query context is in effect whenever a query clause is passed to a query parameter, such as the query parameter in the search API.</p>
<p><strong>Filter context</strong><br>In filter context, a query clause answers the question “Does this document match this query clause?” The answer is a simple Yes or No – no scores are calculated. Filter context is mostly used for filtering structured data, e.g.</p>
<pre><code>Does this timestamp fall into the range 2015 to 2016?
Is the status field set to &quot;published&quot;?
</code></pre><p>Frequently used filters will be cached automatically by Elasticsearch, to speed up performance.</p>
<p>Filter context is in effect whenever a query clause is passed to a filter parameter, such as the filter or must_not parameters in the bool query, the filter parameter in the constant_score query, or the filter aggregation.</p>
<h2 id="11-1-Match-All-Query"><a href="#11-1-Match-All-Query" class="headerlink" title="11.1 Match All Query"></a>11.1 Match All Query</h2><p>The most simple query, which matches all documents, giving them all a _score of 1.0.</p>
<pre><code>{ &quot;match_all&quot;: {} }
</code></pre><h2 id="11-2-Full-text-queries"><a href="#11-2-Full-text-queries" class="headerlink" title="11.2 Full text queries"></a>11.2 Full text queries</h2><p>The high-level full text queries are usually used for running full text queries on full text fields like the body of an email. They understand how the field being queried is analyzed and will apply each field’s analyzer (or search_analyzer) to the query string before executing.</p>
<p>The queries in this group are:</p>
<ul>
<li>match query:The standard query for performing full text queries, including fuzzy matching and phrase or proximity queries.</li>
<li>multi_match query: The multi-field version of the match query.</li>
<li>common_terms query: A more specialized query which gives more preference to uncommon words.</li>
<li>query_string query: Supports the compact Lucene query string syntax, allowing you to specify AND|OR|NOT conditions and multi-field search within a single query string. For expert users only.</li>
<li>simple_query_string: A simpler, more robust version of the query_string syntax suitable for exposing directly to users.</li>
</ul>
<h3 id="11-2-1-Match-Query"><a href="#11-2-1-Match-Query" class="headerlink" title="11.2.1 Match Query"></a>11.2.1 Match Query</h3><p>A family of match queries that accepts text/numerics/dates, analyzes them, and constructs a query. For example:</p>
<pre><code>POST /productindex/_search
{
  &quot;query&quot;: {
    &quot;match&quot;: {
      &quot;productName.productName_ansj&quot;: &quot;vans鞋&quot;
    }
  }
}
</code></pre><p>There are three types of match query: boolean, phrase, and phrase_prefix:</p>
<p><strong>boolean</strong><br>The default match query is of type boolean. It means that the text provided is analyzed and the analysis process constructs a boolean query from the provided text. The <em>operator</em> flag can be set to or or and to control the boolean clauses (defaults to or). The minimum number of optional should clauses to match can be set using the <em>minimum_should_match</em> parameter.</p>
<p>The <em>analyzer</em> can be set to control which analyzer will perform the analysis process on the text. It defaults to the field explicit mapping definition, or the default search analyzer.</p>
<p>The <em>lenient</em> parameter can be set to true to ignore exceptions caused by data-type mismatches, such as trying to query a numeric field with a text query string. Defaults to false.</p>
<pre><code>POST /productindex/_search
{
  &quot;query&quot;: {
    &quot;match&quot;: {
      &quot;productName.productName_ansj&quot;: {
        &quot;query&quot;: &quot;vans鞋&quot;,
        &quot;operator&quot;: &quot;and&quot;
      }
    }
  }
}
</code></pre><p><strong>phrase</strong><br>The match_phrase query analyzes the text and creates a phrase query out of the analyzed text. For example:</p>
<pre><code>{
    &quot;match_phrase&quot; : {
        &quot;message&quot; : &quot;this is a test&quot;
    }
}
</code></pre><p>Since match_phrase is only a type of a match query, it can also be used in the following manner:</p>
<pre><code>{
    &quot;match&quot; : {
        &quot;message&quot; : {
            &quot;query&quot; : &quot;this is a test&quot;,
            &quot;type&quot; : &quot;phrase&quot;
        }
    }
}
</code></pre><p><strong>match_phrase_prefix</strong><br>The match_phrase_prefix is the same as match_phrase, except that it allows for prefix matches on the last term in the text. For example:</p>
<pre><code>{
    &quot;match_phrase_prefix&quot; : {
        &quot;message&quot; : &quot;quick brown f&quot;
    }
}
</code></pre><h3 id="11-2-2-Multi-Match-Query"><a href="#11-2-2-Multi-Match-Query" class="headerlink" title="11.2.2 Multi Match Query"></a>11.2.2 Multi Match Query</h3><p>The multi_match query builds on the match query to allow multi-field queries:</p>
<pre><code>{
  &quot;multi_match&quot; : {
    &quot;query&quot;:    &quot;this is a test&quot;, 
    &quot;fields&quot;: [ &quot;subject^3&quot;, &quot;message&quot; ] 
  }
}
</code></pre><p>Individual fields can be boosted with the caret (^) notation.</p>
<p><strong>Types of multi_match query:</strong></p>
<ul>
<li>best_fields: (default) Finds documents which match any field, but uses the _score from the best field. </li>
<li>most_fields: Finds documents which match any field and combines the _score from each field. </li>
<li>cross_fields: Treats fields with the same analyzer as though they were one big field. Looks for each word in any field. </li>
<li>phrase: Runs a match_phrase query on each field and combines the _score from each field. </li>
<li>phrase_prefix: Runs a match_phrase_prefix query on each field and combines the _score from each field. </li>
</ul>
<p><strong>best_fields</strong></p>
<pre><code>{
  &quot;multi_match&quot; : {
    &quot;query&quot;:      &quot;brown fox&quot;,
    &quot;type&quot;:       &quot;best_fields&quot;,
    &quot;fields&quot;:     [ &quot;subject^3&quot;, &quot;message&quot; ],
    &quot;tie_breaker&quot;: 0.3
  }
}
</code></pre><p>Normally the best_fields type uses the score of the single best matching field, but if tie_breaker is specified, then it calculates the score as follows:</p>
<ul>
<li>the score from the best matching field</li>
<li>plus tie_breaker * _score for all other matching fields</li>
</ul>
<p>Also, accepts analyzer, boost, operator, minimum_should_match, fuzziness, prefix_length, max_expansions, rewrite, zero_terms_query and cutoff_frequency, as explained in match query.</p>
<p>The best_fields and most_fields types are field-centric – they generate a match query per field. This means that the operator and minimum_should_match parameters are applied to each field individually, which is probably not what you want.In other words, <strong>all terms</strong> must be present <strong>in a single field</strong> for a document to match.</p>
<p><strong>cross_fields</strong><br>The cross_fields type is particularly useful with structured documents where multiple fields should match. For instance, when querying the first_name and last_name fields for “Will Smith”, the best match is likely to have “Will” in one field and “Smith” in the other.</p>
<p>This sounds like a job for most_fields but there are two problems with that approach：</p>
<ul>
<li>The first problem is that operator and minimum_should_match are applied per-field, instead of per-term (see explanation above).</li>
<li>The second problem is to do with relevance: the different term frequencies in the first_name and last_name fields can produce unexpected results.</li>
</ul>
<p>In other words, <strong>all terms</strong> must be present <strong>in at least one field</strong> for a document to match. (Compare this to the logic used for best_fields and most_fields.)</p>
<p>One way of dealing with these types of queries is simply to index the first_name and last_name fields into a single full_name field. Of course, this can only be done at index time.</p>
<p><strong>most_fields</strong><br>The most_fields type is most useful when querying multiple fields that contain the same text analyzed in different ways. For instance, the main field may contain synonyms, stemming and terms without diacritics. A second field may contain the original terms, and a third field might contain shingles. By combining scores from all three fields we can match as many documents as possible with the main field, but use the second and third fields to push the most similar results to the top of the list.</p>
<p><strong>phrase and phrase_prefix</strong><br>The phrase and phrase_prefix types behave just like best_fields, but they use a match_phrase or match_phrase_prefix query instead of a match query.</p>
<h3 id="11-2-3-Common-Terms-Query-Advanced-Topic"><a href="#11-2-3-Common-Terms-Query-Advanced-Topic" class="headerlink" title="11.2.3 Common Terms Query (Advanced Topic)"></a>11.2.3 Common Terms Query (Advanced Topic)</h3><p>The common terms query is a modern alternative to stopwords which improves the precision and recall of search results (by taking stopwords into account), without sacrificing performance.</p>
<p>The common terms query divides the query terms into two groups: more important (ie low frequency terms) and less important (ie high frequency terms which would previously have been stopwords).</p>
<p>First it searches for documents which match the more important terms. These are the terms which appear in fewer documents and have a greater impact on relevance.</p>
<p>Then, it executes a second query for the less important terms – terms which appear frequently and have a low impact on relevance. But instead of calculating the relevance score for all matching documents, it only calculates the _score for documents already matched by the first query. In this way the high frequency terms can improve the relevance calculation without paying the cost of poor performance.</p>
<h3 id="11-2-4-Query-String-Query-Advanced-Topic"><a href="#11-2-4-Query-String-Query-Advanced-Topic" class="headerlink" title="11.2.4 Query String Query (Advanced Topic)"></a>11.2.4 Query String Query (Advanced Topic)</h3><p>A query that uses a query parser in order to parse its content. Here is an example:</p>
<pre><code>{
    &quot;query_string&quot; : {
        &quot;default_field&quot; : &quot;content&quot;,
        &quot;query&quot; : &quot;this AND that OR thus&quot;
    }
}
</code></pre><h3 id="11-2-5-Simple-Query-String-Query-Advanced-Topic"><a href="#11-2-5-Simple-Query-String-Query-Advanced-Topic" class="headerlink" title="11.2.5 Simple Query String Query (Advanced Topic)"></a>11.2.5 Simple Query String Query (Advanced Topic)</h3><p>A query that uses the SimpleQueryParser to parse its context. Unlike the regular query_string query, the simple_query_string query will never throw an exception, and discards invalid parts of the query. Here is an example:</p>
<pre><code>{
    &quot;simple_query_string&quot; : {
        &quot;query&quot;: &quot;\&quot;fried eggs\&quot; +(eggplant | potato) -frittata&quot;,
        &quot;analyzer&quot;: &quot;snowball&quot;,
        &quot;fields&quot;: [&quot;body^5&quot;,&quot;_all&quot;],
        &quot;default_operator&quot;: &quot;and&quot;
    }
}
</code></pre><h2 id="11-3-Term-level-queries"><a href="#11-3-Term-level-queries" class="headerlink" title="11.3 Term level queries"></a>11.3 Term level queries</h2><p>While the full text queries will analyze the query string before executing, the term-level queries operate on the exact terms that are stored in the inverted index.</p>
<p>These queries are usually used for structured data like numbers, dates, and enums, rather than full text fields. Alternatively, they allow you to craft low-level queries, foregoing the analysis process.</p>
<p>The queries in this group are:</p>
<ul>
<li>term query: Find documents which contain the exact term specified in the field specified.</li>
<li>terms query: Find documents which contain any of the exact terms specified in the field specified.</li>
<li>range query: Find documents where the field specified contains values (dates, numbers, or strings) in the range specified.</li>
<li>exists query: Find documents where the field specified contains any non-null value.</li>
<li>missing query: Find documents where the field specified does is missing or contains only null values.</li>
<li>prefix query: Find documents where the field specified contains terms which begin with the exact prefix specified.</li>
<li>wildcard query: Find documents where the field specified contains terms which match the pattern specified, where the pattern supports single character wildcards (?) and multi-character wildcards (*)</li>
<li>regexp query: Find documents where the field specified contains terms which match the regular expression specified.</li>
<li>fuzzy query: Find documents where the field specified contains terms which are fuzzily similar to the specified term. Fuzziness is measured as a Levenshtein edit distance of 1 or 2.</li>
<li>type query: Find documents of the specified type.</li>
<li>ids query: Find documents with the specified type and IDs.</li>
</ul>
<h3 id="11-3-1-Term-Query"><a href="#11-3-1-Term-Query" class="headerlink" title="11.3.1 Term Query"></a>11.3.1 Term Query</h3><pre><code>{
  &quot;query&quot;: {
    &quot;term&quot;: {
      &quot;productSkn&quot;: &quot;51022624&quot;
    }
  }
}
</code></pre><p>A boost parameter can be specified to give this term query a higher relevance score than another query, for instance:</p>
<pre><code>GET /_search
{
  &quot;query&quot;: {
    &quot;bool&quot;: {
      &quot;should&quot;: [
        {
          &quot;term&quot;: {
            &quot;status&quot;: {
              &quot;value&quot;: &quot;urgent&quot;,
              &quot;boost&quot;: 2.0 
            }
          }
        },
        {
          &quot;term&quot;: {
            &quot;status&quot;: &quot;normal&quot; 
          }
        }
      ]
    }
  }
}
</code></pre><h3 id="11-3-2-Terms-Query"><a href="#11-3-2-Terms-Query" class="headerlink" title="11.3.2 Terms Query"></a>11.3.2 Terms Query</h3><p>Filters documents that have fields that match any of the provided terms (not analyzed). For example:</p>
<pre><code>{
  &quot;query&quot;: {
    &quot;terms&quot;: {
      &quot;brandId&quot;: [
        &quot;144&quot;,
        &quot;248&quot;
      ]
    }
  }
}

更高效的方法是使用constant_score.filter
{
    &quot;constant_score.&quot; : {
        &quot;filter&quot; : {
            &quot;terms&quot; : { &quot;brandId&quot; : [&quot;144&quot;, &quot;248&quot;]}
        }
    }
}

search on all the tweets that match the followers of user 2
curl -XGET localhost:9200/tweets/_search -d &apos;{
  &quot;query&quot; : {
    &quot;terms&quot; : {
      &quot;user&quot; : {
        &quot;index&quot; : &quot;users&quot;,
        &quot;type&quot; : &quot;user&quot;,
        &quot;id&quot; : &quot;2&quot;,
        &quot;path&quot; : &quot;followers&quot;
      }
    }
  }
}&apos;
</code></pre><h3 id="11-3-3-Range-Query"><a href="#11-3-3-Range-Query" class="headerlink" title="11.3.3 Range Query"></a>11.3.3 Range Query</h3><p>Matches documents with fields that have terms within a certain range. The type of the Lucene query depends on the field type, for string fields, the TermRangeQuery, while for number/date fields, the query is a NumericRangeQuery. </p>
<pre><code>{
  &quot;query&quot;: {
    &quot;range&quot;: {
      &quot;salesNum&quot;: {
        &quot;gte&quot;: &quot;5&quot;,
        &quot;lte&quot;: &quot;100&quot;
      }
    }
  }
}
</code></pre><p>The range query accepts the following parameters:</p>
<ul>
<li>gte： Greater-than or equal to</li>
<li>gt： Greater-than</li>
<li>lte： Less-than or equal to</li>
<li>lt： Less-than</li>
<li>boost： Sets the boost value of the query, defaults to 1.0</li>
</ul>
<h3 id="11-3-4-Exists-Query"><a href="#11-3-4-Exists-Query" class="headerlink" title="11.3.4 Exists Query"></a>11.3.4 Exists Query</h3><p>Returns documents that have at least one non-null value in the original field:</p>
<pre><code>{
    &quot;exists&quot; : { &quot;field&quot; : &quot;user&quot; }
}
</code></pre><p>The exists query can advantageously replace the missing query (Missing Query Deprecated in 2.2.0) when used inside a must_not clause as follows:</p>
<pre><code>&quot;bool&quot;: {
    &quot;must_not&quot;: {
        &quot;exists&quot;: {
            &quot;field&quot;: &quot;user&quot;
        }
    }
}
</code></pre><p>This query returns documents that have no value in the user field.</p>
<h3 id="11-3-5-Prefix-Query"><a href="#11-3-5-Prefix-Query" class="headerlink" title="11.3.5 Prefix Query"></a>11.3.5 Prefix Query</h3><p>Matches documents that have fields containing terms with a specified prefix (not analyzed). The prefix query maps to Lucene PrefixQuery. The following matches documents where the user field contains a term that starts with ki:</p>
<pre><code>{
  &quot;query&quot;: {
    &quot;prefix&quot;: {
      &quot;productName&quot;: &quot;VA&quot;
    }
  }
}
</code></pre><h3 id="11-3-6-Wildcard-Query"><a href="#11-3-6-Wildcard-Query" class="headerlink" title="11.3.6 Wildcard Query"></a>11.3.6 Wildcard Query</h3><p>Matches documents that have fields matching a wildcard expression (not analyzed). Supported wildcards are <em>, which matches any character sequence (including the empty one), and ?, which matches any single character. Note that this query can be slow, as it needs to iterate over many terms. In order to prevent extremely slow wildcard queries, a wildcard term should not start with one of the wildcards </em> or ?. The wildcard query maps to Lucene WildcardQuery.</p>
<pre><code>{
  &quot;query&quot;: {
    &quot;wildcard&quot;: {
      &quot;productName&quot;: &quot;VA*鞋*&quot;
    }
  }
}
</code></pre><h3 id="11-3-7-Regexp-Query"><a href="#11-3-7-Regexp-Query" class="headerlink" title="11.3.7 Regexp Query"></a>11.3.7 Regexp Query</h3><p>The regexp query allows you to use regular expression term queries. See Regular expression syntax for details of the supported regular expression language. The “term queries” in that first sentence means that Elasticsearch will apply the regexp to the terms produced by the tokenizer for that field, and not to the original text of the field.</p>
<p>Note: The performance of a regexp query heavily depends on the regular expression chosen. Matching everything like .<em> is very slow as well as using lookaround regular expressions. If possible, you should try to use a long prefix before your regular expression starts. Wildcard matchers like .</em>?+ will mostly lower performance.</p>
<pre><code>{
    &quot;regexp&quot;:{
        &quot;name.first&quot;: &quot;s.*y&quot;
    }
}
</code></pre><h3 id="11-3-8-Fuzzy-Query"><a href="#11-3-8-Fuzzy-Query" class="headerlink" title="11.3.8 Fuzzy Query"></a>11.3.8 Fuzzy Query</h3><p>The fuzzy query uses similarity based on Levenshtein edit distance for string fields, and a +/- margin on numeric and date fields.</p>
<p>The fuzzy query generates all possible matching terms that are within the maximum edit distance specified in fuzziness and then checks the term dictionary to find out which of those generated terms actually exist in the index.</p>
<p>Here is a simple example:</p>
<pre><code>{
    &quot;fuzzy&quot; : { &quot;user&quot; : &quot;ki&quot; }
}
</code></pre><h3 id="11-3-9-Type-Query"><a href="#11-3-9-Type-Query" class="headerlink" title="11.3.9 Type Query"></a>11.3.9 Type Query</h3><p>Filters documents matching the provided document / mapping type.</p>
<pre><code>{
    &quot;type&quot; : {
        &quot;value&quot; : &quot;my_type&quot;
    }
}
</code></pre><h3 id="11-3-10-Ids-Query"><a href="#11-3-10-Ids-Query" class="headerlink" title="11.3.10 Ids Query"></a>11.3.10 Ids Query</h3><p>Filters documents that only have the provided ids. Note, this query uses the _uid field.</p>
<pre><code>{
    &quot;ids&quot; : {
        &quot;type&quot; : &quot;my_type&quot;,
        &quot;values&quot; : [&quot;1&quot;, &quot;4&quot;, &quot;100&quot;]
    }
}
</code></pre><p>The type is optional and can be omitted, and can also accept an array of values. If no type is specified, all types defined in the index mapping are tried.</p>
<h2 id="11-4-Compound-queries"><a href="#11-4-Compound-queries" class="headerlink" title="11.4 Compound queries"></a>11.4 Compound queries</h2><p>Compound queries wrap other compound or leaf queries, either to combine their results and scores, to change their behaviour, or to switch from query to filter context.</p>
<p>The queries in this group are:</p>
<ul>
<li>constant_score query: A query which wraps another query, but executes it in filter context. All matching documents are given the same “constant” _score.</li>
<li>bool query: The default query for combining multiple leaf or compound query clauses, as must, should, must_not, or filter clauses. The must and should clauses have their scores combined – the more matching clauses, the better – while the must_not and filter clauses are executed in filter context.</li>
<li>dis_max query: A query which accepts multiple queries, and returns any documents which match any of the query clauses. While the bool query combines the scores from all matching queries, the dis_max query uses the score of the single best- matching query clause.</li>
<li>function_score query: Modify the scores returned by the main query with functions to take into account factors like popularity, recency, distance, or custom algorithms implemented with scripting.</li>
<li>boosting query: Return documents which match a positive query, but reduce the score of documents which also match a negative query.</li>
<li>indices query: Execute one query for the specified indices, and another for other indices.</li>
</ul>
<h3 id="11-4-1-Constant-Score-Query"><a href="#11-4-1-Constant-Score-Query" class="headerlink" title="11.4.1 Constant Score Query"></a>11.4.1 Constant Score Query</h3><p>A query that wraps another query and simply returns a constant score equal to the query boost for every document in the filter. Maps to Lucene ConstantScoreQuery.</p>
<pre><code>{
    &quot;constant_score&quot; : {
        &quot;filter&quot; : {
            &quot;term&quot; : { &quot;user&quot; : &quot;kimchy&quot;}
        },
        &quot;boost&quot; : 1.2
    }
}
</code></pre><h3 id="11-4-2-Bool-Query"><a href="#11-4-2-Bool-Query" class="headerlink" title="11.4.2 Bool Query"></a>11.4.2 Bool Query</h3><p>A query that matches documents matching boolean combinations of other queries. The bool query maps to Lucene BooleanQuery. It is built using one or more boolean clauses, each clause with a typed occurrence. The occurrence types are:</p>
<table>
<thead>
<tr>
<th>Occur</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>must</td>
<td>The clause (query) must appear in matching documents and will contribute to the score.</td>
</tr>
<tr>
<td>filter</td>
<td>The clause (query) must appear in matching documents. However unlike must the score of the query will be ignored.</td>
</tr>
<tr>
<td>should</td>
<td>The clause (query) should appear in the matching document. In a boolean query with no must or filter clauses, one or more should clauses must match a document. The minimum number of should clauses to match can be set using the minimum_should_match parameter.</td>
</tr>
<tr>
<td>must_not</td>
<td>The clause (query) must not appear in the matching documents.</td>
</tr>
</tbody>
</table>
<p><strong>If this query is used in a filter context and it has should clauses then at least one should clause is required to match.</strong></p>
<p>The bool query takes a more-matches-is-better approach, so the score from each matching <em>must</em> or <em>should</em> clause will be added together to provide the final _score for each document.</p>
<pre><code>{
    &quot;bool&quot; : {
        &quot;must&quot; : {
            &quot;term&quot; : { &quot;user&quot; : &quot;kimchy&quot; }
        },
        &quot;filter&quot;: {
            &quot;term&quot; : { &quot;tag&quot; : &quot;tech&quot; }
        },
        &quot;must_not&quot; : {
            &quot;range&quot; : {
                &quot;age&quot; : { &quot;from&quot; : 10, &quot;to&quot; : 20 }
            }
        },
        &quot;should&quot; : [
            {
                &quot;term&quot; : { &quot;tag&quot; : &quot;wow&quot; }
            },
            {
                &quot;term&quot; : { &quot;tag&quot; : &quot;elasticsearch&quot; }
            }
        ],
        &quot;minimum_should_match&quot; : 1,
        &quot;boost&quot; : 1.0
    }
}
</code></pre><h3 id="11-4-3-Dis-Max-Query"><a href="#11-4-3-Dis-Max-Query" class="headerlink" title="11.4.3 Dis Max Query"></a>11.4.3 Dis Max Query</h3><p>A query that generates the union of documents produced by its subqueries, and that scores each document with the maximum score for that document as produced by any subquery, plus a tie breaking increment for any additional matching subqueries.</p>
<p>This is useful when searching for a word in multiple fields with different boost factors (so that the fields cannot be combined equivalently into a single search field). We want the primary score to be the one associated with the highest boost, not the sum of the field scores (as Boolean Query would give). If the query is “albino elephant” this ensures that “albino” matching one field and “elephant” matching another gets a higher score than “albino” matching both fields. To get this result, use both Boolean Query and DisjunctionMax Query: for each term a DisjunctionMaxQuery searches for it in each field, while the set of these DisjunctionMaxQuery’s is combined into a BooleanQuery.</p>
<p>The tie breaker capability allows results that include the same term in multiple fields to be judged better than results that include this term in only the best of those multiple fields, without confusing this with the better case of two different terms in the multiple fields.The default tie_breaker is 0.0.</p>
<p>This query maps to Lucene DisjunctionMaxQuery.</p>
<pre><code>{
    &quot;dis_max&quot; : {
        &quot;tie_breaker&quot; : 0.7,
        &quot;boost&quot; : 1.2,
        &quot;queries&quot; : [
            {
                &quot;term&quot; : { &quot;age&quot; : 34 }
            },
            {
                &quot;term&quot; : { &quot;age&quot; : 35 }
            }
        ]
    }
}
</code></pre><h3 id="11-4-4-Function-Score-Query"><a href="#11-4-4-Function-Score-Query" class="headerlink" title="11.4.4 Function Score Query"></a>11.4.4 Function Score Query</h3><p>The function_score allows you to modify the score of documents that are retrieved by a query. This can be useful if, for example, a score function is computationally expensive and it is sufficient to compute the score on a filtered set of documents.</p>
<p>To use function_score, the user has to define a query and one or more functions, that compute a new score for each document returned by the query.</p>
<pre><code>&quot;function_score&quot;: {
    &quot;query&quot;: {},
    &quot;boost&quot;: &quot;boost for the whole query&quot;,
    &quot;functions&quot;: [
        {
            &quot;filter&quot;: {},
            &quot;FUNCTION&quot;: {}, 
            &quot;weight&quot;: number
        },
        {
            &quot;FUNCTION&quot;: {} 
        },
        {
            &quot;filter&quot;: {},
            &quot;weight&quot;: number
        }
    ],
    &quot;max_boost&quot;: number,
    &quot;score_mode&quot;: &quot;(multiply|max|...)&quot;,
    &quot;boost_mode&quot;: &quot;(multiply|replace|...)&quot;,
    &quot;min_score&quot; : number
}
</code></pre><p>++<em>The</em> scores produced by the filtering query of each function do not matter.++</p>
<p>First, each document is scored by the defined functions. The parameter score_mode specifies how the computed scores are combined:</p>
<ul>
<li>multiply: scores are multiplied (default)</li>
<li>sum: scores are summed</li>
<li>avg: scores are averaged</li>
<li>first: the first function that has a matching filter is applied</li>
<li>max: maximum score is used</li>
<li>min: minimum score is used</li>
</ul>
<p>The newly computed score is combined with the score of the query. The parameter boost_mode defines how:</p>
<ul>
<li>multiply: query score and function score is multiplied (default)</li>
<li>replace: only function score is used, the query score is ignored</li>
<li>sum: query score and function score are added</li>
<li>avg: average</li>
<li>max: max of query score and function score</li>
<li>min: min of query score and function score</li>
</ul>
<p>The function_score query provides several types of score functions: </p>
<ul>
<li>script_score</li>
<li>weight</li>
<li>random_score</li>
<li>field_value_factor</li>
<li>decay functions: gauss, linear, exp</li>
</ul>
<p><strong>Field Value factor</strong><br>The field_value_factor function allows you to use a field from a document to influence the score. It’s similar to using the script_score function, however, it avoids the overhead of scripting. If used on a multi-valued field, only the first value of the field is used in calculations.</p>
<p>As an example, imagine you have a document indexed with a numeric popularity field and wish to influence the score of a document with this field, an example doing so would look like:</p>
<pre><code>&quot;field_value_factor&quot;: {
  &quot;field&quot;: &quot;popularity&quot;,
  &quot;factor&quot;: 1.2,
  &quot;modifier&quot;: &quot;sqrt&quot;,
  &quot;missing&quot;: 1
}
</code></pre><p>Which will translate into the following formula for scoring:</p>
<pre><code>sqrt(1.2 * doc[&apos;popularity&apos;].value)
</code></pre><p>There are a number of options for the field_value_factor function:</p>
<ul>
<li>field: Field to be extracted from the document.</li>
<li>factor: Optional factor to multiply the field value with, defaults to 1.</li>
<li>modifier: Modifier to apply to the field value, can be one of: none, log, log1p, log2p, ln, ln1p, ln2p, square, sqrt, or reciprocal. Defaults to none.</li>
</ul>
<table>
<thead>
<tr>
<th>Modifier</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>none</td>
<td>Do not apply any multiplier to the field value</td>
</tr>
<tr>
<td>log</td>
<td>Take the logarithm of the field value</td>
</tr>
<tr>
<td>log1p</td>
<td>Add 1 to the field value and take the logarithm</td>
</tr>
<tr>
<td>log2p</td>
<td>Add 2 to the field value and take the logarithm</td>
</tr>
<tr>
<td>ln</td>
<td>Take the natural logarithm of the field value</td>
</tr>
<tr>
<td>ln1p</td>
<td>Add 1 to the field value and take the natural logarithm</td>
</tr>
<tr>
<td>ln2p</td>
<td>Add 2 to the field value and take the natural logarithm</td>
</tr>
<tr>
<td>square</td>
<td>Square the field value (multiply it by itself)</td>
</tr>
<tr>
<td>sqrt</td>
<td>Take the square root of the field value</td>
</tr>
<tr>
<td>reciprocal</td>
<td>Reciprocate the field value, same as 1/x where x is the field’s value</td>
</tr>
</tbody>
</table>
<h3 id="11-4-5-Boosting-Query"><a href="#11-4-5-Boosting-Query" class="headerlink" title="11.4.5 Boosting Query"></a>11.4.5 Boosting Query</h3><p>The boosting query can be used to effectively demote results that match a given query. Unlike the “NOT” clause in bool query, this still selects documents that contain undesirable terms, but reduces their overall score.</p>
<pre><code>{
    &quot;boosting&quot; : {
        &quot;positive&quot; : {
            &quot;term&quot; : {
                &quot;field1&quot; : &quot;value1&quot;
            }
        },
        &quot;negative&quot; : {
            &quot;term&quot; : {
                &quot;field2&quot; : &quot;value2&quot;
            }
        },
        &quot;negative_boost&quot; : 0.2
    }
}
</code></pre><h3 id="11-4-6-Indices-Query"><a href="#11-4-6-Indices-Query" class="headerlink" title="11.4.6 Indices Query"></a>11.4.6 Indices Query</h3><p>The indices query is useful in cases where a search is executed across multiple indices. It allows to specify a list of index names and an inner query that is only executed for indices matching names on that list. For other indices that are searched, but that don’t match entries on the list, the alternative no_match_query is executed.</p>
<pre><code>{
    &quot;indices&quot; : {
        &quot;indices&quot; : [&quot;index1&quot;, &quot;index2&quot;],
        &quot;query&quot; : {
            &quot;term&quot; : { &quot;tag&quot; : &quot;wow&quot; }
        },
        &quot;no_match_query&quot; : {
            &quot;term&quot; : { &quot;tag&quot; : &quot;kow&quot; }
        }
    }
}
</code></pre><h2 id="11-5-Joining-queries"><a href="#11-5-Joining-queries" class="headerlink" title="11.5 Joining queries"></a>11.5 Joining queries</h2><p>Performing full SQL-style joins in a distributed system like Elasticsearch is prohibitively expensive. Instead, Elasticsearch offers two forms of join which are designed to scale horizontally.</p>
<ul>
<li>nested query: Documents may contains fields of type nested. These fields are used to index arrays of objects, where each object can be queried (with the nested query) as an independent document.</li>
<li>has_child and has_parent queries: A parent-child relationship can exist between two document types within a single index. The has_child query returns parent documents whose child documents match the specified query, while the has_parent query returns child documents whose parent document matches the specified query.</li>
</ul>
<p>Also see the terms-lookup mechanism in the terms query, which allows you to build a terms query from values contained in another document.</p>
<h2 id="11-6-Geo-queries-Advanced-Topic"><a href="#11-6-Geo-queries-Advanced-Topic" class="headerlink" title="11.6 Geo queries  (Advanced Topic)"></a>11.6 Geo queries  (Advanced Topic)</h2><h2 id="11-7-Specialized-queries-Advanced-Topic"><a href="#11-7-Specialized-queries-Advanced-Topic" class="headerlink" title="11.7 Specialized queries (Advanced Topic)"></a>11.7 Specialized queries (Advanced Topic)</h2><p>This group contains queries which do not fit into the other groups:</p>
<ul>
<li>more_like_this query: This query finds documents which are similar to the specified text, document, or collection of documents.</li>
<li>template query: The template query accepts a Mustache template (either inline, indexed, or from a file), and a map of parameters, and combines the two to generate the final query to execute.</li>
<li>script query: This query allows a script to act as a filter. Also see the function_score query.</li>
</ul>
<h2 id="11-8-Span-queries-Advanced-Topic"><a href="#11-8-Span-queries-Advanced-Topic" class="headerlink" title="11.8 Span queries (Advanced Topic)"></a>11.8 Span queries (Advanced Topic)</h2><p>Span queries are low-level positional queries which provide expert control over the order and proximity of the specified terms. These are typically used to implement very specific queries on legal documents or patents.</p>
<p>Span queries cannot be mixed with non-span queries (with the exception of the span_multi query).</p>
<p>The queries in this group are:</p>
<ul>
<li>span_term query: The equivalent of the term query but for use with other span queries.</li>
<li>span_multi query: Wraps a term, range, prefix, wildcard, regexp, or fuzzy query.</li>
<li>span_first query: Accepts another span query whose matches must appear within the first N positions of the field.</li>
<li>span_near query: Accepts multiple span queries whose matches must be within the specified distance of each other, and possibly in the same order.</li>
<li>span_or query: Combines multiple span queries – returns documents which match any of the specified queries.</li>
<li>span_not query: Wraps another span query, and excludes any documents which match that query.</li>
<li>span_containing query: Accepts a list of span queries, but only returns those spans which also match a second span query.</li>
<li>span_within query: The result from a single span query is returned as long is its span falls within the spans returned by a list of other span queries.</li>
</ul>
<h1 id="12-Mapping"><a href="#12-Mapping" class="headerlink" title="12 Mapping"></a>12 Mapping</h1><p>Mapping is the process of defining how a document, and the fields it contains, are stored and indexed. For instance, use mappings to define:</p>
<pre><code>which string fields should be treated as full text fields.
which fields contain numbers, dates, or geolocations.
whether the values of all fields in the document should be indexed into the catch-all _all field.
the format of date values.
custom rules to control the mapping for dynamically added fields.
</code></pre><p><strong>Mapping Types</strong><br>Each index has one or more mapping types, which are used to divide the documents in an index into logical groups. User documents might be stored in a user type, and blog posts in a blogpost type.</p>
<p>Each mapping type has:</p>
<ul>
<li>Meta-fields: Meta-fields are used to customize how a document’s metadata associated is treated. Examples of meta-fields include the document’s _index, _type, _id, and _source fields.</li>
<li>Fields or properties: Each mapping type contains a list of fields or properties pertinent to that type. A user type might contain title, name, and age fields, while a blogpost type might contain title, body, user_id and created fields. Fields with the same name in different mapping types in the same index must have the same mapping.</li>
</ul>
<p><strong>Field datatypes</strong><br>Each field has a data type which can be:</p>
<pre><code>a simple type like string, date, long, double, boolean or ip.
a type which supports the hierarchical nature of JSON such as object or nested.
or a specialised type like geo_point, geo_shape, or completion.
</code></pre><p>It is often useful to index the same field in different ways for different purposes. For instance, a string field could be indexed as an analyzed field for full-text search, and as a not_analyzed field for sorting or aggregations. Alternatively, you could index a string field with the standard analyzer, the english analyzer, and the french analyzer.</p>
<p>This is the purpose of multi-fields. Most datatypes support multi-fields via the fields parameter.</p>
<p><strong>Dynamic mapping</strong><br>Fields and mapping types do not need to be defined before being used. Thanks to dynamic mapping, new mapping types and new field names will be added automatically, just by indexing a document. New fields can be added both to the top-level mapping type, and to inner object and nested fields.</p>
<p>The dynamic mapping rules can be configured to customise the mapping that is used for new types and new fields.</p>
<p><strong>Explicit mappings</strong><br>You know more about your data than Elasticsearch can guess, so while dynamic mapping can be useful to get started, at some point you will want to specify your own explicit mappings.</p>
<p>You can create mapping types and field mappings when you create an index, and you can add mapping types and fields to an existing index with the PUT mapping API.</p>
<p><strong>Updating existing mappings</strong><br>Other than where documented, <strong>existing type and field mappings cannot be updated</strong>. Changing the mapping would mean invalidating already indexed documents. Instead, you should create a new index with the correct mappings and reindex your data into that index.</p>
<p><strong>Fields are shared across mapping types</strong><br>Mapping types are used to group fields, but the fields in each mapping type are not independent of each other. Fields with:</p>
<pre><code>the same name
in the same index
in different mapping types
map to the same field internally,
and must have the same mapping.
</code></pre><p>If a title field exists in both the user and blogpost mapping types, the title fields must have exactly the same mapping in each type. The only exceptions to this rule are the copy_to, dynamic, enabled, ignore_above, include_in_all, and properties parameters, which may have different settings per field.</p>
<p>Usually, fields with the same name also contain the same type of data, so having the same mapping is not a problem. When conflicts do arise, these can be solved by choosing more descriptive names, such as user_title and blog_title.</p>
<p><strong>Example mapping</strong><br>A mapping for the example described above could be specified when creating the index, as follows:</p>
<pre><code>PUT my_index 
{
  &quot;mappings&quot;: {
    &quot;user&quot;: { 
      &quot;_all&quot;:       { &quot;enabled&quot;: false  }, 
      &quot;properties&quot;: { 
        &quot;title&quot;:    { &quot;type&quot;: &quot;string&quot;  }, 
        &quot;name&quot;:     { &quot;type&quot;: &quot;string&quot;  }, 
        &quot;age&quot;:      { &quot;type&quot;: &quot;integer&quot; }  
      }
    },
    &quot;blogpost&quot;: { 
      &quot;properties&quot;: { 
        &quot;title&quot;:    { &quot;type&quot;: &quot;string&quot;  }, 
        &quot;body&quot;:     { &quot;type&quot;: &quot;string&quot;  }, 
        &quot;user_id&quot;:  {
          &quot;type&quot;:   &quot;string&quot;, 
          &quot;index&quot;:  &quot;not_analyzed&quot;
        },
        &quot;created&quot;:  {
          &quot;type&quot;:   &quot;date&quot;, 
          &quot;format&quot;: &quot;strict_date_optional_time||epoch_millis&quot;
        }
      }
    }
  }
}
</code></pre><h2 id="12-1-Field-datatypes"><a href="#12-1-Field-datatypes" class="headerlink" title="12.1 Field datatypes"></a>12.1 Field datatypes</h2><p>Elasticsearch supports a number of different datatypes for the fields in a document:</p>
<h3 id="Core-datatypes"><a href="#Core-datatypes" class="headerlink" title="Core datatypes"></a>Core datatypes</h3><p><strong>String datatype: string</strong><br>The following parameters are accepted by string fields:</p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>analyzer</td>
<td>The analyzer which should be used for analyzed string fields, both at index-time and at search-time (unless overridden by the search_analyzer). Defaults to the default index analyzer, or the standard analyzer.</td>
</tr>
<tr>
<td>boost</td>
<td>Field-level index time boosting. Accepts a floating point number, defaults to 1.0.</td>
</tr>
<tr>
<td>doc_values</td>
<td>Should the field be stored on disk in a column-stride fashion, so that it can later be used for sorting, aggregations, or scripting? Accepts true or false. Defaults to true for not_analyzed fields. Analyzed fields do not support doc values.</td>
</tr>
<tr>
<td>fielddata</td>
<td>Can the field use in-memory fielddata for sorting, aggregations, or scripting? Accepts disabled or paged_bytes (default). Not analyzed fields will use doc values in preference to fielddata.</td>
</tr>
<tr>
<td>fields</td>
<td>Multi-fields allow the same string value to be indexed in multiple ways for different purposes, such as one field for search and a multi-field for sorting and aggregations, or the same string value analyzed by different analyzers.</td>
</tr>
<tr>
<td>ignore_above</td>
<td>Do not index or analyze any string longer than this value. Defaults to 0 (disabled).</td>
</tr>
<tr>
<td>include_in_all</td>
<td>Whether or not the field value should be included in the _all field? Accepts true or false. Defaults to false if index is set to no, or if a parent object field sets include_in_all to false. Otherwise defaults to true.</td>
</tr>
<tr>
<td>index</td>
<td>Should the field be searchable? Accepts analyzed (default, treat as full-text field), not_analyzed (treat as keyword field) and no.</td>
</tr>
<tr>
<td>index_options</td>
<td>What information should be stored in the index, for search and highlighting purposes. Defaults to positions for analyzed fields, and to docs for not_analyzed fields.</td>
</tr>
<tr>
<td>norms</td>
<td>Whether field-length should be taken into account when scoring queries. Defaults depend on the index setting: analyzed fields default to { “enabled”: true, “loading”: “lazy” }; not_analyzed fields default to { “enabled”: false }.</td>
</tr>
<tr>
<td>null_value</td>
<td>Accepts a string value which is substituted for any explicit null values. Defaults to null, which means the field is treated as missing. If the field is analyzed, the null_value will also be analyzed.</td>
</tr>
<tr>
<td>position_increment_gap</td>
<td>The number of fake term position which should be inserted between each element of an array of strings. Defaults to the position_increment_gap configured on the analyzer which defaults to 100. 100 was chosen because it prevents phrase queries with reasonably large slops (less than 100) from matching terms across field values.</td>
</tr>
<tr>
<td>store</td>
<td>Whether the field value should be stored and retrievable separately from the _source field. Accepts true or false (default).</td>
</tr>
<tr>
<td>search_analyzer</td>
<td>The analyzer that should be used at search time on analyzed fields. Defaults to the analyzer setting.</td>
</tr>
<tr>
<td>search_quote_analyzer</td>
<td>The analyzer that should be used at search time when a phrase is encountered. Defaults to the search_analyzer setting.</td>
</tr>
<tr>
<td>similarity</td>
<td>Which scoring algorithm or similarity should be used. Defaults to default, which uses TF/IDF.</td>
</tr>
<tr>
<td>term_vector</td>
<td>Whether term vectors should be stored for an analyzed field. Defaults to no.</td>
</tr>
</tbody>
</table>
<p><strong>Numeric datatypes: long, integer, short, byte, double, float</strong></p>
<p><strong>Date datatype: date </strong><br>JSON doesn’t have a date datatype, so dates in Elasticsearch can either be:</p>
<ul>
<li>strings containing formatted dates, e.g. “2015-01-01” or “2015/01/01 12:10:30”</li>
<li>a long number representing milliseconds-since-the-epoch.</li>
<li>an integer representing seconds-since-the-epoch.)</li>
</ul>
<p><strong>Boolean datatype: boolean</strong><br><strong>Binary datatype: binary </strong><br>The binary type accepts a binary value as a Base64 encoded string. The field is not stored by default and is not searchable.</p>
<h3 id="Complex-datatypes"><a href="#Complex-datatypes" class="headerlink" title="Complex datatypes"></a>Complex datatypes</h3><p><strong>Array datatype</strong><br>Array support does not require a dedicated type (In Elasticsearch, there is no dedicated array type. Any field can contain zero or more values by default, however, all values in the array must be of the same datatype.)</p>
<p><strong>Object datatype: object for single JSON objects</strong><br><strong>Nested datatype: nested for arrays of JSON objects</strong></p>
<h3 id="Geo-datatypes-Advanced-Topic"><a href="#Geo-datatypes-Advanced-Topic" class="headerlink" title="Geo datatypes (Advanced Topic)"></a>Geo datatypes (Advanced Topic)</h3><ul>
<li>Geo-point datatype: geo_point for lat/lon points</li>
<li>Geo-Shape datatype: geo_shape for complex shapes like polygons</li>
</ul>
<p>Specialised datatypes</p>
<ul>
<li>IPv4 datatype： ip for IPv4 addresses</li>
<li>Completion datatype： completion to provide auto-complete suggestions</li>
<li>Token count datatype： token_count to count the number of tokens in a string</li>
<li>mapper-murmur3： murmur3 to compute hashes of values at index-time and store them in the index</li>
<li>Attachment datatype： See the mapper-attachments plugin which supports indexing attachments like Microsoft Office formats, Open Document formats, ePub, HTML, etc. into an attachment datatype.</li>
</ul>
<h3 id="Multi-fields"><a href="#Multi-fields" class="headerlink" title="Multi-fields"></a>Multi-fields</h3><p>It is often useful to index the same field in different ways for different purposes. For instance, a string field could be indexed as an analyzed field for full-text search, and as a not_analyzed field for sorting or aggregations. Alternatively, you could index a string field with the standard analyzer, the english analyzer, and the french analyzer.</p>
<p>This is the purpose of multi-fields. Most datatypes support multi-fields via the fields parameter.</p>
<h2 id="12-2-Meta-Fields"><a href="#12-2-Meta-Fields" class="headerlink" title="12.2 Meta-Fields"></a>12.2 Meta-Fields</h2><p>Each document has metadata associated with it, such as the _index, mapping _type, and _id meta-fields. The behaviour of some of these meta-fields can be customised when a mapping type is created.</p>
<h3 id="Identity-meta-fields"><a href="#Identity-meta-fields" class="headerlink" title="Identity meta-fields"></a>Identity meta-fields</h3><ul>
<li>_index: The index to which the document belongs.</li>
<li>_uid: A composite field consisting of the _type and the _id.</li>
<li>_type: The document’s mapping type.</li>
<li>_id: The document’s ID.</li>
</ul>
<h3 id="Document-source-meta-fields"><a href="#Document-source-meta-fields" class="headerlink" title="Document source meta-fields"></a>Document source meta-fields</h3><ul>
<li>_source: The original JSON representing the body of the document.</li>
<li>_size: The size of the _source field in bytes, provided by the mapper-size plugin.</li>
</ul>
<h3 id="Indexing-meta-fields"><a href="#Indexing-meta-fields" class="headerlink" title="Indexing meta-fields"></a>Indexing meta-fields</h3><ul>
<li><p>_all: The _all field is a special catch-all field which concatenates the values of all of the other fields into one big string, using space as a delimiter, which is then analyzed and indexed, but not stored. This means that it can be searched, but not retrieved; The _all field can be completely disabled per-type by setting enabled to false; While there is only a single _all field per index, the copy_to parameter allows the creation of multiple custom _all fields. For instance, first_name and last_name fields can be combined together into the full_name field.</p>
</li>
<li><p>_field_names: The _field_names field indexes the names of every field in a document that contains any value other than null. This field is used by the exists and missing queries to find documents that either have or don’t have any non-null value for a particular field; The value of the _field_name field is accessible in queries, aggregations, and scripts.</p>
</li>
</ul>
<h3 id="Routing-meta-fields"><a href="#Routing-meta-fields" class="headerlink" title="Routing meta-fields"></a>Routing meta-fields</h3><ul>
<li>_parent: Used to create a parent-child relationship between two mapping types.</li>
<li>_routing: A custom routing value which routes a document to a particular shard.</li>
</ul>
<h3 id="Other-meta-field"><a href="#Other-meta-field" class="headerlink" title="Other meta-field"></a>Other meta-field</h3><ul>
<li>_meta: Application specific metadata.</li>
</ul>
<h2 id="12-3-Mapping-parameters"><a href="#12-3-Mapping-parameters" class="headerlink" title="12.3 Mapping parameters"></a>12.3 Mapping parameters</h2><p>The following mapping parameters are common to some or all field datatypes:</p>
<ul>
<li>analyzer</li>
<li>boost</li>
<li>coerce</li>
<li>copy_to: The copy_to parameter allows you to create custom _all fields. In other words, the values of multiple fields can be copied into a group field, which can then be queried as a single field. For instance, the first_name and last_name fields can be copied to the full_name field.</li>
<li>doc_values: Doc values are the on-disk data structure, built at document index time, which makes this data access pattern possible. They store the same values as the _source but in a column-oriented fashion that is way more efficient for sorting and aggregations. Doc values are supported on almost all field types, with the notable exception of analyzed string fields.</li>
<li>dynamic</li>
<li>enabled</li>
<li>fielddata: Most fields can use index-time, on-disk doc_values to support this type of data access pattern, but analyzed string fields do not support doc_values; Instead, analyzed strings use a query-time data structure called fielddata. This data structure is built on demand the first time that a field is used for aggregations, sorting, or is accessed in a script. It is built by reading the entire inverted index for each segment from disk, inverting the term ↔︎ document relationship, and storing the result in memory, in the JVM heap; Loading fielddata is an expensive process so, once it has been loaded, it remains in memory for the lifetime of the segment.</li>
<li>geohash</li>
<li>geohash_precision</li>
<li>geohash_prefix</li>
<li>format</li>
<li>ignore_above</li>
<li>ignore_malformed</li>
<li>include_in_all</li>
<li>index_options</li>
<li>lat_lon</li>
<li>index</li>
<li>fields: It is often useful to index the same field in different ways for different purposes. This is the purpose of multi-fields. For instance, a string field could be indexed as an analyzed field for full-text search, and as a not_analyzed field for sorting or aggregations. </li>
<li>norms: Norms store various normalization factors – a number to represent the relative field length and the index time boost setting – that are later used at query time in order to compute the score of a document relatively to a query; Although useful for scoring, norms also require quite a lot of memory (typically in the order of one byte per document per field in your index, even for documents that don’t have this specific field). As a consequence, if you don’t need scoring on a specific field, you should disable norms on that field. In particular, this is the case for fields that are used solely for filtering or aggregations.</li>
<li>null_value</li>
<li>position_increment_gap</li>
<li>properties: Type mappings, object fields and nested fields contain sub-fields, called properties. These properties may be of any datatype, including object and nested. </li>
<li>search_analyzer</li>
<li>similarity</li>
<li>store: By default, field values are indexed to make them searchable, but they are not stored. This means that the field can be queried, but the original field value cannot be retrieved; Usually this doesn’t matter. The field value is already part of the _source field, which is stored by default. If you only want to retrieve the value of a single field or of a few fields, instead of the whole _source, then this can be achieved with source filtering; In certain situations it can make sense to store a field. For instance, if you have a document with a title, a date, and a very large content field, you may want to retrieve just the title and the date without having to extract those fields from a large _source field.</li>
<li>term_vector: Term vectors contain information about the terms produced by the analysis process, including: a list of terms/the position (or order) of each term/the start and end character offsets mapping the term to its origin in the original string; These term vectors can be stored so that they can be retrieved for a particular document.</li>
</ul>
<p>The term_vector setting accepts:</p>
<ul>
<li>no: No term vectors are stored. (default)</li>
<li>yes: Just the terms in the field are stored.</li>
<li>with_positions: Terms and positions are stored.</li>
<li>with_offsets: Terms and character offsets are stored.</li>
<li>with_positions_offsets: Terms, positions, and character offsets are stored.</li>
</ul>
<h2 id="12-4-Dynamic-Mapping"><a href="#12-4-Dynamic-Mapping" class="headerlink" title="12.4 Dynamic Mapping"></a>12.4 Dynamic Mapping</h2><p>One of the most important features of Elasticsearch is that it tries to get out of your way and let you start exploring your data as quickly as possible. To index a document, you don’t have to first create an index, define a mapping type, and define your fields – you can just index a document and the index, type, and fields will spring to life automatically:</p>
<pre><code>PUT data/counters/1 
{ &quot;count&quot;: 5 }
</code></pre><p>Creates the data index, the counters mapping type, and a field called count with datatype long.</p>
<p>The automatic detection and addition of new types and fields is called dynamic mapping. The dynamic mapping rules can be customised to suit your purposes with:</p>
<ul>
<li><em>default</em> mapping: Configure the base mapping to be used for new mapping types.</li>
<li>Dynamic field mappings: The rules governing dynamic field detection.</li>
<li>Dynamic templates: Custom rules to configure the mapping for dynamically added fields.</li>
</ul>
<h1 id="13-Analysis-See-another-documents"><a href="#13-Analysis-See-another-documents" class="headerlink" title="13 Analysis (See another documents)"></a>13 Analysis (See another documents)</h1><h1 id="14-Modules-Skipped"><a href="#14-Modules-Skipped" class="headerlink" title="14 Modules (Skipped)"></a>14 Modules (Skipped)</h1><h1 id="15-Index-Modules-Skipped"><a href="#15-Index-Modules-Skipped" class="headerlink" title="15 Index Modules (Skipped)"></a>15 Index Modules (Skipped)</h1><h1 id="16-Testing-Skipped"><a href="#16-Testing-Skipped" class="headerlink" title="16 Testing (Skipped)"></a>16 Testing (Skipped)</h1><h1 id="17-Glossary-of-terms-Skipped"><a href="#17-Glossary-of-terms-Skipped" class="headerlink" title="17 Glossary of terms (Skipped)"></a>17 Glossary of terms (Skipped)</h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;花了几天把&lt;a href=&quot;https://www.elastic.co/guide/en/elasticsearch/reference/2.3/index.html&quot;&gt;Elasticsearch的官方文档&lt;/a&gt;读了一遍，随手记一些关键的笔记。&lt;/p&gt;
    
    </summary>
    
      <category term="Elasticsearch" scheme="http://ginobefunny.com/categories/Elasticsearch/"/>
    
    
      <category term="Elasticsearch" scheme="http://ginobefunny.com/tags/Elasticsearch/"/>
    
      <category term="一起读文档" scheme="http://ginobefunny.com/tags/%E4%B8%80%E8%B5%B7%E8%AF%BB%E6%96%87%E6%A1%A3/"/>
    
  </entry>
  
  <entry>
    <title>《深入理解Java虚拟机》读书笔记7：高效并发</title>
    <link href="http://ginobefunny.com/post/deep_in_jvm_notes_part7/"/>
    <id>http://ginobefunny.com/post/deep_in_jvm_notes_part7/</id>
    <published>2017-01-29T07:31:25.000Z</published>
    <updated>2017-01-31T09:50:33.579Z</updated>
    
    <content type="html"><![CDATA[<p>国内JVM相关书籍NO.1，Java程序员必读。读书笔记第七部分对应原书的第十二章和第十三章，主要介绍Java内存模型、先行发生原则、线程安全和虚拟机的锁优化细节。<br><a id="more"></a></p>
<h1 id="第五部分-高效并发"><a href="#第五部分-高效并发" class="headerlink" title="第五部分 高效并发"></a>第五部分 高效并发</h1><h2 id="第十二章-Java内存模型与线程"><a href="#第十二章-Java内存模型与线程" class="headerlink" title="第十二章 Java内存模型与线程"></a>第十二章 Java内存模型与线程</h2><p>并发处理的广泛应用是使得Amdahl定律代替摩尔定律成为计算机性能发展源动力的根本原因，也是人类“压榨”计算机运算能力的最有力武器。</p>
<h3 id="12-1-概述"><a href="#12-1-概述" class="headerlink" title="12.1 概述"></a>12.1 概述</h3><ul>
<li>多任务处理在现代计算机操作系统中几乎已是一项必备的功能了；</li>
<li>除了充分利用计算机处理器的能力外，一个服务端同时对多个客户端提供服务则是另一个更具体的并发应用场景；</li>
<li>服务端是Java语言最擅长的领域之一，不过如何写好并发应用程序却又是服务端程序开发的难点之一，处理好并发方面的问题通常需要更多的编码经验来支持，幸好Java语言和虚拟机提供了许多工具，把并发编码的门槛降低了不少；</li>
</ul>
<h3 id="12-2-硬件的效率与一致性"><a href="#12-2-硬件的效率与一致性" class="headerlink" title="12.2 硬件的效率与一致性"></a>12.2 硬件的效率与一致性</h3><ul>
<li>绝大多数的运算任务不可能只靠处理器计算就能完成，处理器至少要与内存交互，所以现代计算机系统都不得不加入一层读写速度尽可能接近处理器运算速度的高速缓存来作为内存与处理器之间的缓冲：将运算需要使用到的数据复制到缓存中，让运算能快速运行，当运算结束后再从缓存同步回内存之中，这样处理器就无须等待缓慢的内存读写了；</li>
<li>基于高速缓存的存储交互很好地解决了处理器与内存的速度矛盾，但是也为计算机系统带来更高的复杂度，因为它引入了一个新的问题：缓存一致性；为了解决一致性的问题，需要各个处理器访问缓存时都遵循一些协议，在读写时要根据协议来进行操作，这类协议有MSI、MESI、MOSI、Synapse、Firefly及Dragon Protocol等；</li>
<li>本章将会多次提到内存模型一词，可以理解在特定的操作协议下，对特定的内存或高速缓存进行读写访问的过程抽象；不同架构的物理机器可以拥有不一样的内存模型，而Java虚拟机也有自己的内存模型，并且这里介绍的内存访问操作与硬件的缓存访问具有很高的可比性；</li>
<li>除了增加高速缓存之外，为了使得处理器内部的运算单元能尽量被充分利用，处理器可能会对输入代码进行乱序执行优化，处理器会在计算之后将乱序执行的结果重组，保证该结果与顺序执行的结果是一致的；</li>
</ul>
<h3 id="12-3-Java内存模型"><a href="#12-3-Java内存模型" class="headerlink" title="12.3 Java内存模型"></a>12.3 Java内存模型</h3><p>Java虚拟机规范中视图定义一种Java内存模型（JMM）来屏蔽掉各种硬件和操作系统的内存访问差异，以实现让Java程序在各种平台下都能达到一致的内存访问效果。</p>
<h4 id="12-3-1-主内存与工作内存"><a href="#12-3-1-主内存与工作内存" class="headerlink" title="12.3.1 主内存与工作内存"></a>12.3.1 主内存与工作内存</h4><ul>
<li>Java内存模型的主要目标是定义程序中各个变量的访问规则，即在虚拟机中将变量存储到内存和从内存中取出变量这样的底层细节；此处的变量与Java编程中所说的变量有所区别，它包括了实例字段、静态字段和构成数组对象的元素，但不包括局部变量与方法参数，因为后者是线程私有的，不会被共享；</li>
<li>Java内存模型规定了所有的变量都存储在主内存中，每个线程还有自己的工作内存，线程的工作内存中保存了被该线程使用到的变量的主内存副本拷贝，线程对变量的所有操作都必须在工作内存中进行，而不能直接读写主内存中的变量；</li>
<li>这里所讲的主内存、工作内存与第二章所讲的Java内存区域中的Java堆、栈、方法区等并不是同一个层次的内存划分，这两者基本上是没有关系的；线程、主内存和工作内存的关系如下所示：</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/jmm_relation.png" alt="线程、主内存和工作内存的关系"></p>
<h4 id="12-3-2-内存间交互操作"><a href="#12-3-2-内存间交互操作" class="headerlink" title="12.3.2 内存间交互操作"></a>12.3.2 内存间交互操作</h4><p>关于主内存与工作内存之间具体的交互协议，即一个变量如何从主内存拷贝到工作内存、如何从工作内存同步回主内存之类的实现细节，Java内存模型中定义了以下八种操作来完成，虚拟机实现时必须保证下面提及的每一种操作都是原子的、不可再分的（对于double和long类型的变量的某些操作在某些平台允许有例外）：</p>
<ul>
<li>lock</li>
<li>unlock</li>
<li>read</li>
<li>load</li>
<li>use</li>
<li>assign</li>
<li>store</li>
<li>write</li>
</ul>
<p>基于理解难度和严谨性考虑，最新的JSR-133文档中，已经放弃采用这八种操作去定义Java内存模型的访问协议了，后面将会介绍一个等效判断原则 – 先行发生原则，用来确定一个访问在并发环境下是否安全；</p>
<h4 id="12-3-3-对于volatile型变量的特殊规则"><a href="#12-3-3-对于volatile型变量的特殊规则" class="headerlink" title="12.3.3 对于volatile型变量的特殊规则"></a>12.3.3 对于volatile型变量的特殊规则</h4><ul>
<li>关键字volatile可以说是Java虚拟机提供的最轻量级的同步机制；</li>
<li>当一个变量定义为volatile之后，它将具备两种特性：第一是保证此变量对所有线程的可见性，这里的可见性是指当一个线程修改了这个变量的值，新的值对于其他线程来说是可以立即得知的，而普通的变量的值在线程间传递均需要通过主内存来完成；另外一个是禁止指令重排序优化，普通的变量仅仅会保证在该方法的执行过程中所有依赖赋值结果的地方都能获取到正确的结果，而不能保证变量赋值操作的顺序与程序代码中的执行顺序一致；</li>
<li>volatile变量在各个线程的工作内存中不存在一致性问题，但是Java里面的运算并非原子操作，导致volatile变量的运算在并发下一样是不安全的；</li>
<li>在不符合以下两条规则的运算场景中，我们仍然要通过加锁来保证原子性：运算结果并不依赖变量的当前值或者能够确保只有单一的线程修改变量的值、变量不需要与其他的状态变量共同参与不变约束；</li>
<li>volatile变量读操作的性能消耗与普通变量几乎没有任何差别，但是写操作则可能会慢一些；不过大多数场景下volatile的总开销仍然要比锁低，我们在volatile与锁之中选择的唯一依据仅仅是volatile的语义能否满足使用场景的需求；</li>
</ul>
<h4 id="12-3-4-对于long和double型变量的特殊规则"><a href="#12-3-4-对于long和double型变量的特殊规则" class="headerlink" title="12.3.4 对于long和double型变量的特殊规则"></a>12.3.4 对于long和double型变量的特殊规则</h4><ul>
<li>允许虚拟机将没有被volatile修饰的64位数据的读写操作划分为两次32位的操作来进行，即允许虚拟机实现选择可以不保证64位数据类型的load、store、read和write这4个操作的原子性，这点就是所谓的long和double的非原子性协定；</li>
<li>但允许虚拟机选择把这些操作实现为具有原子性的操作，目前各种平台下的商用虚拟机几乎都选择把64位数据的读写操作作为原子操作来对待；</li>
</ul>
<h4 id="12-3-5-原子性、可见性与有序性"><a href="#12-3-5-原子性、可见性与有序性" class="headerlink" title="12.3.5 原子性、可见性与有序性"></a>12.3.5 原子性、可见性与有序性</h4><ul>
<li>原子性（Atomicity）：由Java内存模型来直接保证的原子性变量操作包括read、load、assign、use、store和write；在synchronized块之间的操作也具备原子性；</li>
<li>可见性（Visibility）：是指当一个线程修改了共享变量的值，其他线程能够立即得知这个修改；除了volatile之外，Java还有synchronized和final关键字能实现可见性；</li>
<li>有序性（Ordering）：如果在本线程内观察，所有的操作都是有序的；如果在一个线程中观察另一个线程，所有的操作都是无序的；Java语言提供了volatile和synchronized两个关键字来保证线程之间操作的有序性；</li>
</ul>
<h4 id="12-3-6-先行发生原则"><a href="#12-3-6-先行发生原则" class="headerlink" title="12.3.6 先行发生原则"></a>12.3.6 先行发生原则</h4><ul>
<li>先行发生是Java内存模型中定义的两项操作之间的偏序关系，如果说操作A先行发生于操作B，其实就是说在发生操作B之前，操作A产生的影响能被操作B观察到，影响包括了修改了内存中共享变量的值、发送了消息、调用了方法等；</li>
<li>下面是Java内存模型下一些天然的先行发生关系：程序次序规则、管程锁定规则、volatile变量规则、线程启动规则、线程终止规则、线程中断规则、对象终结规则、传递性；</li>
<li>时间先后顺序与先行发生原则之间基本没有太大的关系，所以我们衡量并发安全问题的时候不要受到时间顺序的干扰，一切必须以先行发生原则为准；</li>
</ul>
<h3 id="12-4-Java与线程"><a href="#12-4-Java与线程" class="headerlink" title="12.4 Java与线程"></a>12.4 Java与线程</h3><h4 id="12-4-1-线程的实现"><a href="#12-4-1-线程的实现" class="headerlink" title="12.4.1 线程的实现"></a>12.4.1 线程的实现</h4><ul>
<li>线程是比进程更轻量级的调度执行单位，线程的引入可以把一个进程的资源分配和执行调度分开，各个线程既可以共享进程资源又可以独立调度；</li>
<li>Thread类与大部分的Java API有显著的差别，它的所有关键方法都是声明为Native的；</li>
<li>实现线程主要有三种方式：使用内核线程实现（系统调用代价相对较高、一个系统支持轻量级进程的数量是有限的）、使用用户线程实现（优势在于不需要系统内核支援，劣势在于所有线程操作都需要用户程序自己处理）和使用用户线程加轻量级进程混合实现（用户线程是完全建立在用户空间中，因此用户线程的创建、切换等操作依然廉价，并且可以支持大规模的用户线程并发；而操作系统提供支持的轻量级进程则作为用户线程和内核线程之间的桥梁，这样可以使用内核提供的线程调度功能及处理器映射，并且用户线程的系统调用要通过轻量级线程来完成，大大降低了整个进程被完全阻塞的风险）；</li>
<li>对于Sun JDK来说，它的Windows版与Linux版都是使用一对一的线程模型实现的，一条Java线程就映射到一条轻量级进程之中，因为Windows和Linux系统提供的线程模式就是一对一的；</li>
</ul>
<h4 id="12-4-2-Java线程调度"><a href="#12-4-2-Java线程调度" class="headerlink" title="12.4.2 Java线程调度"></a>12.4.2 Java线程调度</h4><ul>
<li>线程调度是指系统为线程分配处理器使用权的过程，主要调度方式有两种，分别是协同式线程调度（线程的执行时间由线程本身来控制）和抢占式线程调度（线程由系统来分配执行时间，线程的切换不由线程本身来决定）；</li>
<li>Java语言一共设置了10个级别的线程优先级，不过线程优先级并不是太靠谱，原因就是操作系统的线程优先级不见得总是与Java线程的优先级一一对应，另外优先级还可能被系统自行改变；</li>
</ul>
<h4 id="12-4-3-状态转换"><a href="#12-4-3-状态转换" class="headerlink" title="12.4.3 状态转换"></a>12.4.3 状态转换</h4><ul>
<li>Java语言定义了五种线程状态，在任意一个时间点，一个线程只能有且只有其中一种状态，分别是新建（New）、运行（Runnable）、无限期等待（Waiting）、限期等待（Timed Waiting）、阻塞（Blocled）、结束（Terminated）。它们之间相互的转换关系如下所示：</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/thread_status.png" alt="线程状态转换关系"></p>
<h3 id="12-5-本章小结"><a href="#12-5-本章小结" class="headerlink" title="12.5 本章小结"></a>12.5 本章小结</h3><p>本章我们首先了解了虚拟机Java内存模型的结构及操作，然后讲解了原子性、可见性、有序性在Java内存模型中的体现，最后介绍了先行发生原则的规则及使用。另外，我们还了解了线程在Java语言之中是如何实现的。</p>
<p>在本章主要介绍了虚拟机如何实现并发，而在下一章我们主要关注点将是虚拟机如何实现高效，以及虚拟机对我们编写的并发代码提供了什么样的优化手段。</p>
<h2 id="第十三章-线程安全与锁优化"><a href="#第十三章-线程安全与锁优化" class="headerlink" title="第十三章 线程安全与锁优化"></a>第十三章 线程安全与锁优化</h2><h3 id="13-1-概述"><a href="#13-1-概述" class="headerlink" title="13.1 概述"></a>13.1 概述</h3><ul>
<li>首先需要保证并发的正确性，然后在此基础上实现高效；</li>
</ul>
<h3 id="13-2-线程安全"><a href="#13-2-线程安全" class="headerlink" title="13.2 线程安全"></a>13.2 线程安全</h3><p>Brian Goetz对线程安全有一个比较恰当的定义：当多个线程访问一个对象时，如果不用考虑这些线程在运行时环境下的调度和交替执行，也不需要进行额外的同步，或者在调用方进行任何其他的协调操作，调用这个对象的行为都可以获得正确的结果，那这个对象是线程安全的。</p>
<h4 id="13-2-1-Java语言中的线程安全"><a href="#13-2-1-Java语言中的线程安全" class="headerlink" title="13.2.1 Java语言中的线程安全"></a>13.2.1 Java语言中的线程安全</h4><ul>
<li>我们可以将Java语言中各个操作共享的数据分为以下五类：不可变、绝对线程安全、相对线程安全、线程兼容和线程对立；</li>
<li>不可变：不可变带来的安全性是最简单和最纯粹的，如final的基本数据类型；如果共享的数据是一个对象，那就需要保证对象的行为不会对其状态产生任何影响才行，比如String类的substring、replace方法；Number类型的大部分子类都符合不可变要求的类型，但是AtomicInteger和AtomicLong则并非不可变的；</li>
<li>线程绝对安全：Java API中标注自己是线程安全的类，大多数都不是绝对的线程安全；比如java.util.Vector，不意味着调用它的是时候永远都不再需要同步手段了；</li>
<li>线程相对安全：是我们通常意义上所讲的线程安全，在Java语言中，大部分的线程安全类都属于这种类型；</li>
<li>线程兼容：指对象本身并不是线程安全的，但是可以通过在调用端正确地使用同步手段来保证对象在并发环境中可以安全地使用；我们说一个类不是线程安全的，绝大多数时候指的是这一种情况；</li>
<li>线程对立：无论调用端是否采取了同步措施，都无法在多线程环境中并发使用的代码，Java语言中很少出现；</li>
</ul>
<h4 id="13-2-2-线程安全的实现方法"><a href="#13-2-2-线程安全的实现方法" class="headerlink" title="13.2.2 线程安全的实现方法"></a>13.2.2 线程安全的实现方法</h4><ul>
<li>互斥同步：同步是指在多个线程并发访问共享数据时，保证共享数据在同一个时刻只被一个线程使用，而互斥是实现同步的一种手段，临界区、互斥量和信号量都是主要的互斥实现方式；Java中最基本的互斥同步手段就是synchronized关键字，它对同一个线程来说是可重入的且会阻塞后面其他线程的进入；另外还可以使用java.util.concurrent包中的重入锁（ReentrantLock）来实现同步，相比synchronized关键字ReentrantLock增加了一些高级功能：等待可中断、可实现公平锁以及锁可以绑定多个条件；</li>
<li>非阻塞同步：互斥同步最主要的问题就是进行线程阻塞和唤醒带来的性能问题，其属于一种悲观的并发策略；随着硬件指令集的发展，我们有了另外一个选择即基于冲突检测的乐观并发策略，就是先进行操作，如果没有其他线程争用共享数据那就操作成功了，如果有争用产生了冲突，那就再采取其他的补偿措施（最常见的就是不断重试直至成功），这种同步操作称为非阻塞同步；Java并发包的整数原子类，其中的compareAndSet和getAndIncrement等方法都使用了Unsafe类的CAS操作；</li>
<li>无同步方案：要保证线程安全，并不是一定就要进行同步；有一些代码天生就是线程安全的，比如可重入代码和线程本地存储的代码；</li>
</ul>
<h3 id="13-3-锁优化"><a href="#13-3-锁优化" class="headerlink" title="13.3 锁优化"></a>13.3 锁优化</h3><h4 id="13-3-1-自旋锁与自适应自旋"><a href="#13-3-1-自旋锁与自适应自旋" class="headerlink" title="13.3.1 自旋锁与自适应自旋"></a>13.3.1 自旋锁与自适应自旋</h4><ul>
<li>互斥同步对性能最大的影响是阻塞的实现，挂起线程和恢复线程的操作都需要转入内核态中完成，这些操作给系统的并发性能带来了很大的压力；另外在共享数据的锁定状态只会持续很短的一段时间，为了这段时间去挂起和恢复线程并不值得，如果让两个或以上的线程同时并行执行，让后面请求锁的那个线程稍等一下，但不放弃处理器的执行时间，看看持有锁的线程是否很快就会释放锁；为了让线程等待，我们只需让线程执行一个忙循环，这些技术就是所谓的自旋锁；</li>
<li>在JDK 1.6已经默认开启自旋锁；如果锁被占用的时间很短自旋等待的效果就会非常好，反之则会白白消耗处理器资源；</li>
<li>在JDK 1.6中引入了自适应的自旋锁，这意味着自旋的时间不再固定，而是由前一次在同一个锁上的自旋时间及锁的拥有者的状态来决定；</li>
</ul>
<h4 id="13-3-2-锁消除"><a href="#13-3-2-锁消除" class="headerlink" title="13.3.2 锁消除"></a>13.3.2 锁消除</h4><ul>
<li>锁消除是指虚拟机即时编译器在运行时，对一些代码上要求同步，但是被检测到不可能存在共享数据竞争的锁进行消除；</li>
<li>锁消除的主要判断依据来源于逃逸分析的数据支持；</li>
</ul>
<h4 id="13-3-3-锁粗化"><a href="#13-3-3-锁粗化" class="headerlink" title="13.3.3 锁粗化"></a>13.3.3 锁粗化</h4><ul>
<li>原则上总是推荐将同步块的作用范围限制得尽量小 – 只有在共享数据的实际作用域中才进行同步，这样是为了使得需要同步的操作数量尽可能变小，如果存在锁竞争，那等待锁的线程也能尽快拿到锁；</li>
<li>但是如果一系列的连续操作都对同一个对象反复加锁和解锁，甚至加锁操作是出现在循环体中的，那即使没有线程竞争，频繁地进行互斥同步操作也会导致不必要的性能损耗；  </li>
</ul>
<h4 id="13-3-4-轻量级锁"><a href="#13-3-4-轻量级锁" class="headerlink" title="13.3.4 轻量级锁"></a>13.3.4 轻量级锁</h4><ul>
<li>轻量级锁是JDK 1.6之中加入的新型锁机制，它是相对于使用操作系统互斥量来实现的传统锁而言的；它并不是用来代替重量级锁的，它的本意是在没有多线程竞争的前提下，减少传统的重量级锁使用操作系统互斥量产生的性能消耗；</li>
<li>要理解轻量级锁，以及后面会讲到的偏向锁的原理和运作过程，必须从HotSpot虚拟机的对象的内存布局开始介绍；HotSpot虚拟机的对象头分为两部分信息：第一部分用于存储对象自身的运行时数据，如哈希码、GC分代年龄等，这部分官方称之为Mark Word，是实现轻量级锁和偏向锁的关键，另外一部分用于存储指向方法区对象类型数据的指针； Mark Word被设计成一个非固定的数据结构以便在极小的空间存储尽量多的信息，在32位的HotSpot虚拟机中对象未被锁定的状态下，25bit用于存储对象哈希码，4bit用于存储对象分代年龄，2bit用于存储锁标志位，1bit固定为0；在其他状态（轻量级锁定、重量级锁定、GC标志、可偏向）下对象的存储内容如下：</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/mark_word_object_header.png" alt="HotSpot虚拟机对象头"></p>
<ul>
<li>在代码进入同步块的时候，如果此同步对象没有被锁定，虚拟机首先将在当前线程的栈帧中建立一个名为锁记录（Lock Record）的空间，用于存储对象目前的Mark Word的拷贝（官方称之为Displaced Mark Word）；然后虚拟机将使用CAS操作尝试将对象的Mark Word更新为指向Lock Record的指针，如果更新成功了那么这个线程就拥有了该对象的锁，并且对象Mark Word的锁标志位将转变为“00”，即表示此对象处于轻量级锁定状态；如果这个更新操作失败了，虚拟机首先会检查对象的Mark Word是否指向当前线程的栈帧，如果是就可以直接进入同步块继续执行，否则说明这个锁对象已经被其他线程抢占了；如果有两条以上的线程争用同一个锁，那轻量级锁就不再有效，要膨胀为重量级锁，锁标志的状态值变为“10”，Mark Word中存储的就是指向重量级锁的指针，后面等待锁的线程也要进行阻塞状态；</li>
<li>轻量级锁能提升程序同步性能的依据是“对于绝大部分的锁，在整个同步周期内都是不存在竞争的”，这是一个经验数据；</li>
</ul>
<h4 id="13-3-5-偏向锁"><a href="#13-3-5-偏向锁" class="headerlink" title="13.3.5 偏向锁"></a>13.3.5 偏向锁</h4><ul>
<li>偏向锁也是JDK 1.6中引入的一项锁优化，它的目的是消除数据在无竞争情况下的同步原语，进一步提高程序的运行性能；如果说轻量级锁是在无竞争的情况下使用CAS操作去消除同步使用的互斥量，那偏向锁就是在无竞争的情况下把整个同步都消除掉，连CAS操作都不做了；</li>
<li>偏向锁会偏向于第一个获得它的线程，如果在接下来的执行过程中，该锁没有被其他的线程获取，则持有偏向锁的线程将永远不需要再进行同步；</li>
<li>假设当前虚拟机启动了偏向锁，那么当锁对象第一次被线程获取的时候，虚拟机将会把对象头中的标志位设为“01”，即偏向模式；同时使用CAS操作把获取到这个锁的线程ID记录在对象的Mark Word之中；如果CAS操作成功，持有偏向锁的线程以后每次进入这个锁相关的同步块时，虚拟机都可以不再进行任何同步操作；当有另外一个线程去尝试获取这个锁时，偏向模式就宣告结束，根据锁对象目前是否被锁定的状态，撤销偏向后恢复到未锁定或轻量级锁定的状态，后续的同步操作就如上面介绍的轻量级锁那样执行；偏向锁、轻量级锁的状态转化以及对象Mark Work的关系如下图所示：</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/mark_word_for_lock.png" alt="偏向锁、轻量级锁的状态转化"></p>
<ul>
<li>偏向锁可以提高带有同步但无竞争的程序性能，它同样是一个带有效益权衡性质的优化；</li>
</ul>
<h3 id="本章小结"><a href="#本章小结" class="headerlink" title="本章小结"></a>本章小结</h3><p>本章介绍了线程安全所涉及的概念和分类、同步实现的方式及虚拟机的底层运行原理，并且介绍了虚拟机为了实现高效并发所采取的一系列锁优化措施。</p>
<p><strong>系列读书笔记</strong></p>
<ul>
<li><a href="http://ginobefunny.com/post/deep_in_jvm_notes_part1">《深入理解Java虚拟机》读书笔记1：Java技术体系、Java内存区域和内存溢出异常</a></li>
<li><a href="http://ginobefunny.com/post/deep_in_jvm_notes_part2">《深入理解Java虚拟机》读书笔记2：垃圾收集器与内存分配策略</a></li>
<li><a href="http://ginobefunny.com/post/deep_in_jvm_notes_part3">《深入理解Java虚拟机》读书笔记3：虚拟机性能监控与调优实战</a></li>
<li><a href="http://ginobefunny.com/post/deep_in_jvm_notes_part4">《深入理解Java虚拟机》读书笔记4：类文件结构</a></li>
<li><a href="http://ginobefunny.com/post/deep_in_jvm_notes_part5">《深入理解Java虚拟机》读书笔记5：类加载机制与字节码执行引擎</a></li>
<li><a href="http://ginobefunny.com/post/deep_in_jvm_notes_part6">《深入理解Java虚拟机》读书笔记6：程序编译与代码优化</a></li>
<li><a href="http://ginobefunny.com/post/deep_in_jvm_notes_part7">《深入理解Java虚拟机》读书笔记7：高效并发</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;国内JVM相关书籍NO.1，Java程序员必读。读书笔记第七部分对应原书的第十二章和第十三章，主要介绍Java内存模型、先行发生原则、线程安全和虚拟机的锁优化细节。&lt;br&gt;
    
    </summary>
    
      <category term="JVM" scheme="http://ginobefunny.com/categories/JVM/"/>
    
    
      <category term="Java" scheme="http://ginobefunny.com/tags/Java/"/>
    
      <category term="读书笔记" scheme="http://ginobefunny.com/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
      <category term="JVM" scheme="http://ginobefunny.com/tags/JVM/"/>
    
      <category term="虚拟机" scheme="http://ginobefunny.com/tags/%E8%99%9A%E6%8B%9F%E6%9C%BA/"/>
    
      <category term="HotSpot" scheme="http://ginobefunny.com/tags/HotSpot/"/>
    
      <category term="内存模型" scheme="http://ginobefunny.com/tags/%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/"/>
    
      <category term="volatile" scheme="http://ginobefunny.com/tags/volatile/"/>
    
      <category term="synchronized" scheme="http://ginobefunny.com/tags/synchronized/"/>
    
      <category term="锁" scheme="http://ginobefunny.com/tags/%E9%94%81/"/>
    
      <category term="同步" scheme="http://ginobefunny.com/tags/%E5%90%8C%E6%AD%A5/"/>
    
      <category term="并发" scheme="http://ginobefunny.com/tags/%E5%B9%B6%E5%8F%91/"/>
    
      <category term="多线程" scheme="http://ginobefunny.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"/>
    
      <category term="先行发生原则" scheme="http://ginobefunny.com/tags/%E5%85%88%E8%A1%8C%E5%8F%91%E7%94%9F%E5%8E%9F%E5%88%99/"/>
    
  </entry>
  
  <entry>
    <title>《深入理解Java虚拟机》读书笔记6：程序编译与代码优化</title>
    <link href="http://ginobefunny.com/post/deep_in_jvm_notes_part6/"/>
    <id>http://ginobefunny.com/post/deep_in_jvm_notes_part6/</id>
    <published>2017-01-26T09:30:57.000Z</published>
    <updated>2017-01-31T09:50:48.931Z</updated>
    
    <content type="html"><![CDATA[<p>国内JVM相关书籍NO.1，Java程序员必读。读书笔记第六部分对应原书的第十章和第十一章，主要介绍javac编译过程、HotSpot的即时编译器以及常见的编译优化技术，通过了解这部分的内容有利于我们更好的编码。<br><a id="more"></a></p>
<h1 id="第四部分-程序编译与代码优化"><a href="#第四部分-程序编译与代码优化" class="headerlink" title="第四部分 程序编译与代码优化"></a>第四部分 程序编译与代码优化</h1><h2 id="第十章-早期（编译器）优化"><a href="#第十章-早期（编译器）优化" class="headerlink" title="第十章 早期（编译器）优化"></a>第十章 早期（编译器）优化</h2><h3 id="10-1-概述"><a href="#10-1-概述" class="headerlink" title="10.1 概述"></a>10.1 概述</h3><ul>
<li>前端编译器（或叫编译器前端）：把<em>.java文件转变为</em>.class文件的过程，比如Sun的javac、Eclipse JDT中的ECJ；</li>
<li>后端运行编译器（JIT编译器）：把字节码转变为机器码的过程，比如HotSpot VM的C1、C2编译器；</li>
<li>静态提前编译器（AOT编译器）：直接把*.java文件编译成本地机器代码的过程，比如GNU Compiler for the Java；</li>
<li>本章主要针对第一类，把第二类的编译过程留到下一章讨论；</li>
<li>javac这类编译器对代码运行效率几乎没有任何优化措施，虚拟机设计团队把对性能的优化集中到了后端的即时编译器中，这样那些不是由javac产生的Class文件也同样能享受到编译器优化所带来的好处；</li>
<li>javac做了许多针对Java语言编码过程的优化措施来改善程序员的编码风格和提高编码效率；可以说，Java中即时编译器在运行期的优化过程对于程序运行来说更重要，而前端编译器在编译器的优化过程对于程序编码来说关系更加密切；</li>
</ul>
<h3 id="10-2-javac编译器"><a href="#10-2-javac编译器" class="headerlink" title="10.2 javac编译器"></a>10.2 javac编译器</h3><p>javac编译器本身就是一个由Java语言编写的程序，这为纯Java的程序员了解它的编译过程带来了很大的便利。</p>
<h4 id="10-2-1-javac的源码与调试"><a href="#10-2-1-javac的源码与调试" class="headerlink" title="10.2.1 javac的源码与调试"></a>10.2.1 javac的源码与调试</h4><ul>
<li>javac的源码存放在JDK_SRC_HOME/langtools/src/share/classes/com/sun/tools/javac，除了JDK自身的API外，就只引用了JDK_SRC_HOME/langtools/src/share/classes/com/sun/*里面的代码；</li>
<li>导入javac的源码后就可以运行com.sun.tools.javac.Main的main方法来执行编译了；</li>
<li>javac编译过程大概可以分为3个过程：解析与填充符号表过程、插入式注解处理器的注解处理过程、分析与字节码生成过程；</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/javac_compiler.png" alt="javac编译过程的主体代码"></p>
<h4 id="10-2-2-解析与填充符号表"><a href="#10-2-2-解析与填充符号表" class="headerlink" title="10.2.2 解析与填充符号表"></a>10.2.2 解析与填充符号表</h4><ul>
<li>解析步骤由parseFiles方法完成；</li>
<li>词法分析将源代码的字符流转变为标记（Token）集合，由com.sun.tools.javac.parser.Scanner类完成；</li>
<li>语法分析是根据Token序列构造抽象语法树（AST，一种用来描述程序代码语法结构的树形表示方式）的过程，由com.sun.tools.javac.parser.Parser类实现，AST由com.sun.tools.javac.tree.JCTree类表示；</li>
<li>填充符号表：由enterTrees方法完成；符号表是由一组符号地址和符号信息构成的表格，所登记的信息在编译的不同阶段都要用到，在语义分析中用于语义检查，在目标代码生成时用于地址分配；由com.sun.tools.javac.comp.Enter类实现；</li>
</ul>
<h4 id="10-2-3-注解处理器"><a href="#10-2-3-注解处理器" class="headerlink" title="10.2.3 注解处理器"></a>10.2.3 注解处理器</h4><ul>
<li>在JDK 1.6中实现了JSR-269规范，提供了一组插入式注解处理器的标准API在编译期间对注解进行处理，可以读取、修改、添加抽象语法树中的任意元素；</li>
<li>通过插入式注解处理器实现的插件在功能上有很大的发挥空间，程序员可以使用插入式注解处理器来实现许多原本只能在编码中完成的事情；</li>
<li>javac中，在initProcessAnnotations初始化，在processAnnotations执行，如果有新的注解处理器，通过com.sun.tools.javac.processing.JavacProcessingEnviroment类的doProcessing方法生成一个新的JavaCompiler对象对编译的后续步骤进行处理；</li>
</ul>
<h4 id="10-2-4-语义分析与字节码生成"><a href="#10-2-4-语义分析与字节码生成" class="headerlink" title="10.2.4 语义分析与字节码生成"></a>10.2.4 语义分析与字节码生成</h4><ul>
<li>语义分析的主要任务是对结构上正确的源程序进行上下文有关性质的审查，主要包括标注检查、数据及控制流分析两个步骤；</li>
<li>解语法糖（Syntactic Sugar，添加的某种对语言功能没有影响但方便程序员使用的语法）：Java中最常用的语法糖主要是泛型、变长参数、自动装箱等，他们在编译阶段还原回简单的基础语法结构；在com.sun.tools.javac.comp.TransTypes类和com.sun.tools.javac.comp.Lower类中完成；</li>
<li>字节码生成：javac编译的最后一个阶段，不仅仅是把前面各个步骤所生成的信息转化为字节码写入到磁盘中，编译器还进行了少量的代码添加和转换工作（如实例构造器<init>方法和类构造器<clinit>方法）；由com.sun.tools.javac.jvm.ClassWriter类的writeClass方法输出字节码，生成最终的Class文件；</clinit></init></li>
</ul>
<h3 id="10-3-Java语法糖的味道"><a href="#10-3-Java语法糖的味道" class="headerlink" title="10.3 Java语法糖的味道"></a>10.3 Java语法糖的味道</h3><h4 id="10-3-1-泛型与类型擦除"><a href="#10-3-1-泛型与类型擦除" class="headerlink" title="10.3.1 泛型与类型擦除"></a>10.3.1 泛型与类型擦除</h4><ul>
<li>Java语言的泛型只在程序源码中存在，在编译后的字节码文件中，就已经替换为原来的原生类型了，并且在相应的地方插入了强制转换，这种基于类型擦除的泛型实现是一种伪泛型；</li>
<li>JCP组织引入了Signature属性，它的作用就是存储一个方法在字节码层面的特征签名，这个属性中保存的参数类型并不是原生类型，而是包括了参数化类型的信息，这样我们就可以通过反射手段获取参数化类型；</li>
</ul>
<h4 id="10-3-2-自动装箱、拆箱与遍历循环"><a href="#10-3-2-自动装箱、拆箱与遍历循环" class="headerlink" title="10.3.2 自动装箱、拆箱与遍历循环"></a>10.3.2 自动装箱、拆箱与遍历循环</h4><ul>
<li>它们的实现比较简单，但却是Java语言里使用最多的语法糖；</li>
</ul>
<h4 id="10-3-3-条件编译"><a href="#10-3-3-条件编译" class="headerlink" title="10.3.3 条件编译"></a>10.3.3 条件编译</h4><ul>
<li>Java语言之中并没有使用预处理器，因为Java编译器并非一个个地编译Java文件，而是将所有编译单元的语法树顶级节点输入到待处理列表后再进行编译；</li>
<li>Java语言可以使用条件为常量的if语句进行条件编译；编译器将会把分支中不成立的代码块消除掉；</li>
</ul>
<h3 id="10-4-实战：插入式注解处理器"><a href="#10-4-实战：插入式注解处理器" class="headerlink" title="10.4 实战：插入式注解处理器"></a>10.4 实战：插入式注解处理器</h3><ul>
<li>实战目标：使用注解处理器API来编写一款拥有自己编码风格的校验工具；</li>
<li>代码实现：继承javax.annotation.processing.AbstractProcessor，实现process方法，从第一个参数annotations获取此注解处理器所要处理的注解集合，从第二个参数roundEnv中访问到当前这个Round中的语法树节点；另外还有一个很常用的实例变量processingEnv，它代表了注解处理器框架提供的一个上下文环境；可以配合使用的@SupportedAnnotationTypes和@SupportedSourceVersion注解；</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="meta">@SupportedAnnotationTypes</span>(<span class="string">"*"</span>)</div><div class="line"><span class="meta">@SupportedSourceVersion</span>(SourceVersion.RELEASE_6)</div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">NameCheckProcessor</span> <span class="keyword">extends</span> <span class="title">AbstractProcessor</span></span>&#123;</div><div class="line">    </div><div class="line">    <span class="keyword">private</span> NameChecker nameChecker;</div><div class="line">    </div><div class="line">    <span class="meta">@Override</span></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">init</span><span class="params">(ProcessingEnviroment processingEnv)</span></span>&#123;</div><div class="line">        <span class="keyword">super</span>.init(processingEnv);</div><div class="line">        nameChecker = <span class="keyword">new</span> NameChecker(processingEnv);</div><div class="line">    &#125;</div><div class="line">    </div><div class="line">    <span class="meta">@Override</span></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">process</span><span class="params">(Set&lt;? extends TypeElement&gt; annotations, RoundEnviroment roundEnv)</span></span>&#123;</div><div class="line">        <span class="keyword">if</span>(!roundEnv.processingOver)&#123;</div><div class="line">            <span class="keyword">for</span>(Element element : roundEnv.getRootElements())&#123;</div><div class="line">                nameChecker.checkNames(element);</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">        </div><div class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</div><div class="line">    &#125;</div><div class="line">    </div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="10-5-本章小结"><a href="#10-5-本章小结" class="headerlink" title="10.5 本章小结"></a>10.5 本章小结</h3><p>本章我们从编译器源码实现的层次上了解了javac源代码编译为字节码的过程，分析了Java语言中多种语法糖的前因后果，并实战实习了如何使用插入式注解处理器来完成一个检查程序命名规范的编译器插件。下一章我们将会介绍即时编译器的运作和优化过程。</p>
<h2 id="第十一章-晚期（运行期）优化"><a href="#第十一章-晚期（运行期）优化" class="headerlink" title="第十一章 晚期（运行期）优化"></a>第十一章 晚期（运行期）优化</h2><h3 id="11-1-概述"><a href="#11-1-概述" class="headerlink" title="11.1 概述"></a>11.1 概述</h3><ul>
<li>为了提高热点代码的执行效率，在运行时虚拟机将会把这些代码编译成与本地平台相关的机器码，并进行各种层次的优化，完成这个任务的编译器称为即时编译器（JIT）；</li>
<li>JIT不是虚拟机必需的，但是其编译性能的好坏、代码优化程度的高低却是衡量一款商用虚拟机优秀与否的最关键的指标之一，它也是虚拟机中最核心且最能体现虚拟机技术水平的部分；</li>
</ul>
<h3 id="11-2-HotSpot虚拟机内的即时编译器"><a href="#11-2-HotSpot虚拟机内的即时编译器" class="headerlink" title="11.2 HotSpot虚拟机内的即时编译器"></a>11.2 HotSpot虚拟机内的即时编译器</h3><h4 id="11-2-1-解释器与编译器"><a href="#11-2-1-解释器与编译器" class="headerlink" title="11.2.1 解释器与编译器"></a>11.2.1 解释器与编译器</h4><ul>
<li>当程序需要迅速启动和执行的时候，解释器可以先发挥作用，省去编译的时间立即执行；在程序运行后，随着时间的推移，编译器把越来越多的代码编译成本地代码提升执行效率；</li>
<li>HotSpot虚拟机中内置了两个即时编译器，分别为Client Compiler和Server Compiler，或简称为C1编译器和C2编译器；虚拟机会根据自身版本与宿主机器的硬件性能自动选择运行模式，也可以使用“-client”或“-server”参数去强制指定运行模式；</li>
<li>想要编译出优化程度更高的代码，解释器可能还要替编译器收集性能监控信息，为了在程序启动响应速度与运行效率之间达到最佳平衡，HotSpot虚拟机还会逐渐启动分层编译的策略：第0层，程序解释运行；第1层，C1编译；第2层，C2编译；</li>
<li>实施分层编译后，Client Compiler和Server Compiler将会同时工作，许多代码都可能会被多次编译，用Client Compiler获取更高的编译速度，用Server Compiler来获取更好的编译质量，在解释执行的时候也无须再承担性能收集监控信息的任务；</li>
</ul>
<h4 id="11-2-2-编译对象与触发条件"><a href="#11-2-2-编译对象与触发条件" class="headerlink" title="11.2.2 编译对象与触发条件"></a>11.2.2 编译对象与触发条件</h4><ul>
<li>被JIT编译的热点代码有两类：被多次调用的方法、被多次执行的循环体；对于前者编译器会以整个方法作为编译对象，属于标准的JIT编译方式；对于后者尽管编译动作是由循环体所触发的，但编译器依然会以整个方法作为编译对象，这种编译方式称之为栈上替换（OSR编译）；</li>
<li>热点探测：基于采样的热点探测和基于计数器的热点探测，在HotSpot虚拟机中使用的是第二种，通过方法计数器和回边计数器进行热点探测。方法调用计数器触发的即时编译交互过程如下图所示：</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/javac_jit.png" alt="调用计数器触发的即时编译"></p>
<h4 id="11-2-3-编译过程"><a href="#11-2-3-编译过程" class="headerlink" title="11.2.3 编译过程"></a>11.2.3 编译过程</h4><ul>
<li>对于Client Compiler来说，它是一个简单快速的三段式编译器，主要的关注点在于局部性的优化，而放弃了很多耗时较长的全局优化手段；第一阶段一个平台独立的前端将字节码构造成一个高级中间代码表示（HIR），第二阶段一个平台相关的后端从HIR中产生低级中间代码表示（LIR），最后阶段是在平台相关的后端使用线性扫描算法在LIR上分配寄存器，并在LIR上做窥孔优化，然后产生机器代码。其大致过程如下所示：</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/javac_client_compiler.png" alt="Client Compiler架构"></p>
<ul>
<li>Server Compiler是专门面向服务端的典型应用并为服务端的性能配置特别调整过的编译器，也是一个充分优化过的高级编译器，几乎能达到GNU C++编译器使用-02参数时的优化强大，它会执行所有经典的优化动作，如无用代码消除、循环展开、循环表达式外提、消除公共子表达式、常量传播、基本块重排序等，还会实现如范围检查消除、空值检查消除等Java语言特性密切相关的优化技术；</li>
</ul>
<h4 id="11-2-4-查看及分析即时编译结果"><a href="#11-2-4-查看及分析即时编译结果" class="headerlink" title="11.2.4 查看及分析即时编译结果"></a>11.2.4 查看及分析即时编译结果</h4><ul>
<li>本节的运行参数有一部分需要Debug或FastDebug版虚拟机的支持；</li>
<li>要知道某个方法是否被编译过，可以使用参数-XX:+PrintCompilation要求虚拟机在即时编译时将被编译成本地代码的方法名称打印出来；</li>
<li>还可以加上参数-XX:+PrintInlining要求虚拟机输出方法内联信息，输出内容如下：</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/javac_jit_viewer.png" alt="编译结果"></p>
<ul>
<li>除了查看那些方法被编译之外，还可以进一步查看即时编译器生成的机器码内容，这个需要结合虚拟机提供的反汇编接口来阅读；</li>
</ul>
<h3 id="11-3-编译优化技术"><a href="#11-3-编译优化技术" class="headerlink" title="11.3 编译优化技术"></a>11.3 编译优化技术</h3><h4 id="11-3-1-优化技术概览"><a href="#11-3-1-优化技术概览" class="headerlink" title="11.3.1 优化技术概览"></a>11.3.1 优化技术概览</h4><p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/javac_jit_tech1.png" alt="优化技术概览1"><br><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/javac_jit_tech2.png" alt="优化技术概览2"></p>
<h4 id="11-3-2-公共子表达式消除"><a href="#11-3-2-公共子表达式消除" class="headerlink" title="11.3.2 公共子表达式消除"></a>11.3.2 公共子表达式消除</h4><ul>
<li>如果一个表达式E已经计算过了，并且从先前的计算到现在E中所有变量的值都没有发生变化，那么E的这次出现就成为了公共子表达式，只需要直接用前面计算过的表达式结果代替E就可以了；</li>
</ul>
<h4 id="11-3-3-数组边界检查消除"><a href="#11-3-3-数组边界检查消除" class="headerlink" title="11.3.3 数组边界检查消除"></a>11.3.3 数组边界检查消除</h4><ul>
<li>对于虚拟机的执行子系统来说，每次数组元素的读写都带有一次隐含的条件判断，对于拥有大量数组访问的程序代码无疑是一种性能负担；</li>
</ul>
<h4 id="11-3-4-方法内联"><a href="#11-3-4-方法内联" class="headerlink" title="11.3.4 方法内联"></a>11.3.4 方法内联</h4><ul>
<li>除了消除方法调用的成本外更重要的意义是为其他优化手段建立良好的基础；</li>
<li>为了解决虚方法的内联问题，引入了类型继承关系分析（CHA）技术和内联缓存（Inline Cache）来完成方法内联；</li>
</ul>
<h4 id="11-3-5-逃逸分析"><a href="#11-3-5-逃逸分析" class="headerlink" title="11.3.5 逃逸分析"></a>11.3.5 逃逸分析</h4><ul>
<li>逃逸分析的基本行为就是分析对象动态作用域，当一个对象在方法中被定义后，它可能被外部方法所引用（方法逃逸），甚至还可能被外部线程所访问到（线程逃逸）；如果能证明一个对象不会逃逸到方法或线程之外，则可能为这个变量进行一些高效的优化，比如栈上分配（减轻垃圾收集的压力）、同步消除（读写不会有竞争）、标量替换；</li>
</ul>
<h3 id="11-4-Java与C-C-的编译器对比"><a href="#11-4-Java与C-C-的编译器对比" class="headerlink" title="11.4 Java与C/C++的编译器对比"></a>11.4 Java与C/C++的编译器对比</h3><ul>
<li>Java虚拟机的即时编译器与C/C++的静态优化编译器相比，可能会由于下列这些原因而导致输出的本地代码有一些劣势：即时编译器运行占用用户程序运行时间、动态类型安全语言导致的频繁检查、运行时对方法接收者进行多态选择的频率大、可以动态扩展导致很多全局的优化难以运行、大部分对象在堆上分配导致垃圾收集机制的效率低；</li>
<li>Java语言的特性换取了开发效率的提升、还有许多优化是静态优化编译器不好做的，比如别名分析、还有一些以运行期性能监控为基础的优化措施如调用频率预测等；</li>
</ul>
<h3 id="11-5-本章小结"><a href="#11-5-本章小结" class="headerlink" title="11.5 本章小结"></a>11.5 本章小结</h3><p>本章我们着重了解了虚拟机的热点探测方法、HotSpot的即时编译器、编译触发条件以及如何从虚拟机外部观察和分析JIT编译的数据和结果，还选择了集中场景的编译期优化技术进行讲解。对Java编译器的深入了解，有助于在工作中分辨哪些代码是编译器可以帮我们处理的，哪些代码需要自己调节以便更适合编译器的优化。</p>
<p><strong>系列读书笔记</strong></p>
<ul>
<li><a href="http://ginobefunny.com/post/deep_in_jvm_notes_part1">《深入理解Java虚拟机》读书笔记1：Java技术体系、Java内存区域和内存溢出异常</a></li>
<li><a href="http://ginobefunny.com/post/deep_in_jvm_notes_part2">《深入理解Java虚拟机》读书笔记2：垃圾收集器与内存分配策略</a></li>
<li><a href="http://ginobefunny.com/post/deep_in_jvm_notes_part3">《深入理解Java虚拟机》读书笔记3：虚拟机性能监控与调优实战</a></li>
<li><a href="http://ginobefunny.com/post/deep_in_jvm_notes_part4">《深入理解Java虚拟机》读书笔记4：类文件结构</a></li>
<li><a href="http://ginobefunny.com/post/deep_in_jvm_notes_part5">《深入理解Java虚拟机》读书笔记5：类加载机制与字节码执行引擎</a></li>
<li><a href="http://ginobefunny.com/post/deep_in_jvm_notes_part6">《深入理解Java虚拟机》读书笔记6：程序编译与代码优化</a></li>
<li><a href="http://ginobefunny.com/post/deep_in_jvm_notes_part7">《深入理解Java虚拟机》读书笔记7：高效并发</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;国内JVM相关书籍NO.1，Java程序员必读。读书笔记第六部分对应原书的第十章和第十一章，主要介绍javac编译过程、HotSpot的即时编译器以及常见的编译优化技术，通过了解这部分的内容有利于我们更好的编码。&lt;br&gt;
    
    </summary>
    
      <category term="JVM" scheme="http://ginobefunny.com/categories/JVM/"/>
    
    
      <category term="Java" scheme="http://ginobefunny.com/tags/Java/"/>
    
      <category term="读书笔记" scheme="http://ginobefunny.com/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
      <category term="JVM" scheme="http://ginobefunny.com/tags/JVM/"/>
    
      <category term="虚拟机" scheme="http://ginobefunny.com/tags/%E8%99%9A%E6%8B%9F%E6%9C%BA/"/>
    
      <category term="字节码" scheme="http://ginobefunny.com/tags/%E5%AD%97%E8%8A%82%E7%A0%81/"/>
    
      <category term="HotSpot" scheme="http://ginobefunny.com/tags/HotSpot/"/>
    
      <category term="编译器" scheme="http://ginobefunny.com/tags/%E7%BC%96%E8%AF%91%E5%99%A8/"/>
    
      <category term="即时编译器" scheme="http://ginobefunny.com/tags/%E5%8D%B3%E6%97%B6%E7%BC%96%E8%AF%91%E5%99%A8/"/>
    
      <category term="解释器" scheme="http://ginobefunny.com/tags/%E8%A7%A3%E9%87%8A%E5%99%A8/"/>
    
  </entry>
  
  <entry>
    <title>基于Elasticsearch实现搜索建议</title>
    <link href="http://ginobefunny.com/post/search_suggestion_implemention_based_elasticsearch/"/>
    <id>http://ginobefunny.com/post/search_suggestion_implemention_based_elasticsearch/</id>
    <published>2017-01-23T09:14:44.000Z</published>
    <updated>2017-01-23T12:30:46.686Z</updated>
    
    <content type="html"><![CDATA[<p>搜索建议是搜索的一个重要组成部分，一个搜索建议的实现通常需要考虑建议词的来源、匹配、排序、聚合、关联的文档数和拼写纠错等，本文介绍一个基于Elasticsearch实现的搜索建议。</p>
<a id="more"></a>
<h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>电商网站的搜索是最基础最重要的功能之一，搜索框上面的良好体验能为电商带来更高的收益，我们先来看看淘宝、京东、亚马逊网站的搜索建议。</p>
<p>在淘宝的搜索框输入【卫衣】时，下方的搜索建议包括建议词以及相关的标签：<br><img src="http://oi46mo3on.bkt.clouddn.com/11_es_suggestion/suggestion_taobao.png" alt="淘宝的搜索建议"></p>
<p>在京东的搜索框输入【卫衣】时，下方搜索建议右方显示建议词关联的商品数量：<br><img src="http://oi46mo3on.bkt.clouddn.com/11_es_suggestion/suggestion_jindong.png" alt="京东的搜索建议"></p>
<p>在亚马逊的搜索框输入【卫衣】时，搜索建议上部分能支持在特定的分类下进行搜索：<br><img src="http://oi46mo3on.bkt.clouddn.com/11_es_suggestion/suggestion_amazon.png" alt="亚马逊的搜索建议"></p>
<p>通过上述对比可以看出，不同的电商对于搜索建议的侧重点略有不同，但核心的问题包括：</p>
<ul>
<li>匹配：能够通过用户的输入进行前缀匹配；</li>
<li>排序：根据建议词的优先级进行排序；</li>
<li>聚合：能够根据建议词关联的商品进行聚合，比如聚合分类、聚合标签等；</li>
<li>纠错：能够对用户的输入进行拼写纠错；</li>
</ul>
<h2 id="搜索建议实现"><a href="#搜索建议实现" class="headerlink" title="搜索建议实现"></a>搜索建议实现</h2><p>在我们的搜索建议实现里，主要考虑了建议词的来源、匹配、排序、关联的商品数量和拼写纠错。</p>
<h3 id="SuggestionDiscovery"><a href="#SuggestionDiscovery" class="headerlink" title="SuggestionDiscovery"></a>SuggestionDiscovery</h3><ul>
<li>SuggestionDiscovery的职责是发现建议词；</li>
<li>建议词的来源可以是商品的分类名称、品牌名称、商品标签、商品名称的高频词、热搜词，也可以是一些组合词，比如“分类 + 性别”和“分类 + 标签”，还可以是一些自定义添加的词；</li>
<li>建议词维护的时候需要考虑去重，比如“卫衣男”和“卫衣 男”应该是相同的，“Nike”和“nike”也应该是相同的；</li>
<li>由于建议词的来源通常比较稳定，所以执行的周期可以比较长一点，比如每周一次；</li>
</ul>
<h3 id="SuggestionCounter"><a href="#SuggestionCounter" class="headerlink" title="SuggestionCounter"></a>SuggestionCounter</h3><ul>
<li>SuggestionCounter的职责是获取建议词关联的商品数量，如果需要可以进行一些聚合操作，比如聚合分类和标签；</li>
<li>SuggestionCounter的实现的时候由于要真正地调用搜索接口，应该尽量避免对用户搜索的影响，比如在凌晨执行并且使用单线程调用；</li>
<li>为了提升效率，应该使用Elasticsearch的Multi Search接口批量进行count，同时批量更新数据库里建议词的count值；</li>
<li>由于SuggestionCounter是比较耗资源的，可以考虑延长执行的周期，但是这可能会带来count值与实际搜索时误差较大的问题，这个需要根据实际情况考虑；</li>
</ul>
<h3 id="SuggestionIndexRebuiler"><a href="#SuggestionIndexRebuiler" class="headerlink" title="SuggestionIndexRebuiler"></a>SuggestionIndexRebuiler</h3><ul>
<li>SuggestionIndexRebuiler的职责是负责重建索引；</li>
<li>考虑到用户的搜索习惯，可以使用<a href="https://www.elastic.co/guide/en/elasticsearch/reference/2.3/multi-fields.html#_multi_fields_with_multiple_analyzers" target="_blank" rel="external">Multi-fields</a>来给建议词增加多个分析器。比如对于【卫衣 套头】的建议词使用Multi-fields增加不分词字段、拼音分词字段、拼音首字母分词字段、IK分词字段，这样输入【weiyi】和【套头】都可以匹配到该建议词；</li>
<li>重建索引时通过是通过bulk批量添加到临时索引中，然后通过别名来更新；</li>
<li>重建索引的数据依赖于SuggestionCounter，因此其执行的周期应该与SuggestionCounter保持一致；</li>
</ul>
<h3 id="SuggestionService"><a href="#SuggestionService" class="headerlink" title="SuggestionService"></a>SuggestionService</h3><ul>
<li>SuggestionService是真正处于用户搜索建议的服务类；</li>
<li>通常的实现是先到缓存中查询是否能匹配到缓存记录，如果能匹配到则直接返回；否则的话调用Elasticsearch的<a href="https://www.elastic.co/guide/en/elasticsearch/reference/2.3/query-dsl-prefix-query.html" target="_blank" rel="external">Prefix Query</a>进行搜索，由于我们在重建索引的时候定义了Multi-fields，在搜索的时候应该用boolQuery来处理；如果此时Elasticsearch返回不为空的结果数据，那么加入缓存并返回即可；</li>
</ul>
<pre><code>POST /suggestion/_search
{
  &quot;from&quot; : 0,
  &quot;size&quot; : 10,
  &quot;query&quot; : {
    &quot;bool&quot; : {
      &quot;must&quot; : {
        &quot;bool&quot; : {
          &quot;should&quot; : [ {
            &quot;prefix&quot; : {
              &quot;keyword&quot; : &quot;卫衣&quot;
            }
          }, {
            &quot;prefix&quot; : {
              &quot;keyword.keyword_ik&quot; : &quot;卫衣&quot;
            }
          }, {
            &quot;prefix&quot; : {
              &quot;keyword.keyword_pinyin&quot; : &quot;卫衣&quot;
            }
          }, {
            &quot;prefix&quot; : {
              &quot;keyword.keyword_first_py&quot; : &quot;卫衣&quot;
            }
          } ]
        }
      },
      &quot;filter&quot; : {
        &quot;range&quot; : {
          &quot;count&quot; : {
            &quot;from&quot; : 5,
            &quot;to&quot; : null,
            &quot;include_lower&quot; : true,
            &quot;include_upper&quot; : true
          }
        }
      }
    }
  },
  &quot;sort&quot; : [ {
    &quot;weight&quot; : {
      &quot;order&quot; : &quot;desc&quot;
    }
  }, {
    &quot;count&quot; : {
      &quot;order&quot; : &quot;desc&quot;
    }
  } ]
}
</code></pre><ul>
<li>如果Elasticsearch返回的是空结果，此时应该需要增加拼写纠错的处理（拼写纠错也可以在调用Elasticsearch搜索的时候带上，但是通常情况下用户并没有拼写错误，所以建议还是在后面单独调用suggester）；如果返回的suggest不为空，则根据新的词调用建议词服务；比如用户输入了【adidss】，调用Elasticsearch的suggester获取到的结果是【adidas】，则再根据adidas进行搜索建议词处理。</li>
</ul>
<pre><code>POST /suggestion/_search
{
  &quot;size&quot; : 0,
  &quot;suggest&quot; : {
    &quot;keyword_suggestion&quot; : {
      &quot;text&quot; : &quot;adidss&quot;,
      &quot;term&quot; : {
        &quot;field&quot; : &quot;keyword&quot;,
        &quot;size&quot; : 1
      }
    }
  }
}
</code></pre><ul>
<li>关于排序：在我们的实现里面是通过weight和count进行排序的，weight目前只考虑了建议词的类型（比如分类 &gt; 品牌 &gt; 标签）；</li>
</ul>
<h2 id="实现效果和后续改进"><a href="#实现效果和后续改进" class="headerlink" title="实现效果和后续改进"></a>实现效果和后续改进</h2><ul>
<li>通过上面的实现，我们已经能实现一个比较强大的搜索建议词了，实际的效果如下所示：</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/11_es_suggestion/suggestion_yoho.png" alt="最终效果"></p>
<ul>
<li>后续可以考虑的改进：参考亚马逊增加分类的聚合展示、增加用户个性化的处理支持更好的建议词排序、基于用户的搜索历史支持更好的建议词推荐；</li>
</ul>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/2.3/query-dsl-prefix-query.html" target="_blank" rel="external">Elasticsearch Prefix Query</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/2.3/search-suggesters.html" target="_blank" rel="external">Elasticsearch Suggester</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;搜索建议是搜索的一个重要组成部分，一个搜索建议的实现通常需要考虑建议词的来源、匹配、排序、聚合、关联的文档数和拼写纠错等，本文介绍一个基于Elasticsearch实现的搜索建议。&lt;/p&gt;
    
    </summary>
    
      <category term="Elasticsearch" scheme="http://ginobefunny.com/categories/Elasticsearch/"/>
    
    
      <category term="Elasticsearch" scheme="http://ginobefunny.com/tags/Elasticsearch/"/>
    
      <category term="搜索建议" scheme="http://ginobefunny.com/tags/%E6%90%9C%E7%B4%A2%E5%BB%BA%E8%AE%AE/"/>
    
  </entry>
  
</feed>
