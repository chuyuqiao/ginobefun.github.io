<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>GinoBeFunny</title>
  <subtitle>Be a creator of knowledge.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://ginobefunny.com/"/>
  <updated>2017-03-28T11:51:39.026Z</updated>
  <id>http://ginobefunny.com/</id>
  
  <author>
    <name>Gino Zhang</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>基于word2vec和Elasticsearch实现个性化搜索</title>
    <link href="http://ginobefunny.com/post/personalized_search_implemention_based_word2vec_and_elasticsearch/"/>
    <id>http://ginobefunny.com/post/personalized_search_implemention_based_word2vec_and_elasticsearch/</id>
    <published>2017-03-28T07:51:02.000Z</published>
    <updated>2017-03-28T11:51:39.026Z</updated>
    
    <content type="html"><![CDATA[<p>在<a href="http://ginobefunny.com/post/learning_word2vec/">word2vec学习小记</a>一文中我们曾经学习了word2vec这个工具，它基于神经网络语言模型并在其基础上进行优化，最终能获取词向量和语言模型。在我们的商品搜索系统里，采用了word2vec的方式来计算用户向量和商品向量，并通过Elasticsearch的function_score评分机制和自定义的脚本插件来实现个性化搜索。</p>
<a id="more"></a> 
<h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h1><p>先来看下<a href="https://en.wikipedia.org/wiki/Personalized_search" target="_blank" rel="external">维基百科</a>上对于个性化搜索的定义和介绍：</p>
<blockquote>
<p>Personalized search refers to web search experiences that are tailored specifically to an individual’s interests by incorporating information about the individual beyond specific query provided. Pitkow et al. describe two general approaches to personalizing search results, one involving modifying the user’s query and the other re-ranking search results.</p>
</blockquote>
<p>由此我们可以得到两个重要的信息：</p>
<ol>
<li>个性化搜索需要充分考虑到用户的偏好，将<strong>用户感兴趣的内容</strong>优先展示给用户；</li>
<li>另外是对于实现个性化的方式上主要有<strong>查询修改和对搜索结果的重排序</strong>两种。</li>
</ol>
<p>而对我们电商网站来说，个性化搜索的重点是当用户搜索某个关键字，如【卫衣】时，能将用户最感兴趣最可能购买的商品（如用户偏好的品牌或款式）优先展示给用户，以提升用户体验和点击转化。</p>
<h1 id="设计思路"><a href="#设计思路" class="headerlink" title="设计思路"></a>设计思路</h1><ol>
<li>在此之前我们曾经有一般的个性化搜索实现，其主要是通过计算用户和商品的一些重要属性（比如品牌、品类、性别等）的权重，然后得到一个用户和商品之间的关联系数，然后根据该系数进行重排序。</li>
<li>但是这一版从效果来看并不是很好，我个人觉得主要的原因有以下几点：用户对商品的各个属性的重视程度并不是一样的，另外考虑的商品的属性并不全，且没有去考虑商品和商品直接的关系；</li>
<li>在新的版本的设计中，我们考虑通过<strong>用户的浏览记录这种时序数据</strong>来获取用户和商品以及商品和商品直接的关联关系，其核心就是通过类似于语言模型的词出现的顺序来训练向量表示结果；</li>
<li>在获取用户向量和商品向量表示后，我们就可以<strong>根据向量直接的距离来计算相关性</strong>，从而将用户感兴趣的商品优先展示；</li>
</ol>
<h1 id="实现细节"><a href="#实现细节" class="headerlink" title="实现细节"></a>实现细节</h1><h2 id="商品向量的计算"><a href="#商品向量的计算" class="headerlink" title="商品向量的计算"></a>商品向量的计算</h2><ul>
<li>根据用户最近某段时间（如30天内）的浏览记录，获取得到浏览SKN的列表并使用空格分隔；核心的逻辑如下面的SQL所示：</li>
</ul>
<pre><code>select concat_ws(&apos; &apos;, collect_set(product_skn)) as skns 
from 
 (select uid, cast(product_skn as string) as product_skn, click_time_stamp 
  from product_click_record 
  where date_id &lt;= $date_id and date_id &gt;= $date_id_30_day_ago
  order by uid, click_time_stamp) as a 
group by uid;
</code></pre><ul>
<li>将该SQL的执行结果写入文件作为word2vec训练的输入；</li>
<li>调用word2vec执行训练，并保存训练的结果：</li>
</ul>
<pre><code>time ./word2vec -train $prepare_file -output $result_file -cbow 1 -size 20 
-window 8 -negative 25 -hs 0 -sample 1e-4 -threads 20 -iter 15 
</code></pre><ul>
<li>读取训练结果的向量，保存到搜索库的商品向量表。</li>
</ul>
<h2 id="用户向量的计算"><a href="#用户向量的计算" class="headerlink" title="用户向量的计算"></a>用户向量的计算</h2><ul>
<li>在计算用户向量时采用了一种简化的处理，即通过用户最近某段时间（如30天内）的商品浏览记录，根据这些商品的向量进行每一维的求平均值处理来计算用户向量，核心的逻辑如下：</li>
</ul>
<pre><code>vec_list = []
for i in range(feature_length):
    vec_list.append(&quot;avg(coalesce(b.vec[%s], 0))&quot; % (str(i)))
vec = &apos;, &apos;.join(vec_list)


select a.uid as uid, array(%s) as vec 
from 
 (select * from product_click_record where date_id &lt;= $date_id and date_id &gt;= $date_id_30_day_ago) as a
left outer join
 (select * from product_w2v where date_id = $date_id) as b
on a.product_skn = b.product_skn
group by a.uid;
</code></pre><ul>
<li>将计算获取的用户向量，保存到Redis里供搜索服务获取。</li>
</ul>
<h2 id="搜索服务时增加个性化评分"><a href="#搜索服务时增加个性化评分" class="headerlink" title="搜索服务时增加个性化评分"></a>搜索服务时增加个性化评分</h2><ul>
<li>商品索引重建构造器在重建索引时设置商品向量到product_index的某个字段中，比如下面例子的productFeatureVector字段；</li>
<li>搜索服务在默认的cross_fields相关性评分的机制下，需要增加个性化的评分，这个可以通过<strong>function_score</strong>来实现。</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">Map&lt;String, Object&gt; scriptParams = <span class="keyword">new</span> HashMap&lt;&gt;();</div><div class="line">scriptParams.put(<span class="string">"field"</span>, <span class="string">"productFeatureVector"</span>);</div><div class="line">scriptParams.put(<span class="string">"inputFeatureVector"</span>, userVector);</div><div class="line">scriptParams.put(<span class="string">"version"</span>, version);</div><div class="line">Script script = <span class="keyword">new</span> Script(<span class="string">"feature_vector_scoring_script"</span>, ScriptService.ScriptType.INLINE, <span class="string">"native"</span>, scriptParams);</div><div class="line">functionScoreQueryBuilder.add(ScoreFunctionBuilders.scriptFunction(script));</div></pre></td></tr></table></figure>
<ul>
<li>这里采用了<a href="https://github.com/ginobefun/elasticsearch-feature-vector-scoring" target="_blank" rel="external">elasticsearch-feature-vector-scoring插件</a>来进行相关性评分，其核心是向量的余弦距离表示，具体见下面一小节的介绍。在脚本参数中，field表示索引中保存商品特征向量的字段；inputFeatureVector表示输入的向量，在这里为用户的向量；</li>
<li>这里把version参数单独拿出来解释一下，因为每天计算出来的向量是不一样的，向量的每一维并没有对应商品的某个具体的属性（至少我们现在看不出来这种关联），因此我们要特别避免不同时间计算出来的向量的之间计算相关性。在实现的时候，我们是通过一个中间变量来表示最新的版本，即在完成商品向量和用户向量的计算和推送给搜索之后，再更新这个中间向量；搜索的索引构造器定期轮询这个中间变量，当发现发生更新之后，就将商品的特征向量批量更新到ES中，在后面的搜索中就可以采用新版本的向量了；</li>
</ul>
<h2 id="elasticsearch-feature-vector-scoring插件"><a href="#elasticsearch-feature-vector-scoring插件" class="headerlink" title="elasticsearch-feature-vector-scoring插件"></a>elasticsearch-feature-vector-scoring插件</h2><p>这是我自己写的一个插件，具体的使用可以看下<a href="https://github.com/ginobefun/elasticsearch-feature-vector-scoring" target="_blank" rel="external">项目主页</a>，其核心也就一个类，我将其主要的代码和注释贴一下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FeatureVectorScoringSearchScript</span> <span class="keyword">extends</span> <span class="title">AbstractSearchScript</span> </span>&#123;</div><div class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> ESLogger LOGGER = Loggers.getLogger(<span class="string">"feature-vector-scoring"</span>);</div><div class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String SCRIPT_NAME = <span class="string">"feature_vector_scoring_script"</span>;</div><div class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">double</span> DEFAULT_BASE_CONSTANT = <span class="number">1.0</span>D;</div><div class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">double</span> DEFAULT_FACTOR_CONSTANT = <span class="number">1.0</span>D;</div><div class="line"></div><div class="line">    <span class="comment">// field in index to store feature vector</span></div><div class="line">    <span class="keyword">private</span> String field;</div><div class="line"></div><div class="line">    <span class="comment">// version of feature vector, if it isn't null, it should match version of index</span></div><div class="line">    <span class="keyword">private</span> String version;</div><div class="line"></div><div class="line">    <span class="comment">// final_score = baseConstant + factorConstant * cos(X, Y)</span></div><div class="line">    <span class="keyword">private</span> <span class="keyword">double</span> baseConstant;</div><div class="line"></div><div class="line">    <span class="comment">// final_score = baseConstant + factorConstant * cos(X, Y)</span></div><div class="line">    <span class="keyword">private</span> <span class="keyword">double</span> factorConstant;</div><div class="line"></div><div class="line">    <span class="comment">// input feature vector</span></div><div class="line">    <span class="keyword">private</span> <span class="keyword">double</span>[] inputFeatureVector;</div><div class="line"></div><div class="line">    <span class="comment">// cos(X, Y) = Σ(Xi * Yi) / ( sqrt(Σ(Xi * Xi)) * sqrt(Σ(Yi * Yi)) )</span></div><div class="line">    <span class="comment">// the inputFeatureVectorNorm is sqrt(Σ(Xi * Xi))</span></div><div class="line">    <span class="keyword">private</span> <span class="keyword">double</span> inputFeatureVectorNorm;</div><div class="line"></div><div class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">ScriptFactory</span> <span class="keyword">implements</span> <span class="title">NativeScriptFactory</span> </span>&#123;</div><div class="line">        <span class="meta">@Override</span></div><div class="line">        <span class="function"><span class="keyword">public</span> ExecutableScript <span class="title">newScript</span><span class="params">(@Nullable Map&lt;String, Object&gt; params)</span> <span class="keyword">throws</span> ScriptException </span>&#123;</div><div class="line">            <span class="keyword">return</span> <span class="keyword">new</span> FeatureVectorScoringSearchScript(params);</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="meta">@Override</span></div><div class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">needsScores</span><span class="params">()</span> </span>&#123;</div><div class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">private</span> <span class="title">FeatureVectorScoringSearchScript</span><span class="params">(Map&lt;String, Object&gt; params)</span> <span class="keyword">throws</span> ScriptException </span>&#123;</div><div class="line">        <span class="keyword">this</span>.field = (String) params.get(<span class="string">"field"</span>);</div><div class="line">        String inputFeatureVectorStr = (String) params.get(<span class="string">"inputFeatureVector"</span>);</div><div class="line">        <span class="keyword">if</span> (<span class="keyword">this</span>.field == <span class="keyword">null</span> || inputFeatureVectorStr == <span class="keyword">null</span> || inputFeatureVectorStr.trim().length() == <span class="number">0</span>) &#123;</div><div class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> ScriptException(<span class="string">"Initialize script "</span> + SCRIPT_NAME + <span class="string">" failed!"</span>);</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="keyword">this</span>.version = (String) params.get(<span class="string">"version"</span>);</div><div class="line">        <span class="keyword">this</span>.baseConstant = params.get(<span class="string">"baseConstant"</span>) != <span class="keyword">null</span> ? Double.parseDouble(params.get(<span class="string">"baseConstant"</span>).toString()) : DEFAULT_BASE_CONSTANT;</div><div class="line">        <span class="keyword">this</span>.factorConstant = params.get(<span class="string">"factorConstant"</span>) != <span class="keyword">null</span> ? Double.parseDouble(params.get(<span class="string">"factorConstant"</span>).toString()) : DEFAULT_FACTOR_CONSTANT;</div><div class="line"></div><div class="line">        String[] inputFeatureVectorArr = inputFeatureVectorStr.split(<span class="string">","</span>);</div><div class="line">        <span class="keyword">int</span> dimension = inputFeatureVectorArr.length;</div><div class="line">        <span class="keyword">double</span> sumOfSquare = <span class="number">0.0</span>D;</div><div class="line">        <span class="keyword">this</span>.inputFeatureVector = <span class="keyword">new</span> <span class="keyword">double</span>[dimension];</div><div class="line">        <span class="keyword">double</span> temp;</div><div class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> index = <span class="number">0</span>; index &lt; dimension; index++) &#123;</div><div class="line">            temp = Double.parseDouble(inputFeatureVectorArr[index].trim());</div><div class="line">            <span class="keyword">this</span>.inputFeatureVector[index] = temp;</div><div class="line">            sumOfSquare += temp * temp;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="keyword">this</span>.inputFeatureVectorNorm = Math.sqrt(sumOfSquare);</div><div class="line">        LOGGER.debug(<span class="string">"FeatureVectorScoringSearchScript.init, version:&#123;&#125;, norm:&#123;&#125;, baseConstant:&#123;&#125;, factorConstant:&#123;&#125;."</span></div><div class="line">                , <span class="keyword">this</span>.version, <span class="keyword">this</span>.inputFeatureVectorNorm, <span class="keyword">this</span>.baseConstant, <span class="keyword">this</span>.factorConstant);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="meta">@Override</span></div><div class="line">    <span class="function"><span class="keyword">public</span> Object <span class="title">run</span><span class="params">()</span> </span>&#123;</div><div class="line">        <span class="keyword">if</span> (<span class="keyword">this</span>.inputFeatureVectorNorm == <span class="number">0</span>) &#123;</div><div class="line">            <span class="keyword">return</span> <span class="keyword">this</span>.baseConstant;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="keyword">if</span> (!doc().containsKey(<span class="keyword">this</span>.field) || doc().get(<span class="keyword">this</span>.field) == <span class="keyword">null</span>) &#123;</div><div class="line">            LOGGER.error(<span class="string">"cannot find field &#123;&#125;."</span>, field);</div><div class="line">            <span class="keyword">return</span> <span class="keyword">this</span>.baseConstant;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        String docFeatureVectorStr = ((ScriptDocValues.Strings) doc().get(<span class="keyword">this</span>.field)).getValue();</div><div class="line">        <span class="keyword">return</span> calculateScore(docFeatureVectorStr);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">double</span> <span class="title">calculateScore</span><span class="params">(String docFeatureVectorStr)</span> </span>&#123;</div><div class="line">        <span class="comment">// 1. check docFeatureVector</span></div><div class="line">        <span class="keyword">if</span> (docFeatureVectorStr == <span class="keyword">null</span>) &#123;</div><div class="line">            <span class="keyword">return</span> <span class="keyword">this</span>.baseConstant;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        docFeatureVectorStr = docFeatureVectorStr.trim();</div><div class="line">        <span class="keyword">if</span> (docFeatureVectorStr.isEmpty()) &#123;</div><div class="line">            <span class="keyword">return</span> <span class="keyword">this</span>.baseConstant;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="comment">// 2. check version and get feature vector array of document</span></div><div class="line">        String[] docFeatureVectorArr;</div><div class="line">        <span class="keyword">if</span> (<span class="keyword">this</span>.version != <span class="keyword">null</span>) &#123;</div><div class="line">            String versionPrefix = version + <span class="string">"|"</span>;</div><div class="line">            <span class="keyword">if</span> (!docFeatureVectorStr.startsWith(versionPrefix)) &#123;</div><div class="line">                <span class="keyword">return</span> <span class="keyword">this</span>.baseConstant;</div><div class="line">            &#125;</div><div class="line"></div><div class="line">            docFeatureVectorArr = docFeatureVectorStr.substring(versionPrefix.length()).split(<span class="string">","</span>);</div><div class="line">        &#125; <span class="keyword">else</span> &#123;</div><div class="line">            docFeatureVectorArr = docFeatureVectorStr.split(<span class="string">","</span>);</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="comment">// 3. check the dimension of input and document</span></div><div class="line">        <span class="keyword">int</span> dimension = <span class="keyword">this</span>.inputFeatureVector.length;</div><div class="line">        <span class="keyword">if</span> (docFeatureVectorArr == <span class="keyword">null</span> || docFeatureVectorArr.length != dimension) &#123;</div><div class="line">            <span class="keyword">return</span> <span class="keyword">this</span>.baseConstant;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="comment">// 4. calculate the relevance score of the two feature vector</span></div><div class="line">        <span class="keyword">double</span> sumOfSquare = <span class="number">0.0</span>D;</div><div class="line">        <span class="keyword">double</span> sumOfProduct = <span class="number">0.0</span>D;</div><div class="line">        <span class="keyword">double</span> tempValueInDouble;</div><div class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; dimension; i++) &#123;</div><div class="line">            tempValueInDouble = Double.parseDouble(docFeatureVectorArr[i].trim());</div><div class="line">            sumOfProduct += tempValueInDouble * <span class="keyword">this</span>.inputFeatureVector[i];</div><div class="line">            sumOfSquare += tempValueInDouble * tempValueInDouble;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="keyword">if</span> (sumOfSquare == <span class="number">0</span>) &#123;</div><div class="line">            <span class="keyword">return</span> <span class="keyword">this</span>.baseConstant;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="keyword">double</span> cosScore = sumOfProduct / (Math.sqrt(sumOfSquare) * inputFeatureVectorNorm);</div><div class="line">        <span class="keyword">return</span> <span class="keyword">this</span>.baseConstant + <span class="keyword">this</span>.factorConstant * cosScore;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h1 id="总结与后续改进"><a href="#总结与后续改进" class="headerlink" title="总结与后续改进"></a>总结与后续改进</h1><ul>
<li>基于word2vec、Elasticsearch和自定义的脚本插件，我们就实现了一个个性化的搜索服务，相对于原有的实现，新版的点击率和转化率都有大幅的提升；</li>
<li>基于word2vec的商品向量还有一个可用之处，就是可以用来实现相似商品的推荐；</li>
<li>但是以我个人的理解，使用word2vec来实现个性化搜索或个性化推荐是有一定局限性的，因为它只能处理用户点击历史这样的时序数据，而无法全面的去考虑用户偏好，这个还是有很大的改进和提升的空间；</li>
<li>后续的话我们会更多的参考业界的做法，更多地更全面地考虑用户的偏好，另外还需要考虑时效性的问题，以优化商品排序和推荐。</li>
</ul>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul>
<li><a href="https://en.wikipedia.org/wiki/Personalized_search" target="_blank" rel="external">Personalized search Wiki</a></li>
<li><a href="http://blog.csdn.net/soso_blog/article/details/6050346" target="_blank" rel="external">搜索下一站：个性化搜索基本方法和简单实验</a></li>
<li><a href="http://www.infoq.com/cn/presentations/jingdong-personalized-search-engine-based-on-big-data-technology" target="_blank" rel="external">京东基于大数据技术的个性化电商搜索引擎</a></li>
<li><a href="https://www.zhihu.com/question/35574888" target="_blank" rel="external">淘宝为什么还不能实现个性化推荐和搜索？</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在&lt;a href=&quot;http://ginobefunny.com/post/learning_word2vec/&quot;&gt;word2vec学习小记&lt;/a&gt;一文中我们曾经学习了word2vec这个工具，它基于神经网络语言模型并在其基础上进行优化，最终能获取词向量和语言模型。在我们的商品搜索系统里，采用了word2vec的方式来计算用户向量和商品向量，并通过Elasticsearch的function_score评分机制和自定义的脚本插件来实现个性化搜索。&lt;/p&gt;
    
    </summary>
    
      <category term="Elasticsearch" scheme="http://ginobefunny.com/categories/Elasticsearch/"/>
    
    
      <category term="Elasticsearch" scheme="http://ginobefunny.com/tags/Elasticsearch/"/>
    
      <category term="word2vec" scheme="http://ginobefunny.com/tags/word2vec/"/>
    
      <category term="个性化" scheme="http://ginobefunny.com/tags/%E4%B8%AA%E6%80%A7%E5%8C%96/"/>
    
      <category term="搜索" scheme="http://ginobefunny.com/tags/%E6%90%9C%E7%B4%A2/"/>
    
      <category term="推荐" scheme="http://ginobefunny.com/tags/%E6%8E%A8%E8%8D%90/"/>
    
  </entry>
  
  <entry>
    <title>基于Elasticsearch实现搜索推荐</title>
    <link href="http://ginobefunny.com/post/search_recommendation_implemention_based_elasticsearch/"/>
    <id>http://ginobefunny.com/post/search_recommendation_implemention_based_elasticsearch/</id>
    <published>2017-03-21T09:18:39.000Z</published>
    <updated>2017-03-24T01:32:41.619Z</updated>
    
    <content type="html"><![CDATA[<p>在<a href="http://ginobefunny.com/post/search_suggestion_implemention_based_elasticsearch/">基于Elasticsearch实现搜索建议</a>一文中我们曾经介绍过如何基于Elasticsearch来实现搜索建议，而本文是在此基础上进一步优化搜索体验，在当搜索无结果或结果过少时提供推荐搜索词给用户。</p>
<a id="more"></a>
<h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h1><p>在根据用户输入和筛选条件进行搜索后，有时返回的是无结果或者结果很少的情况，为了提升用户搜索体验，需要能够给用户推荐一些相关的搜索词，比如用户搜索【迪奥】时没有找到相关的商品，可以推荐搜索【香水】、【眼镜】等关键词。</p>
<h1 id="设计思路"><a href="#设计思路" class="headerlink" title="设计思路"></a>设计思路</h1><p>首先需要分析搜索无结果或者结果过少可能的原因，我总结了一下，主要包括主要可能：</p>
<ol>
<li>搜索的关键词在本网不存在，比如【迪奥】；</li>
<li>搜索的关键词在本网的商品很少，比如【科比】；</li>
<li>搜索的关键词拼写有问题，比如把【阿迪达斯】写成了【阿迪大斯】；</li>
<li>搜索的关键词过多，由于我们采用的是cross_fields，在一个商品内不可能包含所有的Term，导致无结果，比如【阿迪达斯 耐克 卫衣 运动鞋】；</li>
</ol>
<p>那么针对以上情况，可以采用以下方式进行处理：</p>
<ol>
<li>搜索的关键词在本网不存在，可以通过爬虫的方式获取相关知识，然后根据<strong>搜索建议词</strong>去提取，比如去百度百科的<a href="http://baike.baidu.com/item/%E8%BF%AA%E5%A5%A5/291012?sefr=cr" target="_blank" rel="external">迪奥</a>词条里就能提取出【香水】、【香氛】和【眼镜】等关键词；当然基于爬虫的知识可能存在偏差，此时需要能够有人工审核或人工更正的部分；</li>
<li>搜索的关键词在本网的商品很少，有两种解决思路，一种是通过方式1的爬虫去提取关键词，另外一种是通过返回商品的信息去聚合出关键词，如品牌、品类、风格、标签等，这里我们采用的是后者（在测试后发现后者效果更佳）；</li>
<li>搜索的关键词拼写有问题，这就需要<strong>拼写纠错</strong>出场了，先纠错然后根据纠错后的词去提供搜索推荐；</li>
<li>搜索的关键词过多，有两种解决思路，一种是识别关键词的类型，如是品牌、品类、风格还是性别，然后通过一定的组合策略来实现搜索推荐；另外一种则是根据用户的输入到搜索建议词里去匹配，设置最小匹配为一个匹配到一个Term即可，这种方式实现比较简单而且效果也不错，所以我们采用的是后者。</li>
</ol>
<p>所以，我们在实现搜索推荐的核心是之前讲到的搜索建议词，它提供了本网主要的关键词，另外一个很重要的是它本身包含了<strong>关联商品数的属性</strong>，这样就可以保证推荐给用户的关键词是可以搜索出结果的。</p>
<h1 id="实现细节"><a href="#实现细节" class="headerlink" title="实现细节"></a>实现细节</h1><h2 id="整体设计"><a href="#整体设计" class="headerlink" title="整体设计"></a>整体设计</h2><p>整体设计框架如下图所示：</p>
<p><img src="http://oi46mo3on.bkt.clouddn.com/11_es_suggestion/search_recommadation_overview.png" alt="搜索推荐整体设计"></p>
<h2 id="搜索建议词索引"><a href="#搜索建议词索引" class="headerlink" title="搜索建议词索引"></a>搜索建议词索引</h2><p>在<a href="http://ginobefunny.com/post/search_suggestion_implemention_based_elasticsearch/">基于Elasticsearch实现搜索建议</a>一文已有说明，请移步阅读。此次增加了一个keyword.keyword_lowercase的字段用于拼写纠错，这里列取相关字段的索引：</p>
<pre><code>PUT /suggest_index
{
  &quot;mappings&quot;: {
    &quot;suggest&quot;: {
      &quot;properties&quot;: {
        &quot;keyword&quot;: {
          &quot;fields&quot;: {
            &quot;keyword&quot;: {
              &quot;type&quot;: &quot;string&quot;,
              &quot;index&quot;: &quot;not_analyzed&quot;
            },
            &quot;keyword_lowercase&quot;: {
              &quot;type&quot;: &quot;string&quot;,
              &quot;analyzer&quot;: &quot;lowercase_keyword&quot;
            },
            &quot;keyword_ik&quot;: {
              &quot;type&quot;: &quot;string&quot;,
              &quot;analyzer&quot;: &quot;ik_smart&quot;
            },
            &quot;keyword_pinyin&quot;: {
              &quot;type&quot;: &quot;string&quot;,
              &quot;analyzer&quot;: &quot;pinyin_analyzer&quot;
            },
            &quot;keyword_first_py&quot;: {
              &quot;type&quot;: &quot;string&quot;,
              &quot;analyzer&quot;: &quot;pinyin_first_letter_keyword_analyzer&quot;
            }
          },
          &quot;type&quot;: &quot;multi_field&quot;
        },
        &quot;type&quot;: {
          &quot;type&quot;: &quot;long&quot;
        },
        &quot;weight&quot;: {
          &quot;type&quot;: &quot;long&quot;
        },
        &quot;count&quot;: {
          &quot;type&quot;: &quot;long&quot;
        }
      }
    }
  }
}
</code></pre><h2 id="商品数据索引"><a href="#商品数据索引" class="headerlink" title="商品数据索引"></a>商品数据索引</h2><p>这里只列取相关字段的mapping：</p>
<pre><code>PUT /product_index
{
  &quot;mappings&quot;: {
    &quot;product&quot;: {
      &quot;properties&quot;: {
        &quot;productSkn&quot;: {
          &quot;type&quot;: &quot;long&quot;
        },
        &quot;productName&quot;: {
          &quot;type&quot;: &quot;string&quot;,
          &quot;analyzer&quot;: &quot;ik_smart&quot;
        },
        &quot;brandName&quot;: {
          &quot;type&quot;: &quot;string&quot;,
          &quot;analyzer&quot;: &quot;ik_smart&quot;
        },
        &quot;sortName&quot;: {
          &quot;type&quot;: &quot;string&quot;,
          &quot;analyzer&quot;: &quot;ik_smart&quot;
        },
        &quot;style&quot;: {
          &quot;type&quot;: &quot;string&quot;,
          &quot;analyzer&quot;: &quot;ik_smart&quot;
        }
      }
    }
  }
}
</code></pre><h2 id="关键词映射索引"><a href="#关键词映射索引" class="headerlink" title="关键词映射索引"></a>关键词映射索引</h2><p>主要就是source和dest直接的映射关系。</p>
<pre><code>PUT /conversion_index
{
  &quot;mappings&quot;: {
    &quot;conversion&quot;: {
      &quot;properties&quot;: {
        &quot;source&quot;: {
          &quot;type&quot;: &quot;string&quot;,
          &quot;analyzer&quot;: &quot;lowercase_keyword&quot;
        },
        &quot;dest&quot;: {
          &quot;type&quot;: &quot;string&quot;,
          &quot;index&quot;: &quot;not_analyzed&quot;
        }
      }
    }
  }
}
</code></pre><h2 id="爬虫数据爬取"><a href="#爬虫数据爬取" class="headerlink" title="爬虫数据爬取"></a>爬虫数据爬取</h2><p>在实现的时候，我们主要是爬取了百度百科上面的词条，在实际的实现中又分为了全量爬虫和增加爬虫。</p>
<h3 id="全量爬虫"><a href="#全量爬虫" class="headerlink" title="全量爬虫"></a>全量爬虫</h3><p>全量爬虫我这边是从网上下载了一份<a href="https://pan.baidu.com/s/1gfcHXvX" target="_blank" rel="external">他人汇总的词条URL资源</a>，里面根据一级分类包含多个目录，每个目录又根据二级分类包含多个词条，每一行的内容的格式如下：</p>
<pre><code>李宁!http://baike.baidu.com/view/10670.html?fromTaglist
diesel!http://baike.baidu.com/view/394305.html?fromTaglist
ONLY!http://baike.baidu.com/view/92541.html?fromTaglist
lotto!http://baike.baidu.com/view/907709.html?fromTaglist
</code></pre><p>这样在启动的时候我们就可以使用多线程甚至分布式的方式爬虫自己感兴趣的词条内容作为初始化数据保持到爬虫数据表。为了保证幂等性，如果再次全量爬取时就需要排除掉数据库里已有的词条。</p>
<h3 id="增量爬虫"><a href="#增量爬虫" class="headerlink" title="增量爬虫"></a>增量爬虫</h3><ol>
<li>在商品搜索接口中，如果搜索某个关键词关联的商品数为0或小于一定的阈值（如20条），就通过Redis的ZSet进行按天统计；</li>
<li>统计的时候是区分搜索无结果和结果过少两个Key的，因为两种情况实际上是有所区别的，而且后续在搜索推荐查询时也有用到这个统计结果；</li>
<li>增量爬虫是每天凌晨运行，根据前一天统计的关键词进行爬取，爬取前需要排除掉已经爬过的关键词和黑名单中的关键词；</li>
<li>所谓黑名单的数据包含两种：一种是每天增量爬虫失败的关键字（一般会重试几次，确保失败后加入黑名单），一种是人工维护的确定不需要爬虫的关键词；</li>
</ol>
<h2 id="爬虫数据关键词提取"><a href="#爬虫数据关键词提取" class="headerlink" title="爬虫数据关键词提取"></a>爬虫数据关键词提取</h2><ol>
<li>首先需要明确关键词的范围，这里我们采用的是suggest中类型为品牌、品类、风格、款式的词作为关键词；</li>
<li>关键词提取的核心步骤就是对爬虫内容和关键词分别分词，然后进行分词匹配，看该爬虫数据是否包含关键词的所有Term（如果就是一个Term就直接判断包含就好了）；在处理的时候还可以对匹配到关键词的次数进行排序，最终的结果就是一个key-value的映射，如{迪奥 -&gt; [香水,香氛,时装,眼镜], 纪梵希 -&gt; [香水,时装,彩妆,配饰,礼服]}；</li>
</ol>
<h2 id="管理关键词映射"><a href="#管理关键词映射" class="headerlink" title="管理关键词映射"></a>管理关键词映射</h2><ol>
<li>由于爬虫数据提取的关键词是和词条的内容相关联的，因此很有可能提取的关键词效果不大好，因此就需要人工管理；</li>
<li>管理动作主要是包括添加、修改和置失效关键词映射，然后增量地更新到conversion_index索引中；</li>
</ol>
<h2 id="搜索推荐服务的实现"><a href="#搜索推荐服务的实现" class="headerlink" title="搜索推荐服务的实现"></a>搜索推荐服务的实现</h2><ol>
<li>首先如果对搜索推荐的入口进行判断，一些非法的情况不进行推荐（比如关键词太短或太长），另外由于搜索推荐并非核心功能，可以增加一个全局动态参数来控制是否进行搜索推荐；</li>
<li>在<strong>设计思路</strong>里面我们分析过可能有4中场景需要搜索推荐，如何高效、快速地找到具体的场景从而减少不必要的查询判断是推荐服务实现的关键；这个在设计的时候就需要综合权衡，我们通过一段时间的观察后，目前采用的逻辑的伪代码如下：</li>
</ol>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> JSONObject <span class="title">recommend</span><span class="params">(SearchResult searchResult, String queryWord)</span> </span>&#123;</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">        String keywordsToSearch = queryWord;</div><div class="line"></div><div class="line">        <span class="comment">// 搜索推荐分两部分</span></div><div class="line">        <span class="comment">// 1) 第一部分是最常见的情况，包括有结果、根据SKN搜索、关键词未出现在空结果Redis ZSet里</span></div><div class="line">        <span class="keyword">if</span> (containsProductInSearchResult(searchResult)) &#123;</div><div class="line">            <span class="comment">// 1.1） 搜索有结果的 优先从搜索结果聚合出品牌等关键词进行查询</span></div><div class="line">            String aggKeywords = aggKeywordsByProductList(searchResult);</div><div class="line">            keywordsToSearch = queryWord + <span class="string">" "</span> + aggKeywords;</div><div class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (isQuerySkn(queryWord)) &#123;</div><div class="line">            <span class="comment">// 1.2） 如果是查询SKN 没有查询到的 后续的逻辑也无法推荐 所以直接到ES里去获取关键词</span></div><div class="line">            keywordsToSearch = aggKeywordsBySkns(queryWord);</div><div class="line">            <span class="keyword">if</span> (StringUtils.isEmpty(keywordsToSearch)) &#123;</div><div class="line">                <span class="keyword">return</span> defaultSuggestRecommendation();</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        Double count = searchKeyWordService.getKeywordCount(RedisKeys.SEARCH_KEYWORDS_EMPTY, queryWord);</div><div class="line">        <span class="keyword">if</span> (count == <span class="keyword">null</span> || queryWord.length() &gt;= <span class="number">5</span>) &#123;</div><div class="line">            <span class="comment">// 1.3) 如果该关键词一次都没有出现在空结果列表或者长度大于5 则该词很有可能是可以搜索出结果的</span></div><div class="line">            <span class="comment">//      因此优先取suggest_index去搜索一把 减少后面的查询动作</span></div><div class="line">            JSONObject recommendResult = recommendBySuggestIndex(queryWord, keywordsToSearch, <span class="keyword">false</span>);</div><div class="line">            <span class="keyword">if</span> (isNotEmptyResult(recommendResult)) &#123;</div><div class="line">                <span class="keyword">return</span> recommendResult;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="comment">// 2) 第二部分是通过Conversion和拼写纠错去获取关键词 由于很多品牌的拼写可能比较相近 因此先走Conversion然后再拼写检查</span></div><div class="line">        String spellingCorrentWord = <span class="keyword">null</span>, dest = <span class="keyword">null</span>;</div><div class="line">        <span class="keyword">if</span> (allowGetingDest(queryWord) &amp;&amp; StringUtils.isNotEmpty((dest = getSuggestConversionDestBySource(queryWord)))) &#123;</div><div class="line">            <span class="comment">// 2.1) 爬虫和自定义的Conversion处理</span></div><div class="line">            keywordsToSearch = dest;</div><div class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (allowSpellingCorrent(queryWord) </div><div class="line">		         &amp;&amp; StringUtils.isNotEmpty((spellingCorrentWord = suggestService.getSpellingCorrectKeyword(queryWord)))) &#123;</div><div class="line">            <span class="comment">// 2.2) 执行拼写检查 由于在搜索建议的时候会进行拼写检查 所以缓存命中率高</span></div><div class="line">            keywordsToSearch = spellingCorrentWord;</div><div class="line">        &#125; <span class="keyword">else</span> &#123;</div><div class="line">            <span class="comment">// 2.3) 如果两者都没有 则直接返回</span></div><div class="line">            <span class="keyword">return</span> defaultSuggestRecommendation();</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        JSONObject recommendResult = recommendBySuggestIndex(queryWord, keywordsToSearch, dest != <span class="keyword">null</span>);</div><div class="line">        <span class="keyword">return</span> isNotEmptyResult(recommendResult) ? recommendResult : defaultSuggestRecommendation();</div><div class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</div><div class="line">        logger.error(<span class="string">"[func=recommend][queryWord="</span> + queryWord + <span class="string">"]"</span>, e);</div><div class="line">        <span class="keyword">return</span> defaultSuggestRecommendation();</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>其中涉及到的几个函数简单说明下：</p>
<ul>
<li>aggKeywordsByProductList方法用商品列表的结果，聚合出出现次数最多的几个品牌和品类（比如各2个），这样我们就可以得到4个关键词，和原先用户的输入拼接后调用recommendBySuggestIndex获取推荐词；</li>
<li>aggKeywordsBySkns方法是根据用户输入的SKN先到product_index索引获取商品列表，然后再调用aggKeywordsByProductList去获取品牌和品类的关键词列表；</li>
<li>getSuggestConversionDestBySource方法是查询conversion_index索引去获取关键词提取的结果，这里在调用recommendBySuggestIndex时有个参数，该参数主要是用于处理是否限制只能是输入的关键词；</li>
<li>getSpellingCorrectKeyword方法为拼写检查，在调用suggest_index处理时有个地方需要注意一下，拼写检查是基于编辑距离的，大小写不一致的情况会导致Elasticsearch Suggester无法得到正确的拼写建议，因此在处理时需要两边都转换为小写后进行拼写检查；</li>
<li>最终都需要调用recommendBySuggestIndex方法获取搜索推荐，因为通过suggest_index索引可以确保推荐出去的词是有意义的且关联到商品的。该方法核心逻辑的伪代码如下：</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">private</span> JSONObject <span class="title">recommendBySuggestIndex</span><span class="params">(String srcQueryWord, String keywordsToSearch, <span class="keyword">boolean</span> isLimitKeywords)</span> </span>&#123;</div><div class="line">    <span class="comment">// 1) 先对keywordsToSearch进行分词</span></div><div class="line">    List&lt;String&gt; terms = <span class="keyword">null</span>;</div><div class="line">    <span class="keyword">if</span> (isLimitKeywords) &#123;</div><div class="line">        terms = Arrays.stream(keywordsToSearch.split(<span class="string">","</span>)).filter(term -&gt; term != <span class="keyword">null</span> &amp;&amp; term.length() &gt; <span class="number">1</span>)</div><div class="line">                      .distinct().collect(Collectors.toList());</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">        terms = searchAnalyzeService.getAnalyzeTerms(keywordsToSearch, <span class="string">"ik_smart"</span>);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">if</span> (CollectionUtils.isEmpty(terms)) &#123;</div><div class="line">        <span class="keyword">return</span> <span class="keyword">new</span> JSONObject();</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">// 2) 根据terms搜索构造搜索请求</span></div><div class="line">    SearchParam searchParam = <span class="keyword">new</span> SearchParam();</div><div class="line">    searchParam.setPage(<span class="number">1</span>);</div><div class="line">    searchParam.setSize(<span class="number">3</span>);</div><div class="line"></div><div class="line">    <span class="comment">// 2.1) 构建FunctionScoreQueryBuilder</span></div><div class="line">    QueryBuilder queryBuilder = isLimitKeywords ? buildQueryBuilderByLimit(terms)</div><div class="line">                                  : buildQueryBuilder(keywordsToSearch, terms);</div><div class="line">    searchParam.setQuery(queryBuilder);</div><div class="line">    </div><div class="line">    <span class="comment">// 2.2) 设置过滤条件</span></div><div class="line">    BoolQueryBuilder boolFilter = QueryBuilders.boolQuery();</div><div class="line">    boolFilter.must(QueryBuilders.rangeQuery(<span class="string">"count"</span>).gte(<span class="number">20</span>));</div><div class="line">    boolFilter.mustNot(QueryBuilders.termQuery(<span class="string">"keyword.keyword_lowercase"</span>, srcQueryWord.toLowerCase()));</div><div class="line">    <span class="keyword">if</span> (isLimitKeywords) &#123;</div><div class="line">        boolFilter.must(QueryBuilders.termsQuery(<span class="string">"keyword.keyword_lowercase"</span>, terms.stream()</div><div class="line">            .map(String::toLowerCase).collect(Collectors.toList())));</div><div class="line">    &#125;</div><div class="line">    searchParam.setFiter(boolFilter);</div><div class="line"></div><div class="line">    <span class="comment">// 2.3) 按照得分、权重、数量的规则降序排序</span></div><div class="line">    List&lt;SortBuilder&gt; sortBuilders = <span class="keyword">new</span> ArrayList&lt;&gt;(<span class="number">3</span>);</div><div class="line">    sortBuilders.add(SortBuilders.fieldSort(<span class="string">"_score"</span>).order(SortOrder.DESC));</div><div class="line">    sortBuilders.add(SortBuilders.fieldSort(<span class="string">"weight"</span>).order(SortOrder.DESC));</div><div class="line">    sortBuilders.add(SortBuilders.fieldSort(<span class="string">"count"</span>).order(SortOrder.DESC));</div><div class="line">    searchParam.setSortBuilders(sortBuilders);</div><div class="line"></div><div class="line">    <span class="comment">// 4) 先从缓存中获取</span></div><div class="line">    <span class="keyword">final</span> String indexName = SearchConstants.INDEX_NAME_SUGGEST;</div><div class="line">    JSONObject suggestResult = searchCacheService.getJSONObjectFromCache(indexName, searchParam);</div><div class="line">    <span class="keyword">if</span> (suggestResult != <span class="keyword">null</span>) &#123;</div><div class="line">        <span class="keyword">return</span> suggestResult;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">// 5) 调用ES执行搜索</span></div><div class="line">    SearchResult searchResult = searchCommonService.doSearch(indexName, searchParam);</div><div class="line"></div><div class="line">    <span class="comment">// 6) 构建结果加入缓存</span></div><div class="line">    suggestResult = <span class="keyword">new</span> JSONObject();</div><div class="line">    List&lt;String&gt; resultTerms = searchResult.getResultList().stream()</div><div class="line">            .map(map -&gt; (String) map.get(<span class="string">"keyword"</span>)).collect(Collectors.toList());</div><div class="line">    suggestResult.put(<span class="string">"search_recommendation"</span>, resultTerms);</div><div class="line">    searchCacheService.addJSONObjectToCache(indexName, searchParam, suggestResult);</div><div class="line">    <span class="keyword">return</span> suggestResult;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">private</span> QueryBuilder <span class="title">buildQueryBuilderByLimit</span><span class="params">(List&lt;String&gt; terms)</span> </span>&#123;</div><div class="line">    FunctionScoreQueryBuilder functionScoreQueryBuilder</div><div class="line">        = <span class="keyword">new</span> FunctionScoreQueryBuilder(QueryBuilders.matchAllQuery());</div><div class="line"></div><div class="line">    <span class="comment">// 给品类类型的关键词加分</span></div><div class="line">    functionScoreQueryBuilder.add(QueryBuilders.termQuery(<span class="string">"type"</span>, Integer.valueOf(<span class="number">2</span>)),</div><div class="line">        ScoreFunctionBuilders.weightFactorFunction(<span class="number">3</span>));</div><div class="line"></div><div class="line">    <span class="comment">// 按词出现的顺序加分</span></div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; terms.size(); i++) &#123;</div><div class="line">        functionScoreQueryBuilder.add(QueryBuilders.termQuery(<span class="string">"keyword.keyword_lowercase"</span>, </div><div class="line">   terms.get(i).toLowerCase()),</div><div class="line">            ScoreFunctionBuilders.weightFactorFunction(terms.size() - i));</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    functionScoreQueryBuilder.boostMode(CombineFunction.SUM);</div><div class="line">    <span class="keyword">return</span> functionScoreQueryBuilder;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">private</span> QueryBuilder <span class="title">buildQueryBuilder</span><span class="params">(String keywordsToSearch, Set&lt;String&gt; termSet)</span> </span>&#123;</div><div class="line">    <span class="comment">// 1) 对于suggest的multi-fields至少要有一个字段匹配到 匹配得分为常量1</span></div><div class="line">    MultiMatchQueryBuilder queryBuilder = QueryBuilders.multiMatchQuery(keywordsToSearch.toLowerCase(),</div><div class="line">            <span class="string">"keyword.keyword_ik"</span>, <span class="string">"keyword.keyword_pinyin"</span>, </div><div class="line">            <span class="string">"keyword.keyword_first_py"</span>, <span class="string">"keyword.keyword_lowercase"</span>)</div><div class="line">        .analyzer(<span class="string">"ik_smart"</span>)</div><div class="line">        .type(MultiMatchQueryBuilder.Type.BEST_FIELDS)</div><div class="line">        .operator(MatchQueryBuilder.Operator.OR)</div><div class="line">        .minimumShouldMatch(<span class="string">"1"</span>);</div><div class="line"></div><div class="line">    FunctionScoreQueryBuilder functionScoreQueryBuilder</div><div class="line">        = <span class="keyword">new</span> FunctionScoreQueryBuilder(QueryBuilders.constantScoreQuery(queryBuilder));</div><div class="line">			</div><div class="line">    <span class="keyword">for</span> (String term : termSet) &#123;</div><div class="line">        <span class="comment">// 2) 对于完全匹配Term的加1分</span></div><div class="line">        functionScoreQueryBuilder.add(QueryBuilders.termQuery(<span class="string">"keyword.keyword_lowercase"</span>, term.toLowerCase()),</div><div class="line">            ScoreFunctionBuilders.weightFactorFunction(<span class="number">1</span>));</div><div class="line"></div><div class="line">        <span class="comment">// 3) 对于匹配到一个Term的加2分</span></div><div class="line">        functionScoreQueryBuilder.add(QueryBuilders.termQuery(<span class="string">"keyword.keyword_ik"</span>, term),</div><div class="line">            ScoreFunctionBuilders.weightFactorFunction(<span class="number">2</span>));</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    functionScoreQueryBuilder.boostMode(CombineFunction.SUM);</div><div class="line">    <span class="keyword">return</span> functionScoreQueryBuilder;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>最后，从实际运行的统计来看，有90%以上的查询都能在1.3)的情况下返回推荐词，而这一部分还没有进行拼写纠错和conversion_index索引的查询，因此还是比较高效的；剩下的10%在最坏的情况且缓存都没有命中的情况下，最多还需要进行三次ES的查询，性能是比较差的，但是由于有缓存而且大部分的无结果的关键词都比较集中，因此也在可接受的范围，这一块可以考虑再增加一个动态参数，在大促的时候进行关闭处理。</p>
<h1 id="小结与后续改进"><a href="#小结与后续改进" class="headerlink" title="小结与后续改进"></a>小结与后续改进</h1><ul>
<li>通过以上的设计和实现，我们实现了一个效果不错的搜索推荐功能，线上使用效果如下：</li>
</ul>
<pre><code>//搜索【迪奥】，本站无该品牌商品
没有找到 &quot;迪奥&quot; 相关的商品， 为您推荐 &quot;香水&quot; 的搜索结果。或者试试 &quot;香氛&quot;  &quot;眼镜&quot; 

//搜索【puma 运动鞋 上衣】，关键词太多无法匹配
没有找到 &quot;puma 运动鞋 上衣&quot; 相关的商品， 为您推荐 &quot;PUMA 运动鞋&quot; 的搜索结果。或者试试 &quot;PUMA 运动鞋 女&quot;  &quot;PUMA 运动鞋 男&quot;

//搜索【puma 上衣】，结果太少
&quot;puma 上衣&quot; 搜索结果太少了，试试 &quot;上衣&quot;  &quot;PUMA&quot;  &quot;PUMA 休闲&quot; 关键词搜索

//搜索【51489312】特定的SKN，结果太少
&quot;51489312&quot; 搜索结果太少了，试试 &quot;夹克&quot;  &quot;PUMA&quot;  &quot;户外&quot; 关键词搜索

//搜索【blackjauk】，拼写错误
没有找到 &quot;blackjauk&quot; 相关的商品， 为您推荐 &quot;BLACKJACK&quot; 的搜索结果。或者试试 &quot;BLACKJACK T恤&quot;  &quot;BLACKJACK 休闲裤&quot; 
</code></pre><ul>
<li>后续考虑的改进包括：1.继续统计各种无结果或结果太少场景出现的频率和对应推荐词的实现，优化搜索推荐服务的效率；2.爬取更多的语料资源，提升Conversion的准确性；3.考虑增加个性化的功能，给用户推荐Ta最感兴趣的内容。</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在&lt;a href=&quot;http://ginobefunny.com/post/search_suggestion_implemention_based_elasticsearch/&quot;&gt;基于Elasticsearch实现搜索建议&lt;/a&gt;一文中我们曾经介绍过如何基于Elasticsearch来实现搜索建议，而本文是在此基础上进一步优化搜索体验，在当搜索无结果或结果过少时提供推荐搜索词给用户。&lt;/p&gt;
    
    </summary>
    
      <category term="Elasticsearch" scheme="http://ginobefunny.com/categories/Elasticsearch/"/>
    
    
      <category term="Elasticsearch" scheme="http://ginobefunny.com/tags/Elasticsearch/"/>
    
      <category term="搜索" scheme="http://ginobefunny.com/tags/%E6%90%9C%E7%B4%A2/"/>
    
      <category term="推荐" scheme="http://ginobefunny.com/tags/%E6%8E%A8%E8%8D%90/"/>
    
      <category term="拼写纠错" scheme="http://ginobefunny.com/tags/%E6%8B%BC%E5%86%99%E7%BA%A0%E9%94%99/"/>
    
  </entry>
  
  <entry>
    <title>一个简易的Elasticsearch动态同义词插件</title>
    <link href="http://ginobefunny.com/post/elasticsearch_dynamic_synonym_plugin/"/>
    <id>http://ginobefunny.com/post/elasticsearch_dynamic_synonym_plugin/</id>
    <published>2017-03-15T07:07:22.000Z</published>
    <updated>2017-03-16T02:47:30.376Z</updated>
    
    <content type="html"><![CDATA[<p>Elasticsearch自带了一个synonym同义词插件，但是该插件只能使用文件或在分析器中静态地配置同义词，如果需要添加或修改，需要修改配置文件和重启，使用方式不够友好。通过学习Elasticsearch的synonym代码，自研了一个可动态维护同义词的插件，并以运用于生产环境，供大家参考。</p>
<a id="more"></a>
<h1 id="Elasticsearch自带的SynonymTokenFilter"><a href="#Elasticsearch自带的SynonymTokenFilter" class="headerlink" title="Elasticsearch自带的SynonymTokenFilter"></a>Elasticsearch自带的SynonymTokenFilter</h1><p>Elasticsearch自带的同义词过滤器支持在分析器配置（使用synonyms参数）和文件中配置（使用synonyms_path参数）同义词，配置方式如下：</p>
<pre><code>{
    &quot;index&quot; : {
        &quot;analysis&quot; : {
            &quot;analyzer&quot; : {
                &quot;synonym_analyzer&quot; : {
                    &quot;tokenizer&quot; : &quot;whitespace&quot;,
                    &quot;filter&quot; : [&quot;my_synonym&quot;]
                }
            },
            &quot;filter&quot; : {
                &quot;my_synonym&quot; : {
                    &quot;type&quot; : &quot;synonym&quot;,
                    &quot;expand&quot;: true,
                    &quot;ignore_case&quot;: true, 
                    &quot;synonyms_path&quot; : &quot;analysis/synonym.txt&quot;
                    &quot;synonyms&quot; : [&quot;阿迪, 阿迪达斯, adidasi =&gt; Adidas&quot;,&quot;Nike, 耐克, naike&quot;]
                }
            }
        }
    }
}
</code></pre><p>在配置同义词规则时有<a href="https://www.elastic.co/guide/en/elasticsearch/reference/2.3/analysis-synonym-tokenfilter.html#_solr_synonyms" target="_blank" rel="external">Solr synonyms</a>和<a href="https://www.elastic.co/guide/en/elasticsearch/reference/2.3/analysis-synonym-tokenfilter.html#_wordnet_synonyms" target="_blank" rel="external">WordNet synonyms</a>，一般我们使用的都是Solr synonyms。在配置时又存在映射和对等两种方式，区别如下：</p>
<pre><code>// 精确映射同义词，【阿迪】、【阿迪达斯】和【adidasi】的token将会转换为【Adidas】存入倒排索引中
阿迪, 阿迪达斯, adidasi =&gt; Adidas

// 对等同义词
// 当expand为true时，当出现以下任何一个token，三个token都会存入倒排索引中
// 当expand为false时，当出现以下任何一个token，第一个token也就是【Nike】会存入倒排索引中
Nike, 耐克, naike
</code></pre><h1 id="DynamicSynonymTokenFilter"><a href="#DynamicSynonymTokenFilter" class="headerlink" title="DynamicSynonymTokenFilter"></a>DynamicSynonymTokenFilter</h1><h2 id="实现方式"><a href="#实现方式" class="headerlink" title="实现方式"></a>实现方式</h2><ul>
<li>DynamicSynonymTokenFilter参考了SynonymTokenFilter的方式，但又予以简化，使用一个HashMap来保存同义词之间的转换关系；</li>
<li>DynamicSynonymTokenFilter只支持Solr synonyms，同时也支持expand和ignore_case参数的配置；</li>
<li>DynamicSynonymTokenFilter通过数据库来管理同义词的配置，并轮询数据库（通过version字段判断是否存在规则变化）实现同义词的动态管理；</li>
</ul>
<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>1.下载插件源码</p>
<pre><code>git clone git@github.com:ginobefun/elasticsearch-dynamic-synonym.git
</code></pre><p>2.使用maven编译插件</p>
<pre><code>mvn clean install -DskipTests
</code></pre><p>3.在ES_HOME/plugin目录新建dynamic-synonym目录，并将target/releases/elasticsearch-dynamic-synonym-VERSION.zip文件解压到该目录</p>
<p>4.在MySQL中创建Elasticsearch同义词数据库并创建用户</p>
<pre><code>create database elasticsearch;
DROP TABLE IF EXISTS `dynamic_synonym_rule`;
CREATE TABLE `dynamic_synonym_rule` (
  `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `rule` varchar(255) NOT NULL,
  `status` tinyint(1) NOT NULL DEFAULT &apos;1&apos; COMMENT &apos;1: available, 0:unavailable&apos;,
  `version` int(11) NOT NULL,
  PRIMARY KEY (`id`),
  KEY `IDX_DYNAMIC_SYNONYM_VERSION` (`version`),
  KEY `IDX_DYNAMIC_SYNONYM_RULE` (`rule`)
) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=utf8;

-- ----------------------------
-- insert sample records
-- ----------------------------
INSERT INTO `dynamic_synonym_rule` VALUES (&apos;1&apos;, &apos;阿迪, 阿迪达斯, adidasi =&gt; Adidas&apos;, &apos;1&apos;, &apos;1&apos;);
INSERT INTO `dynamic_synonym_rule` VALUES (&apos;2&apos;, &apos;Nike, 耐克, naike&apos;, &apos;1&apos;, &apos;2&apos;);
</code></pre><p>5.重启Elasticsearch</p>
<h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><p>在Elasticsearch的elasticsearch.yml文件或在API创建索引时配置分析器和过滤器：</p>
<pre><code>index:
  analysis:
    filter:
      my_synonym:
        type: dynamic-synonym
        expand: true
        ignore_case: true
        tokenizer: whitespace
        db_url: jdbc:mysql://localhost:3306/elasticsearch?user=test_user&amp;password=test_pwd&amp;useUnicode=true&amp;characterEncoding=UTF8
    analyzer:
      analyzer_with_dynamic_synonym:
        type: custom
        tokenizer: whitespace
        filter: [&quot;my_synonym&quot;]
</code></pre><h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><p>测试分析器效果【阿迪】</p>
<pre><code>http://localhost:9200/test/_analyze?analyzer=analyzer_with_dynamic_synonym&amp;text=阿迪

{
  &quot;tokens&quot;: [
    {
      &quot;token&quot;: &quot;adidas&quot;,
      &quot;start_offset&quot;: 0,
      &quot;end_offset&quot;: 2,
      &quot;type&quot;: &quot;SYNONYM&quot;,
      &quot;position&quot;: 0
    }
  ]
}
</code></pre><p>测试分析器效果【耐克】</p>
<pre><code>http://localhost:9200/test/_analyze?analyzer=analyzer_with_dynamic_synonym&amp;text=耐克

{
  &quot;tokens&quot;: [
    {
      &quot;token&quot;: &quot;nike&quot;,
      &quot;start_offset&quot;: 0,
      &quot;end_offset&quot;: 2,
      &quot;type&quot;: &quot;SYNONYM&quot;,
      &quot;position&quot;: 0
    },
    {
      &quot;token&quot;: &quot;耐克&quot;,
      &quot;start_offset&quot;: 0,
      &quot;end_offset&quot;: 2,
      &quot;type&quot;: &quot;SYNONYM&quot;,
      &quot;position&quot;: 1
    },
    {
      &quot;token&quot;: &quot;naike&quot;,
      &quot;start_offset&quot;: 0,
      &quot;end_offset&quot;: 2,
      &quot;type&quot;: &quot;SYNONYM&quot;,
      &quot;position&quot;: 2
    }
  ]
}
</code></pre><p>往数据库中插入一条同义词，测试【范斯】</p>
<pre><code>INSERT INTO `dynamic_synonym_rule` VALUES (&apos;3&apos;, &apos;Vans, 范斯&apos;, &apos;1&apos;, &apos;3&apos;);

// wait for 2 minutes to reload 
[2017-03-15 15:52:28,895][INFO ][node                     ] [node-local] started
[2017-03-15 15:55:29,645][INFO ][dynamic-synonym          ] Start to reload synonym rule...
[2017-03-15 15:55:29,661][INFO ][dynamic-synonym          ] Succeed to reload 3 synonym rule!

http://localhost:9200/test/_analyze?analyzer=analyzer_with_dynamic_synonym&amp;text=范斯

{
  &quot;tokens&quot;: [
    {
      &quot;token&quot;: &quot;vans&quot;,
      &quot;start_offset&quot;: 0,
      &quot;end_offset&quot;: 2,
      &quot;type&quot;: &quot;SYNONYM&quot;,
      &quot;position&quot;: 0
    },
    {
      &quot;token&quot;: &quot;范斯&quot;,
      &quot;start_offset&quot;: 0,
      &quot;end_offset&quot;: 2,
      &quot;type&quot;: &quot;SYNONYM&quot;,
      &quot;position&quot;: 1
    }
  ]
}
</code></pre><h1 id="总结与后续改进"><a href="#总结与后续改进" class="headerlink" title="总结与后续改进"></a>总结与后续改进</h1><ul>
<li>通过学习Elasticsearch源码自己实现了一个简易版的同义词插件，通过同义词的配置可以实现同义词规则的增删改的动态更新；</li>
<li>需要注意的是，同义词的动态更新存在一个很重要的问题是原本在索引中已存在的数据不受同义词更新动态的影响，因此在使用时需要考虑是否可以容忍该问题，一个通常的做法是在某个时刻集中管理同义词，更新后执行索引重建动作；</li>
<li>另外该插件目前存在一个问题，就是同义词的映射关系在内存中是一个全局数据，因此如果有多个不同的同义词过滤器则会存在问题，代码初始化时以第一个成功初始化的过滤器生成的映射关系为准，这个后续版本考虑改进。</li>
</ul>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/guide/current/using-synonyms.html" target="_blank" rel="external">Using Synonyms</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/2.3/analysis-synonym-tokenfilter.html" target="_blank" rel="external">Synonym Token Filter</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Elasticsearch自带了一个synonym同义词插件，但是该插件只能使用文件或在分析器中静态地配置同义词，如果需要添加或修改，需要修改配置文件和重启，使用方式不够友好。通过学习Elasticsearch的synonym代码，自研了一个可动态维护同义词的插件，并以运用于生产环境，供大家参考。&lt;/p&gt;
    
    </summary>
    
      <category term="Elasticsearch" scheme="http://ginobefunny.com/categories/Elasticsearch/"/>
    
    
      <category term="Elasticsearch" scheme="http://ginobefunny.com/tags/Elasticsearch/"/>
    
      <category term="同义词" scheme="http://ginobefunny.com/tags/%E5%90%8C%E4%B9%89%E8%AF%8D/"/>
    
      <category term="synonym" scheme="http://ginobefunny.com/tags/synonym/"/>
    
      <category term="插件" scheme="http://ginobefunny.com/tags/%E6%8F%92%E4%BB%B6/"/>
    
  </entry>
  
  <entry>
    <title>阅读随手记 201703</title>
    <link href="http://ginobefunny.com/post/reading_record_201703/"/>
    <id>http://ginobefunny.com/post/reading_record_201703/</id>
    <published>2017-03-03T00:33:19.000Z</published>
    <updated>2017-03-29T08:49:59.077Z</updated>
    
    <content type="html"><![CDATA[<p>关键字：微服务, 架构, Event Sourcing, CQRS, Redis, TDD, 消息中间件, 缓存, RPC, 监控, 高性能, 高并发, 高可用, 机器学习, 深度学习, 人工智能。<br><a id="more"></a></p>
<h3 id="Martin-Fowler谈如何理解事件驱动和CQRS-Martin-Fowler-薛命灯"><a href="#Martin-Fowler谈如何理解事件驱动和CQRS-Martin-Fowler-薛命灯" class="headerlink" title="Martin Fowler谈如何理解事件驱动和CQRS Martin Fowler/薛命灯"></a><a href="http://mp.weixin.qq.com/s?__biz=MzA5Nzc4OTA1Mw==&amp;mid=2659599045&amp;idx=1&amp;sn=02aee31a7e947626df37165e3484953f" target="_blank" rel="external">Martin Fowler谈如何理解事件驱动和CQRS</a> Martin Fowler/薛命灯</h3><p>原文地址:<a href="https://martinfowler.com/articles/201701-event-driven.html" target="_blank" rel="external">https://martinfowler.com/articles/201701-event-driven.html</a></p>
<ul>
<li>为了帮助读者理解“事件驱动”的含义，软件大师Martin Fowler在他的博客上总结出了四种基于事件驱动的模型。</li>
<li>事件通知（Event Notification）：事件通知是最基本也是最简单的模型，当一个系统发生了变更，它会通过发送事件消息的形式通知其他系统，发送消息的系统不要求接收消息的系统返回任何响应，即使有响应返回，它也不对其进行任何处理。事件通知的好处在于它的简单性，并且有助于降低系统间的耦合性。不过太多的事件通知可能会带来问题，太多的事件难以跟踪，发生问题难以调试；另外通知事件不会包含太多的数据，额外的请求不仅会造成延迟；</li>
<li>事件传递状态转移（Event-Carried State Transfer）：它比事件通知更进一步，这个模型最大的特点是事件里包含了发生变更的数据。对于接收事件的系统来说，无需再次向源系统发起请求，从而降低了延迟。而且就算源系统宕机，也不会影响到后续的流程。不过，既然把变更数据放在事件里进行传输，那么占用更多的带宽是不可避免的了。</li>
<li>事件溯源（Event-Sourcing）：其核心理念是在对系统的状态做出变更时，把每次变更记录为一个事件，在未来的任何时刻，都可以通过重新处理这些事件来重建系统的状态。事件存储是主要的事件来源，可以从事件存储中重建系统的状态。其好处是存储结构简单，易于存储，不需要用到事务控制从而可以避免使用锁，事件本身还可以充当审计日志的作用。不足之处在于如果事件很多，重放事件是一个耗时的过程，而且在重放过程中可能会涉及与第三方外部系统发生交互，所以需要做一些额外的操作。</li>
<li>CQRS：是Command Query Resposibility Segregation的缩写，它将读操作和写操作进行分离，不仅让逻辑更清晰，而且可以各自进行优化。对于读多写少的系统来说，就特别适合使用CQRS，因为可以针对读性能和写性能进行优化，而且可以进行横向扩展。不过CQRS的概念虽然简单，但是实现起来相对复杂，而且涉及到很多领域驱动设计的概念，最好结合事件溯源一起使用。</li>
</ul>
<h3 id="Event-Sourcing-Martin-Fowler"><a href="#Event-Sourcing-Martin-Fowler" class="headerlink" title="Event Sourcing Martin Fowler"></a><a href="https://martinfowler.com/eaaDev/EventSourcing.html" target="_blank" rel="external">Event Sourcing</a> Martin Fowler</h3><blockquote>
<p>Event Sourcing ensures that all changes to application state are stored as a sequence of events. Not just can we query these events, we can also use the event log to reconstruct past states, and as a foundation to automatically adjust the state to cope with retroactive changes.</p>
<p>The fundamental idea of Event Sourcing is that of ensuring every change to the state of an application is captured in an event object, and that these event objects are themselves stored in the sequence they were applied for the same lifetime as the application state itself.</p>
<p>The most obvious thing we’ve gained by using Event Sourcing is that we now have a log of all the changes. Not just can we see where each ship is, we can see where it’s been. </p>
<p>The key to Event Sourcing is that we guarantee that all changes to the domain objects are initiated by the event objects. This leads to a number of facilities that can be built on top of the event log: Complete Rebuild, Temporal Query, Event Replay.</p>
<p>In many applications it’s more common to request recent application states, if so a faster alternative is to store the current application state and if someone wants the special features that Event Sourcing offers then that additional capability is built on top.</p>
<p>There are a number of choices about where to put the logic for handling events. The primary choice is whether to put the logic in Transaction Scripts or Domain Model. As usual Transaction Scripts are better for simple logic and a Domain Model is better when things get more complicated.</p>
<p>As well as events playing themselves forwards, it’s also often useful for them to be able to reverse themselves.</p>
<p>Many of the advantages of Event Sourcing stem from the ability to replay events at will, but if these events cause update messages to be sent to external systems, then things will go wrong because those external systems don’t know the difference between real processing and replays. To handle this you’ll need to wrap any external systems with a Gateway. This in itself isn’t too onerous since it’s a thoroughly good idea in any case. The gateway has to be a bit more sophisticated so it can deal with any replay processing that the Event Sourcing system is doing.</p>
<p>When to Use It: Packaging up every change to an application as an event is an interface style that not everyone is comfortable with, and many find to be awkward. As a result it’s not a natural choice and to use it means that you expect to get some form of return. One obvious form of return is that it’s easy to serialize the events to make an Audit Log. Another use for this kind of complete Audit Log is to help with debugging. Event Sourcing is the foundation for Parallel Models or Retroactive Events. If you want to use either of those patterns you will need to use Event Sourcing first. </p>
</blockquote>
<h3 id="CQRS-Martin-Fowler"><a href="#CQRS-Martin-Fowler" class="headerlink" title="CQRS Martin Fowler"></a><a href="https://martinfowler.com/bliki/CQRS.html" target="_blank" rel="external">CQRS</a> Martin Fowler</h3><blockquote>
<p>CQRS stands for Command Query Responsibility Segregation. At its heart is the notion that you can use a different model to update information than the model you use to read information. For some situations, this separation can be valuable, but beware that for most systems CQRS adds risky complexity.</p>
<p>The change that CQRS introduces is to split that conceptual model into separate models for update and display, which it refers to as Command and Query respectively following the vocabulary of CommandQuerySeparation. The rationale is that for many problems, particularly in more complicated domains, having the same conceptual model for commands and queries leads to a more complex model that does neither well.</p>
</blockquote>
<p><img src="http://oi46mo3on.bkt.clouddn.com/15_reading_201702/cqrs.png" alt="CQRS"></p>
<blockquote>
<p>The two models might not be separate object models, it could be that the same objects have different interfaces for their command side and their query side, rather like views in relational databases. But usually when I hear of CQRS, they are clearly separate models.</p>
<p>When to use it: CQRS is a significant mental leap for all concerned, so shouldn’t be tackled unless the benefit is worth the jump. In particular CQRS should only be used on specific portions of a system (a BoundedContext in DDD lingo) and not the system as a whole. So far I see benefits in two directions. Firstly is that a few complex domains may be easier to tackle by using CQRS. The other main benefit is in handling high performance applications. </p>
</blockquote>
<h3 id="RxJava2实例解析-Victor-Grazi-薛命灯Rays"><a href="#RxJava2实例解析-Victor-Grazi-薛命灯Rays" class="headerlink" title="RxJava2实例解析  Victor Grazi/薛命灯Rays"></a><a href="http://www.infoq.com/cn/articles/rxjava2-by-example" target="_blank" rel="external">RxJava2实例解析</a>  Victor Grazi/薛命灯Rays</h3><ul>
<li>响应式编程是一种处理异步数据流的规范，它为数据流的转换和聚合以及数据流的控制管理提供了工具支持，它让考量程序整体设计的工作变得简单。但它使用起来并不简单，它的学习曲线也并不平坦。</li>
<li>传统的编程模式以对象为基础，而响应式以事件流为基础。事件可能以多种形式出现，比如对象、数据源、鼠标移动信息或者异常。</li>
<li>首先要记住的是，响应式里所有的东西都是流。Observable封装了流，是最基本的单元。流可以包含零个或多个事件，有未完成和已完成两种状态，可以正常结束也可以发生错误。如果一个流正常完成或者发生错误，说明处理结束了，虽然有些工具可以对错误进行重试或者使用不同的流替换发生错误的流。</li>
<li>一个Observable对象必须要有一个订阅者来处理它所生成的事件。所幸的是，现在Java支持Lambda表达式，我们就可以使用简洁的声明式风格来表示订阅操作：</li>
</ul>
<pre><code>Observable&lt;String&gt; howdy = Observable.just(&quot;Howdy!&quot;);
howdy.subscribe(System.out::println);
</code></pre><ul>
<li>zip操作通过成对的“zip”映射转换把源流的元素跟另一个给定流的元素组合起来，其中的映射可以使用Lambda表达式来表示。只要其中的一个流完成操作，整个zip操作也跟着停止，另一个未完成的流剩下的事件就会被忽略。zip可以支持最多9个源流的zip操作。zipWith操作可以把一个指定流合并到一个已存在的流里。我们可以使用range和zipWith操作加入编号，并用String.format做映射转换：</li>
</ul>
<pre><code>Observable.fromIterable(words)
 .zipWith(Observable.range(1, Integer.MAX_VALUE), (string, count)-&gt;String.format(&quot;%2d. %s&quot;, count, string))
 .subscribe(System.out::println);
</code></pre><ul>
<li>我们从小被告知“quick brown fox”这个全字母短句包含了英语里所有的字母，现在让我们对这些字母进行排序看看：</li>
</ul>
<pre><code>List&lt;String&gt; words = Arrays.asList(&quot;the&quot;, &quot;quick&quot;, &quot;brown&quot;, &quot;fox&quot;, &quot;jumped&quot;, 
    &quot;over&quot;, &quot;the&quot;, &quot;lazy&quot;, &quot;dogs&quot;);

Observable.fromIterable(words)
 .flatMap(word -&gt; Observable.fromArray(word.split(&quot;&quot;)))
 .distinct()
 .sorted()
 .zipWith(Observable.range(1, Integer.MAX_VALUE), (string, count) -&gt; String.format(&quot;%2d. %s&quot;, count, string))
 .subscribe(System.out::println);
</code></pre><ul>
<li>但是到目前为止，所有的代码都跟Java 8里引入的Streams API很相似，不过这种相似只是一种巧合，因为响应式包含的内容远不止这些。响应式引入了执行时间、节流、流量控制等概念，而且它们可以被连接到“永不停止”的处理流程里。响应式产生的结果虽然不是集合，但你可以用任何期望的方式来处理这些结果。</li>
<li>在RxJava的前期版本中，即使对于无需流控制的小型流，Observable也给出了流控制方法。为符合响应式的规范，RxJava2将流控制从Observable类中移除，并引入了新的Flowable类。Flowable可以看作是提供流控制的Observable。</li>
<li>要连接到长期运行的现有数据源上，除非是提供背压控制，我们通常会选择使用Flowable，使用一种Observable的并行语法。a. 调用Flowable的publish方法生成一个新的ConnectableFlowable； b. 调用ConnectableFlowable的connect方法开始生成数据；</li>
<li>要连接到一个已有的数据源上，可以在这个数据源上添加监听器（如果你喜欢这么做），监听器会把事件传播给订阅者，然后在每个事件发生时调用订阅者的onNext方法。在实现监听器的时候要确保每个订阅者仍然处于订阅状态，否则就要停止把事件传播给它，同时要注意回压信号。所幸的是，这些工作可以由Flowabled的create方法来处理。</li>
</ul>
<h3 id="Redis的内存优化-CacheCloud"><a href="#Redis的内存优化-CacheCloud" class="headerlink" title="Redis的内存优化 CacheCloud"></a><a href="https://cachecloud.github.io/2017/02/16/Redis%E5%86%85%E5%AD%98%E4%BC%98%E5%8C%96/" target="_blank" rel="external">Redis的内存优化</a> CacheCloud</h3><ul>
<li>Redis存储的所有值对象在内部定义为redisObject结构体，内部结构如下图所示：</li>
</ul>
<p><img src="http://i1.itc.cn/20170216/3084_2c3a0c00_6cbc_c4d9_01fe_8852cc497653_1.png" alt="RedisObject"></p>
<ul>
<li>Redis存储的数据都使用redisObject来封装，包括string,hash,list,set,zset在内的所有数据类型。理解redisObject对内存优化非常有帮助。type字段表示当前对象使用的数据类型，encoding字段表示Redis内部编码类型，lru字段记录对象最后一次被访问的时间，refcount字段记录当前对象被引用的次数，*ptr字段与对象的数据内容相关，如果是整数直接存储数据，否则表示指向数据的指针。</li>
<li>降低Redis内存使用最直接的方式就是缩减键（key）和值（value）的长度。值对象缩减比较复杂，常见需求是把业务对象序列化成二进制数组放入Redis。首先应该在业务上精简业务对象，去掉不必要的属性避免存储无效数据。其次在序列化工具选择上，应该选择更高效的序列化工具来降低字节数组大小。</li>
<li>对象共享池指Redis内部维护[0-9999]的整数对象池。创建大量的整数类型redisObject存在内存开销，每个redisObject内部结构至少占16字节，甚至超过了整数自身空间消耗。所以Redis内存维护一个[0-9999]的整数对象池，用于节约内存。除了整数值对象，其他类型如list,hash,set,zset内部元素也可以使用整数对象池。因此开发中在满足需求的前提下，尽量使用整数对象以节省内存。</li>
<li>字符串优化：Redis没有采用原生C语言的字符串类型而是自己实现了字符串结构，内部简单动态字符串，简称SDS；因为字符串(SDS)存在预分配机制，日常开发中要小心预分配带来的内存浪费，从测试数据看，同样的数据追加后内存消耗非常严重；字符串之所以采用预分配的方式是防止修改操作需要不断重分配内存和字节数据拷贝，但同样也会造成内存的浪费，字符串预分配每次并不都是翻倍扩容；字符串重构指不一定把每份数据作为字符串整体存储，像json这样的数据可以使用hash结构，使用二级结构存储也能帮我们节省内存，同时可以使用hmget,hmset命令支持字段的部分读取修改，而不用每次整体存取。</li>
<li>编码优化：Redis对外提供了string,list,hash,set,zet等类型，但是Redis内部针对不同类型存在编码的概念。Redis作者想通过不同编码实现效率和空间的平衡，比如当我们的存储只有10个元素的列表，当使用双向链表数据结构时，必然需要维护大量的内部字段如每个元素需要前置指针、后置指针、数据指针等，造成空间浪费，如果采用连续内存结构的压缩列表(ziplist)，将会节省大量内存，而由于数据长度较小，存取操作时间复杂度即使为O(n2)性能也可满足需求。</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/15_reading_201702/type_encoding_relation.png" alt="类型和编码关系"></p>
<ul>
<li>控制key的数量：当使用Redis存储大量数据时，通常会存在大量键，过多的键同样会消耗大量内存。Redis本质是一个数据结构服务器，它为我们提供多种数据结构，如hash，list，set，zset等结构。使用Redis时不要进入一个误区，大量使用get/set这样的API，把Redis当成Memcached使用。对于存储相同的数据内容利用Redis的数据结构降低外层键的数量，也可以节省大量内存。</li>
</ul>
<h3 id="Redis架构之防雪崩设计：网站不宕机背后的兵法-付磊，张益军"><a href="#Redis架构之防雪崩设计：网站不宕机背后的兵法-付磊，张益军" class="headerlink" title="Redis架构之防雪崩设计：网站不宕机背后的兵法 付磊，张益军"></a><a href="https://mp.weixin.qq.com/s/TBCEwLVAXdsTszRVpXhVug" target="_blank" rel="external">Redis架构之防雪崩设计：网站不宕机背后的兵法</a> 付磊，张益军</h3><ul>
<li>缓存穿透预防及优化：缓存穿透是指查询一个根本不存在的数据，缓存层和存储层都不会命中；缓存穿透将导致不存在的数据每次请求都要到存储层去查询，失去了缓存保护后端存储的意义；解决方法：缓存空对象和布隆过滤器拦截；</li>
<li>缓存雪崩问题优化：由于缓存层承载着大量请求，有效的保护了存储层，但是如果缓存层由于某些原因整体不能提供服务，于是所有的请求都会达到存储层，存储层的调用量会暴增，造成存储层也会挂掉的情况；预防和解决缓存雪崩问题，可以从以下三个方面进行着手：保证缓存层服务高可用性、依赖隔离组件为后端限流并降级、提前演练。</li>
<li>缓存热点key重建优化：开发人员使用缓存和过期时间的策略既可以加速数据读写，又保证数据的定期更新，这种模式基本能够满足绝大部分需求。但如果热点Key和重建缓存耗时两个问题同时出现，可能就会对应用造成致命的危害；解决方法：互斥锁（只允许一个线程重建缓存）、永远不过期（唯一不足的就是重构缓存期间会出现数据不一致的情况）。</li>
</ul>
<h3 id="现代化的缓存设计方案-Benjamin-Manes-简直"><a href="#现代化的缓存设计方案-Benjamin-Manes-简直" class="headerlink" title="现代化的缓存设计方案 Benjamin Manes/简直"></a><a href="http://ifeve.com/design-of-a-modern-cache/" target="_blank" rel="external">现代化的缓存设计方案</a> Benjamin Manes/简直</h3><ul>
<li>缓存是提升性能的通用方法，现在大多数的缓存实现都使用了经典的技术。这篇文章中，我们会发掘Caffeine中的现代化的实现方法。Caffeine 是一个开源的Java缓存库，它能提供高命中率和出色的并发能力。期望读者们能被这些想法激发，进而将它们应用到任何你喜欢的编程语言中。</li>
<li>驱逐策略：缓存的驱逐策略是为了预测哪些数据在短期内最可能被再次用到，从而提升缓存的命中率。LRU策略或许是最流行的驱逐策略，但LRU通过历史数据来预测未来是局限的，它会认为最后到来的数据是最可能被再次访问的。现代缓存扩展了对历史数据的使用，结合就近程度和访问频次来更好的预测数据。其中一种保留历史信息的方式是使用popularity sketch（一种压缩、概率性的数据结构）来从一大堆访问事件中定位频繁的访问者。Window TinyLFU（W-TinyLFU）算法将 sketch 作为过滤器，当新来的数据比要驱逐的数据高频时，这个数据才会被缓存接纳。这个许可窗口给予每个数据项积累热度的机会，而不是立即过滤掉。对于长期保留的数据，W-TinyLFU 使用了分段 LRU（Segmented LRU，缩写 SLRU）策略。起初，一个数据项存储被存储在试用段（probationary segment）中，在后续被访问到时，它会被提升到保护段（protected segment）中（保护段占总容量的 80%）。保护段满后，有的数据会被淘汰回试用段，这也可能级联的触发试用段的淘汰。这套机制确保了访问间隔小的热数据被保存下来，而被重复访问少的冷数据则被回收。</li>
<li>过期策略：鉴于大多数场景里不同数据项使用的都是固定的过期时长，Caffien采用了统一过期时间的方式。这个限制让用 O（1）的有序队列组织数据成为可能。针对数据的写后过期，维护了一个写入顺序队列，针对读后过期，维护了一个读取顺序队列。缓存能复用驱逐策略下的队列以及下面将要介绍的并发机制，让过期的数据项在缓存的维护阶段被抛弃掉。</li>
<li>并发：由于在大多数的缓存策略中，数据的读取都会伴随对缓存状态的写操作，并发的缓存读取被视为一个难点问题。在 Caffeine 中，有一组缓冲区被用来记录读写。一次访问首先会被因线程而异的哈希到 stripped ring buffer 上，当检测到竞争时，缓冲区会自动扩容。一个 ring buffer 容量满载后，会触发异步的执行操作，而后续的对该 ring buffer 的写入会被丢弃，直到这个 ring buffer 可被使用。虽然因为 ring buffer 容量满而无法被记录该访问，但缓存值依然会返回给调用方。这种策略信息的丢失不会带来大的影响，因为 W-TinyLFU 能识别出我们希望保存的热点数据。通过使用因线程而异的哈希算法替代在数据项的键上做哈希，缓存避免了瞬时的热点 key 的竞争问题。写数据时，采用更传统的并发队列，每次变更会引起一次立即的执行。</li>
</ul>
<h3 id="大话程序猿眼里的高并发之续篇-SFLYQ"><a href="#大话程序猿眼里的高并发之续篇-SFLYQ" class="headerlink" title="大话程序猿眼里的高并发之续篇 SFLYQ"></a><a href="https://blog.thankbabe.com/2017/02/27/high-concurrency-scheme-xp/" target="_blank" rel="external">大话程序猿眼里的高并发之续篇</a> SFLYQ</h3><ul>
<li>分层（将系统在横向维度上切分成几个部分，每个部门负责一部分相对简单并比较单一的职责，然后通过上层对下层的依赖和调度组成一个完整的系统），分割（在纵向方面对业务进行切分，将一块相对复杂的业务分割成不同的模块单元），分布式（分布式应用和服务，将分层或者分割后的业务分布式部署，独立的应用服务器、数据库和缓存服务器，当业务达到一定用户量的时候，再进行服务器均衡负载，数据库、缓存主从集群）；</li>
<li>集群：对于用户访问集中的业务独立部署服务器，应用服务器，数据库，nosql数据库。核心业务基本上需要搭建集群，即多台服务器部署相同的应用构成一个集群，通过负载均衡设备共同对外提供服务，服务器集群能够为相同的服务提供更多的并发支持；</li>
<li>异步：在高并发业务中如果涉及到数据库操作，主要压力都是在数据库服务器上面，虽然使用主从分离，但是数据库操作都是在主库上操作，单台数据库服务器连接池允许的最大连接数量是有限的，像这种涉及数据库操作的高并发的业务，就要考虑使用异步了，客户端发起接口请求，服务端快速响应，客户端展示结果给用户，数据库操作通过异步同步；</li>
<li>缓存：数据不经常变化，我们可以把数据进行缓存，Cache是直接存储在应用服务器中，读取速度快，内存数据库服务器允许连接数可以支撑到很大，而且数据存储在内存，读取速度快，再加上主从集群，可以支撑很大的并发查询；</li>
<li>面向服务：使用服务化思维，将核心业务或者通用的业务功能抽离成服务独立部署，对外提供接口的方式提供功能。最理想化的设计是可以把一个复杂的系统抽离成多个服务，共同组成系统的业务，优点：松耦合，高可用性，高伸缩性，易维护。通过面向服务化设计，独立服务器部署，均衡负载，数据库集群，可以让服务支撑更高的并发；</li>
<li>冗余，自动化：当高并发业务所在的服务器出现宕机的时候，需要有备用服务器进行快速的替代，在应用服务器压力大的时候可以快速添加机器到集群中，所以我们就需要有备用机器可以随时待命。 最理想的方式是可以通过自动化监控服务器资源消耗来进行报警，自动切换降级方案，自动的进行服务器替换和添加操作等，通过自动化可以减少人工的操作的成本，而且可以快速操作，避免人为操作上面的失误。</li>
</ul>
<h3 id="浅谈机器学习基础-我偏笑-NSNirvana"><a href="#浅谈机器学习基础-我偏笑-NSNirvana" class="headerlink" title="浅谈机器学习基础 我偏笑_NSNirvana"></a><a href="http://www.jianshu.com/p/ed9ae5385b89" target="_blank" rel="external">浅谈机器学习基础</a> 我偏笑_NSNirvana</h3><p>下篇的地址为:<a href="http://www.jianshu.com/p/0359e3b7bb1b" target="_blank" rel="external">http://www.jianshu.com/p/0359e3b7bb1b</a></p>
<h3 id="浅谈深度学习基础-我偏笑-NSNirvana"><a href="#浅谈深度学习基础-我偏笑-NSNirvana" class="headerlink" title="浅谈深度学习基础 我偏笑_NSNirvana"></a><a href="http://www.jianshu.com/p/df9a4473d6d4" target="_blank" rel="external">浅谈深度学习基础</a> 我偏笑_NSNirvana</h3><p>下篇的地址为:<a href="http://www.jianshu.com/p/3d1ddfce1563" target="_blank" rel="external">http://www.jianshu.com/p/3d1ddfce1563</a></p>
<h3 id="机器学习算法实现解析——word2vec源码解析-zhiyong-will"><a href="#机器学习算法实现解析——word2vec源码解析-zhiyong-will" class="headerlink" title=" 机器学习算法实现解析——word2vec源码解析 zhiyong_will"></a><a href="http://blog.csdn.net/google19890102/article/details/51887344#" target="_blank" rel="external"> 机器学习算法实现解析——word2vec源码解析</a> zhiyong_will</h3><h3 id="分布式系列文章——Paxos算法原理与推导-linbingdong"><a href="#分布式系列文章——Paxos算法原理与推导-linbingdong" class="headerlink" title="分布式系列文章——Paxos算法原理与推导 linbingdong"></a><a href="http://linbingdong.com/2017/03/17/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0%E2%80%94%E2%80%94Paxos%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%E4%B8%8E%E6%8E%A8%E5%AF%BC/" target="_blank" rel="external">分布式系列文章——Paxos算法原理与推导</a> linbingdong</h3><h3 id="分布式一致性算法：Raft-算法-linbingdong"><a href="#分布式一致性算法：Raft-算法-linbingdong" class="headerlink" title="分布式一致性算法：Raft 算法 linbingdong"></a><a href="http://linbingdong.com/2017/02/19/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%80%E8%87%B4%E6%80%A7%E7%AE%97%E6%B3%95%EF%BC%9ARaft%20%E7%AE%97%E6%B3%95%EF%BC%88Raft%20%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%EF%BC%89/" target="_blank" rel="external">分布式一致性算法：Raft 算法</a> linbingdong</h3><h3 id="解决业务代码里的分布式事务一致性问题-陶文"><a href="#解决业务代码里的分布式事务一致性问题-陶文" class="headerlink" title="解决业务代码里的分布式事务一致性问题 陶文"></a><a href="https://zhuanlan.zhihu.com/p/25346771" target="_blank" rel="external">解决业务代码里的分布式事务一致性问题</a> 陶文</h3><ul>
<li>微服务架构解决了很多问题，但是同时引入了很多问题，本文要探讨的是如何解决下面这几个问题：有大量的同步RPC依赖，如何保证自身的可靠性？RPC调用失败，降级处理之后如何保证数据可修复？消息队列是一个RPC主流程的旁路流程，怎么保证可靠性？消息队列怎么保持与数据库的事务一致？</li>
<li>同步转异步，解决稳定性问题：在平时的时候，都是RPC同步调用，如果调用失败了，则自动把同步调用降级为异步的，消息此时进入队列，然后异步被重试。</li>
</ul>
<p><img src="http://pic2.zhimg.com/v2-9f5a5418027eae507551518a6aaa1179_b.png" alt="同步转异步"></p>
<ul>
<li>把消息队列放入到主流程：如果要把重要的业务逻辑挂在消息队列后面，必须要保证消息队列里的数据的完整性，不能有丢失的情况，所以不能是把消息队列的写入作为一个旁路的逻辑。如果消息队列写入失败或者超时，都应该直接返回错误，而不是允许继续执行。在无法及时写入的情况，我们需要使用本地文件充当一个缓冲。实际上是通过引入本地文件队列结合远程分布式队列构成一个可用性更高，延迟更低的组合队列方案。这个本地的队列如果能封装到一个 Kafka 的 Agent 作为本地写入的代理，那是最理想的实现方式。</li>
</ul>
<p><img src="http://pic2.zhimg.com/v2-eeccde7413dc2331257eece53558dce1_b.png" alt="把消息队列放入到主流程"></p>
<ul>
<li>保障分布式事务一致性：我们需要一个延迟队列，在业务入口的时候挂一个延迟job，然后执行完了取消它。如果没有执行完，则延迟队列负责去触发这个延迟任务，把整个业务流程重复执行一遍。这样我们就可以保证任意rpc操作流程的最终一致性了。而入kafka消息队列作为RPC操作的一种，自然也是可以得到保证的了。</li>
</ul>
<p><img src="http://pic1.zhimg.com/v2-a8904d61ad34784fd77c32b1932e8f78_b.png" alt="保障分布式事务一致性"></p>
<h3 id="TensorFlow和Caffe、CNTK、MXNet等其他7种深度学习框架的对比-黄文坚-唐源"><a href="#TensorFlow和Caffe、CNTK、MXNet等其他7种深度学习框架的对比-黄文坚-唐源" class="headerlink" title="TensorFlow和Caffe、CNTK、MXNet等其他7种深度学习框架的对比 黄文坚/唐源"></a><a href="http://mp.weixin.qq.com/s?__biz=MzA5NzkxMzg1Nw==&amp;mid=2653162065&amp;idx=1&amp;sn=c0f50fe72cb495dc19b9861d3bd5d67f" target="_blank" rel="external">TensorFlow和Caffe、CNTK、MXNet等其他7种深度学习框架的对比</a> 黄文坚/唐源</h3><h4 id="TensorFlow"><a href="#TensorFlow" class="headerlink" title="TensorFlow"></a>TensorFlow</h4><ul>
<li>TensorFlow是相对高阶的机器学习库，用户可以方便地用它设计神经网络结构，而不必为了追求高效率的实现亲自写C++或CUDA18代码。</li>
<li>它和Theano一样都支持自动求导，用户不需要再通过反向传播求解梯度。</li>
<li>其核心代码和Caffe一样是用C++编写的，使用C++简化了线上部署的复杂度，并让手机这种内存和CPU资源都紧张的设备可以运行复杂模型（Python则会比较消耗资源，并且执行效率不高）。</li>
<li>除了核心代码的C++接口，TensorFlow还有官方的Python、Go和Java接口，是通过SWIG（Simplified Wrapper and Interface Generator）实现的，这样用户就可以在一个硬件配置较好的机器中用Python进行实验，并在资源比较紧张的嵌入式环境或需要低延迟的环境中用C++部署模型。SWIG支持给C/C++代码提供各种语言的接口，因此其他脚本语言的接口未来也可以通过SWIG方便地添加。</li>
<li>TensorFlow也有内置的TF.Learn和TF.Slim等上层组件可以帮助快速地设计新网络，并且兼容Scikit-learn estimator接口，可以方便地实现evaluate、grid search、cross validation等功能。</li>
<li>同时TensorFlow不只局限于神经网络，其数据流式图支持非常自由的算法表达，当然也可以轻松实现深度学习以外的机器学习算法。事实上，只要可以将计算表示成计算图的形式，就可以使用TensorFlow。</li>
<li>用户可以写内层循环代码控制计算图分支的计算，TensorFlow会自动将相关的分支转为子图并执行迭代运算。TensorFlow也可以将计算图中的各个节点分配到不同的设备执行，充分利用硬件资源。</li>
<li>在数据并行模式上，TensorFlow和Parameter Server很像，但TensorFlow有独立的Variable node，不像其他框架有一个全局统一的参数服务器，因此参数同步更自由。</li>
<li>TensorFlow和Spark的核心都是一个数据计算的流式图，Spark面向的是大规模的数据，支持SQL等操作，而TensorFlow主要面向内存足以装载模型参数的环境，这样可以最大化计算效率。</li>
<li>TensorFlow的另外一个重要特点是它灵活的移植性，可以将同一份代码几乎不经过修改就轻松地部署到有任意数量CPU或GPU的PC、服务器或者移动设备上。</li>
<li>TensorBoard是TensorFlow的一组Web应用，用来监控TensorFlow运行过程，或可视化Computation Graph。</li>
<li>TensorFlow拥有产品级的高质量代码，有Google强大的开发、维护能力的加持，整体架构设计也非常优秀。</li>
</ul>
<h4 id="Caffe"><a href="#Caffe" class="headerlink" title="Caffe"></a>Caffe</h4><ul>
<li>Caffe全称为Convolutional Architecture for Fast Feature Embedding，是一个被广泛使用的开源深度学习框架，目前由伯克利视觉学中心进行维护。Caffe的创始人是加州大学伯克利的Ph.D.贾扬清，他同时也是TensorFlow的作者之一。</li>
<li>Caffe的主要优势包括如下几点：容易上手，网络结构都是以配置文件形式定义，不需要用代码设计网络；训练速度快，能够训练state-of-the-art的模型与大规模的数据；组件模块化，可以方便地拓展到新的模型和学习任务上。</li>
<li>Caffe的核心概念是Layer，每一个神经网络的模块都是一个Layer。Layer接收输入数据，同时经过内部计算产生输出数据。设计网络结构时，只需要把各个Layer拼接在一起构成完整的网络（通过写protobuf配置文件定义）。</li>
<li>Caffe最开始设计时的目标只针对于图像，没有考虑文本、语音或者时间序列的数据，因此Caffe对卷积神经网络的支持非常好，但对时间序列RNN、LSTM等支持得不是特别充分。同时，基于Layer的模式也对RNN不是非常友好，定义RNN结构时比较麻烦。</li>
</ul>
<h4 id="Theano"><a href="#Theano" class="headerlink" title="Theano"></a>Theano</h4><ul>
<li>Theano诞生于2008年，由蒙特利尔大学Lisa Lab团队开发并维护，是一个高性能的符号计算及深度学习库。因其出现时间早，可以算是这类库的始祖之一，也一度被认为是深度学习研究和应用的重要标准之一。</li>
<li>Theano的核心是一个数学表达式的编译器，专门为处理大规模神经网络训练的计算而设计。它可以将用户定义的各种计算编译为高效的底层代码，并链接各种可以加速的库，比如BLAS、CUDA等。</li>
</ul>
<h4 id="Torch"><a href="#Torch" class="headerlink" title="Torch"></a>Torch</h4><ul>
<li>Torch给自己的定位是LuaJIT上的一个高效的科学计算库，支持大量的机器学习算法，同时以GPU上的计算优先。</li>
<li>Torch的历史非常悠久，但真正得到发扬光大是在Facebook开源了其深度学习的组件之后，此后包括Google、Twitter、NYU、IDIAP、Purdue等组织都大量使用Torch。</li>
<li>Torch的目标是让设计科学计算算法变得便捷，它包含了大量的机器学习、计算机视觉、信号处理、并行运算、图像、视频、音频、网络处理的库，同时和Caffe类似，Torch拥有大量的训练好的深度学习模型。</li>
<li>它可以支持设计非常复杂的神经网络的拓扑图结构，再并行化到CPU和GPU上，在Torch上设计新的Layer是相对简单的。</li>
</ul>
<h4 id="Keras"><a href="#Keras" class="headerlink" title="Keras"></a>Keras</h4><ul>
<li>Keras是一个崇尚极简、高度模块化的神经网络库，使用Python实现，并可以同时运行在TensorFlow和Theano上。它旨在让用户进行最快速的原型实验，让想法变为结果的这个过程最短。</li>
<li>Theano和TensorFlow的计算图支持更通用的计算，而Keras则专精于深度学习。Theano和TensorFlow更像是深度学习领域的NumPy，而Keras则是这个领域的Scikit-learn。</li>
<li>它提供了目前为止最方便的API，用户只需要将高级的模块拼在一起，就可以设计神经网络，它大大降低了编程开销和阅读别人代码时的理解开销。</li>
</ul>
<h4 id="MXNet"><a href="#MXNet" class="headerlink" title="MXNet"></a>MXNet</h4><ul>
<li>MXNet是DMLC（Distributed Machine Learning Community）开发的一款开源的、轻量级、可移植的、灵活的深度学习库，它让用户可以混合使用符号编程模式和指令式编程模式来最大化效率和灵活性，目前已经是AWS官方推荐的深度学习框架。</li>
<li>它是各个框架中率先支持多GPU和分布式的，同时其分布式性能也非常高。MXNet的核心是一个动态的依赖调度器，支持自动将计算任务并行化到多个GPU或分布式集群。</li>
<li>它上层的计算图优化算法可以让符号计算执行得非常快，而且节约内存，开启mirror模式会更加省内存，甚至可以在某些小内存GPU上训练其他框架因显存不够而训练不了的深度学习模型，也可以在移动设备上运行基于深度学习的图像识别等任务。</li>
</ul>
<h4 id="CNTK"><a href="#CNTK" class="headerlink" title="CNTK"></a>CNTK</h4><ul>
<li>CNTK（Computational Network Toolkit）是微软研究院（MSR）开源的深度学习框架。它最早由start the deep learning craze的演讲人创建，目前已经发展成一个通用的、跨平台的深度学习系统，在语音识别领域的使用尤其广泛。</li>
<li>CNTK通过一个有向图将神经网络描述为一系列的运算操作，这个有向图中子节点代表输入或网络参数，其他节点代表各种矩阵运算。CNTK支持各种前馈网络，包括MLP、CNN、RNN、LSTM、Sequence-to-Sequence模型等，也支持自动求解梯度。</li>
<li>CNTK有丰富的细粒度的神经网络组件，使得用户不需要写底层的C++或CUDA，就能通过组合这些组件设计新的复杂的Layer。</li>
<li>CNTK拥有产品级的代码质量，支持多机、多GPU的分布式训练。</li>
</ul>
<h4 id="Deeplearning4J"><a href="#Deeplearning4J" class="headerlink" title="Deeplearning4J"></a>Deeplearning4J</h4><ul>
<li>Deeplearning4J（简称DL4J）是一个基于Java和Scala的开源的分布式深度学习库，由Skymind于2014年6月发布，其核心目标是创建一个即插即用的解决方案原型。</li>
<li>DL4J拥有一个多用途的n-dimensional array的类，可以方便地对数据进行各种操作；拥有多种后端计算核心，用以支持CPU及GPU加速，在图像识别等训练任务上的性能与Caffe相当。</li>
<li>可以与Hadoop及Spark自动整合，同时可以方便地在现有集群上进行扩展，同时DL4J的并行化是根据集群的节点和连接自动优化，不像其他深度学习库那样可能需要用户手动调整。</li>
</ul>
<h3 id="微服务与RPC-凤凰牌老熊"><a href="#微服务与RPC-凤凰牌老熊" class="headerlink" title="微服务与RPC 凤凰牌老熊"></a><a href="http://mp.weixin.qq.com/s?__biz=MzI4OTQ3MTI2NA==&amp;mid=2247483731&amp;idx=1&amp;sn=5f32d6a9757a48d0dcdb5921c131e2bd" target="_blank" rel="external">微服务与RPC</a> 凤凰牌老熊</h3><ul>
<li>RPC vs Restful：RPC支持多种语言（但不是所有语言），四层通讯协议，性能高，节省带宽，相对Restful协议，使用Thrift RPC，在同等硬件条件下，带宽使用率仅为前者的20%，性能却提升一个数量级，但是这种协议最大的问题在于无法穿透防火墙。而以Spring Cloud为代表所支持的Restful协议，优势在于能够穿透防火墙，使用方便，语言无关，基本上可以使用各种开发语言实现的系统，都可以接受Restful的请求，但性能和带宽占用上有劣势。所以业内对微服务的实现，基本是确定一个组织边界，在该边界内使用RPC，边界外使用Restful。</li>
<li>RPC选型：Apache Thrift是目前最为成熟的框架，优点在于稳定、高性能，缺点在于它仅提供RPC服务，其他的功能，包括限流、熔断、服务治理等，都需要自己实现，或者使用第三方软件。Google Protobuf一直只有数据模型的实现，而2015年才推出的gRPC还缺乏重量级的用户。Thrift 提供多种高性能的传输协议，但在数据定义上，不如Protobuf强大，而Protobuf的劣势在于其RPC服务的实现性能不佳，为此，Apache Thrift + Protobuf的RPC实现，成为不少公司的选择。</li>
<li>服务注册与发现：Spring cloud提供了服务注册和发现功能，如果需要自己实现，可以考虑使用Apache Zookeeper作为注册表，使用Apache Curator 来管理Zookeeper的链接；对服务注册来说，注册表结构需要详细设计，一般注册表结构会按照如下方式组织：机房区域-部门-服务类型-服务名称-服务器地址。</li>
<li>连接池：RPC服务访问和数据库类似，建立链接是一个耗时的过程，连接池是服务调用的标配。目前还没有成熟的开源Apache Thrift链接池，一般互联网公司都会开发内部自用的链接池。自己实现可以基于JDBC链接池做改进，比如参考Apache commons DBCP链接池，使用Apache Pools来管理链接。连接池实现的主要难点在于如何从多个服务器中选举出来为当前调用提供服务的连接。比如目前有10台机器在提供服务，上一次分配的是第4台服务器，本次应该分配哪一台？在实现上，需要收集每台机器的QOS以及当前的负担，分配一个最佳的连接。</li>
<li>API网关：如果有一个应用需要调用多个服务，对这个应用来说，就需要维护和多个服务器之间的链接。服务的重启，都会对连接池以及客户端的访问带来影响。为此，在微服务中，广泛会使用到API网关。API网关可以认为是一系列服务集合的访问入口。从面向对象设计的角度看，它与外观模式类似，实现对所提供服务的封装。</li>
<li>熔断与限流：熔断一般采用电路熔断器模式(Circuit Breaker Patten)，当某个服务发生错误，每秒错误次数达到阈值时，不再响应请求，直接返回服务器忙的错误给调用方，延迟一段时间后，尝试开放50%的访问，如果错误还是高，则继续熔断，否则恢复到正常情况。限流指按照访问方、IP地址或者域名等方式对服务访问进行限制，一旦超过给定额度，则禁止其访问。 除了使用Hystrix，如果要自己实现，可以考虑使用使用Guava RateLimiter。</li>
<li>服务演化：随着服务访问量的增加，服务的实现也会不断演化以提升性能，主要的方法有读写分离、缓存等。</li>
</ul>
<h3 id="CMU论文：一部深度学习发展史，看神经网络兴衰更替-Haohan-Wang-Bhiksha-Raj-张易"><a href="#CMU论文：一部深度学习发展史，看神经网络兴衰更替-Haohan-Wang-Bhiksha-Raj-张易" class="headerlink" title="CMU论文：一部深度学习发展史，看神经网络兴衰更替  Haohan Wang/Bhiksha Raj/张易"></a><a href="http://mp.weixin.qq.com/s/xNERrFKU4tY3lGmwBIJC9w" target="_blank" rel="external">CMU论文：一部深度学习发展史，看神经网络兴衰更替</a>  Haohan Wang/Bhiksha Raj/张易</h3><ul>
<li>论文地址：<a href="https://128.84.21.199/abs/1702.07800" target="_blank" rel="external">https://128.84.21.199/abs/1702.07800</a></li>
<li>从亚里士多德的联想主义心理学到神经网络的优化方法，CMU的这篇最新论文回顾解析了深度学习的演化历史，不仅提供了一个全面的背景知识，而且总结了一座座发展里程碑背后的闪光思想，为未来的深度学习研究提供了方向。</li>
<li>人工智能的发展或许可以追溯到公元前仰望星空的古希腊人，当亚里士多德为了解释人类大脑的运行规律而提出了联想主义心理学的时候，他恐怕不会想到，两千多年后的今天，人们正在利用联想主义心理学衍化而来的人工神经网络，构建超级人工智能，一起又一次地挑战人类大脑认知的极限；</li>
<li>联想主义心理学是一种理论，认为人的意识是一组概念元素，被这些元素之间的关联组织在一起。受柏拉图的启发，亚里士多德审视了记忆和回忆的过程，提出了四种联想法则：邻接（空间或时间上接近的事物或事件倾向于在意识中相关联）、频率（两个事件的发生次数与这两个事件之间的关联强度成正比）、相似性（关于一个事件的思维倾向于触发类似事件的思维）、对比（关于一个事件的思维倾向于触发相反事件的思维）。</li>
<li>1949年，Hebb提出了那条著名的规则：一起发射的神经元连在一起。更具体的表述是：“当神经元A的轴突和神经元B足够接近并反复或持续激发它时，其中一个或两个神经元就会发生增长或新陈代谢的变化，例如激发B的神经元之一——A efficiency——会增加。” </li>
<li>尽管Hebbian学习规则被视为奠定了神经网络的基础，但今天看来它的缺陷是显而易见的：随着共同出现的次数增加，连接的权重不断增加，主信号的权重将呈指数增长。这就是Hebbian学习规则的不稳定性。</li>
<li>将感知器放在一起，就变成了基本的神经网络。通过并列放置感知器，我们能得到一个单层神经网络。通过堆叠一个单层神经网络，我们会得到一个多层神经网络，这通常被称为多层感知器（MLP ）。单层神经网络具有局限性，正是这种局限性导致了相关的研究曾经一度停滞了进二十年，但同时，也正是这种局限性刺激了神经网络向更高层结构进发，渐渐迎来了如今的深度学习时代。</li>
<li>神经网络的一个显著特性，即众所周知的通用逼近属性，可以被粗略描述为MLP可以表示任何函数。可以从以下三方面探讨这一属性：布尔逼近（一个隐藏层的MLP可以准确的表示布尔函数）、连续逼近（一个隐藏层的MLP可以以任意精度逼近任何有界连续函数）、任意逼近（两个隐藏层的MLP可以以任意精度逼近任何函数）。</li>
<li>universal approximation成为如今神经网络与深度学习一片繁荣景象的重要理论基石，universal approximation的相关理论——一个多层神经网络具备表达任何方程的能力——已经成为深度学习的标志性特点。</li>
<li>从八十年代的Self Organizing Map到 Hopfield Network, 再到鼎鼎大名的Boltzmann Machine和Restricted Boltzmann  Machine，直到Hinton塑造的Deep Belief Network。深度学习的研究一路走来，悠长的历史之中，作者带领我们研读了这几个璀璨明星的诞生过程，以及这些作品诞生时的内在联系。下图总结了涉及的模型，水平轴代表这些模型的计算复杂度，而垂直轴代表表达能力，这是六个里程碑式的模型。</li>
</ul>
<p><img src="http://mmbiz.qpic.cn/mmbiz_png/UicQ7HgWiaUb0KmiabiaAYL2leR3beqyIhQyc0z6hq5kRy6wr4g89VHsarib8ALSd1I6okiarOdLrkH5qT7z2nE0icQsw/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1" alt=""></p>
<ul>
<li>卷积神经网络的谱系主要是从对人类视觉皮层的认识演变而来。卷积神经网络的视觉问题的成功原因之一是：复制人类视觉系统的仿生设计。卷积作为一个非常有效的视觉特征提取工具，几乎是深度学习在计算机视觉问题上如此成功的基石。</li>
<li>递归神经网络（RNN）是神经网络的一种，其单位的连接形成了有向循环; 这种性质赋予了其处理时间数据的能力。</li>
<li>优化是深度学习发展历史上不可回避的课题。目前存在的优化方式有：梯度法、剔除法、BatchNormalization。</li>
</ul>
<h3 id="北大AI公开课第1讲：雷鸣评人工智能前沿与行业结合点-新智元"><a href="#北大AI公开课第1讲：雷鸣评人工智能前沿与行业结合点-新智元" class="headerlink" title="北大AI公开课第1讲：雷鸣评人工智能前沿与行业结合点 新智元"></a><a href="http://mp.weixin.qq.com/s/UPsHwCqF4MJ1uTAunfza3g" target="_blank" rel="external">北大AI公开课第1讲：雷鸣评人工智能前沿与行业结合点</a> 新智元</h3><p>视频回放链接：<a href="http://www.iqiyi.com/l_19rrf6l46v.html" target="_blank" rel="external">http://www.iqiyi.com/l_19rrf6l46v.html</a></p>
<h3 id="北大AI公开课第2讲：雷鸣-amp-余凯漫谈嵌入式AI-新智元"><a href="#北大AI公开课第2讲：雷鸣-amp-余凯漫谈嵌入式AI-新智元" class="headerlink" title="北大AI公开课第2讲：雷鸣&amp;余凯漫谈嵌入式AI 新智元"></a><a href="http://mp.weixin.qq.com/s/cl8FMJagC9GZvSIdVAmrJQ" target="_blank" rel="external">北大AI公开课第2讲：雷鸣&amp;余凯漫谈嵌入式AI</a> 新智元</h3><p>视频回放链接：<a href="http://www.iqiyi.com/w_19rtza2dh9.html" target="_blank" rel="external">http://www.iqiyi.com/w_19rtza2dh9.html</a></p>
<h3 id="北大AI公开课第3讲：蚂蚁金服漆远-人工智能驱动的金融生活服务-新智元"><a href="#北大AI公开课第3讲：蚂蚁金服漆远-人工智能驱动的金融生活服务-新智元" class="headerlink" title="北大AI公开课第3讲：蚂蚁金服漆远 人工智能驱动的金融生活服务 新智元"></a><a href="http://mp.weixin.qq.com/s/Sj0jUshXXLCfJsxvoH5eYg" target="_blank" rel="external">北大AI公开课第3讲：蚂蚁金服漆远 人工智能驱动的金融生活服务</a> 新智元</h3><p>视频回放链接：<a href="http://www.iqiyi.com/l_19rrfk4wof.html" target="_blank" rel="external">http://www.iqiyi.com/l_19rrfk4wof.html</a></p>
<h3 id="北大AI公开课第4讲：吴甘沙：智能驾驶，有多少AI可以重来-新智元"><a href="#北大AI公开课第4讲：吴甘沙：智能驾驶，有多少AI可以重来-新智元" class="headerlink" title="北大AI公开课第4讲：吴甘沙：智能驾驶，有多少AI可以重来 新智元"></a><a href="http://mp.weixin.qq.com/s/deWPwaw4IWY4GcC-LK7Ilw" target="_blank" rel="external">北大AI公开课第4讲：吴甘沙：智能驾驶，有多少AI可以重来</a> 新智元</h3><p>视频回放链接：<a href="http://www.iqiyi.com/w_19ru020epp.html" target="_blank" rel="external">http://www.iqiyi.com/w_19ru020epp.html</a></p>
<h3 id="北大AI公开课第5讲：小米黄江吉-产品化引领人工智能硬件发展-新智元"><a href="#北大AI公开课第5讲：小米黄江吉-产品化引领人工智能硬件发展-新智元" class="headerlink" title="北大AI公开课第5讲：小米黄江吉 产品化引领人工智能硬件发展 新智元"></a><a href="http://mp.weixin.qq.com/s/Wdz0wNSpf_5FWkUOJ-XfEw" target="_blank" rel="external">北大AI公开课第5讲：小米黄江吉 产品化引领人工智能硬件发展</a> 新智元</h3><p>视频回放链接：<a href="http://www.iqiyi.com/l_19rrfgd203.html" target="_blank" rel="external">http://www.iqiyi.com/l_19rrfgd203.html</a></p>
<h3 id="实例化DevOps原则-伍斌"><a href="#实例化DevOps原则-伍斌" class="headerlink" title="实例化DevOps原则 伍斌"></a><a href="http://mp.weixin.qq.com/s?__biz=MjM5MjY3OTgwMA==&amp;mid=2652456410&amp;idx=1&amp;sn=e1e54a5b790969e1cd86f70f4d4363db" target="_blank" rel="external">实例化DevOps原则</a> 伍斌</h3><ul>
<li>DevOps的起源可以分为两条线：比利时独立咨询师Patrick Debois思考能否把敏捷的实践引入Ops团队；图片分享网站Flickr的两个开发者于2009年发表了一个引燃DevOps的演讲－－《每天部署10次以上：Flickr公司的Dev与Ops的合作》；</li>
<li>Flickr公司的两位演讲者所表达的“Dev和Ops的共同目标是让业务所要求的那些变化能随时上线可用”这一观点，其实就是DevOps的愿景。而要达到这一点，可以使用一个现成的工具：精益。源自丰田生产方式的“精益”的愿景就是“Shortest lead time”，即用最短的时间来完成从客户下订单到收到货物的全过程。这恰好能帮助实现DevOps的上述愿景。</li>
<li>从上面DevOps的起源中能够看出三点：DevOps源自草根社区，最初并没有什么自上而下设计出来的理论框架；DevOps背后的原则，就是上面两条线中所涉及的敏捷和精益的原则；DevOps的愿景是让业务所要求的那些变化能随时上线可用。 </li>
<li>一些DevOps从业者，纷纷设定自己的DevOps框架。其中比较有名的框架有Damon Edwards所定义并被Jez Humble所修订的CALMS（Culture, Automation, Lean, Metrics, ），和Gene Kim所定义的The Three Ways。</li>
<li></li>
</ul>
<h3 id="TDD-is-dead-Long-live-testing-David-Heinemeier-Hansson"><a href="#TDD-is-dead-Long-live-testing-David-Heinemeier-Hansson" class="headerlink" title="TDD is dead. Long live testing.  David Heinemeier Hansson"></a><a href="http://david.heinemeierhansson.com/2014/tdd-is-dead-long-live-testing.html" target="_blank" rel="external">TDD is dead. Long live testing.</a>  David Heinemeier Hansson</h3><ul>
<li>DHH是Ruby on Rails的创始人，Basecamp公司的创始人和CTO；</li>
<li>DHH先批判了测试先行的教条主义，虽然它能提升开发人员对软件质量的自信，且对于自动回归测试是有帮助的，但它不应该作为每日工作的教条；而业界近些年来对于TDD的推崇以及对未采用TDD开发的嘲讽更让DHH发火；当DHH多次尝试TDD而发现这会伤害程序的设计时，DHH声明不以TDD的方式开发软件了；</li>
<li>DHH的建议：重新平衡单元测试和系统测试，减少对于单元测试的重视，将更多的精力投入到系统测试；但千万不要跳入只做系统测试的极端里。</li>
</ul>
<h3 id="Is-TDD-Dead-Martin-Fowler-David-Heinemeier-Hansson-Kent-Beck"><a href="#Is-TDD-Dead-Martin-Fowler-David-Heinemeier-Hansson-Kent-Beck" class="headerlink" title="Is TDD Dead? Martin Fowler/David Heinemeier Hansson/Kent Beck"></a><a href="https://martinfowler.com/articles/is-tdd-dead/" target="_blank" rel="external">Is TDD Dead?</a> Martin Fowler/David Heinemeier Hansson/Kent Beck</h3><ul>
<li>Part 1: <a href="https://www.youtube.com/watch?v=z9quxZsLcfo" target="_blank" rel="external">https://www.youtube.com/watch?v=z9quxZsLcfo</a></li>
<li>Part 2: <a href="https://www.youtube.com/watch?v=JoTB2mcjU7w" target="_blank" rel="external">https://www.youtube.com/watch?v=JoTB2mcjU7w</a></li>
<li>Part 3: <a href="https://www.youtube.com/watch?v=YNw4baDz6WA" target="_blank" rel="external">https://www.youtube.com/watch?v=YNw4baDz6WA</a></li>
<li>Part 4: <a href="https://www.youtube.com/watch?v=dGtasFJnUxI" target="_blank" rel="external">https://www.youtube.com/watch?v=dGtasFJnUxI</a></li>
<li>Part 5 &amp; Part 6: <a href="https://www.youtube.com/watch?v=gWD6REVeKW4" target="_blank" rel="external">https://www.youtube.com/watch?v=gWD6REVeKW4</a> </li>
</ul>
<h3 id="让我们再聊聊TDD-刘冉"><a href="#让我们再聊聊TDD-刘冉" class="headerlink" title="让我们再聊聊TDD 刘冉"></a><a href="http://mp.weixin.qq.com/s/dmreBAzk2Mz94YfkXN8PHg" target="_blank" rel="external">让我们再聊聊TDD</a> 刘冉</h3><ul>
<li>总结一下，技术人员拒绝TDD的主要原因在于难度大、工作量大、Mock的大量使用导致很难测试业务价值等。这些理解主要是建立在片面的理解和实践之上，而在我的认知中，TDD的核心是：<strong>先写测试，并使用它帮助开发人员来驱动软件开发</strong>。</li>
<li>首先是先写测试，这里的测试并不只是单元测试，也不是说一定要使用mock和stub来做测试。这里的测试就是指软件测试本身，可以是基于代码单元的单元测试，可以是基于业务需求的功能测试，也可以是基于特定验收条件的验收测试。其次是帮助开发人员，主要是帮助开发人员理解软件的功能需求和验收条件，帮助其思考和设计代码，从而达到驱动开发的目的</li>
<li>TDD是包含两部分：ATDD（验收驱动测试开发，首先BA或者QA编写验收测试用例，然后Dev通过验收测试来理解需求和验收条件，并编写实现代码直到验收测试用例通过）与UTDD（单元驱动测试开发，首先Dev编写单元测试用例，然后编写实现代码直到单元测试通过）。</li>
</ul>
<p><img src="http://mmbiz.qpic.cn/mmbiz_png/aaVJqS7LaMLN7tU0IqFick0jH1uSDLQ5fVd3nXpDRicibEZ3JMjw6CDr0KUk3F1jJ1TmdkcLOaPLQaPUrHZicvRJFQ/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1" alt="TDD包括ATDD和UTDD"></p>
<ul>
<li>TDD不是银弹，不要期望它能解决任何问题，无论是UTDD、EDD还是BDD，根据自己项目的实际情况，比如资金、人力资源、时间、组织架构等，合理的选择。  </li>
<li>TDD并没有死，死的是你的持续学习、思考、实践与总结。TDD其实早已融入日常的软件开发工作中，只是很多人还没有意识到。</li>
</ul>
<h3 id="让我们再聊聊TDD-续——人人都在做TDD-刘冉"><a href="#让我们再聊聊TDD-续——人人都在做TDD-刘冉" class="headerlink" title="让我们再聊聊TDD 续——人人都在做TDD 刘冉"></a><a href="https://mp.weixin.qq.com/s/tm70lIohbbhlZ50LoFD5HA" target="_blank" rel="external">让我们再聊聊TDD 续——人人都在做TDD</a> 刘冉</h3><ul>
<li>现实世界中TDD的实施一般分为三个阶段，即无意识的TDD、被动通过技术实现的TDD、以及有意识和主动通过技术实现的TDD。</li>
<li>无意识的TDD：当拿到一个新的软件需求时，首先会思考如何实现，其中包括当前软件架构、业务分解、实现设计、代码分层、代码实现等，然后通过思考和设计所得到的产出物来驱动代码实现，进而在代码实现中会思考如何通过一个或多个函数或者算法来实现业务逻辑，这类思考其实已经是意识思维上的TDD，它帮助开发人员先在大脑里面设计并验证代码实现，甚至帮助其重构代码；其实开发人员在开发前思考测试逻辑和用例的过程就是在做TDD了；只不过这是初级的无意识的TDD，没有明确的产出来协助和规范这个测试驱动开发方式，也缺乏快速反馈、度量、传递和协作等；</li>
<li>被动通过技术实现TDD：由于意识层面上的难易程度和工作量都比技术层面上相对较小，所以前者实施起来相对容易一些，而后者则相对较难，所以如果通过了各种手段强行实施TDD，而没有主动去摆正做TDD的意识，甚至没有足够的技术能力，那么这样的TDD就是一个倒三角，非常容易倒塌；</li>
<li>有意识和主动通过技术实现TDD：首先要突破思维意识的局限，认识到TDD的普遍存在性和适用性，不要害怕和排斥TDD这种思维和开发模式；其次要主动学习，并刻意练习TDD的技术实现，提升自己的技术能力，从而在技术层面能更容易的实现TDD，摆脱被动TDD的困境；只有大量的刻意练习才能让你在真实的代码编写过程中去思考和理解TDD，去运用你通过学习得到的知识，最终才能做到有意识和主动的通过技术去实现TDD，TDD的倒三角才能变成一个稳定的砖块，然后哪里需要往哪里搬。</li>
</ul>
<p><img src="http://mmbiz.qpic.cn/mmbiz_png/aaVJqS7LaMKMVkVm2qvJxgkWcicPgeIgXShj5d81Z98AMXJcfvASJlstibhFHSLDw3tI2XxSMuUSGpDMic3rLTSibg/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1" alt="TDD"></p>
<h3 id="推行TDD的思考-张逸"><a href="#推行TDD的思考-张逸" class="headerlink" title="推行TDD的思考  张逸"></a><a href="http://mp.weixin.qq.com/s/c2xmaqjD9U07PWX4BMOyZA" target="_blank" rel="external">推行TDD的思考</a>  张逸</h3><ul>
<li>开发人员的质量意识：由开发人员编写测试带来的收益，最重要的一点不在于测试本身，而在于它能促进开发、测试以及需求分析人员的交流与沟通；也能让开发者从消费者角度去思考接口的设计；软件质量除了外部质量之外，内部质量同等重要，维护成本的增加主要归咎于内部质量的糟糕；</li>
<li>需求分析与任务分解：TDD要求我们在编写测试之前要做好合理的任务分解。若没有很好地理解需求，任务分解就无法顺利进行；任务分解应该是有层次的，即业务价值——&gt;业务功能——&gt;业务实现；任务分解是TDD的核心，是驱动设计和开发的重要力量；</li>
<li>测试先行的编程习惯：任务分解应该是TDD的起点，多数开发者未能形成任务分解的习惯，因此在改变为测试先行的时候，错以为应该一上来就写测试；测试驱动开发仍可进行事先设计，设计并不仅包含技术层面的设计如对OO思想乃至设计模式的运用，它本身还包括对需求的分析与建模；测试驱动开发提倡的任务分解，实际上就是一种需求的分析，如何寻找职责，以及识别职责的承担者则可以视为建模设计；在开始测试驱动开发之前，做适度的事先设计，还有利于我们仔细思考技术实现的解决方案，它与测试驱动接口的设计并不相悖；</li>
<li>重构能力：TDD的核心是红——绿——重构，没有好的重构能力，TDD就会有缺失，若说代码的内部质量是生命的话，重构就是灵魂；重构手法与代码坏味道一一对应，若有测试保障，重构就变得安全；重要的是要找到重构的节奏感，即小步前行，每次重构必运行测试的良好习惯；在TDD过程中，若能结对自然是上佳选择，当一个人在掌控键盘时，另一个人就可以重点关注代码的可读性，看看代码是否散发出臭味；</li>
<li>单元测试的基础设施：最好能找到一些开源的测试框架，包括生成测试数据，模拟测试行为等，因为你遇到的问题，别人可能早已遇见过。</li>
</ul>
<h3 id="大数据时代的新型数据库-—-图数据库-Neo4j-的应用-张帜"><a href="#大数据时代的新型数据库-—-图数据库-Neo4j-的应用-张帜" class="headerlink" title="大数据时代的新型数据库 — 图数据库 Neo4j 的应用 张帜"></a><a href="http://mp.weixin.qq.com/s?__biz=MzI3MzEzMDI1OQ==&amp;mid=2651816597&amp;idx=1&amp;sn=e061823e2020258a729dcf7498f8aba4" target="_blank" rel="external">大数据时代的新型数据库 — 图数据库 Neo4j 的应用</a> 张帜</h3><ul>
<li>什么是图数据库：是基于数学里的图论的理论和算法而实现的高效处理复杂关系网络的新型数据库系统，它实际上就是处理关系的、处理网络的数据库系统。图数据库是善于处理大量的、复杂的、互联的、多变的数据，它处理这些数据的效率，远远高于关系型数据库。</li>
<li>从数据库的结构来看，它包含的概念非常的简单，他包含的概念只有节点和关系。节点可以带标签，节点和关系也都可以带属性。</li>
</ul>
<p><img src="http://mmbiz.qpic.cn/mmbiz_jpg/3xsFRgx4kHrLeb7AhVmAnDeMrVwXM98asCP41iaOpd0dLez09Cv4aK9yfrbT1IcnbOHm1P7iadtwSaAUTBhzeqpA/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1" alt="节点、关系和属性"></p>
<p><img src="http://mmbiz.qpic.cn/mmbiz_jpg/3xsFRgx4kHrLeb7AhVmAnDeMrVwXM98aeUtYBEyanQich093rOm4o0HjRsyx6IPs5uq0TXWE0XSvjefZO81Yllw/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1" alt="创建节点、关系"></p>
<p><img src="http://mmbiz.qpic.cn/mmbiz_jpg/3xsFRgx4kHrLeb7AhVmAnDeMrVwXM98aPQnqgn8hBNeDNLz2NFBlUz8No2KTn27CgZ1iaiayoxq5cKSN8PSPV9ww/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1" alt="查询"></p>
<ul>
<li>为什么要用图数据库：世界本来就是由各种关系组成的；关系型数据库处理复杂关系的时候，建模难、性能低、查询难、扩展难；图数据库它是专门为处理复杂关系而创建出来的，它具有开发的优势和部署的优势。</li>
<li>Neo4j的关键产品特征：社区版不支持集群，免费；企业版支持集群，是收费的。</li>
</ul>
<p><img src="http://mmbiz.qpic.cn/mmbiz_jpg/3xsFRgx4kHrLeb7AhVmAnDeMrVwXM98ajpQAxkojh6gQpO5NicDiaTgkiaiaYBoboxVGicTN3KPx6Prw3icgBXg1C5NA/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1" alt="Neo4j的关键产品特征"></p>
<h3 id="Apache-Spark和Apache-Storm的区别-fuqingchuan"><a href="#Apache-Spark和Apache-Storm的区别-fuqingchuan" class="headerlink" title="Apache Spark和Apache Storm的区别 fuqingchuan"></a><a href="http://mp.weixin.qq.com/s/GgUhOTTbmliGsvdp6qE3OA" target="_blank" rel="external">Apache Spark和Apache Storm的区别</a> fuqingchuan</h3><ul>
<li>Apache Spark是基于内存的分布式数据分析平台，旨在解决快速批处理分析任务、迭代机器学习任务、交互查询以及图处理任务。其最主要的特点在于，Spark使用了RDD或者说弹性分布式数据集。RDD非常适合用于计算的流水线式并行操作。RDD的不变性(immutable)保证，使其具有很好的容错能力。</li>
</ul>
<p><img src="http://mmbiz.qpic.cn/mmbiz_png/8y6qG4b7IlKAuicCqd4ODeOibt1TS0XFrVRtG19JexY5T9g7IcUibNMtHRIJzjbpEQmwddqvx7GSoYhxic8D6Ex0RQ/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1" alt="Spark架构"> </p>
<ul>
<li>Apache Storm专注于流处理或者一些调用复杂的事件处理。Storm实现了一种容错方法，用于在事件流入系统时执行计算或流水线化多个计算。人们可以使用Storm在非结构化数据流入系统到期望的格式时对其进行转换。</li>
</ul>
<p><img src="http://mmbiz.qpic.cn/mmbiz_png/8y6qG4b7IlKAuicCqd4ODeOibt1TS0XFrVniaOm0DboNBGE137aFpFtCDWkcQtrIn5IXbunp41caucuSXCpECMyDA/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1" alt="Storm架构"></p>
<ul>
<li>Storm和Spark专注于相当不同的应用场景。对比Storm Trident和Spark Streaming，应该是更加公平的比较。由于Spark的RDD本质上是不可变的，Spark Streaming实现了一种方法，用于在用户定义的时间间隔中“批处理”传入的更新，并将其转换为自己的RDD。 然后Spark通用的并行运算符就可以对这些RDD执行计算。这与Storm处理每个事件不同，Storm是真正的流式处理。</li>
<li>总而言之，这两种技术之间的一个主要区别是Spark执行数据并行计算，而Storm执行任务并行计算，这两种设计都是各自领域内的权衡。</li>
</ul>
<h3 id="大数据系统的Lambda架构-张逸"><a href="#大数据系统的Lambda架构-张逸" class="headerlink" title="大数据系统的Lambda架构 张逸"></a><a href="http://mp.weixin.qq.com/s?__biz=MzA4NTkwODkyMQ==&amp;mid=208690964&amp;idx=1&amp;sn=03497b08e36f091c4afdf4b30532ea8f" target="_blank" rel="external">大数据系统的Lambda架构</a> 张逸</h3><ul>
<li>在大数据处理系统中，如何有效地将real time与batch job结合起来，既发挥前者对响应的实时性，又能解决对海量数据的分析与处理？答案就是Lambda架构思想。</li>
<li>传统系统的问题：无法很好地支持系统的可伸缩性；数据库对于分区是不了解的，无法帮助你应对分区、复制与分布式查询；最糟糕的问题是系统并没有为人为错误进行工程设计，仅靠备份是不能治本的；</li>
<li>数据系统的概念：如果数据系统通过查找过去的数据去回答问题，则通常需要访问整个数据集。因此可以给data system的最通用的定义：Query = function(all data)</li>
<li>一个大数据系统必须具备的属性包括：健壮性和容错性（Robustness和Fault Tolerance）、低延迟的读与更新（Low Latency reads and updates）、可伸缩性（Scalability）、通用性（Generalization）、可扩展性（Extensibility）、内置查询（Ad hoc queries）、维护最小（Minimal maintenance）、可调试性（Debuggability）；</li>
<li>Lambda架构：主要思想就是将大数据系统构建为多个层次，如下图所示</li>
</ul>
<p><img src="http://mmbiz.qpic.cn/mmbiz/d3Qa7X5fakCejz6If7PwqE84e6DhKj3LsYAy0WogdNxlZZCVk6RtBdjvWrkYOgLFaIq5GblDUZvD0BF1ksbaWA/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1" alt="Lambda架构分层"></p>
<ul>
<li>理想状态下，任何数据访问都可以从表达式Query = function(all data)开始，但是，若数据达到相当大的一个级别（例如PB），且还需要支持实时查询时，就需要耗费非常庞大的资源。一个解决方式是预运算查询函数（precomputed query funciton）。Mathan Marz将这种预运算查询函数称之为Batch View，当需要执行查询时，可以从Batch View中读取结果。这样一个预先运算好的View是可以建立索引的，因而可以支持随机读取。于是系统就变成：</li>
</ul>
<pre><code>batch view = function(all data)
query = function(batch view)
</code></pre><ul>
<li>Batch Layer：在Lambda架构中，实现batch view = function(all data)的部分被称之为batch layer。它承担了两个职责：<br>存储Master Dataset，这是一个不变的持续增长的数据集；针对这个Master Dataset进行预运算。利用Batch Layer进行预运算的作用实际上就是将大数据变小，从而有效地利用资源，改善实时查询的性能。</li>
<li>Serving Layer：Serving Layer负责对batch view进行操作，从而为最终的实时查询提供支撑。因此Serving Layer的职责包含：对batch view的随机访问；更新batch view。Serving Layer应该是一个专用的分布式数据库，以支持对batch view的加载、随机读取以及更新。注意，它并不支持对batch view的随机写，因为随机写会为数据库引来许多复杂性。</li>
<li>Speed Layer：从对数据的处理来看，speed layer与batch layer非常相似，它们之间最大的区别是前者只处理最近的数据，后者则要处理所有的数据。另一个区别是为了满足最小的延迟，speed layer并不会在同一时间读取所有的新数据，相反，它会在接收到新数据时，更新realtime view，而不会像batch layer那样重新运算整个view。speed layer是一种增量的计算，而非重新运算（recomputation）。因而，Speed Layer的作用包括：对更新到serving layer带来的高延迟的一种补充；快速、增量的算法；最终Batch Layer会覆盖speed layer；</li>
<li>总结下来，Lambda架构就是如下的三个等式：</li>
</ul>
<pre><code>batch view = function(all data)
realtime view = function(realtime view, new data)
query = function(batch view . realtime view)
</code></pre><p>整个Lambda架构如下图所示：</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/d3Qa7X5fakCejz6If7PwqE84e6DhKj3LxUnITFzls4Vzdzo0okkC97nMrOQtg83MBXVwD3BExm2zicd1AlJlcoA/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1" alt="Lambda架构"></p>
<h3 id="Lambda架构与推荐在电商网站实践-王富平"><a href="#Lambda架构与推荐在电商网站实践-王富平" class="headerlink" title="Lambda架构与推荐在电商网站实践 王富平"></a><a href="http://mp.weixin.qq.com/s?__biz=MzAwMDU1MTE1OQ==&amp;mid=401800864&amp;idx=1&amp;sn=e86e31a4aa6279f5b515f9116da47d59" target="_blank" rel="external">Lambda架构与推荐在电商网站实践</a> 王富平</h3><ul>
<li>Lambda架构：Lambda架构由Storm的作者Nathan Marz提出。旨在设计出一个能满足实时大数据系统关键特性的架构，具有高容错、低延时和可扩展等特性。Lambda架构整合离线计算和实时计算，融合不可变性（Immutability）、读写分离和复杂性隔离等一系列架构原则，可集成Hadoop、Kafka、Storm、Spark、HBase等各类大数据组件。</li>
<li>Lambda有两个假设：不可变假设（Lambda架构要求data不可变）、Monoid假设（理想情况下满足Monoid 的function可以转换为 query = function(all data/ 2) + function(all data/ 2)）；</li>
<li>Lambda三层架构：批处理层（批量处理数据，生成离线结果）、实时处理层（实时处理在线数据，生成增量结果）、服务层（结合离线、在线计算结果，推送上层）；</li>
</ul>
<p><img src="http://mmbiz.qpic.cn/mmbiz/8XkvNnTiapONibSkm2GM78fSeyNicjYLiaS4KLn4NnjP1tN9xKmiapauPIKMw9EW4Nu6N18W2DlDVWUQu2RxU7ljWkA/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1" alt="Lambda三层架构"></p>
<ul>
<li>Lambda架构缺点：Lambda需要将所有的算法实现两次，一次是为批处理系统，另一次是为实时系统，还要求查询得到的是两个系统结果的合并。考虑到将复杂算法正确地实现一次都是一个挑战，执行两次这样的任务以及调试不可避免的问题显然是难上加难。除此之外，运维两个分布式多节点的服务肯定比运维一个更难。【参加<a href="http://www.infoq.com/cn/news/2014/09/lambda-architecture-questions】" target="_blank" rel="external">http://www.infoq.com/cn/news/2014/09/lambda-architecture-questions】</a></li>
<li>推荐系统的最终目的是提高转化率，手段是推送用户感兴趣的、需要的产品。1号店会根据你实时浏览、加车、收藏、从购物车删除、下单等行为，计算相关产品的权重，把相应的产品立刻更新到猜你喜欢栏位。</li>
<li>Netflix推荐架构：批处理层（从Hive、pig数据仓库，离线计算推荐模型，生成离线推荐结果）、实时处理层（从消息队列实时拉取用户行为数据与事件，生成在线推荐结果）、服务层（结合离线、在线推荐结果，为用户生成推荐列表）；</li>
</ul>
<p><img src="http://mmbiz.qpic.cn/mmbiz/8XkvNnTiapONibSkm2GM78fSeyNicjYLiaS4uGGpcicBjCDXz90x0gib9Z6A1flicrwFmicyTt3DyvDpS3t3Ub5uu4OjKw/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1" alt="Netflix推荐架构"></p>
<ul>
<li>1号店推荐系统实践：推荐引擎组件包括用户意图、用户画像、千人千面、情境推荐、反向推荐、主题推荐；</li>
</ul>
<p><img src="http://mmbiz.qpic.cn/mmbiz/8XkvNnTiapONibSkm2GM78fSeyNicjYLiaS4FYPdPln0wy1kLaxOKPc6geibsjAkVNgWbvHr5gt1pib07OqvWMF10lBQ/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1" alt="1号店推荐系统实践"></p>
<h3 id="技术的边界-阮一峰"><a href="#技术的边界-阮一峰" class="headerlink" title="技术的边界 阮一峰"></a><a href="http://www.ruanyifeng.com/blog/2017/03/boundary.html" target="_blank" rel="external">技术的边界</a> 阮一峰</h3><ul>
<li>当代社会就像一座机器组成的监狱，学会技术可以摆脱牢房。</li>
<li>我们理应享受技术成果，问题是，技术正变得越来越先进，也越来越难懂，大多数人已经不能够理解技术了。</li>
<li>技术已经到了这样一个地步：我们走一步看一步，谁也不知道十年后，技术会突破到什么程度。</li>
<li>我最近常想，技术有没有边界呢？一旦接近”绝境长城”，我们会自觉停在那里，不再往下发展吗？举例来说，人工智能领域有一个概念，叫做”终极智能”。意思是，当机器的智能达到这种程度时，就不需要人类再做发明创造了，因为机器自己就会发明创造。如果这种”终极智能”真的可能实现，技术要不要去实现它呢？</li>
<li>一个依赖技术的高科技、高度自动化的社会，也是一个非常脆弱的社会。整个人类正坐在一架软件驾驶的飞机里面，只能祈祷软件运行永远不发生错误。一旦发生问题，人类就会坠机。</li>
</ul>
<h3 id="分布式开放消息系统-RocketMQ-的原理与实践-CHEN川"><a href="#分布式开放消息系统-RocketMQ-的原理与实践-CHEN川" class="headerlink" title="分布式开放消息系统(RocketMQ)的原理与实践 CHEN川"></a><a href="http://www.jianshu.com/p/453c6e7ff81c" target="_blank" rel="external">分布式开放消息系统(RocketMQ)的原理与实践</a> CHEN川</h3><ul>
<li>分布式消息系统作为实现分布式系统可扩展、可伸缩性的关键组件，需要具有高吞吐量、高可用等特点。而谈到消息系统的设计，就回避不了两个问题：消息的顺序问题和消息的重复问题；</li>
<li>消息有序指的是可以按照消息的发送顺序来消费。要实现严格的顺序消息，简单且可行的办法就是：保证“生产者 - MQServer - 消费者”是一对一对一的关系，但这样并行度就会成为消息系统的瓶颈，而且还需要更多的异常处理；有些问题，看起来很重要，但实际上我们可以通过合理的设计或者将问题分解来规避。如果硬要把时间花在解决问题本身，实际上不仅效率低下，而且也是一种浪费。从这个角度来看消息的顺序问题，我们可以得出两个结论：不关注乱序的应用实际大量存在、队列无序并不意味着消息无序；RocketMQ通过轮询所有队列的方式来确定消息被发送到哪一个队列，在获取到路由信息以后，会根据MessageQueueSelector实现的算法来选择一个队列，同一个OrderId获取到的肯定是同一个队列。</li>
<li>造成消息重复的根本原因是：网络不可达。只要通过网络交换数据，就无法避免这个问题。所以解决这个问题的办法就是绕过这个问题。那么问题就变成了：如果消费端收到两条一样的消息，应该怎样处理？消费端处理消息的业务逻辑保持幂等性，保证每条消息都有唯一编号且保证消息处理成功与去重表的日志同时出现；RocketMQ不保证消息不重复，如果你的业务需要保证严格的不重复消息，需要你自己在业务端去重。</li>
<li>RocketMQ除了支持普通消息，顺序消息，另外还支持事务消息。将大事务拆分成多个小事务异步执行，这样基本上能够将跨机事务的执行效率优化到与单机一致。</li>
<li>Producer轮询某topic下的所有队列的方式来实现发送方的负载均衡，如果Producer发送消息失败，会自动重试；</li>
<li>RocketMQ的消息存储是由consume queue和commit log配合完成的。</li>
<li>RocketMQ消息订阅有两种模式，一种是Push模式，即MQServer主动向消费端推送；另外一种是Pull模式，即消费端在需要时，主动到MQServer拉取。但在具体实现时，Push和Pull模式都是采用消费端主动拉取的方式。</li>
</ul>
<h3 id="当前服务器配置能承受多大的QPS？如何评估？-Susie-Xia-Anant-Rao-薛命灯"><a href="#当前服务器配置能承受多大的QPS？如何评估？-Susie-Xia-Anant-Rao-薛命灯" class="headerlink" title="当前服务器配置能承受多大的QPS？如何评估？ Susie Xia/Anant Rao/薛命灯"></a><a href="https://mp.weixin.qq.com/s?__biz=MzA5Nzc4OTA1Mw==&amp;mid=2659599084&amp;idx=1&amp;sn=a67802f91645f2a3864b584842676803" target="_blank" rel="external">当前服务器配置能承受多大的QPS？如何评估？</a> Susie Xia/Anant Rao/薛命灯</h3><ul>
<li>LinkedIn的基础设施上运行着数百个应用，它们为4.67亿的LinkedIn会员提供服务。为了能够准确地对服务容量极限进行评估，并有效地识别出容量瓶颈，我们的解决方案需要具备如下特点：利用生产环境来突破实验室的局限、使用真实的流量作为负载、最小化对用户体验造成的影响、低运营成本和开销、自动伸缩；</li>
<li>我们使用生产环境的真实流量，通过Redliner实现自动化的容量评估和准确的余量分析。Redliner在目标服务上运行压力测试，逐步增加流量，直到服务无法处理更多的流量为止，以此来评估服务的吞吐量。</li>
<li>Redliner自动从生产环境引入流量，并确保对用户只有很小的影响。在设计Redliner时，我们遵循了两个原则：影响最小化和完全自动化。</li>
<li>在进行流量重定向时，最主要的问题是如何避免对站点和用户造成影响。Redliner使用以下的策略来缓解对生产环境性能造成的影响。首先，通过增量的方式将流量导向redline实例。其次，Redliner对服务进行实时的监控，并根据实际情况来分发流量。Redliner捕捉实时的性能指标，并基于EKG健康评估规则的计算结果来确定服务的健康状况。</li>
<li>我们借助LinkedIn的技术平台保证Redliner自动化的健壮性和伸缩性。Redliner能够运行调度测试，通过EKG检测性能状态，还能利用A/B测试平台XLNT来动态地调整导向目标服务的流量。在经过几轮的迭代之后，Redliner就可以确定单个服务能够处理的最大QPS。一般整个过程需要不到一个小时的时间。</li>
<li>下图是Redliner的架构图，包含导流组件和容量评估组件。主要组件如下：导流层（代理/负载均衡器）、服务健康状态分析器和服务度量指标收集器。</li>
</ul>
<p><img src="http://mmbiz.qpic.cn/mmbiz_png/LaW7jDBKBg2hgd0MSVb4auiavNkXAR4ich8icBvIq0CqO733atJrBW2ueCWpWpjOhtwkibnJj96Wx9k1BmtLX2Yq1w/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1" alt="Redliner的架构图"></p>
<h3 id="解读GraphQL-王亦凡"><a href="#解读GraphQL-王亦凡" class="headerlink" title="解读GraphQL 王亦凡"></a><a href="https://mp.weixin.qq.com/s/U5WhVHKp4Q5ZW6JUamlufQ" target="_blank" rel="external">解读GraphQL</a> 王亦凡</h3><ul>
<li>GraphQL是Facebook推出的一个查询语言，可能和听起来不同，它并不是一个数据库查询语言，而是你可以用任何其他语言实现的一个用于查询的抽象层。</li>
<li>通常你可以通过GraphQL让你的客户端请求有权决定获取的数据结构，也可以通过GraphQL获得更好的多版本API兼容性。并且与大多数查询语言不同的是，GraphQL是一个静态类型的查询语言，这意味着你可以通过GraphQL获得更强大更安全的开发体验。</li>
<li>为什么要选择GraphQL？ GraphQL的核心目标就是取代RESTful API。</li>
<li>REST有什么问题？ 资源分类导致性能受限、在现代场景中难于维护、缺乏约束、严格，抽象，但并不能解决客户端问题；</li>
<li>GraphQL好在哪？Github宣布他们打算拥抱GraphQL；GraphQL用来构建客户端API，但它并不关心视图，也不关心服务的到底是什么客户端；至于请求什么数据，数据怎么组织，全都是客户端说了算；GraphQL的意思，顾名思义就是图查询语言；静态类型；兼容多版本；</li>
<li>GraphQL潜在的问题？ 可能会存在服务器性能隐患、安全问题、需要重新思考Cache策略。</li>
</ul>
<p><strong>免责声明：相关链接版本为原作者所有，如有侵权，请告知删除。</strong></p>
<p><strong>随手记系列：</strong></p>
<ul>
<li><a href="http://ginobefunny.com/post/reading_record_201702/">阅读随手记 201702</a></li>
<li><a href="http://ginobefunny.com/post/reading_record_201701/">阅读随手记 201701</a></li>
<li><a href="http://ginobefunny.com/post/reading_record_201612/">阅读随手记 201612</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;关键字：微服务, 架构, Event Sourcing, CQRS, Redis, TDD, 消息中间件, 缓存, RPC, 监控, 高性能, 高并发, 高可用, 机器学习, 深度学习, 人工智能。&lt;br&gt;
    
    </summary>
    
      <category term="Reading Record" scheme="http://ginobefunny.com/categories/Reading-Record/"/>
    
    
      <category term="微服务" scheme="http://ginobefunny.com/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"/>
    
      <category term="RPC" scheme="http://ginobefunny.com/tags/RPC/"/>
    
      <category term="机器学习" scheme="http://ginobefunny.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="http://ginobefunny.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="高性能" scheme="http://ginobefunny.com/tags/%E9%AB%98%E6%80%A7%E8%83%BD/"/>
    
      <category term="高并发" scheme="http://ginobefunny.com/tags/%E9%AB%98%E5%B9%B6%E5%8F%91/"/>
    
      <category term="高可用" scheme="http://ginobefunny.com/tags/%E9%AB%98%E5%8F%AF%E7%94%A8/"/>
    
      <category term="架构" scheme="http://ginobefunny.com/tags/%E6%9E%B6%E6%9E%84/"/>
    
      <category term="消息中间件" scheme="http://ginobefunny.com/tags/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
      <category term="缓存" scheme="http://ginobefunny.com/tags/%E7%BC%93%E5%AD%98/"/>
    
      <category term="监控" scheme="http://ginobefunny.com/tags/%E7%9B%91%E6%8E%A7/"/>
    
      <category term="Event Sourcing" scheme="http://ginobefunny.com/tags/Event-Sourcing/"/>
    
      <category term="CQRS" scheme="http://ginobefunny.com/tags/CQRS/"/>
    
      <category term="Redis" scheme="http://ginobefunny.com/tags/Redis/"/>
    
      <category term="TDD" scheme="http://ginobefunny.com/tags/TDD/"/>
    
      <category term="人工智能" scheme="http://ginobefunny.com/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
  </entry>
  
  <entry>
    <title>word2vec学习小记</title>
    <link href="http://ginobefunny.com/post/learning_word2vec/"/>
    <id>http://ginobefunny.com/post/learning_word2vec/</id>
    <published>2017-02-13T06:39:39.000Z</published>
    <updated>2017-02-14T09:18:41.387Z</updated>
    
    <content type="html"><![CDATA[<p>word2vec是Google于2013年开源推出的一个用于获取词向量的工具包，它简单、高效，因此引起了很多人的关注。最近项目组使用word2vec来实现个性化搜索，在阅读资料的过程中做了一些笔记，用于后面进一步学习。<br><a id="more"></a></p>
<p><strong>前注：word2vec涉及的相关理论和推导是非常严(ku)格(zao)的，本文作为一个初学者的学习笔记，希望能从自己的理解中尽量用简单的描述，如有错误或者歧义的地方，欢迎指正。</strong></p>
<h2 id="word2vec是什么？"><a href="#word2vec是什么？" class="headerlink" title="word2vec是什么？"></a>word2vec是什么？</h2><blockquote>
<p>This tool provides an efficient implementation of the continuous bag-of-words and skip-gram architectures for computing vector representations of words. These representations can be subsequently used in many natural language processing applications and for further research.</p>
</blockquote>
<p>从官方的介绍可以看出word2vec是一个将词表示为一个向量的工具，通过该向量表示，可以用来进行更深入的自然语言处理，比如机器翻译等。</p>
<p>为了理解word2vec的设计思想，我们有必要先学习一下自然语言处理的相关发展历程和基础知识。</p>
<h2 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h2><h3 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h3><ul>
<li>语言模型其实就是看一句话是不是正常人说出来的。这玩意很有用，比如机器翻译、语音识别得到若干候选之后，可以利用语言模型挑一个尽量靠谱的结果。在 NLP 的其它任务里也都能用到。</li>
<li>用数学表达的话，就是给定T个词<strong>w1,w2,…wT</strong>，看它是自然语言的概率P，公式如下所示：</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/14_learning_w2v/language_model.png" alt="语言模型形式化表达"></p>
<ul>
<li>举例来说，比如一段语音识别为“我喜欢吃梨”和“我喜欢吃力”，根据分词和上述公式，可以得到两种表述的概率计算分别为下面的公式，而其中每一个子项的概率我们可以事先通过大量的语料统计得到，这样我们就可以得到更好的识别效果。</li>
</ul>
<pre><code>P(&apos;我喜欢吃梨&apos;) = P(&apos;我&apos;) * P(&apos;喜欢&apos;|&apos;我&apos;) * P(&apos;吃&apos;|&apos;我&apos;,&apos;喜欢&apos;) * P(&apos;梨&apos;|&apos;我&apos;,&apos;喜欢&apos;,&apos;吃&apos;)
P(&apos;我喜欢吃力&apos;) = P(&apos;我&apos;) * P(&apos;喜欢&apos;|&apos;我&apos;) * P(&apos;吃力&apos;|&apos;我&apos;,&apos;喜欢&apos;)
</code></pre><h3 id="N-gram模型"><a href="#N-gram模型" class="headerlink" title="N-gram模型"></a>N-gram模型</h3><ul>
<li>通过上面的语言模型计算的例子，大家可以发现，如果一个句子比较长，那么它的计算量会很大；</li>
<li>牛逼的科学家们想出了一个<strong>N-gram模型</strong>来简化计算，在计算某一项的概率时Context不是考虑前面所有的词，而是前<strong>N-1</strong>个词；</li>
<li>当然牛逼的科学家们还在此模型上继续优化，比如<strong>N-pos模型</strong>从语法的角度出发，先对词进行词性标注分类，在此基础上来计算模型的概率；后面还有一些针对性的语言模型改进，这里就不一一介绍。</li>
<li>通过上面简短的语言模型介绍，我们可以看出核心的计算在于P(wi|Contenti)，对于其的计算主要有两种思路：一种是基于统计的思路，另外一种是通过函数拟合的思路；前者比较容易理解但是实际运用的时候有一些问题（比如如果组合在语料里没出现导致对应的条件概率变为0等），而函数拟合的思路就是通过语料的输入训练出一个函数P(wi|Contexti) = f(wi,Contexti;θ)，这样对于测试数据就直接套用函数计算概率即可，这也是机器学习中惯用的思路之一。</li>
</ul>
<h3 id="词向量表示"><a href="#词向量表示" class="headerlink" title="词向量表示"></a>词向量表示</h3><ul>
<li>自然语言理解的问题要转化为机器学习的问题，第一步肯定是要找一种方法把这些符号数学化。</li>
<li>最直观的就是把每个词表示为一个很长的向量。这个向量的维度是词表的大小，其中绝大多数元素为0，只有一个维度的值为1，这个维度就代表了当前的词。这种表示方式被称为<strong>One-hot Representation</strong>。这种方式的优点在于简洁，但是却无法描述词与词之间的关系。</li>
<li>另外一种表示方法是通过一个低维的向量（通常为50维、100维或200维），其基于“<strong>具有相似上下文的词，应该具有相似的语义</strong>”的假说，这种表示方式被称为<strong>Distributed Representation</strong>。它是一个稠密、低维的实数向量，它的每一维表示词语的一个潜在特征，该特征捕获了有用的句法和语义特征。其特点是将词语的不同句法和语义特征分布到它的每一个维度上去表示。这种方式的好处是可以通过空间距离或者余弦夹角来描述词与词之间的相似性。</li>
<li>以下我们来举个例子看看两者的区别：</li>
</ul>
<pre><code>// One-hot Representation 向量的维度是词表的大小，比如有10w个词，该向量的维度就是10w
v(&apos;足球&apos;) = [0 1 0 0 0 0 0 ......]
v(&apos;篮球&apos;) = [0 0 0 0 0 1 0 ......]

// Distributed Representation 向量的维度是某个具体的值如50
v(&apos;足球&apos;) = [0.26 0.49 -0.54 -0.08 0.16 0.76 0.33 ......]
v(&apos;篮球&apos;) = [0.31 0.54 -0.48 -0.01 0.28 0.94 0.38 ......]
</code></pre><ul>
<li>最后需要说明的是一个词的向量表示对于不同的语料和场景结果是不同的。下面就介绍一种最常用的计算词向量的语言模型。</li>
</ul>
<h3 id="神经网络概率语言模型"><a href="#神经网络概率语言模型" class="headerlink" title="神经网络概率语言模型"></a>神经网络概率语言模型</h3><ul>
<li>神经网络概率语言模型（NNLM）把词向量作为输入（初始的词向量是随机值），训练语言模型的同时也在训练词向量，最终可以同时得到语言模型和词向量。</li>
<li>Bengio等牛逼的科学家们用了一个三层的神经网络来构建语言模型，同样也是N-gram 模型。 网络的第一层是输入层，是是上下文的N-1个向量组成的(n-1)m维向量；第二层是隐藏层，使用tanh作为激活函数；第三层是输出层，每个节点表示一个词的未归一化概率，最后使用softmax激活函数将输出值归一化。</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/14_learning_w2v/nnlm.png" alt="神经网络概率语言模型"></p>
<ul>
<li>得到这个模型，然后就可以利用梯度下降法把模型优化出来，最终得到语言模型和词向量表示。</li>
</ul>
<h3 id="word2vec的核心模型"><a href="#word2vec的核心模型" class="headerlink" title="word2vec的核心模型"></a>word2vec的核心模型</h3><ul>
<li>word2vec在NNLM和<a href="http://techblog.youdao.com/?p=915" target="_blank" rel="external">其他语言模型</a>的基础进行了优化，有CBOW模型和Skip-Gram模型，还有Hierarchical Softmax和Negative Sampling两个降低复杂度的近似方法，两两组合出四种实现。</li>
<li>无论是哪种模型，其基本网络结构都是在下图的基础上，省略掉了隐藏层；</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/14_learning_w2v/w2v_model.jpg" alt="word2vec的核心模型"></p>
<p>CBOW和Skip-gram模型<br><img src="http://oi46mo3on.bkt.clouddn.com/14_learning_w2v/w2v_model_2.jpg" alt="word2vec的核心模型"></p>
<ul>
<li>CBOW（Continuous Bag-of-Words Model）是一种根据上下文的词语预测当前词语的出现概率的模型，其图示如上图左。CBOW是已知上下文，估算当前词语的语言模型；</li>
<li>而Skip-gram只是逆转了CBOW的因果关系而已，即已知当前词语，预测上下文，其图示如上图右；</li>
<li>Hierarchical Softmax使用的办法其实是借助了分类的概念。假设我们是把所有的词都作为输出，那么“足球”、“篮球”都是混在一起的。而Hierarchical Softmax则是把这些词按照类别进行区分的，二叉树上的每一个节点可以看作是一个使用哈夫曼编码构造的二分类器。在算法的实现中，模型会赋予这些抽象的中间节点一个合适的向量，真正的词会共用这些向量。这种近似的处理会显著带来性能上的提升同时又不会丢失很大的准确性。</li>
<li>Negative Sampling也是用二分类近似多分类，区别在于它会采样一些负例，调整模型参数使得可以区分正例和负例。换一个角度来看，就是Negative Sampling有点懒，它不想把分母中的所有词都算一次，就稍微选几个算算。</li>
<li>这一部分的模型实现比较复杂，网上也有很多资料可以参考，感兴趣的可以读读这两篇：<a href="http://www.hankcs.com/nlp/word2vec.html" target="_blank" rel="external">word2vec原理推导与代码分析</a>和<a href="http://blog.csdn.net/mytestmy/article/details/26969149" target="_blank" rel="external">深度学习word2vec笔记之算法篇</a>。</li>
</ul>
<h2 id="通过实践了解word2vec"><a href="#通过实践了解word2vec" class="headerlink" title="通过实践了解word2vec"></a>通过实践了解word2vec</h2><p>学习了上面的基础知识之后，我们就通过一个例子来感受一下word2vec的效果。</p>
<h3 id="下载语料"><a href="#下载语料" class="headerlink" title="下载语料"></a>下载语料</h3><p>从搜狗实验室下载<a href="http://www.sogou.com/labs/resource/ca.php" target="_blank" rel="external">全网新闻数据(SogouCA)</a>，该语料来自若干新闻站点2012年6月—7月期间国内、国际、体育、社会、娱乐等18个频道的新闻数据。</p>
<p>下载该文件解压后大约为1.5G，包含120w条以上的新闻，文件的内容格式如下图所示：</p>
<p><img src="http://oi46mo3on.bkt.clouddn.com/14_learning_w2v/corpus_format.png" alt="搜狗CA语料内容格式"></p>
<h3 id="获取新闻内容"><a href="#获取新闻内容" class="headerlink" title="获取新闻内容"></a>获取新闻内容</h3><p>这里我们先对语料进行初步处理，只获取新闻内容部分。可以执行以下命令获取content部分：</p>
<pre><code>cat news_tensite_xml.dat | iconv -f gbk -t utf-8 -c | grep &quot;&lt;content&gt;&quot;  &gt; corpus.txt
</code></pre><h3 id="分词处理"><a href="#分词处理" class="headerlink" title="分词处理"></a>分词处理</h3><p>由于word2vec处理的数据是单词分隔的语句，对于中文来说，需要先进行分词处理。这里采用的是中国自然语言处理开源组织开源的<a href="https://github.com/NLPchina/ansj_seg" target="_blank" rel="external">ansj_seg</a>分词器，核心代码如下所示：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordAnalyzer</span> </span>&#123;</div><div class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String TAG_START_CONTENT = <span class="string">"&lt;content&gt;"</span>;</div><div class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String TAG_END_CONTENT = <span class="string">"&lt;/content&gt;"</span>;</div><div class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String INPUT_FILE = <span class="string">"/home/test/w2v/corpus.txt"</span>;</div><div class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String OUTPUT_FILE = <span class="string">"/home/test/w2v/corpus_out.txt"</span>;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">        BufferedReader reader = <span class="keyword">null</span>;</div><div class="line">        PrintWriter pw = <span class="keyword">null</span>;</div><div class="line">        <span class="keyword">try</span> &#123;</div><div class="line">            System.out.println(<span class="string">"开始处理分词..."</span>);</div><div class="line">            reader = IOUtil.getReader(INPUT_FILE, <span class="string">"UTF-8"</span>);</div><div class="line">            pw = <span class="keyword">new</span> PrintWriter(OUTPUT_FILE);</div><div class="line">            <span class="keyword">long</span> start = System.currentTimeMillis();</div><div class="line">            <span class="keyword">int</span> totalCharactorLength = <span class="number">0</span>;</div><div class="line">            <span class="keyword">int</span> totalTermCount = <span class="number">0</span>;</div><div class="line">            Set&lt;String&gt; set = <span class="keyword">new</span> HashSet&lt;String&gt;();</div><div class="line">            String temp = <span class="keyword">null</span>;</div><div class="line">            <span class="keyword">while</span> ((temp = reader.readLine()) != <span class="keyword">null</span>) &#123;</div><div class="line">                temp = temp.trim();</div><div class="line">                <span class="keyword">if</span> (temp.startsWith(TAG_START_CONTENT)) &#123;</div><div class="line">                    <span class="comment">//System.out.println("处理文本:" + temp);</span></div><div class="line">                    <span class="keyword">int</span> end = temp.indexOf(TAG_END_CONTENT);</div><div class="line">                    String content = temp.substring(TAG_START_CONTENT.length(), end);</div><div class="line">                    totalCharactorLength += content.length();</div><div class="line">                    Result result = ToAnalysis.parse(content);</div><div class="line">                    <span class="keyword">for</span> (Term term : result) &#123;</div><div class="line">                        String item = term.getName().trim();</div><div class="line">                        totalTermCount++;</div><div class="line">                        pw.print(item + <span class="string">" "</span>);</div><div class="line">                        set.add(item);</div><div class="line">                    &#125;</div><div class="line">                    pw.println();</div><div class="line">                &#125;</div><div class="line">            &#125;</div><div class="line"></div><div class="line">            <span class="keyword">long</span> end = System.currentTimeMillis();</div><div class="line">            System.out.println(<span class="string">"共"</span> + totalTermCount + <span class="string">"个Term，共"</span> </div><div class="line">                + set.size() + <span class="string">"个不同的Term，共 "</span> </div><div class="line">                + totalCharactorLength + <span class="string">"个字符，每秒处理字符数:"</span> </div><div class="line">                + (totalCharactorLength * <span class="number">1000.0</span> / (end - start)));</div><div class="line">        &#125; <span class="keyword">finally</span> &#123;</div><div class="line">            <span class="comment">// close reader and pw</span></div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>分词处理之后的文件内容如下所示：<br><img src="http://oi46mo3on.bkt.clouddn.com/14_learning_w2v/corpus_ansj_analyze.png" alt="分词之后的语料"></p>
<h3 id="下载word2vec源码并编译"><a href="#下载word2vec源码并编译" class="headerlink" title="下载word2vec源码并编译"></a>下载word2vec源码并编译</h3><p>这里我没有从官网下载而是从github上的<a href="https://github.com/svn2github/word2vec" target="_blank" rel="external">svn2github/word2vec</a>项目下载源码，下载之后执行make命令编译，这个过程很快就可以结束。</p>
<h3 id="开始word2vec处理"><a href="#开始word2vec处理" class="headerlink" title="开始word2vec处理"></a>开始word2vec处理</h3><p>编译成功后开始处理。我这里用的是CentOS 64位的虚拟机，八核CPU，32G内存，整个处理过程耗时大约4个小时。</p>
<pre><code>./word2vec -train ../corpus_out.txt -output vectors.bin -cbow 0 -size 200 -window 5 -negative 0 -hs 1 -sample 1e-3 -threads 12 -binary 1

// 参数解释
-train 训练数据 
-output 结果输入文件，即每个词的向量 
-cbow 是否使用cbow模型，0表示使用skip-gram模型，1表示使用cbow模型，默认情况下是skip-gram模型，cbow模型快一些，skip-gram模型效果好一些 
-size 表示输出的词向量维数 
-window 为训练的窗口大小，5表示每个词考虑前5个词与后5个词（实际代码中还有一个随机选窗口的过程，窗口大小&lt;=5) 
-negative 表示是否使用负例采样方法0表示不使用，其它的值目前还不是很清楚 
-hs 是否使用Hierarchical Softmax方法，0表示不使用，1表示使用 
-sample 表示采样的阈值，如果一个词在训练样本中出现的频率越大，那么就越会被采样
-binary 表示输出的结果文件是否采用二进制存储，0表示不使用（即普通的文本存储，可以打开查看），1表示使用，即vectors.bin的存储类型
</code></pre><h3 id="测试处理结果"><a href="#测试处理结果" class="headerlink" title="测试处理结果"></a>测试处理结果</h3><p>处理结束之后，使用distance命令可以测试处理结果，以下是分别测试【足球】和【改革】的效果：</p>
<p><img src="http://oi46mo3on.bkt.clouddn.com/14_learning_w2v/w2v_test_football.png" alt="测试足球一词的词向量"></p>
<p><img src="http://oi46mo3on.bkt.clouddn.com/14_learning_w2v/w2v_test_reform.png" alt="测试改革一词的词向量"></p>
<h2 id="学习小结"><a href="#学习小结" class="headerlink" title="学习小结"></a>学习小结</h2><ul>
<li>word2vec的模型是基于神经网络来训练词向量的工具；</li>
<li>word2vec通过一系列的模型和框架对原有的NNLM进行优化，简化了计算但准确度还是保持得很好；</li>
<li>word2vec的主要的应用还是自然语言的处理，通过训练出来的词向量，可以进行聚类等处理，或者作为其他深入学习的输入。另外，word2vec还适用于一些时序数据的挖掘，比如用户商品的浏览分析、用户APP的下载等，通过这些数据的分析，可以得到商品或者APP的向量表示，从而用于个性化搜索和推荐。</li>
</ul>
<h2 id="参考材料"><a href="#参考材料" class="headerlink" title="参考材料"></a>参考材料</h2><ul>
<li><a href="https://code.google.com/archive/p/word2vec/" target="_blank" rel="external">word2vec HomePage</a></li>
<li><a href="https://arxiv.org/pdf/1301.3781.pdf" target="_blank" rel="external">Efficient Estimation of Word Representations in Vector Space</a></li>
<li><a href="http://licstar.net/archives/328" target="_blank" rel="external">Deep Learning in NLP （一）词向量和语言模型</a></li>
<li><a href="http://blog.csdn.net/itplus/article/details/37969635" target="_blank" rel="external">word2vec 中的数学原理详解</a></li>
<li><a href="http://blog.csdn.net/zhaoxinfan/article/details/11069485" target="_blank" rel="external">利用word2vec对关键词进行聚类</a></li>
<li><a href="http://blog.csdn.net/eastmount/article/details/50637476" target="_blank" rel="external">word2vec词向量训练及中文文本相似度计算</a></li>
<li><a href="http://www.cnblogs.com/Determined22/p/5780305.html" target="_blank" rel="external">词表示模型（一）：表示学习；syntagmatic与paradigmatic两类模型；基于矩阵的LSA和GloVe</a></li>
<li><a href="http://www.cnblogs.com/Determined22/p/5804455.html" target="_blank" rel="external">词表示模型（二）：基于神经网络的模型：NPLM；word2vec（CBOW/Skip-gram）</a></li>
<li><a href="http://www.cnblogs.com/Determined22/p/5807362.html" target="_blank" rel="external">词表示模型（三）：word2vec（CBOW/Skip-gram）的加速：Hierarchical Softmax与Negative Sampling</a></li>
<li><a href="http://blog.csdn.net/mytestmy/article/details/26961315" target="_blank" rel="external">深度学习word2vec笔记之基础篇</a></li>
<li><a href="https://www.zhihu.com/question/21661274/answer/19331979" target="_blank" rel="external">Google 开源项目 word2vec 的分析？_杨超的回答</a></li>
<li><a href="https://www.zybuluo.com/Dounm/note/591752" target="_blank" rel="external">Word2Vec-知其然知其所以然</a></li>
<li><a href="http://www.nustm.cn/blog/index.php/archives/842" target="_blank" rel="external">word2vec 原理篇</a></li>
<li><a href="http://www.hankcs.com/nlp/word2vec.html" target="_blank" rel="external">word2vec原理推导与代码分析</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;word2vec是Google于2013年开源推出的一个用于获取词向量的工具包，它简单、高效，因此引起了很多人的关注。最近项目组使用word2vec来实现个性化搜索，在阅读资料的过程中做了一些笔记，用于后面进一步学习。&lt;br&gt;
    
    </summary>
    
      <category term="NLP" scheme="http://ginobefunny.com/categories/NLP/"/>
    
    
      <category term="Google" scheme="http://ginobefunny.com/tags/Google/"/>
    
      <category term="入门" scheme="http://ginobefunny.com/tags/%E5%85%A5%E9%97%A8/"/>
    
      <category term="教程" scheme="http://ginobefunny.com/tags/%E6%95%99%E7%A8%8B/"/>
    
      <category term="实例" scheme="http://ginobefunny.com/tags/%E5%AE%9E%E4%BE%8B/"/>
    
      <category term="word2vec" scheme="http://ginobefunny.com/tags/word2vec/"/>
    
      <category term="NLP" scheme="http://ginobefunny.com/tags/NLP/"/>
    
      <category term="词向量" scheme="http://ginobefunny.com/tags/%E8%AF%8D%E5%90%91%E9%87%8F/"/>
    
      <category term="自然语言处理" scheme="http://ginobefunny.com/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>阅读随手记 201702</title>
    <link href="http://ginobefunny.com/post/reading_record_201702/"/>
    <id>http://ginobefunny.com/post/reading_record_201702/</id>
    <published>2017-02-08T04:31:23.000Z</published>
    <updated>2017-03-29T03:18:45.777Z</updated>
    
    <content type="html"><![CDATA[<p>关键字：微服务, 分布式, 配置中心, Java编程, 推荐系统, 运维, 高并发, 高可用, 机器学习, 深度学习。<br><a id="more"></a></p>
<h3 id="Microservices-A-definition-of-this-new-architectural-term-Martin-Fowler"><a href="#Microservices-A-definition-of-this-new-architectural-term-Martin-Fowler" class="headerlink" title="Microservices: A definition of this new architectural term Martin Fowler"></a><a href="https://martinfowler.com/articles/microservices.html" target="_blank" rel="external">Microservices: A definition of this new architectural term</a> Martin Fowler</h3><h4 id="微服务架构风格"><a href="#微服务架构风格" class="headerlink" title="微服务架构风格"></a>微服务架构风格</h4><ul>
<li>一组微型的服务/独立开发/通过轻量级机制通信/自动化的独立部署/各服务之间可采用不同语言和技术方案；</li>
<li>很难给微服务下一个具体的定义，Martin Fowler通过九大特性来阐述微服务；</li>
</ul>
<blockquote>
<p>the microservice architectural style is an approach to developing a single application as a suite of small services, each running in its own process and communicating with lightweight mechanisms, often an HTTP resource API. These services are built around business capabilities and independently deployable by fully automated deployment machinery. There is a bare minimum of centralized management of these services, which may be written in different programming languages and use different data storage technologies.</p>
</blockquote>
<h4 id="微服务的九大特性"><a href="#微服务的九大特性" class="headerlink" title="微服务的九大特性"></a>微服务的九大特性</h4><h5 id="1-组件化与服务"><a href="#1-组件化与服务" class="headerlink" title="1. 组件化与服务"></a>1. 组件化与服务</h5><ul>
<li><strong>组件</strong>（component）是一个可以独立更换和升级的软件单元；</li>
<li><strong>软件库）</strong>（libraries是能被链接到程序且通过方法调用的组件；</li>
<li><strong>服务</strong>（services）是进程外的组件，通过web service或rpc的机制来通信；</li>
<li>使用服务而非软件库的方式来组件化的主要原因是服务可被独立部署（避免一个组件修改导致需要重新部署整个应用），另外一个好处是能获得更加显式的组件接口（想想单体应用中的API依赖）；</li>
<li>当然也有不足之处，比如远程调用比进程内的调用要更加昂贵；</li>
</ul>
<h5 id="2-围绕业务功能组织团队"><a href="#2-围绕业务功能组织团队" class="headerlink" title="2. 围绕业务功能组织团队"></a>2. 围绕业务功能组织团队</h5><ul>
<li>对一个大型的应用进行分解时，通常是按照技术层面来进行划分，比如分为前端开发、后端开发和数据库开发（当然实际情况比这更细），但是这样会带来一个问题就是一个很小的需求都需要跨团队的项目合作和进度安排；这就是<strong>康威定律</strong>（任何设计系统的组织，都会产生这样一个设计，即该设计的结构与该组织的沟通结构相一致）起作用的例子；</li>
<li>而微服务是通过<strong>业务功能</strong>将系统分解为若干服务，这些服务针对该业务领域提供多层次广泛的软件实现；因此团队是<strong>跨职能</strong>的，它拥有软件开发所需的全方位的技能（如用户体验、数据库和项目管理）；</li>
<li>大型单体应用系统也可以根据业务功能进行模块化设计，但是通常的问题是一个团队会包含太多的功能，而团队之间的边界也不够清晰；</li>
</ul>
<h5 id="3-做产品而非做项目"><a href="#3-做产品而非做项目" class="headerlink" title="3. 做产品而非做项目"></a>3. 做产品而非做项目</h5><ul>
<li>大部分的应用开发都使用这样一个产品模型：一旦某项软件功能已交付，就会将软件移交给维护团队，而开发团队随之被解散；</li>
<li>而微服务主张<strong>“一个团队应该拥有该产品的整个生命周期”</strong>（原子亚马逊的“you build, you run it”）这样的理念，即一个开发团队对一个生产环境下运行的软件负全责；</li>
<li>这样的产品理念是与业务功能相绑定的，它不会把软件开成是一系列待完成功能的集合，而是一种持续提升客户业务功能的关系；</li>
<li>当然，单体应用也可以采用上述的产品理念，但是更细粒度的服务能使服务的开发者和它的用户更近；</li>
</ul>
<h5 id="4-智能终端和傻瓜管道"><a href="#4-智能终端和傻瓜管道" class="headerlink" title="4. 智能终端和傻瓜管道"></a>4. 智能终端和傻瓜管道</h5><ul>
<li>在不同的进程进行通信时，多数的产品或方法会在其中加入大量的智能特性，有个典型的例子就是ESB（企业服务总线），它通常包括消息路由、编制、转换和业务规则应用；</li>
<li>而微服务主张采用另一种做法：<strong>智能终端</strong>（smart endpoints）和<strong>傻瓜管道</strong>（dumb pipes）。这里的理解是应该尽可能地简化进程间的通信，将一些需要智能处理的逻辑（比如路由、重试等）交给服务处理；</li>
<li>微服务最常用的两种协议是：带有资源API的HTTP请求-响应协议和轻量级的消息发送协议（如RabbitMQ、ZeroMQ）；</li>
<li>将一个单体应用拆分为微服务的最大挑战就是改变原有的通信模式，如果直接将原先的进程内方法调用改为RPC会导致微服务直接产生繁琐的通信，因此应该考虑更粗粒度的方式调用；</li>
</ul>
<h5 id="5-去中心化的治理"><a href="#5-去中心化的治理" class="headerlink" title="5. 去中心化的治理"></a>5. 去中心化的治理</h5><ul>
<li>集中治理的一个问题是会趋向于在单一技术平台上制定标准从而带来局限性；</li>
<li>微服务中的<strong>分权治理</strong>（去中心化的）使得每个服务可以选择不同的技术，比如选择不同的语言和数据库；</li>
<li>相比于选用一组已定义好的标准，微服务的开发者更喜欢自己编写一些有用的工具，这些工具通常源于他们的微服务实施过程并分享给更多的团队；比如Nteflix就是一个很好的例子，这家提供网络视频点播的公司开源了一系列的实施微服务的工具；</li>
<li>对微服务社区来说，像<a href="https://martinfowler.com/bliki/TolerantReader.html" target="_blank" rel="external"><strong>容错读取</strong></a>（Tolerant Reader）和<a href="https://martinfowler.com/articles/consumerDrivenContracts.html" target="_blank" rel="external"><strong>消费者驱动的契约</strong></a>（Consumer-Driven Contracts）的模式已经被用于日常管理；</li>
<li>像Netflix这样的公司已经开始推行分权治理，这样可以令程序员更加注重质量（谁都不想半夜被电话叫去修复问题对吧），而这些与之前传统的集中治理有着天壤之别；</li>
</ul>
<h5 id="6-去中心化的数据管理"><a href="#6-去中心化的数据管理" class="headerlink" title="6. 去中心化的数据管理"></a>6. 去中心化的数据管理</h5><ul>
<li><strong>去中心化的数据管理</strong>从最抽象的层面看，意味着各个系统对客观世界所构建的概念模型将彼此不同，比如客户的概念对于销售和支撑团队就有所不同；</li>
<li>思考这类问题的一个有用的方法就是使用<strong>领域驱动设计</strong>（Domain-Driven Design, DDD）中的<a href="http://martinfowler.com/bliki/BoundedContext.html" target="_blank" rel="external"><strong>上下文边界</strong></a>（Bounded Context）的概念；DDD将一个复杂的领域划分为多个上下文边界，并且将它们的相互关系用图表示出来；</li>
<li>不同于单体应用喜欢共用一个单独的数据库（也许是被商业数据库的license逼出来的），微服务更喜欢让每一个服务管理其自有的数据库，可以采用相同数据库技术的不同实例，也可以采用完全不同的数据库系统；</li>
<li>但是去中心化的数据管理会带来数据一致性的问题，对此微服务强调的是<strong>无事务的协调</strong>（transactionless coordination between services），这源自微服务社区明确地认识到以下两点：即数据一致性可能只要求数据最终一致性，并且一致性问题能够通过补偿操作来进行处理；</li>
</ul>
<h5 id="7-基础设施自动化"><a href="#7-基础设施自动化" class="headerlink" title="7. 基础设施自动化"></a>7. 基础设施自动化</h5><ul>
<li>云的发展已经很大程度上降低了构建、部署和运维微服务的复杂性了；</li>
<li>许多使用微服务构建的团队都具备<strong>持续交付</strong>（Continuous Delivery）的经验，这样的团队广泛采用了基础设施自动化的技术，如下图的构建流水线所示：</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/15_reading_201702/basic_pipeline.png" alt="构建流水线"></p>
<ul>
<li>持续交付需要大量的<strong>自动化测试</strong>以及<strong>自动化部署的能力</strong>，通过这两个关键特点使得我们能更有信心、更愉快地部署功能到生成环境；另外，微服务的独立性也使得部署更加容易（这里的容易是指只需要部署修改的服务而不需要部署整个应用），当然也会带来困难（原来只需要部署一个系统，而现在需要部署更多的服务），此时就需要在部署工具上投入精力改进；</li>
</ul>
<h5 id="8-容错设计"><a href="#8-容错设计" class="headerlink" title="8. 容错设计"></a>8. 容错设计</h5><ul>
<li>使用微服务作为组件，需要设计成能容忍这些服务所出现的故障；一旦某个服务出现故障，其他任何对该服务的调用都会出现故障，客户端需要尽可能优雅地处理这种情况；与单体应用相比，这是微服务引入的额外的复杂性；</li>
<li>Netflix公司开源的<a href="https://github.com/Netflix/SimianArmy" target="_blank" rel="external"><strong>Simian Army</strong></a>能够诱导服务发生故障来测试应用的弹性和监控能力；</li>
<li><a href="http://martinfowler.com/bliki/CircuitBreaker.html" target="_blank" rel="external"><strong>断路器</strong></a>（Circuit Breaker）是一种用于隔离故障的模式，Netflix公司的这篇很精彩的<a href="http://techblog.netflix.com/2012/02/fault-tolerance-in-high-volume.html" target="_blank" rel="external">博客</a>解释了这些模式是如何应用的；</li>
<li><strong>容错设计</strong>要求能够快速地检测出故障，而且在可能的情况下自动恢复服务；此时<strong>实时监控</strong>就变得非常重要，它可用来检查架构元素指标（比如数据库每秒接收到多少请求）和业务相关指标（如系统每分钟收到多少订单）以便预测故障的发生；这对于微服务来说尤为重要，因为微服务对于服务编排和事件协作的偏好更易导致突发行为；</li>
<li>采用微服务的团队通常希望使用仪表盘或者日志记录装置来监控服务的运行和各项指标；</li>
</ul>
<h5 id="9-演进式设计"><a href="#9-演进式设计" class="headerlink" title="9. 演进式设计"></a>9. 演进式设计</h5><ul>
<li>每当试图将软件系统拆分为各个组件时，都会面对一个棘手的问题，即如何拆分，拆分的原则是什么？</li>
<li>一个组件的关键属性，是具有<strong>独立可替换性和可升级性</strong>（independent replacement and upgradeability），这使得我们在重写一个组件时更多的是聚焦于功能而非与不用担心与其他的关联组件；</li>
<li>而事实上，许多做微服务的团队会更进一步，他们明确地预期许多服务将来会报废而不是长期演进；比如英国卫报网站，他们依然使用原先的单体应用作为网站核心，而对于一些新的功能，比如增加报道一个体育赛事的页面，就会采用微服务的方式来添加，一旦赛事结束了，这个服务就可以被废除；</li>
<li>这种强调可更换性的特点是模块化设计一般性原则的一个特例，通过<strong>变化模式</strong>（the pattern of change）来驱动进行模块化的实现；将那些能在同时发生变化的东西放到相同的模块中，如果发现需要同时反复变更两个服务是，这就是它们两个需要被合并的一个信号；</li>
<li>将一个个组件放入一个个服务中增加了做出更精细化版本发布的机会，对于单体应用，任何变化都需要做一次整个应用系统的全量构建和部署，而对微服务来说，只需要部署修改的服务即可；</li>
</ul>
<h4 id="微服务是未来的方向吗？"><a href="#微服务是未来的方向吗？" class="headerlink" title="微服务是未来的方向吗？"></a>微服务是未来的方向吗？</h4><ul>
<li>作者通过该文阐述了微服务的主要思路和原则，在当时已经有一些公司如亚马逊和Netflix提供了正面的经验，收到了不少正面的评价；</li>
<li>架构决策所产生的真正效果通常需要若干年后才能真正显现，当时考虑的限制微服务的因素主要有组件拆分的难度、组件直接的复杂关联关系以及团队技能，但从目前的发展来看，已经有越来越多的企业采用了微服务的架构；</li>
</ul>
<h3 id="Netflix-Conductor：一个微服务编制引擎-Abel-Avram-杨雷"><a href="#Netflix-Conductor：一个微服务编制引擎-Abel-Avram-杨雷" class="headerlink" title="Netflix Conductor：一个微服务编制引擎 Abel Avram/杨雷"></a><a href="http://www.infoq.com/cn/news/2016/12/netflix-conductor" target="_blank" rel="external">Netflix Conductor：一个微服务编制引擎</a> Abel Avram/杨雷</h3><ul>
<li>Netflix开发了一个叫<a href="https://netflix.github.io/conductor/" target="_blank" rel="external">Conductor</a>的编制引擎，已经在内部生产环境中使用了一年了。在这段时间里，Netflix已经运行了大约260万个处理工作流，包括简单的线性工作流，以及运行数天的动态工作流。</li>
<li>主要特性：能够构建复杂工作流；能够通过微服务执行任务；使用JSON DSL描述的工作流蓝图；执行过程可见、可跟踪；能够暂停、恢复、重启、停止任务；任务执行通常是异步的，也可以强制同步执行；处理工作流能够扩展到百万级别；</li>
<li>Conductor的架构图如下所示；其中API和存储层都是可插拔的，允许使用不同的队列和存储引擎；工作流中的任务分为两种类型：Worker（运行在远端机器上的用户任务和System（运行在引擎的JVM上的任务），后者是用来对Worker执行任务进行branch、fork、join；Worker任务通过HTTP或者gRPC和Conductor通信；</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/15_reading_201702/conductor-architecture.png" alt="Conductor的架构图"></p>
<h3 id="深度学习并不是在“模拟人脑”-周志华"><a href="#深度学习并不是在“模拟人脑”-周志华" class="headerlink" title="深度学习并不是在“模拟人脑” 周志华"></a><a href="https://mp.weixin.qq.com/s?__biz=MzI5NTIxNTg0OA==&amp;mid=2247485059&amp;idx=1&amp;sn=e516921ae4beda15c7ddf5cb4c93bf78" target="_blank" rel="external">深度学习并不是在“模拟人脑”</a> 周志华</h3><ul>
<li>当<strong>特征信息和样本信息</strong>不充分时，机器学习可能就帮不上忙；</li>
<li>用机器学习解决问题更多的时候像一个裁缝，一定要量体裁衣，针对某个问题专门设计有效的方法，这样才能得到一个更好的结果；<strong>按需设计、度身定制</strong>，是在做机器学习应用的时候特别重要的一点；</li>
<li>机器学习有着深厚的理论基础，其中最基本的理论模型叫做<strong>概率近似正确模型</strong>；机器学习做的事情，是你给我数据之后，希望能够以很高的概率给出一个好模型；</li>
<li>从生物机理来说的话，一个神经元收到很多其它神经元发来的电位信号，信号经过放大到达它这里，如果这个累积信号比它自己的电位高了，那这个神经元就被激活了；其实神经网络本质上，是一个简单函数通过多层嵌套叠加形成的一个数学模型，背后其实是数学和工程在做支撑；而神经生理学起的作用，可以说是给了一点点启发，但是远远不像现在很多人说的神经网络研究受到神经生理学的“指导”，或者是“模拟脑”；</li>
<li>深度学习火起来的3个因素：有了大量的训练数据、有很强的计算设备、使用大量的“窍门”（Trick）；</li>
</ul>
<h3 id="分布式配置管理平台的设计与实现-架构文摘"><a href="#分布式配置管理平台的设计与实现-架构文摘" class="headerlink" title="分布式配置管理平台的设计与实现 架构文摘"></a><a href="http://mp.weixin.qq.com/s?__biz=MzIyNjE4NjI2Nw==&amp;mid=2652558155&amp;idx=1&amp;sn=351b10f4ecb80756bc91a25487d482be" target="_blank" rel="external">分布式配置管理平台的设计与实现</a> 架构文摘</h3><ul>
<li>分布式配置平台的一些应用场景：对某些配置的更新，不想要重启应用，并且能近似实时生效；希望将配置进行统一管理，而非放入各应用的配置文件中；</li>
<li>分布式配置平台需要满足的一些基本特性：高可用性（服务器集群应该无单点故障）、容错性（主要针对客户端，应保证即便在配置平台不可用时，也不影响客户端的正常运行）、高性能、可靠的存储、近似实时生效、负载均衡、扩展性；</li>
<li>分布式配置平台Diablo的设计与实现：对等的服务器集群（Server被视为是对等的，没有主从关系）、高性能处理（客户端应用获取配置时，仅会从本地缓存中获取，开发人员在控制台更改配置后，会通知客户端刷新缓冲）、使用Redis作存储、重试等待（diablo会通过重试等待等机制保证，在服务端集群不可用时，也不会影响客户端应用的正常运行，而是等待集群恢复）、请求负载（使用一致性哈希分配客户端连接是Server）、配置更新实时生效（diablo使用了特殊Pull模式，即长轮询）；</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/15_reading_201702/diablo.jpg" alt="Diablo的架构设计"></p>
<h3 id="一篇好TM长的关于配置中心的文章-坤宇"><a href="#一篇好TM长的关于配置中心的文章-坤宇" class="headerlink" title="一篇好TM长的关于配置中心的文章 坤宇"></a><a href="http://jm.taobao.org/2016/09/28/an-article-about-config-center/" target="_blank" rel="external">一篇好TM长的关于配置中心的文章</a> 坤宇</h3><ul>
<li>每一个分布式系统都应该有一个动态配置管理系统</li>
<li>配置与环境：某个配置项，其具体的值域的定义往往跟具体的环境相关联，现实中相当一部分配置在不同的环境必须设定不同的值，但是也有相当的另一部分配置在不同的环境要设定为完全一致的值。配置管理系统应该做的是提供方便的交互方式保证这两种不同的一致性诉求同时得到很好的满足，这种诉求分为3个方面，如下示意图:</li>
</ul>
<p><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1NFSpNpXXXXajapXXXXXXXXXX" alt="配置与环境"></p>
<ul>
<li>另外，一个配置中心也应该具备的能力是配置集的导出\导入功能，可以让应用将A环境中的配置集方便的导出和导入到环境B中的能力；</li>
<li>业界最新动态：Apache Commons Configuration（太繁琐）、<a href="http://owner.aeonbits.org" target="_blank" rel="external">owner</a>（简单易上手，特性看起来很多，但是在很多关键常用的特性反倒是没有）、<a href="http://www.cfg4j.org/" target="_blank" rel="external">cfg4j</a>（简单易上手，cfg4j 支持跟多种后端集成，做配置中心的解决方案，api设计也非常的不错）、Spring Framework、Spring Cloud Config Server；</li>
<li>配置(configuration)与元数据(metadata)：配置的修改基本上都是由人来驱动，并且在ops上实现变更；而元数据的本质是一小段程序元数据，它很多时候是程序产生，程序消费，由程序通过调用Diamond的客户端api来实现变更，中间不会有ops 或者人的介入。Diamond 不光是应用配置存储，其目前存储的数据，很大一部分是metadata，所以Diamond 其实也是一个元数据存储中心。</li>
</ul>
<h3 id="Reddit是如何使用Memcached来存储3TB缓存数据的？-薛命灯"><a href="#Reddit是如何使用Memcached来存储3TB缓存数据的？-薛命灯" class="headerlink" title="Reddit是如何使用Memcached来存储3TB缓存数据的？ 薛命灯"></a><a href="http://mp.weixin.qq.com/s?__biz=MzA5Nzc4OTA1Mw==&amp;mid=2659598891&amp;idx=1&amp;sn=6ae9c213ce167b3a8295b7d7d4804048" target="_blank" rel="external">Reddit是如何使用Memcached来存储3TB缓存数据的？</a> 薛命灯</h3><h4 id="Reddit的缓存规模和基本策略"><a href="#Reddit的缓存规模和基本策略" class="headerlink" title="Reddit的缓存规模和基本策略"></a>Reddit的缓存规模和基本策略</h4><p>Reddit目前使用了54个规格为r3.2xlarge的AWS EC2实例，每个实例拥有61GB内存，也就是说总的缓存大小差不多是3.3TB。Reddit的缓存包含了多种类型的数据，包括数据库对象、查询结果集、函数调用，还有一些看起来不太像缓存的东西，比如限定速率、分布式锁等等。</p>
<p><img src="http://oi46mo3on.bkt.clouddn.com/15_reading_201702/reddit_cache_1.png" alt="Reddit的缓存规模"></p>
<p>如何管理这么大规模的缓存是一件很挑战性的事情，Reddit采用的是“不要把所有鸡蛋放在同一个篮子里”的基本策略。也就是说，他们并不是把3.3TB的内存看成一个总的大缓存池，而是按照负载类型对缓存进行分类，每种类型占用一定数量的缓存空间。</p>
<h4 id="Reddit的缓存类型"><a href="#Reddit的缓存类型" class="headerlink" title="Reddit的缓存类型"></a>Reddit的缓存类型</h4><ul>
<li>数据库对象缓存（thing-cache）：Reddit最大的缓存池。这些对象是无schema的，开发人员可以很容易地对这些对象添加新属性，而无需对数据库schema进行变更。这些对象包括用户评论、链接和账户等等。该类型缓存是Reddit最繁忙也最有用的缓存，命中率高达99%。</li>
<li>主缓存（cache-main）：主缓存是Reddit第二大缓存池。这个缓存是一般性的缓存，里面存放的所有用来展示/r/all的结果集。</li>
<li>渲染缓存（cache-render）：第三大缓存用来存放渲染过的页面模板或页面片段。这个缓存相对安全，就算发生失效，也不会对系统造成太大影响。它的命中率只有大概50%左右，毕竟页面信息需要不断更新，所以渲染过的页面模板或片段也需要更新。</li>
<li>持久缓存（cache-perma）：它的命中率超过了99%。这个缓存用来存放数据库的查询结果，还有用户评论和链接。为什么管这个缓存叫持久缓存，因为他们使用了读-改-写（read-modify-write）的模式。例如，在用户新增一个评论时，他们会同时更新缓存和后端的数据库（Cassandra），而不是简单地让缓存失效，这样就避免了需要再次从数据库加载数据。</li>
<li>非缓存对象池：除了上述的几种缓存，Reddit还使用了速率限定和分布式锁。</li>
</ul>
<h4 id="mcrouter"><a href="#mcrouter" class="headerlink" title="mcrouter"></a>mcrouter</h4><ul>
<li>mcrouter是由Facebook开源的Memcached连接池。就像访问数据库要使用数据库连接池一样，使用连接池可以对连接进行重用和管理，避免了重复创建和销毁连接的开销。</li>
<li>mcrouter提供了多种路由类型，比如PrefixSelectorRoute，它通过匹配key的前缀来决定应该到哪个缓存上获取数据。这样就可以把特定功能的操作路由到特定的缓存上。</li>
<li>如果要往缓存集群里增加新的缓存实例，那么可以使用WarmUpRoute。WarmUpRoute的工作原理是说，把所有写操作路由到“冷”缓存上，而把未命中的读操作路由到“热”缓存上，然后把在“热”缓存上命中的缓存结果异步地更新到“冷”缓存上，那么下次同样的读操作就也可以在“冷”缓存上命中。</li>
<li>mcrouter还提供了FailoverRoute，顾名思义，这个特性可以避免缓存的单点故障；</li>
<li>Reddit还使用了影子缓存，不同于WarmUpRoute，它会把读操作和写操作都拷贝一份到新的实例上，但前提是不改变数据源。</li>
</ul>
<h4 id="自定义监控"><a href="#自定义监控" class="headerlink" title="自定义监控"></a>自定义监控</h4><ul>
<li>基于Memcached的“stats slabs”命令自己写了一个追踪板块度量指标的工具，他们还开发了一个简陋的可视化仪表盘；</li>
<li>Reddit团队还开发了另外一个工具，叫作mcsauna。这个工具被部署在每个缓存服务器上，它可以检测网络流量，并根据配置规则把不同的key保存在不同的bucket里，然后把结果输出到文件上。FilesCollector会收集这些文件，分析里面的key，并以图形化的方式呈现出来。从这些图形上可以看出那些热点的key。</li>
</ul>
<h4 id="展望"><a href="#展望" class="headerlink" title="展望"></a>展望</h4><p>缓存为提升网站的响应速度做出了不可磨灭的贡献。而在如何使用缓存方面，Reddit还有很长的路要走。接下来，他们可能要想着如何通过服务发现来对配置进行自动化，从而实现缓存的自动扩展，而不需要人工的介入。而随着Memcached版本的不断改进，他们也要针对现有系统进行调整，从而最大化缓存的性能。</p>
<h3 id="微信高并发资金交易系统设计方案-方乐明"><a href="#微信高并发资金交易系统设计方案-方乐明" class="headerlink" title="微信高并发资金交易系统设计方案 方乐明"></a><a href="http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2650995437&amp;idx=1&amp;sn=fefff4bff3e183d656a2d242e4c0a382" target="_blank" rel="external">微信高并发资金交易系统设计方案</a> 方乐明</h3><ul>
<li>微信红包的两大业务特点：微信红包业务比普通商品“秒杀”有更海量的并发要求、微信红包业务要求更严格的安全级别；</li>
<li>微信红包系统的技术难点：事务级操作量级大、事务性要求严格；</li>
<li>解决高并发问题常用方案：使用内存操作替代实时的DB事务操作（用内存操作替代磁盘操作，提高了并发性能，但是DB持久化可能会丢数据）、使用乐观锁替代悲观锁（可以提高DB的并发处理能力，但是回滚失败带来很差的用户体验）；</li>
</ul>
<h4 id="微信红包系统的高并发解决方案"><a href="#微信红包系统的高并发解决方案" class="headerlink" title="微信红包系统的高并发解决方案"></a>微信红包系统的高并发解决方案</h4><h5 id="系统垂直SET化，分而治之"><a href="#系统垂直SET化，分而治之" class="headerlink" title="系统垂直SET化，分而治之"></a>系统垂直SET化，分而治之</h5><p>红包系统根据微信红包ID，按一定的规则（如按ID尾号取模等），垂直上下切分。切分后，一个垂直链条上的逻辑Server服务器、DB统称为一个SET。各个SET之间相互独立，互相解耦。并且同一个红包ID的所有请求，包括发红包、抢红包、拆红包、查详情详情等，垂直stick到同一个SET内处理，高度内聚。通过这样的方式，系统将所有红包请求这个巨大的洪流分散为多股小流，互不影响，分而治之。</p>
<h5 id="逻辑Server层将请求排队，解决DB并发问题。"><a href="#逻辑Server层将请求排队，解决DB并发问题。" class="headerlink" title="逻辑Server层将请求排队，解决DB并发问题。"></a>逻辑Server层将请求排队，解决DB并发问题。</h5><p>如果到达DB的事务操作不是并发的，而是串行的，就不会存在“并发抢锁”的问题了。按这个思路，为了使拆红包的事务操作串行地进入DB，只需要将请求在Server层以FIFO的方式排队，就可以达到这个效果。从而问题就集中到Server的FIFO队列设计上。</p>
<p>微信红包系统设计了分布式的、轻巧的、灵活的FIFO队列方案。其具体实现如下：</p>
<ul>
<li>将同一个红包ID的所有请求stick到同一台Server；</li>
<li>设计单机请求排队方案：将stick到同一台Server上的所有请求在被接收进程接收后，按红包ID进行排队。然后串行地进入worker进程（执行业务逻辑）进行处理，从而达到排队的效果；</li>
<li>增加memcached控制并发：利用memcached的CAS原子累增操作，控制同时进入DB执行拆红包事务的请求数，超过预先设定数值则直接拒绝服务。用于DB负载升高时的降级体验；</li>
</ul>
<h5 id="双维度库表设计，保障系统性能稳定"><a href="#双维度库表设计，保障系统性能稳定" class="headerlink" title="双维度库表设计，保障系统性能稳定"></a>双维度库表设计，保障系统性能稳定</h5><p>处理微信红包数据的冷热分离时，系统在以红包ID维度分库表的基础上，增加了以循环天分表的维度，形成了双维度分库表的特色。具体来说，就是分库表规则像db_xx.t_y_dd设计，其中，xx/y是红包ID的hash值后三位，dd的取值范围在01~31，代表一个月天数最多31天。通过这种双维度分库表方式，解决了DB单表数据量膨胀导致性能下降的问题，保障了系统性能的稳定性。同时，在热冷分离的问题上，又使得数据搬迁变得简单而优雅。</p>
<h3 id="分布式系统理论基础-一致性、2PC和3PC-bangerlee"><a href="#分布式系统理论基础-一致性、2PC和3PC-bangerlee" class="headerlink" title="分布式系统理论基础 - 一致性、2PC和3PC bangerlee"></a><a href="http://www.cnblogs.com/bangerlee/p/5268485.html" target="_blank" rel="external">分布式系统理论基础 - 一致性、2PC和3PC</a> bangerlee</h3><ul>
<li>狭义的分布式系统指由网络连接的计算机系统，每个节点独立地承担计算或存储任务，节点间通过网络协同工作。广义的分布式系统是一个相对的概念，正如Leslie Lamport所说：</li>
</ul>
<blockquote>
<p>What is a distributed systeme. Distribution is in the eye of the beholder.<br>To the user sitting at the keyboard, his IBM personal computer is a nondistributed system.<br>To a flea crawling around on the circuit board, or to the engineer who designed it, it’s very much a distributed system.</p>
</blockquote>
<ul>
<li>一致性是分布式理论中的根本性问题；何为一致性问题？简单而言，一致性问题就是相互独立的节点之间如何达成一项决议的问题。分布式系统中，进行数据库事务提交(commit transaction)、Leader选举、序列号生成等都会遇到一致性问题。</li>
<li>假设一个具有N个节点的分布式系统，当其满足以下条件时，我们说这个系统满足一致性：全认同(agreement)、值合法(validity)、可结束(termination)；</li>
<li>分布式系统实现起来并不轻松，因为它面临着这些问题：消息传递异步无序(asynchronous)、节点宕机(fail-stop)、节点宕机恢复(fail-recover)、网络分化(network partition)、拜占庭将军问题(byzantine failure)；</li>
<li>一致性还具备两个属性，一个是强一致(safety)，它要求所有节点状态一致、共进退；一个是可用(liveness)，它要求分布式系统24*7无间断对外服务。FLP定理(FLP impossibility)已经证明在一个收窄的模型中(异步环境并只存在节点宕机)，不能同时满足safety和liveness。FLP定理是分布式系统理论中的基础理论，正如物理学中的能量守恒定律彻底否定了永动机的存在，FLP定理否定了同时满足safety和liveness的一致性协议的存在。</li>
</ul>
<h4 id="2PC"><a href="#2PC" class="headerlink" title="2PC"></a>2PC</h4><p>2PC（tow phase commit，两阶段提交）顾名思义它分成两个阶段，先由一方进行提议(propose)并收集其他节点的反馈(vote)，再根据反馈决定提交(commit)或中止(abort)事务。我们将提议的节点称为协调者(coordinator)，其他参与决议节点称为参与者(participants, 或cohorts)。</p>
<p>在阶段一中，coordinator发起一个提议，分别问询各participant是否接受。<br><img src="http://images2015.cnblogs.com/blog/116770/201603/116770-20160313202532507-1396598167.png" alt="2PC阶段一"></p>
<p>在阶段二中，coordinator根据participant的反馈，提交或中止事务，如果participant全部同意则提交，只要有一个participant不同意就中止。<br><img src="http://images2015.cnblogs.com/blog/116770/201603/116770-20160313203429600-179395429.png" alt="2PC阶段二"></p>
<p>在异步环境(asynchronous)并且没有节点宕机(fail-stop)的模型下，2PC可以满足全认同、值合法、可结束，是解决一致性问题的一种协议。但如果再加上节点宕机(fail-recover)的考虑，就要求 coordinator/participant 记录历史状态，以备coordinator宕机后watchdog对participant查询、coordinator宕机恢复后重新找回状态；</p>
<h4 id="3PC"><a href="#3PC" class="headerlink" title="3PC"></a>3PC</h4><p>在2PC中一个participant的状态只有它自己和coordinator知晓，假如coordinator提议后自身宕机，在watchdog启用前一个participant又宕机，其他participant就会进入既不能回滚、又不能强制commit的阻塞状态，直到participant宕机恢复。这引出两个疑问：</p>
<ul>
<li>能不能去掉阻塞，使系统可以在commit/abort前回滚(rollback)到决议发起前的初始状态；</li>
<li>当次决议中，participant间能不能相互知道对方的状态，又或者participant间根本不依赖对方的状态；</li>
</ul>
<p>相比2PC，3PC增加了一个准备提交(prepare to commit)阶段来解决以上问题。coordinator接收完participant的反馈(vote)之后，进入阶段2，给各个participant发送准备提交(prepare to commit)指令。participant接到准备提交指令后可以锁资源，但要求相关操作必须可回滚。coordinator接收完确认(ACK)后进入阶段3、进行commit/abort，3PC的阶段3与2PC的阶段2无异。协调者备份(coordinator watchdog)、状态记录(logging)同样应用在3PC。</p>
<p><img src="http://images2015.cnblogs.com/blog/116770/201603/116770-20160314002734304-489496391.png" alt="3PC"></p>
<p>因为有了准备提交(prepare to commit)阶段，3PC的事务处理延时也增加了1个RTT，变为3个RTT(propose+precommit+commit)，但是它防止participant宕机后整个系统进入阻塞态，增强了系统的可用性，对一些现实业务场景是非常值得的。</p>
<h3 id="分布式系统理论基础-选举、多数派和租约-bangerlee"><a href="#分布式系统理论基础-选举、多数派和租约-bangerlee" class="headerlink" title="分布式系统理论基础 - 选举、多数派和租约 bangerlee"></a><a href="http://www.cnblogs.com/bangerlee/p/5767845.html" target="_blank" rel="external">分布式系统理论基础 - 选举、多数派和租约</a> bangerlee</h3><ul>
<li>选举(election)是分布式系统实践中常见的问题，通过打破节点间的对等关系，选得的leader(或叫master、coordinator)有助于实现事务原子性、提升决议效率。多数派(quorum)的思路帮助我们在网络分化的情况下达成决议一致性，在leader选举的场景下帮助我们选出唯一leader。租约(lease)在一定期限内给予节点特定权利，也可以用于实现leader选举。</li>
<li>选举(electioin)：一致性问题(consistency)是独立的节点间如何达成决议的问题，选出大家都认可的leader本质上也是一致性问题；Bully算法是最常见的选举算法，其要求每个节点对应一个序号，序号最高的节点为leader，leader宕机后次高序号的节点被重选为leader；Bully算法中有2PC的身影，都具有提议(propose)和收集反馈(vote)的过程；在一致性算法Paxos、ZAB、Raft中，为提升决议效率均有节点充当leader的角色；</li>
<li>多数派(quorum)：在网络分化的场景下以上Bully算法会遇到一个问题，被分隔的节点都认为自己具有最大的序号、将产生多个leader；多数派的思路在分布式系统中很常见，其确保网络分化情况下决议唯一；多数派的原理说起来很简单，假如节点总数为2f+1，则一项决议得到多于 f 节点赞成则获得通过。leader选举中，网络分化场景下只有具备多数派节点的部分才可能选出leader，这避免了多leader的产生；</li>
<li>租约(lease)：选举中很重要的一个问题，怎么判断leader不可用、什么时候应该发起重新选举？最先可能想到会通过心跳(heart beat)判别leader状态是否正常，但在网络拥塞或瞬断的情况下，这容易导致出现双主；租约(lease)是解决该问题的常用方法，其最初提出时用于解决分布式缓存一致性问题，后面在分布式锁等很多方面都有应用；租约的原理同样不复杂，中心思想是每次租约时长内只有一个节点获得租约、到期后必须重新颁发租约；租约机制确保了一个时刻最多只有一个leader，避免只使用心跳机制产生双主的问题，在实践应用中，zookeeper、ectd可用于租约颁发。</li>
</ul>
<h3 id="分布式系统理论基础-时间、时钟和事件顺序-bangerlee"><a href="#分布式系统理论基础-时间、时钟和事件顺序-bangerlee" class="headerlink" title="分布式系统理论基础 - 时间、时钟和事件顺序 bangerlee"></a><a href="http://www.cnblogs.com/bangerlee/p/5448766.html" target="_blank" rel="external">分布式系统理论基础 - 时间、时钟和事件顺序</a> bangerlee</h3><ul>
<li>现实生活中时间是很重要的概念，时间可以记录事情发生的时刻、比较事情发生的先后顺序。分布式系统的一些场景也需要记录和比较不同节点间事件发生的顺序，但不同于日常生活使用物理时钟记录时间，分布式系统使用逻辑时钟记录事件顺序关系；</li>
<li>在1978年提出逻辑时钟的概念，并描述了一种逻辑时钟的表示方法，这个方法被称为Lamport时间戳(Lamport timestamps)。分布式系统中按是否存在节点交互可分为三类事件，一类发生于节点内部，二是发送事件，三是接收事件。Lamport时间戳原理如下：</li>
</ul>
<pre><code>每个事件对应一个Lamport时间戳，初始值为0
如果事件在节点内发生，时间戳加1
如果事件属于发送事件，时间戳加1并在消息中带上该时间戳
如果事件属于接收事件，时间戳 = Max(本地时间戳，消息中的时间戳) + 1
</code></pre><p><img src="http://images2015.cnblogs.com/blog/116770/201605/116770-20160501174922566-1686627384.png" alt="Lamport timestamps"></p>
<ul>
<li>Lamport时间戳帮助我们得到事件顺序关系，但还有一种顺序关系不能用Lamport时间戳很好地表示出来，那就是同时发生关系(concurrent)。Vector clock是在Lamport时间戳基础上演进的另一种逻辑时钟方法，它通过vector结构不但记录本节点的Lamport时间戳，同时也记录了其他节点的Lamport时间戳。Vector clock的原理与Lamport时间戳类似，使用图例如下：</li>
</ul>
<p><img src="http://images2015.cnblogs.com/blog/116770/201605/116770-20160502134654404-1109556515.png" alt="Vector clock"></p>
<ul>
<li>基于Vector clock我们可以获得任意两个事件的顺序关系，结果或为先后顺序或为同时发生，识别事件顺序在工程实践中有很重要的引申应用，最常见的应用是发现数据冲突(detect conflict)。分布式系统中数据一般存在多个副本(replication)，多个副本可能被同时更新，这会引起副本间数据不一致，Version vector的实现与Vector clock非常类似，目的用于发现数据冲突。下面通过一个例子说明Version vector的用法：</li>
</ul>
<p><img src="http://images2015.cnblogs.com/blog/116770/201605/116770-20160502183034013-800335383.png" alt="Version vector"></p>
<ul>
<li>Vector clock只用于发现数据冲突，不能解决数据冲突。如何解决数据冲突因场景而异，具体方法有以最后更新为准(last write win)，或将冲突的数据交给client由client端决定如何处理，或通过quorum决议事先避免数据冲突的情况发生。</li>
<li>由于记录了所有数据在所有节点上的逻辑时钟信息，Vector clock和Version vector在实际应用中可能面临的一个问题是vector过大，用于数据管理的元数据(meta data)甚至大于数据本身。解决该问题的方法是使用server id取代client id创建vector (因为server的数量相对client稳定)，或设定最大的size、如果超过该size值则淘汰最旧的vector信息。</li>
<li>小结：以上介绍了分布式系统里逻辑时钟的表示方法，通过Lamport timestamps可以建立事件的全序关系，通过Vector clock可以比较任意两个事件的顺序关系并且能表示无因果关系的事件，将Vector clock的方法用于发现数据版本冲突，于是有了Version vector。</li>
</ul>
<h3 id="分布式系统理论基础-CAP-bangerlee"><a href="#分布式系统理论基础-CAP-bangerlee" class="headerlink" title="分布式系统理论基础 - CAP bangerlee"></a><a href="http://www.cnblogs.com/bangerlee/p/5328888.html" target="_blank" rel="external">分布式系统理论基础 - CAP</a> bangerlee</h3><ul>
<li>CAP由Eric Brewer在2000年PODC会议上提出，是Eric Brewer在Inktomi期间研发搜索引擎、分布式web缓存时得出的关于数据一致性(consistency)、服务可用性(availability)、分区容错性(partition-tolerance)的猜想：</li>
</ul>
<blockquote>
<p>It is impossible for a web service to provide the three following guarantees : Consistency, Availability and Partition-tolerance.</p>
</blockquote>
<ul>
<li>该猜想在提出两年后被证明成立[4]，成为我们熟知的CAP定理：</li>
</ul>
<blockquote>
<p>数据一致性(consistency)：如果系统对一个写操作返回成功，那么之后的读请求都必须读到这个新数据；如果返回失败，那么所有读操作都不能读到这个数据，对调用者而言数据具有强一致性(strong consistency) ；</p>
<p>服务可用性(availability)：所有读写请求在一定时间内得到响应，可终止、不会一直等待；</p>
<p>分区容错性(partition-tolerance)：在网络分区的情况下，被分隔的节点仍能正常对外服务；</p>
</blockquote>
<ul>
<li>在某时刻如果满足AP，分隔的节点同时对外服务但不能相互通信，将导致状态不一致，即不能满足C；如果满足CP，网络分区的情况下为达成C，请求只能一直等待，即不满足A；如果要满足CA，在一定时间内要达到节点状态一致，要求不能出现网络分区，则不能满足P。C、A、P三者最多只能满足其中两个，和FLP定理一样，CAP定理也指示了一个不可达的结果(impossibility result)。</li>
<li>要理解P，我们看回CAP证明中P的定义：</li>
</ul>
<blockquote>
<p>In order to model partition tolerance, the network will be allowed to lose arbitrarily many messages sent from one node to another.</p>
</blockquote>
<p>网络分区的情况符合该定义，网络丢包的情况也符合以上定义，另外节点宕机，其他节点发往宕机节点的包也将丢失，这种情况同样符合定义。现实情况下我们面对的是一个不可靠的网络、有一定概率宕机的设备，这两个因素都会导致Partition，因而分布式系统实现中P是一个必须项，而不是可选项。对于分布式系统工程实践，CAP理论更合适的描述是：在满足分区容错的前提下，没有算法能同时满足数据一致性和服务可用性：</p>
<blockquote>
<p>In a network subject to communication failures, it is impossible for any web service to implement an atomic read/write shared memory that guarantees a response to every request.</p>
</blockquote>
<ul>
<li>CAP定理证明中的一致性指强一致性，强一致性要求多节点组成的被调要能像单节点一样运作、操作具备原子性，数据在时间、时序上都有要求。如果放宽这些要求，还有序列一致性(sequential consistency)和最终一致性(eventual consistency)。工程实践中，较常见的做法是通过异步拷贝副本(asynchronous replication)、quorum/NRW，实现在调用端看来数据强一致、被调端最终一致，在调用端看来服务可用、被调端允许部分节点不可用的效果。</li>
</ul>
<h3 id="分布式系统理论进阶-Paxos-bangerlee"><a href="#分布式系统理论进阶-Paxos-bangerlee" class="headerlink" title="分布式系统理论进阶 - Paxos bangerlee"></a><a href="http://www.cnblogs.com/bangerlee/p/5655754.html" target="_blank" rel="external">分布式系统理论进阶 - Paxos</a> bangerlee</h3><ul>
<li>Paxos协议在节点宕机恢复、消息无序或丢失、网络分化的场景下能保证决议的一致性，是被讨论最广泛的一致性协议。</li>
<li>一致性问题是在节点宕机、消息无序等场景可能出现的情况下，相互独立的节点之间如何达成决议的问题，作为解决一致性问题的协议，Paxos的核心是节点间如何确定并只确定一个值(value)。</li>
<li>和2PC类似，Paxos先把节点分成两类，发起提议(proposal)的一方为proposer，参与决议的一方为acceptor。</li>
</ul>
<pre><code>P1.  一个acceptor接受它收到的第一项提议 //假如只有一个proposer发起提议，并且节点不宕机、消息不丢包
P2.  如果一项值为v的提议被确定，那么后续只确定值为v的提议
P2a. 如果一项值为v的提议被确定，那么acceptor后续只接受值为v的提议
P2b. 如果一项值为v的提议被确定，那么proposer后续只发起值为v的提议
P2c. 对于提议(n,v)，acceptor的多数派S中，如果存在acceptor最近一次(即ID值最大)接受的提议的值为v&apos;，那么要求v = v&apos;；否则v可为任意值
</code></pre><ul>
<li>以上提到的各项约束条件可以归纳为3点，如果proposer/acceptor满足下面3点，那么在少数节点宕机、网络分化隔离的情况下，在“确定并只确定一个值”这件事情上可以保证一致性(consistency)：</li>
</ul>
<pre><code>B1(ß): ß中每一轮决议都有唯一的ID标识
B2(ß): 如果决议B被acceptor多数派接受，则确定决议B
B3(ß): 对于ß中的任意提议B(n,v)，acceptor的多数派中如果存在acceptor最近一次(即ID值最大)接受的提议的值为v&apos;，那么要求v = v&apos;；否则v可为任意值
</code></pre><ul>
<li>至此，proposer/acceptor完成一轮决议可归纳为prepare和accept两个阶段。prepare阶段proposer发起提议问询提议值、acceptor回应问询并进行promise；accept阶段完成决议，图示如下：</li>
</ul>
<p><img src="http://images2015.cnblogs.com/blog/116770/201607/116770-20160712125617045-527200085.png" alt="Paxos"></p>
<h3 id="分布式系统理论进阶-Raft、Zab-bangerlee"><a href="#分布式系统理论进阶-Raft、Zab-bangerlee" class="headerlink" title="分布式系统理论进阶 - Raft、Zab bangerlee"></a><a href="http://www.cnblogs.com/bangerlee/p/5991417.html" target="_blank" rel="external">分布式系统理论进阶 - Raft、Zab</a> bangerlee</h3><ul>
<li>Paxos偏向于理论、对如何应用到工程实践提及较少。理解的难度加上现实的骨感，在生产环境中基于Paxos实现一个正确的分布式系统非常难；Raft在2013年提出，提出的时间虽然不长，但已经有很多系统基于Raft实现。相比Paxos，Raft的买点就是更利于理解、更易于实行。</li>
<li>为达到更容易理解和实行的目的，Raft将问题分解和具体化：Leader统一处理变更操作请求，一致性协议的作用具化为保证节点间操作日志副本(log replication)一致，以term作为逻辑时钟(logical clock)保证时序，节点运行相同状态机(state machine)得到一致结果。Raft协议具体过程如下：</li>
</ul>
<p><img src="http://images2015.cnblogs.com/blog/116770/201610/116770-20161024005549560-244386650.png" alt="Raft"></p>
<pre><code>①Client发起请求，每一条请求包含操作指令
②请求交由Leader处理，Leader将操作指令(entry)追加(append)至操作日志，紧接着对Follower发起AppendEntries请求、尝试让操作日志副本在Follower落地
③如果Follower多数派(quorum)同意AppendEntries请求，Leader进行commit操作、把指令交由状态机处理
④状态机处理完成后将结果返回给Client
</code></pre><ul>
<li>Paxos中Leader的存在是为了提升决议效率，Leader的有无和数目并不影响决议一致性，Raft要求具备唯一Leader，并把一致性问题具体化为保持日志副本的一致性，以此实现相较Paxos而言更容易理解、更容易实现的目标。</li>
<li>Zab的全称是Zookeeper atomic broadcast protocol，是Zookeeper内部用到的一致性协议。相比Paxos，Zab最大的特点是保证强一致性(strong consistency)。和Raft一样，Zab要求唯一Leader参与决议，Zab可以分解成discovery、sync、broadcast三个阶段：</li>
</ul>
<p><img src="http://images2015.cnblogs.com/blog/116770/201610/116770-20161025133734734-658183229.jpg" alt="Zab"></p>
<pre><code>discovery: 选举产生PL(prospective leader)，PL收集Follower epoch(cepoch)，根据Follower的反馈PL产生newepoch(每次选举产生新Leader的同时产生新epoch，类似Raft的term)
sync: PL补齐相比Follower多数派缺失的状态、之后各Follower再补齐相比PL缺失的状态，PL和Follower完成状态同步后PL变为正式Leader(established leader)
broadcast: Leader处理Client的写操作，并将状态变更广播至Follower，Follower多数派通过之后Leader发起将状态变更落地(deliver/commit)
</code></pre><ul>
<li>了解完Zab的基本原理，我们再来看Zab怎样保证强一致性，Zab通过约束事务先后顺序达到强一致性，先广播的事务先commit、FIFO，Zab称之为primary order(以下简称PO)。实现PO的核心是zxid。</li>
<li>Paxos、Raft、Zab和VR都是解决一致性问题的协议，Paxos协议原文倾向于理论，Raft、Zab、VR倾向于实践，一致性保证程度等的不同也导致这些协议间存在差异。</li>
</ul>
<h3 id="亿级规模的Elasticsearch优化实战-王卫华"><a href="#亿级规模的Elasticsearch优化实战-王卫华" class="headerlink" title="亿级规模的Elasticsearch优化实战 王卫华"></a><a href="http://mp.weixin.qq.com/s?__biz=MzAwMDU1MTE1OQ==&amp;mid=209488723&amp;idx=1&amp;sn=d60c0637d7a9f4a4b981a69f10c6b90a" target="_blank" rel="external">亿级规模的Elasticsearch优化实战</a> 王卫华</h3><ul>
<li>索引优化：SSD是经济压力能承受情况下的不二选择。减少碎片也可以提高索引速度，每天进行优化还是很有必要的。在初次索引的时候，把replica设置为0，也能提高索引速度。</li>
<li>索引优化相关参数：threadpool.index.queue_size、indices.memory.index_buffer_size、index.translog.flush_threshold_ops和refresh_interval。</li>
<li>查询优化：可以使用多个集群，每个集群使用不同的routing，比如用户是一个routing维度。在实践中，这个routing非常重要。</li>
<li>索引越来越大，单个shard也很巨大，查询速度也越来越慢。这时候，是选择分索引还是更多的shards？在实践过程中，更多的shards会带来额外的索引压力，即IO 力。我们选择了分索引，比如按照每个大分类一个索引，或者主要的大城市一个索引。然后将他们进行合并查询。如：<a href="http://cluster1:9200/shanghai,beijing/_search?routing=fang；" target="_blank" rel="external">http://cluster1:9200/shanghai,beijing/_search?routing=fang；</a></li>
<li>线程池我们默认使用fixed，使用cached有可能控制不好。主要是比较大的分片relocation时，会导致分片自动下线，集群可能处于危险状态。</li>
<li>128G内存的机器配置一个JVM，然后是巨大的heapsize（如64G）还是配多个JVM instance，较小的 heapsize（如32G）？我的建议是后者。实际使用中，后者也能帮助我们节省不少资源，并提供不错的性能。具体请参阅 “Don’t Cross 32 GB!” （<a href="https://www.elastic.co/guide/en/elasticsearch/guide/current/heap-sizing.html#compressed_oops）" target="_blank" rel="external">https://www.elastic.co/guide/en/elasticsearch/guide/current/heap-sizing.html#compressed_oops）</a></li>
</ul>
<h3 id="怎样读一本书V5-0-ljinkai"><a href="#怎样读一本书V5-0-ljinkai" class="headerlink" title="怎样读一本书V5.0 ljinkai"></a><a href="https://ljinkai.github.io/2017/02/08/how-to-read-a-book" target="_blank" rel="external">怎样读一本书V5.0</a> ljinkai</h3><p><img src="http://oi46mo3on.bkt.clouddn.com/15_reading_201702/how_to_read_a_book.png" alt="怎样读一本书"></p>
<h3 id="JVM为什么需要GC？-周明耀"><a href="#JVM为什么需要GC？-周明耀" class="headerlink" title="JVM为什么需要GC？ 周明耀"></a><a href="http://mp.weixin.qq.com/s/KVj5NwYyXyJdaqKIDu-Avg" target="_blank" rel="external">JVM为什么需要GC？</a> 周明耀</h3><ul>
<li>HotSpot的垃圾回收器总结：如果你想要最小化地使用内存和并行开销，请选Serial GC；如果你想要最大化应用程序的吞吐量，请选Parallel GC；如果你想要最小化GC的中断或停顿时间，请选CMS GC。</li>
<li>G1 GC基本思想：G1 GC是一个压缩收集器，它基于回收最大量的垃圾原理进行设计。G1 GC利用递增、并行、独占暂停这些属性，通过拷贝方式完成压缩目标。此外，它也借助并行、多阶段并行标记这些方式来帮助减少标记、重标记、清除暂停的停顿时间，让停顿时间最小化是它的设计目标之一。</li>
<li>G1 GC的垃圾回收循环组成：年轻代循环、多步骤并行标记循环、混合收集循环、Full GC；</li>
<li>G1的区间设计：在G1中，堆被平均分成若干个大小相等的区域（Region）。每个Region都有一个关联的Remembered Set（简称RS），RS的数据结构是Hash表，里面的数据是Card Table （堆中每512byte映射在card table 1byte）。简单的说RS里面存在的是Region中存活对象的指针。当Region中数据发生变化时，首先反映到Card Table中的一个或多个Card上，RS通过扫描内部的Card Table得知Region中内存使用情况和存活对象。在使用Region过程中，如果Region被填满了，分配内存的线程会重新选择一个新的Region，空闲Region被组织到一个基于链表的数据结构（LinkedList）里面，这样可以快速找到新的Region。</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/15_reading_201702/design_of_g1.png" alt="G1的区间设计"></p>
<h3 id="让机器读懂用户–大数据中的用户画像-杨杰"><a href="#让机器读懂用户–大数据中的用户画像-杨杰" class="headerlink" title="让机器读懂用户–大数据中的用户画像 杨杰"></a><a href="https://mp.weixin.qq.com/s?__biz=MzI2MzM3MzkyMg==&amp;mid=2247484433&amp;idx=1&amp;sn=f30a6a3585becc1a500772aaa78fd937" target="_blank" rel="external">让机器读懂用户–大数据中的用户画像</a> 杨杰</h3><ul>
<li>用户画像（persona）的概念最早由交互设计之父Alan Cooper提出:“Personas are a concrete representation of target users.” 是指真实用户的虚拟代表，是建立在一系列属性数据之上的目标用户模型。随着互联网的发展，现在我们说的用户画像又包含了新的内涵——通常用户画像是根据用户人口学特征、网络浏览内容、网络社交活动和消费行为等信息而抽象出的一个标签化的用户模型。构建用户画像的核心工作，<strong>主要是利用存储在服务器上的海量日志和数据库里的大量数据进行分析和挖掘</strong>，给用户贴“标签”，而“标签”是能表示用户某一维度特征的标识。具体的标签形式可以参考下图某网站给其中一个用户打的标签。</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/15_reading_201702/persona.jpg" alt="用户画像例子"></p>
<ul>
<li>用户画像的作用：精准营销、用户研究、个性服务、业务决策；</li>
<li>用户画像的内容：对于大部分互联网公司，用户画像都会包含<strong>人口属性和行为特征</strong>。人口属性主要指用户的年龄、性别、所在的省份和城市、教育程度、婚姻情况、生育情况、工作所在的行业和职业等。行为特征主要包含活跃度、忠诚度等指标。另外，电商购物网站的用户画像，一般会提取用户的<strong>网购兴趣和消费能力</strong>等指标。</li>
<li>用户画像的生产，大致可以分为以下几步：</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/15_reading_201702/netease_persona.jpg" alt="网易用户画像"></p>
<blockquote>
<ol>
<li>用户建模，指确定提取的用户特征维度，和需要使用到的数据源。</li>
<li>数据收集，通过数据收集工具，如Flume或自己写的脚本程序，把需要使用的数据统一存放到Hadoop集群。</li>
<li>数据清理，数据清理的过程通常位于Hadoop集群，也有可能与数据收集同时进行，这一步的主要工作，是把收集到各种来源、杂乱无章的数据进行字段提取，得到关注的目标特征。</li>
<li>模型训练，有些特征可能无法直接从数据清理得到，比如用户感兴趣的内容或用户的消费水平，那么可以通过收集到的已知特征进行学习和预测。</li>
<li>属性预测，利用训练得到的模型和用户的已知特征，预测用户的未知特征。</li>
<li>数据合并，把用户通过各种数据源提取的特征进行合并，并给出一定的可信度。</li>
<li>数据分发，对于合并后的结果数据，分发到精准营销、个性化推荐、CRM等各个平台，提供数据支持。</li>
</ol>
</blockquote>
<ul>
<li>应用示例之个性化推荐：很多推荐场景都会用到基于商品的协同过滤，而基于商品协同过滤的核心是一个商品相关性矩阵W，假设有n个商品，那么W就是一个n * n的矩阵，矩阵的元素wij代表商品Ii和Ij之间的相关系数。而根据用户访问和购买商品的行为特征，可以把用户表示成一个n维的特征向量U=[ i1, i2, …, in ]。于是U * W可以看成用户对每个商品的感兴趣程度V=[ v1, v2, …, vn ]，这里v1即是用户对商品I1的感兴趣程度，v1= i1*w11 + i2*w12 + in*w1n。如果把相关系数w11, w12, …, w1n 看成要求的变量，那么就可以用LR模型，代入训练集用户的行为向量U，进行求解。这样一个初步的LR模型就训练出来了，效果和基于商品的协同过滤类似。</li>
</ul>
<h3 id="推荐系统本质与网易严选实践-沈燕"><a href="#推荐系统本质与网易严选实践-沈燕" class="headerlink" title="推荐系统本质与网易严选实践  沈燕"></a><a href="http://mp.weixin.qq.com/s?__biz=MzA4Mzc0NjkwNA==&amp;mid=2650782153&amp;idx=1&amp;sn=d90906f57d45d8991a9be1269627315d" target="_blank" rel="external">推荐系统本质与网易严选实践</a>  沈燕</h3><ul>
<li>推荐系统作用本质：有资料称亚马逊的推荐系统带来的GMV占其全站总量的20%-30%；推荐的本质就是提升用户体验，为此它们最主要的方式就是帮助用户快速的找到它需要的产商品，其他的方式还包括给用户新颖感等。</li>
<li>推荐系统工作原理本质：所谓embedding，数学上的意义就是映射。如word2vec通过语料训练把词变成一个数百维的向量，向量的每一维没有明确的物理意义（或者说我们无法理解）。推荐系统如果可以把人很精确地映射成一个向量，把物品也映射成一个同维度同意义的向量，那么推荐就是可以按规则处理的精确的事情了。</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/15_reading_201702/recommendation.png" alt="最佳的推荐形式"></p>
<ul>
<li>电商推荐系统的特点：商品种类数巨大，不同的商品需要不同的embedding；单种商品深度不够，难以有效embedding；人对商品的兴趣大都建立在短期或者瞬时需求之上；大量耐消品的影响；用户理论上对所有商品都会有兴趣。基于以上的原因，在电商领域难以找到完美的embedding方式来实现推荐。其实我们在看各大电商的个性化推荐时，无论宣称背后用怎样复杂的模型融合，从结果看，用户近期行为的权重是非常大的，使得结果非常像itemCF推荐出来的。</li>
</ul>
<h4 id="网易严选推荐实践"><a href="#网易严选推荐实践" class="headerlink" title="网易严选推荐实践"></a>网易严选推荐实践</h4><p>网易严选推荐的基础模型采用的是CTR模型，基于LR（逻辑回归）。</p>
<p><img src="http://oi46mo3on.bkt.clouddn.com/15_reading_201702/netease_recommendation_1.jpg" alt="网易严选推荐模型"></p>
<p>在核心的特征工程方面，网易严选推荐团队将用户的具体属性（性别、收入水平、地域等）、用户在网易严选的行为属性（短期，长期）、及时间上下文（季节、上次购买时间间隔等）作为属性空间，从1层迪卡尔积开始往上构造N层迪卡尔积形成复杂属性空间P，挖掘属性空间与商品的相关，对有明显相关（正相关或负相关）的（属性、物品）对构造特征。</p>
<p>用户属性空间</p>
<p><img src="http://oi46mo3on.bkt.clouddn.com/15_reading_201702/netease_recommendation_2.jpg" alt="用户属性空间"></p>
<p>具体属性应用</p>
<p><img src="http://oi46mo3on.bkt.clouddn.com/15_reading_201702/netease_recommendation_3.jpg" alt="具体属性应用"></p>
<p>行为属性作为抽象属性与具体属性置以相同的地位</p>
<p><img src="http://oi46mo3on.bkt.clouddn.com/15_reading_201702/netease_recommendation_4.jpg" alt="行为属性"></p>
<p>二阶属性（属性的2重迪卡尔积）</p>
<p><img src="http://oi46mo3on.bkt.clouddn.com/15_reading_201702/netease_recommendation_5.jpg" alt="二阶属性"></p>
<p>从结果来看，这一套特征工程方法可以挖出比较全的特征集，在鲁棒性与效果上都有不错的效果，自上线以来各项指标均在稳步提升。</p>
<h3 id="深入探索Java-8-Lambda表达式-Richard-Warburton-Raoul-Urma-Mario-Fusco-段建华"><a href="#深入探索Java-8-Lambda表达式-Richard-Warburton-Raoul-Urma-Mario-Fusco-段建华" class="headerlink" title="深入探索Java 8 Lambda表达式  Richard Warburton/Raoul Urma/Mario Fusco/段建华"></a><a href="http://www.infoq.com/cn/articles/Java-8-Lambdas-A-Peek-Under-the-Hood" target="_blank" rel="external">深入探索Java 8 Lambda表达式</a>  Richard Warburton/Raoul Urma/Mario Fusco/段建华</h3><ul>
<li>为什么匿名内部类不好？编译器会为每一个匿名内部类创建一个类文件，而类在使用之前需要加载类文件并进行验证，这个过程则会影响应用的启动性能，另外类文件的加载很有可能是一个耗时的操作，这其中包含了磁盘IO和解压JAR文件。最重要的，一旦Lambda表达式使用了匿名内部类实现，就会限制了后续Lambda表达式实现的更改，降低了其随着JVM改进而改进的能力。</li>
<li>Lambdas表达式和invokedynamic：将Lambda表达式转化成字节码只需要如下两步：1.生成一个invokedynamic调用点，也叫做Lambda工厂，当调用时返回一个Lambda表达式转化成的函数式接口实例；2.将Lambda表达式的方法体转换成方法供invokedynamic指令调用。需要注意的是编译器对于Lambda表达式的翻译策略并非固定的，因为这样invokedynamic可以使编译器在后期使用不同的翻译实现策略。</li>
<li>性能分析：Lambda工厂的预热准备需要消耗时间，但Lambda工厂方式也会比匿名内部类加载要快，最高可达100倍；如果是不进行捕获变量，这一步会自动进行优化，避免在基于Lambda工厂实现下额外创建对象；对于真实方法的调用，匿名内部类和Lambda表达式执行的操作相同，没有性能上的差别；</li>
<li>对于大多数情况来说，Lambda表达式要比匿名内部类性能更优。然而现状并非完美，基于测量驱动优化，我们仍然有很大的提升空间。</li>
</ul>
<h3 id="红黑树-Hosee"><a href="#红黑树-Hosee" class="headerlink" title="红黑树 Hosee"></a><a href="https://my.oschina.net/hosee/blog/618828" target="_blank" rel="external">红黑树</a> Hosee</h3><ul>
<li>先来看下算法导论对R-B Tree的介绍：</li>
</ul>
<blockquote>
<p>红黑树，一种二叉查找树，但在每个结点上增加一个存储位表示结点的颜色，可以是Red或Black。<br>通过对任何一条从根到叶子的路径上各个结点着色方式的限制，红黑树确保没有一条路径会比其他路径长出俩倍，因而是接近平衡的。</p>
</blockquote>
<ul>
<li>二叉查找树，也称有序二叉树（ordered binary tree），是指一棵空树或者具有下列性质的二叉树：</li>
</ul>
<pre><code>1.若任意节点的左子树不空，则左子树上所有结点的值均小于它的根结点的值；
2.若任意节点的右子树不空，则右子树上所有结点的值均大于它的根结点的值；
3.任意节点的左、右子树也分别为二叉查找树。
4.没有键值相等的节点（no duplicate nodes）。
</code></pre><ul>
<li>红黑树虽然本质上是一棵二叉查找树，但它在二叉查找树的基础上增加了着色和相关的性质使得红黑树相对平衡，从而保证了红黑树的查找、插入、删除的时间复杂度最坏为O(log n)。但它是如何保证一棵n个结点的红黑树的高度始终保持在logn的呢？这就引出了红黑树的5个性质：</li>
</ul>
<pre><code>1.每个结点要么是红的要么是黑的。  
2.根结点是黑的。  
3.每个叶结点（叶结点即指树尾端NIL指针或NULL结点）都是黑的。  
4.如果一个结点是红的，那么它的两个儿子都是黑的。  
5.对于任意结点而言，其到叶结点树尾端NIL指针的每条路径都包含相同数目的黑结点。
</code></pre><p><img src="http://static.oschina.net/uploads/space/2016/0220/152640_Hxex_2243330.png" alt="红黑树"></p>
<ul>
<li>当在对红黑树进行插入和删除等操作时，对树做了修改可能会破坏红黑树的性质。为了继续保持红黑树的性质，可以通过对结点进行重新着色，以及对树进行相关的旋转操作，即通过修改树中某些结点的颜色及指针结构，来达到对红黑树进行插入或删除结点等操作后继续保持它的性质或平衡的目的。</li>
<li>红黑树的插入：首先，将红黑树当作一颗二叉查找树，将节点插入；然后，将节点着色为红色；最后，通过旋转和重新着色等方法来修正该树，使之重新成为一颗红黑树。</li>
<li>红黑树与AVL树的区别：红黑树旋转操作非常局部化，而且次数极少（插入最多两次旋转，删除最多三次旋转），而改变颜色的操作不会影响到用户对树的query操作（即不要lock），另外很多树，如AVL树，2-3树,2-4树都可以转化成红黑树，红黑树能达到O(logn)高度，但是不像AVL树那样严格要求左右子树高度差必需相差不超过1。可以说RB树是目前为止高度要求最灵活的准平衡BST。准平衡是相对完全二叉树来说的，AVL树(比如Fibonacci树)也不是完美平衡的。</li>
</ul>
<h3 id="业界难题-“跨库分页”的四种方案-58沈剑"><a href="#业界难题-“跨库分页”的四种方案-58沈剑" class="headerlink" title="业界难题-“跨库分页”的四种方案 58沈剑"></a><a href="http://mp.weixin.qq.com/s?__biz=MjM5ODYxMDA5OQ==&amp;mid=2651959942&amp;idx=1&amp;sn=e9d3fe111b8a1d44335f798bbb6b9eea" target="_blank" rel="external">业界难题-“跨库分页”的四种方案</a> 58沈剑</h3><p><strong>方法一：全局视野法</strong></p>
<ol>
<li>将order by time offset X limit Y，改写成order by time offset 0 limit X+Y</li>
<li>服务层对得到的N*(X+Y)条数据进行内存排序，内存排序后再取偏移量X后的Y条记录这种方法随着翻页的进行，性能越来越低。</li>
</ol>
<p><strong>方法二：业务折衷法-禁止跳页查询</strong></p>
<ol>
<li>用正常的方法取得第一页数据，并得到第一页记录的time_max</li>
<li>每次翻页，将order by time offset X limit Y，改写成order by time where time&gt;$time_max limit Y 以保证每次只返回一页数据，性能为常量。</li>
</ol>
<p><strong>方法三：业务折衷法-允许模糊数据</strong><br>将order by time offset X limit Y，改写成order by time offset X/N limit Y/N</p>
<p><strong>方法四：二次查询法</strong></p>
<ol>
<li>将order by time offset X limit Y，改写成order by time offset X/N limit Y</li>
<li>找到最小值time_min</li>
<li>between二次查询，order by time between $time_min and $time_i_max</li>
<li>设置虚拟time_min，找到time_min在各个分库的offset，从而得到time_min在全局的offset</li>
<li>得到了time_min在全局的offset，自然得到了全局的offset X limit Y</li>
</ol>
<h3 id="从GITLAB误删除数据库想到的-陈皓"><a href="#从GITLAB误删除数据库想到的-陈皓" class="headerlink" title="从GITLAB误删除数据库想到的 陈皓"></a><a href="http://coolshell.cn/articles/17680.html" target="_blank" rel="external">从GITLAB误删除数据库想到的</a> 陈皓</h3><ul>
<li>事件回顾：Gitlab某员工在做负载均衡工作时需要解决突发情况，误将删除命令敲到生产环境的窗口上导致线上数据库被删除，然后视图通过多种备份机制都无法恢复，最终只能从6小时前的数据库中拷贝回来，导致在这6个小时期间的数据丢失；</li>
<li>人肉运维：一个公司的运维能力的强弱和你上线上环境敲命令是有关的，你越是喜欢上线敲命令你的运维能力就越弱，越是通过自动化来处理问题，你的运维能力就越强。</li>
<li>数据丢失有各种各样的情况，不单单只是人员的误操作，比如，掉电、磁盘损坏、中病毒等等，在这些情况下，你设计的那些想流程、规则、人肉检查、权限系统、checklist等等统统都不管用了，这个时候，你觉得应该怎么做呢？是的，你会发现，你不得不用更好的技术去设计出一个高可用的系统！别无它法。</li>
<li>关于备份：如果你要让你的备份系统随时都可以用，那么你就要让它随时都Live着，而随时都Live着的多结点系统，基本上就是一个分布式的高可用的系统。</li>
<li>非技术方面：故障反思（5 whys分析）、工程师文化（如果你是一个技术公司，你就会更多的相信技术而不是管理）、事件公开（公开所有的细节，会让大众少很多猜测的空间，有利于抵制流言和黑公关，同时，还会赢得大众的理解和支持。）；</li>
</ul>
<h3 id="AWS-的-S3-故障回顾和思考-陈皓"><a href="#AWS-的-S3-故障回顾和思考-陈皓" class="headerlink" title="AWS 的 S3 故障回顾和思考 陈皓"></a><a href="http://coolshell.cn/articles/17737.html" target="_blank" rel="external">AWS 的 S3 故障回顾和思考</a> 陈皓</h3><ul>
<li>故障原因：AWS某员工在修复账务系统问题，需要移除某些子系统时有一条命令搞错了，移除了大量S3的控制系统，包括对象索引服务和位置服务系统。而这两个系统重启花费了非常长时间（由于该系统非常稳定，以及很长时间没有重启过，而数据量级却一直在增长），最终导致服务挂了4个小时；</li>
<li>AWS后续改进措施：改进运维操作工具（让删除服务这个操作变慢一些、任何服务在运行时都应该有一个最小资源数、Review所有和其它的运维工具）；改进恢复过程（分解现有厚重的重要服务成更小的单元、今年内完成对 Index 索引服务的分区计划）；</li>
<li>一个系统的高可用的因素很多，不仅仅只是系统架构，更重要的是——高可用运维。对于高可用的运维，平时的故障演习是很重要的。</li>
</ul>
<h3 id="关于高可用的系统-陈皓"><a href="#关于高可用的系统-陈皓" class="headerlink" title="关于高可用的系统 陈皓"></a><a href="http://coolshell.cn/articles/17459.html" target="_blank" rel="external">关于高可用的系统</a> 陈皓</h3><ul>
<li>理解高可用系统：要做到数据不丢，就必需要持久化；要做到服务高可用，就必需要有备用（复本），无论是应用结点还是数据结点；要做到复制，就会有数据一致性的问题；我们不可能做到100%的高可用，也就是说，我们能做到几个9个的SLA；</li>
<li>高可用系统的技术解决方案：下图基本上来说是目前高可用系统中能看得到的所有的解决方案的基础了。M/S、MM实现起来不难，但是会有很多问题，2PC的问题就是性能不行，而Paxos的问题就是太复杂，实现难度太大。</li>
</ul>
<p><img src="http://coolshell.cn//wp-content/uploads/2014/01/Transaction-Across-DataCenter.jpg" alt="Transaction Across DataCenter"></p>
<ul>
<li>高可用技术方案的示例：MySQL的高可用的方案的SLA（下图下面红色的标识表示了这个方案有几个9）：</li>
</ul>
<p><img src="http://coolshell.cn//wp-content/uploads/2016/08/mysql-high-availability-solutions-feb-2015-webinar-9-638.jpg" alt="MySQL的高可用的方案的SLA"></p>
<pre><code>1.MySQL Repleaction就是传统的异步数据同步或是半同步Semi-Sync这个方式本质上不到2个9；
2.MySQL Fabric简单来说就是数据分片下的M/S的读写分离模式。这个方案的的可用性可以达到99%；
3.DRBD通过底层的磁盘同步技术来解决数据同步的问题，就是RAID 1——把两台以上的主机的硬盘镜像成一个。这个方案不到3个9；
4.Solaris Clustering/Oracle VM ，这个机制监控了包括硬件、操作系统、网络和数据库。这个方案一般会伴随着节点间的“心跳机制”，
而且还会动用到SAN（Storage Area Network）或是本地的分布式存储系统，还会动用虚拟化技术来做虚拟机的迁移以降低宕机时间的概率。这个解决方案完全就是一个“全栈式的解决方案”。这个方案接近4个9；
5.MySQL Cluster是官方的一个开源方案，其把MySQL的集群分成SQL Node 和Data Node，
Data Node是一个自动化sharing和复制的集群NDB，为了更高的可用性，MySQL Cluster采用了“完全同步”的数据复制的机制来冗余数据结点。这个方案接近5个9；
</code></pre><ul>
<li>影响高可用的因素：无计划的宕机原因（系统级的故障、数据和中介的故障、自然灾害、人为破坏等）；有计划的宕机原因（日常任务、运维相关、升级相关）；</li>
<li>要干出高可用的系统，其中包括但不限于：软件的设计、编码、测试、上线和软件配置管理的水平；工程师的人员技能水平；运维的管理和技术水平；数据中心的运营管理水平；依赖于第三方服务的管理水平；</li>
<li>深层交的东西则是——对工程这门科学的尊重：对待技术的态度、一个公司的工程文化、领导者对工程的尊重；</li>
</ul>
<h3 id="专访RocketMQ联合创始人：项目思路、技术细节和未来规划-王小瑞-冯嘉"><a href="#专访RocketMQ联合创始人：项目思路、技术细节和未来规划-王小瑞-冯嘉" class="headerlink" title="专访RocketMQ联合创始人：项目思路、技术细节和未来规划 王小瑞/冯嘉"></a><a href="http://jm.taobao.org/2017/03/03/RocketMQ-future-idea/" target="_blank" rel="external">专访RocketMQ联合创始人：项目思路、技术细节和未来规划</a> 王小瑞/冯嘉</h3><ul>
<li>RocketMQ的由来：第一代，推模式，数据存储采用关系型数据库，典型代表包括Notify、Napoli；第二代，拉模式，自研的专有消息存储，典型代表MetaQ；第三代，以拉模式为主，兼有推模式的高性能、低延迟消息引擎RocketMQ，在二代功能特性的基础上，为电商金融领域添加了可靠重试、基于文件存储的分布式事务等特性，并做了大量优化。</li>
<li>RocketMQ的技术概览：在我们看来，它最大的创新点在于能够通过精巧的横向、纵向扩展，不断满足与日俱增的海量消息在高吞吐、高可靠、低延迟方面的要求。目前RocketMQ主要由NameServer、Broker、Producer以及Consumer四部分构成，所有的集群都具有水平扩展能力，如下图所示：</li>
</ul>
<p><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1FUR8PVXXXXbbXpXXXXXXXXXX" alt="RocketMQ的技术概览"></p>
<ul>
<li>与其他消息中间件比较：RabbitMQ是AMQP规范的参考实现，AMQP是一个线路层协议，面面俱到，很系统，也稍显复杂；ActiveMQ是JMS规范的参考实现，JMS虽说是一个API级别的协议，但其内部还是定义了一些实现约束，不过缺少多语言支撑；而Kafka最初被设计用来做日志处理，是一个不折不扣的大数据通道，追求高吞吐，存在丢消息的可能；RocketMQ天生为金融互联网领域而生，追求高可靠、高可用、高并发、低延迟，是一个阿里巴巴由内而外成功孕育的典范。</li>
</ul>
<p><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1trKoPVXXXXXpXXXXXXXXXXXX" alt="与其他消息中间件比较"></p>
<ul>
<li>三项技术发力点：消息的顺序（全局保序）、消息的去重（目前的版本是不支持去重的，建议用户通过外置全局存储自己做判重处理，后续版本内置解决方案）、分布式的挑战（基于Zab一致性协议，利用分布式锁和通知机制保障多副本数据的一致性）；</li>
<li>新一代RocketMQ：期望构建一套厂商无关的集线路层、API层于一体的规范，这也是第四代消息引擎最大的亮点。</li>
</ul>
<h3 id="分布式事务原理与实践-沈询"><a href="#分布式事务原理与实践-沈询" class="headerlink" title="分布式事务原理与实践 沈询"></a><a href="http://jm.taobao.org/2017/02/09/20170209/" target="_blank" rel="external">分布式事务原理与实践</a> 沈询</h3><ul>
<li>事务的四大特性分别是：原子型、一致性、隔离性和持久性。</li>
<li>事务单元是通过Begin-Traction，然后Commit（Begin-Traction、Commit和Rollback之间所有针对数据的写入、读取的操作都应该添加同步访问），Begin和Commit之间就是一个同步的事务单元。</li>
<li>Two Phase Lock（2PL）是数据库中非常重要的一个概念。数据库操作Insert、Update、Delete都是先读再写的操作，例如Insert操作是先读取数据，读取之后判读数据是否存在，如果不存在，则写入该数据，如果数据存在，则返回错误。</li>
<li>处理事务的常见方法有排队法、排他锁、读写锁、MVCC等方式。事务处理中最重要也是最简单的方案是排队法，单线程地处理一堆数据；有些场景不适合用单线程操作，可以利用排他锁的方式来快速隔离并发读写事务；读写锁的核心是在多次读的操作中，同时允许多个读者来访问共享资源，提高并发性；MVCC本质是Copy On Write，也就是每次写都是以重新开始一个新的版本的方式写入数据，因此，数据库中也就包含了之前的所有版本，在数据读的过程中，先申请一个版本号，如果该版本号小于正在写入的版本号，则数据一定可以查询到，无需等到新版本完全写完即可返回查询结果。</li>
<li>事务的调优原则：尽可能减少锁的覆盖范围、增加锁上可并行的线程数、选择正确锁类型（比如悲观锁适合并发争抢比较严重的场景，乐观锁适合并发争抢不太严重的场景）；</li>
</ul>
<h3 id="对比了解Grafana与Kibana的关键差异-Asaf-Yigal-冬雨"><a href="#对比了解Grafana与Kibana的关键差异-Asaf-Yigal-冬雨" class="headerlink" title="对比了解Grafana与Kibana的关键差异 Asaf Yigal/冬雨"></a><a href="http://www.infoq.com/cn/articles/grafana-vs-kibana-the-key-differences-to-know" target="_blank" rel="external">对比了解Grafana与Kibana的关键差异</a> Asaf Yigal/冬雨</h3><ul>
<li>Kibana是一个分析和可视化平台，它可以让你浏览、可视化存储在Elasticsearch集群上排名靠前的日志数据，并构建仪表盘。你可以执行深入的数据分析并以多种图表、表格和地图方式可视化这些数据。Kibana的仪表盘非常简单易用，任何人都可以使用它，甚至IT技能和知识很少的业务人员也可以使用。</li>
<li>Grafana是一个开源仪表盘工具，它可用于Graphite、InfluxDB与 OpenTSDB一起使用。最新的版本还可以用于其他的数据源，比如Elasticsearch。它包含一个独一无二的Graphite目标解析器，从而可以简化度量和函数的编辑。Grafana快速的客户端渲染默认使用的是 Flot ，即使很长的时间范围也可应对。</li>
<li>日志与度量：Grafana专注于根据CPU和IO利用率之类的特定指标提供时间序列图表，而Kibana能创建一个复杂的日志分析仪表盘；</li>
<li>基于角色的访问：默认情况下Kibana的仪表盘是公开的，Grafana内置的RBA允许你维护用户和团队访问仪表盘的权限；</li>
<li>仪表盘灵活性：虽然Kibana有大量内置的图表类型，但它们之上的控制仍是最初的限制，Grafana包括更多的选择，可以更灵活地浏览和使用图表；</li>
<li>数据源的集成：Grafana支持许多不同的存储后端，它是针对数据源所具备的特性和能力特别定制的，而Kibana原生集成进了ELK栈，这使安装极为简单，对用户非常友好；</li>
<li>开源社区：ELK仍保持着快速的增长，并有潜力在不久的将来保持领先；</li>
<li>共同协作：Kibana和Grafana都是强大的可视化工具。然而，Grafana和InfluxDB组合是用于度量数据的，反之，Kibana是流行的ELK栈的一部分，它可以更为灵活地浏览日志数据。这两个平台都是好的选择，甚至有时还可以互补。首先，用Kibana去分析你的日志。然后，把数据导入到Grafana作为可视化层。这些的前提是需要同一个Elasticsearch库。</li>
</ul>
<h3 id="百度宣布将在Kubernetes上运行其深度学习平台PaddlePaddle-木环"><a href="#百度宣布将在Kubernetes上运行其深度学习平台PaddlePaddle-木环" class="headerlink" title="百度宣布将在Kubernetes上运行其深度学习平台PaddlePaddle 木环"></a><a href="http://www.infoq.com/cn/news/2017/02/baidu-Kubernetes-PaddlePaddle" target="_blank" rel="external">百度宣布将在Kubernetes上运行其深度学习平台PaddlePaddle</a> 木环</h3><ul>
<li>本月初，Kubernetes在其官网上宣布了百度的PaddlePaddle成为目前唯一官方支持Kubernetes的深度学习框架。PaddlePaddle是百度于2016年9月开源的一款深度学习平台，具有易用，高效，灵活和可伸缩等特点，为百度内部多项产品提供深度学习算法支持。</li>
<li>Kubernetes 把很多分散的物理计算资源抽象成一个巨大的资源池，它利用这些资源来帮助用户执行计算任务。对于用户来说，操作一个分散的集群资源可以像使用一台计算机一样简单。对于这个项目，Kubernetes 主要负责将学习任务分配到集群的物理节点上进行运算；如果遇到任务失败的情况，Kubernetes 会自动重启任务。</li>
<li>能不能将框架的作业和任务模式，同“容器”这个全新的部署概念匹配起来，才是现阶段最重要的。毕竟，如果框架连正常运行起来都很困难，再好的资源利用率提升机制也没有用武之地。在这一点上，Kubernetes应该说是现有的容器管理项目中做的最好的。</li>
<li>容器化实施深度学习的优点：轻量级、更高的资源利用率、基于容器的设计模式、高度的可扩展性和容错能力；</li>
</ul>
<h3 id="建设DevOps统一运维监控平台，先从日志监控说起-王海龙"><a href="#建设DevOps统一运维监控平台，先从日志监控说起-王海龙" class="headerlink" title="建设DevOps统一运维监控平台，先从日志监控说起 王海龙"></a><a href="http://mp.weixin.qq.com/s/QqoyLhCdy85gD9ixOdCYqg" target="_blank" rel="external">建设DevOps统一运维监控平台，先从日志监控说起</a> 王海龙</h3><ul>
<li>DevOps浪潮下带来的监控挑战：监控源的多样化挑战、海量数据的分析处理挑战、软硬件数据资源的管理分析挑战；</li>
<li>一个好的统一监控平台，应当具备：高度抽象模型，扩展监控指标、多种监控视图、强大的数据加工能力、多种数据采集技术、多种报警机制、全路径问题跟踪；</li>
<li>统一监控平台由七大角色构成：监控源、数据采集、数据存储、数据分析、数据展现、预警中心、CMDB(企业软硬件资产管理)。</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/15_reading_201702/monitor_system.jpg" alt="统一监控平台"></p>
<ul>
<li>日志监控的技术栈</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/15_reading_201702/monitor_system_tach.jpg" alt="日志监控的技术栈"></p>
<ul>
<li>ELK-日志监控经典方案</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/15_reading_201702/monitor_system_elk.jpg" alt="ELK-日志监控经典方案"> </p>
<ul>
<li>微服务+容器云背景下的日志监控实践：跑在容器中的应用、数据库等软件都会把日志落到容器日志（docker日志），然后在docker系统服务上进行配置，将docker容器日志输出到系统日志服务journald中。这样，容器中的日志就统一到了系统日志中。针对于运行在虚拟机上的系统软件，如kubernetes、etcd等，配置成系统服务service，使用systemd管理，自然也就做到了将其日志输入到journald中。再往上就比较简单了，自实现一个agent，读取journald中的日志，通过tcp协议发送到fluentd中，考虑到现在的日志量并不会太大，所以没有再使用kafka进行缓冲，而是直接经过fluentd的拦截和过滤，将日志发送到Elasticsearch中。</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/15_reading_201702/monitor_system_inuse.jpg" alt="日志监控实践"> </p>
<ul>
<li>如何选择适合自己的日志监控方案：工具能力是否满足、性能对比、看技术能力是否能cover住、监控平台日志量评估；</li>
</ul>
<h3 id="函数式编程入门教程-阮一峰"><a href="#函数式编程入门教程-阮一峰" class="headerlink" title="函数式编程入门教程 阮一峰"></a><a href="http://www.ruanyifeng.com/blog/2017/02/fp-tutorial.html" target="_blank" rel="external">函数式编程入门教程</a> 阮一峰</h3><ul>
<li>函数式编程的起源，是一门叫做范畴论（Category Theory）的数学分支。理解函数式编程的关键，就是理解范畴论。它是一门很复杂的数学，认为世界上所有的概念体系，都可以抽象成一个个的”范畴”（category）。</li>
<li>范畴就是使用箭头连接的物体。也就是说，彼此之间存在某种关系的概念、事物、对象等等，都构成”范畴”。箭头表示范畴成员之间的关系，正式的名称叫做”态射”（morphism）。范畴论认为，同一个范畴的所有成员，就是不同状态的”变形”（transformation）。通过”态射”，一个成员可以变形成另一个成员。</li>
<li>我们可以把”范畴”想象成是一个容器，里面包含两样东西：值（value）和值的变形关系，也就是函数。</li>
<li>本质上，函数式编程只是范畴论的运算方法，跟数理逻辑、微积分、行列式是同一类东西，都是数学方法，只是碰巧它能用来写程序。</li>
<li>如果一个值要经过多个函数，才能变成另外一个值，就可以把所有中间步骤合并成一个函数，这叫做”函数的合成”（compose）。</li>
<li>f(x)和g(x)合成为f(g(x))，有一个隐藏的前提，就是f和g都只能接受一个参数。如果可以接受多个参数就需要函数柯里化了。所谓”柯里化”，就是把一个多参数的函数，转化为单参数函数。</li>
<li>函子是函数式编程里面最重要的数据类型，也是基本的运算单位和功能单位。它首先是一种范畴，也就是说，是一个容器，包含了值和变形关系。比较特殊的是，它的变形关系可以依次作用于每一个值，将当前容器变形成另一个容器。</li>
<li>学习函数式编程，实际上就是学习函子的各种运算。</li>
</ul>
<p><strong>免责声明：相关链接版本为原作者所有，如有侵权，请告知删除。</strong></p>
<p><strong>随手记系列：</strong></p>
<ul>
<li><a href="http://ginobefunny.com/post/reading_record_201701/">阅读随手记 201701</a></li>
<li><a href="http://ginobefunny.com/post/reading_record_201612/">阅读随手记 201612</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;关键字：微服务, 分布式, 配置中心, Java编程, 推荐系统, 运维, 高并发, 高可用, 机器学习, 深度学习。&lt;br&gt;
    
    </summary>
    
      <category term="Reading Record" scheme="http://ginobefunny.com/categories/Reading-Record/"/>
    
    
      <category term="推荐系统" scheme="http://ginobefunny.com/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
      <category term="微服务" scheme="http://ginobefunny.com/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"/>
    
      <category term="机器学习" scheme="http://ginobefunny.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="http://ginobefunny.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="高并发" scheme="http://ginobefunny.com/tags/%E9%AB%98%E5%B9%B6%E5%8F%91/"/>
    
      <category term="高可用" scheme="http://ginobefunny.com/tags/%E9%AB%98%E5%8F%AF%E7%94%A8/"/>
    
      <category term="分布式" scheme="http://ginobefunny.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
      <category term="配置中心" scheme="http://ginobefunny.com/tags/%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83/"/>
    
      <category term="Java编程" scheme="http://ginobefunny.com/tags/Java%E7%BC%96%E7%A8%8B/"/>
    
      <category term="运维" scheme="http://ginobefunny.com/tags/%E8%BF%90%E7%BB%B4/"/>
    
  </entry>
  
  <entry>
    <title>Protocol Buffers简明教程</title>
    <link href="http://ginobefunny.com/post/learning_protobuf/"/>
    <id>http://ginobefunny.com/post/learning_protobuf/</id>
    <published>2017-02-07T11:32:47.000Z</published>
    <updated>2017-02-10T05:12:25.760Z</updated>
    
    <content type="html"><![CDATA[<p>随着微服务架构的流行，RPC框架渐渐地成为服务框架的一个重要部分。在很多RPC的设计中，都采用了高性能的编解码技术，Protocol Buffers就属于其中的佼佼者。<a href="https://github.com/google/protobuf" target="_blank" rel="external">Protocol Buffers</a>是Google开源的一个语言无关、平台无关的通信协议，其小巧、高效和友好的兼容性设计，使其被广泛使用。<br><a id="more"></a></p>
<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><h3 id="protobuf是什么？"><a href="#protobuf是什么？" class="headerlink" title="protobuf是什么？"></a>protobuf是什么？</h3><blockquote>
<p>Protocol buffers are Google’s language-neutral, platform-neutral, extensible mechanism for serializing structured data – think XML, but smaller, faster, and simpler. You define how you want your data to be structured once, then you can use special generated source code to easily write and read your structured data to and from a variety of data streams and using a variety of languages.</p>
</blockquote>
<ul>
<li>Google良心企业出厂的；</li>
<li>是一种序列化对象框架（或者说是编解码框架），其他功能相似的有Java自带的序列化、Facebook的Thrift和JBoss Marshalling等；</li>
<li>通过proto文件定义结构化数据，其他功能相似的比如XML、JSON等；</li>
<li>自带代码生成器，支持多种语言；</li>
</ul>
<h3 id="为什么叫“Protocol-Buffers”？"><a href="#为什么叫“Protocol-Buffers”？" class="headerlink" title="为什么叫“Protocol Buffers”？"></a>为什么叫“Protocol Buffers”？</h3><p>官方如是说：</p>
<blockquote>
<p>The name originates from the early days of the format, before we had the protocol buffer compiler to generate classes for us. At the time, there was a class called ProtocolBuffer which actually acted as a buffer for an individual method. Users would add tag/value pairs to this buffer individually by calling methods like AddValue(tag, value). The raw bytes were stored in a buffer which could then be written out once the message had been constructed.</p>
<p>Since that time, the “buffers” part of the name has lost its meaning, but it is still the name we use. Today, people usually use the term “protocol message” to refer to a message in an abstract sense, “protocol buffer” to refer to a serialized copy of a message, and “protocol message object” to refer to an in-memory object representing the parsed message.</p>
</blockquote>
<h3 id="核心特点"><a href="#核心特点" class="headerlink" title="核心特点"></a>核心特点</h3><ul>
<li>语言无关、平台无关</li>
<li>简洁</li>
<li>高性能</li>
<li>良好的兼容性</li>
</ul>
<h3 id="“变态的”性能表现"><a href="#“变态的”性能表现" class="headerlink" title="“变态的”性能表现"></a>“变态的”性能表现</h3><p>有位网友曾经做过<a href="http://agapple.iteye.com/blog/859052" target="_blank" rel="external">各种通用序列化协议技术的对比</a>，我这里直接拿来给大家感受一下：</p>
<p><strong>序列化响应时间对比</strong></p>
<p><img src="http://oi46mo3on.bkt.clouddn.com/13_learning_protobuf/protobuf_comparation_time.png" alt="序列化响应时间对比"></p>
<p><strong>序列化bytes对比</strong></p>
<p><img src="http://oi46mo3on.bkt.clouddn.com/13_learning_protobuf/protobuf_comparation_bytes.png" alt="序列化bytes对比"></p>
<p><strong>具体的数字</strong></p>
<p><img src="http://oi46mo3on.bkt.clouddn.com/13_learning_protobuf/protobuf_comparation_result.png" alt="具体的数字"></p>
<h2 id="快速开始"><a href="#快速开始" class="headerlink" title="快速开始"></a>快速开始</h2><p>以下示例源码已上传至github：<a href="https://github.com/ginobefun/learning_projects/tree/master/learning-protobuf" target="_blank" rel="external">https://github.com/ginobefun/learning_projects/tree/master/learning-protobuf</a></p>
<h3 id="新建一个maven项目并添加依赖"><a href="#新建一个maven项目并添加依赖" class="headerlink" title="新建一个maven项目并添加依赖"></a>新建一个maven项目并添加依赖</h3><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;
         xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

    &lt;groupId&gt;com.ginobefunny.learning&lt;/groupId&gt;
    &lt;artifactId&gt;leanring-protobuf&lt;/artifactId&gt;
    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;

    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.google.protobuf&lt;/groupId&gt;
            &lt;artifactId&gt;protobuf-java&lt;/artifactId&gt;
            &lt;version&gt;3.2.0&lt;/version&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;
&lt;/project&gt;
</code></pre><h3 id="新建protobuf的消息定义文件addressbook-proto"><a href="#新建protobuf的消息定义文件addressbook-proto" class="headerlink" title="新建protobuf的消息定义文件addressbook.proto"></a>新建protobuf的消息定义文件addressbook.proto</h3><pre><code>syntax = &quot;proto3&quot;; // 声明为protobuf 3定义文件
package tutorial;

option java_package = &quot;com.ginobefunny.learning.protobuf.message&quot;; // 声明生成消息类的java包路径
option java_outer_classname = &quot;AddressBookProtos&quot;;  // 声明生成消息类的类名

message Person {
  string name = 1;
  int32 id = 2;
  string email = 3;

  enum PhoneType {
    MOBILE = 0;
    HOME = 1;
    WORK = 2;
  }

  message PhoneNumber {
    string number = 1;
    PhoneType type = 2;
  }

  repeated PhoneNumber phones = 4;
}

message AddressBook {
  repeated Person people = 1;
}
</code></pre><h3 id="使用protoc工具生成消息对应的Java类"><a href="#使用protoc工具生成消息对应的Java类" class="headerlink" title="使用protoc工具生成消息对应的Java类"></a>使用protoc工具生成消息对应的Java类</h3><ul>
<li>从<a href="https://github.com/google/protobuf/releases/" target="_blank" rel="external">已发布版本</a>中下载protoc工具，比如protoc-3.2.0-win32；</li>
<li>解压后将bin目录添加到path路径；</li>
<li>执行以下protoc命令生成Java类：</li>
</ul>
<pre><code>protoc -I=. --java_out=src/main/java addressbook.proto
</code></pre><h3 id="编写测试类写入和读取序列化文件"><a href="#编写测试类写入和读取序列化文件" class="headerlink" title="编写测试类写入和读取序列化文件"></a>编写测试类写入和读取序列化文件</h3><ul>
<li>AddPerson类通过用户每次添加一个联系人，并序列化保存到指定文件中。</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">AddPerson</span> </span>&#123;</div><div class="line"></div><div class="line">    <span class="comment">// 通过用户输入构建一个Person对象</span></div><div class="line">    <span class="keyword">static</span> AddressBookProtos.<span class="function">Person <span class="title">promptForAddress</span><span class="params">(BufferedReader stdin,</span></span></div><div class="line">                                                     PrintStream stdout) <span class="keyword">throws</span> IOException &#123;</div><div class="line">        AddressBookProtos.Person.Builder person = AddressBookProtos.Person.newBuilder();</div><div class="line"></div><div class="line">        stdout.print(<span class="string">"Enter person ID: "</span>);</div><div class="line">        person.setId(Integer.valueOf(stdin.readLine()));</div><div class="line"></div><div class="line">        stdout.print(<span class="string">"Enter name: "</span>);</div><div class="line">        person.setName(stdin.readLine());</div><div class="line"></div><div class="line">        stdout.print(<span class="string">"Enter email address (blank for none): "</span>);</div><div class="line">        String email = stdin.readLine();</div><div class="line">        <span class="keyword">if</span> (email.length() &gt; <span class="number">0</span>) &#123;</div><div class="line">            person.setEmail(email);</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</div><div class="line">            stdout.print(<span class="string">"Enter a phone number (or leave blank to finish): "</span>);</div><div class="line">            String number = stdin.readLine();</div><div class="line">            <span class="keyword">if</span> (number.length() == <span class="number">0</span>) &#123;</div><div class="line">                <span class="keyword">break</span>;</div><div class="line">            &#125;</div><div class="line"></div><div class="line">            AddressBookProtos.Person.PhoneNumber.Builder phoneNumber =</div><div class="line">                    AddressBookProtos.Person.PhoneNumber.newBuilder().setNumber(number);</div><div class="line"></div><div class="line">            stdout.print(<span class="string">"Is this a mobile, home, or work phone? "</span>);</div><div class="line">            String type = stdin.readLine();</div><div class="line">            <span class="keyword">if</span> (type.equals(<span class="string">"mobile"</span>)) &#123;</div><div class="line">                phoneNumber.setType(AddressBookProtos.Person.PhoneType.MOBILE);</div><div class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (type.equals(<span class="string">"home"</span>)) &#123;</div><div class="line">                phoneNumber.setType(AddressBookProtos.Person.PhoneType.HOME);</div><div class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (type.equals(<span class="string">"work"</span>)) &#123;</div><div class="line">                phoneNumber.setType(AddressBookProtos.Person.PhoneType.WORK);</div><div class="line">            &#125; <span class="keyword">else</span> &#123;</div><div class="line">                stdout.println(<span class="string">"Unknown phone type.  Using default."</span>);</div><div class="line">            &#125;</div><div class="line"></div><div class="line">            person.addPhones(phoneNumber);</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="keyword">return</span> person.build();</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">// 加载指定的序列化文件（如不存在则创建一个新的），再通过用户输入增加一个新的联系人到地址簿，最后序列化到文件中</span></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">        <span class="keyword">if</span> (args.length != <span class="number">1</span>) &#123;</div><div class="line">            System.err.println(<span class="string">"Usage:  AddPerson ADDRESS_BOOK_FILE"</span>);</div><div class="line">            System.exit(-<span class="number">1</span>);</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        AddressBookProtos.AddressBook.Builder addressBook = AddressBookProtos.AddressBook.newBuilder();</div><div class="line"></div><div class="line">        <span class="comment">// Read the existing address book.</span></div><div class="line">        <span class="keyword">try</span> &#123;</div><div class="line">            addressBook.mergeFrom(<span class="keyword">new</span> FileInputStream(args[<span class="number">0</span>]));</div><div class="line">        &#125; <span class="keyword">catch</span> (FileNotFoundException e) &#123;</div><div class="line">            System.out.println(args[<span class="number">0</span>] + <span class="string">": File not found.  Creating a new file."</span>);</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="comment">// Add an address.</span></div><div class="line">        addressBook.addPeople(promptForAddress(<span class="keyword">new</span> BufferedReader(<span class="keyword">new</span> InputStreamReader(System.in)),</div><div class="line">                        System.out));</div><div class="line"></div><div class="line">        <span class="comment">// Write the new address book back to disk.</span></div><div class="line">        FileOutputStream output = <span class="keyword">new</span> FileOutputStream(args[<span class="number">0</span>]);</div><div class="line">        addressBook.build().writeTo(output);</div><div class="line">        output.close();</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<ul>
<li>ListPeople类读取序列化文件并输出所有联系人信息。</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ListPeople</span> </span>&#123;</div><div class="line"></div><div class="line">    <span class="comment">// 打印地址簿中所有联系人信息</span></div><div class="line">    <span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">print</span><span class="params">(AddressBookProtos.AddressBook addressBook)</span> </span>&#123;</div><div class="line">        <span class="keyword">for</span> (AddressBookProtos.Person person: addressBook.getPeopleList()) &#123;</div><div class="line">            System.out.println(<span class="string">"Person ID: "</span> + person.getId());</div><div class="line">            System.out.println(<span class="string">"  Name: "</span> + person.getName());</div><div class="line">            <span class="keyword">if</span> (!person.getPhonesList().isEmpty()) &#123;</div><div class="line">                System.out.println(<span class="string">"  E-mail address: "</span> + person.getEmail());</div><div class="line">            &#125;</div><div class="line"></div><div class="line">            <span class="keyword">for</span> (AddressBookProtos.Person.PhoneNumber phoneNumber : person.getPhonesList()) &#123;</div><div class="line">                <span class="keyword">switch</span> (phoneNumber.getType()) &#123;</div><div class="line">                    <span class="keyword">case</span> MOBILE:</div><div class="line">                        System.out.print(<span class="string">"  Mobile phone #: "</span>);</div><div class="line">                        <span class="keyword">break</span>;</div><div class="line">                    <span class="keyword">case</span> HOME:</div><div class="line">                        System.out.print(<span class="string">"  Home phone #: "</span>);</div><div class="line">                        <span class="keyword">break</span>;</div><div class="line">                    <span class="keyword">case</span> WORK:</div><div class="line">                        System.out.print(<span class="string">"  Work phone #: "</span>);</div><div class="line">                        <span class="keyword">break</span>;</div><div class="line">                &#125;</div><div class="line">                System.out.println(phoneNumber.getNumber());</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">// 加载指定的序列化文件，并输出所有联系人信息</span></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">        <span class="keyword">if</span> (args.length != <span class="number">1</span>) &#123;</div><div class="line">            System.err.println(<span class="string">"Usage:  ListPeople ADDRESS_BOOK_FILE"</span>);</div><div class="line">            System.exit(-<span class="number">1</span>);</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="comment">// Read the existing address book.</span></div><div class="line">        AddressBookProtos.AddressBook addressBook =</div><div class="line">                AddressBookProtos.AddressBook.parseFrom(<span class="keyword">new</span> FileInputStream(args[<span class="number">0</span>]));</div><div class="line"></div><div class="line">        print(addressBook);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="验证效果"><a href="#验证效果" class="headerlink" title="验证效果"></a>验证效果</h3><p>先添加一个联系人Gino</p>
<p><img src="http://oi46mo3on.bkt.clouddn.com/13_learning_protobuf/AddPerson1.png" alt="添加一个联系人Gino"></p>
<p>再添加一个联系人Slightly</p>
<p><img src="http://oi46mo3on.bkt.clouddn.com/13_learning_protobuf/AddPerson2.png" alt="添加一个联系人Gino"></p>
<p>最后显示所有联系人信息</p>
<p><img src="http://oi46mo3on.bkt.clouddn.com/13_learning_protobuf/ListPerson.png" alt="添加一个联系人Gino"></p>
<h3 id="实例小结"><a href="#实例小结" class="headerlink" title="实例小结"></a>实例小结</h3><ul>
<li>通过以上的例子我们能大概感受到开发protobuf序列化的大致步骤：定义proto文件、生成对应的Java类文件、通过消息类的构造器构造对象并通过writeTo序列化、通过parseFrom反序列化对象；</li>
<li>如果查看中间序列化的文件，我们可以发现protobuf序列化的二进制文件非常紧凑，因此文件更小，传输性能更好。</li>
</ul>
<h2 id="深入学习"><a href="#深入学习" class="headerlink" title="深入学习"></a>深入学习</h2><h3 id="关于proto文件"><a href="#关于proto文件" class="headerlink" title="关于proto文件"></a>关于proto文件</h3><h4 id="protobuf版本"><a href="#protobuf版本" class="headerlink" title="protobuf版本"></a>protobuf版本</h4><ul>
<li>protobuf现在主流的有2.X和3.X版本，两者之间相差比较大，对于刚采用的建议使用3.X版本；</li>
<li>如果采用3.X版本，需要再proto文件第一个非注释行声明（就像我们上面的例子那样），因为protobuf默认认为是2.X版本；</li>
</ul>
<h4 id="message结构"><a href="#message结构" class="headerlink" title="message结构"></a>message结构</h4><ul>
<li>在一个proto文件中可以包含多个message定义，message之间可以互相引用，message还可以嵌套message和枚举类；</li>
<li>一个message通常包含一至多个字段；</li>
<li>每个字段包含以下几个部分：字段描述符（可选）、字段类型、字段名称和字段对应的Tag；</li>
</ul>
<h4 id="字段描述符"><a href="#字段描述符" class="headerlink" title="字段描述符"></a>字段描述符</h4><p>字段描述符用于描述字段出现的频率，有以下两个可选值：</p>
<ul>
<li>singular：表示出现0次或1次；如果没有声明描述符，默认为singular；</li>
<li>repeated：表示出现0次或多次；</li>
</ul>
<h4 id="字段类型"><a href="#字段类型" class="headerlink" title="字段类型"></a>字段类型</h4><ul>
<li>基本数据类型：包括double、float、bool、string、bytes、int32、int64、uint32、uint64、sint32、sint64、fixed32、fixed64、sfixed32、sfixed64；</li>
<li>引用其他message类型：这个就有点像我们Java里面的对象引用的方式；</li>
<li>枚举类型：对于枚举类型，protobuf有个约束：枚举的第一项对应的值必须为0；下面是一个包含枚举类型的消息定义：</li>
</ul>
<pre><code>message SearchRequest {
  string query = 1;
  int32 page_number = 2;
  int32 result_per_page = 3;
  enum Corpus {
    UNIVERSAL = 0;
    WEB = 1;
    IMAGES = 2;
    LOCAL = 3;
    NEWS = 4;
    PRODUCTS = 5;
    VIDEO = 6;
  }
  Corpus corpus = 4;
}
</code></pre><h4 id="字段对应的Tag"><a href="#字段对应的Tag" class="headerlink" title="字段对应的Tag"></a>字段对应的Tag</h4><ul>
<li>对应同一个message里面的字段，每个字段的Tag是必须唯一数字；</li>
<li>Tag主要用于说明字段在二进制文件的对应关系，一旦指定字段为对应的Tag，不应该在后续进行变更；</li>
<li>对于Tag的分配，1~15只用一个byte进行编码（因此应该留给那些常用的字段），16~2047用两个byte进行编码，最大支持到536870911，但是中间有一段（19000~19999）是protobuf内部使用的；</li>
<li>可以通过reserved关键字来预留Tag和字段名，还有一种场景是如果某个字段已经被废弃了不希望后续被采用，也可以用reserved关键字声明；</li>
</ul>
<h4 id="字段的默认值"><a href="#字段的默认值" class="headerlink" title="字段的默认值"></a>字段的默认值</h4><p>protobuf 2.X版本是支持在字段中声明默认值的，但是在3.X版本中去掉了默认值的定义，主要是为了区别用户是否设置了一个和默认值一样的值的情况。对于3.X版本，protobuf采用以下规则处理默认值：</p>
<ul>
<li>对应string类型，默认值为一个空字符串；</li>
<li>对于bytes类型，默认值为一个空的byte数组；</li>
<li>对于bool类型，默认值为false；</li>
<li>对于数值类型，默认值为0；</li>
<li>对于枚举类型，默认值为第一项，也即值为0的那个枚举值；</li>
<li>对于引用其他message类型：其默认值和对应的语言是相关的；</li>
</ul>
<h3 id="Map字段类型"><a href="#Map字段类型" class="headerlink" title="Map字段类型"></a>Map字段类型</h3><ul>
<li>protobuf也支持定义Map类型的字段，但是对于Map的key的类型只能是整数型（包括各种int32和int64）和string类型；</li>
<li>Map类型不能定义为repeated；</li>
<li>Map类型的数据是无序的；</li>
<li>以下是一个Map类型的字段定义示例：</li>
</ul>
<pre><code>map&lt;string, Project&gt; projects = 3;
</code></pre><h3 id="导入其他proto文件"><a href="#导入其他proto文件" class="headerlink" title="导入其他proto文件"></a>导入其他proto文件</h3><ul>
<li>可以通过import关键字导入其他proto文件，从而重用message类型；下面是一个import的示例：</li>
</ul>
<pre><code>import &quot;myproject/other_protos.proto&quot;;
</code></pre><h3 id="如果proto中的message要扩展怎么办？"><a href="#如果proto中的message要扩展怎么办？" class="headerlink" title="如果proto中的message要扩展怎么办？"></a>如果proto中的message要扩展怎么办？</h3><p>proto具有很好的扩展性，但是也要遵循以下原则：</p>
<ul>
<li>不能修改原有字段的Tag；</li>
<li>如果新增一个字段，对于老的二进制序列化文件处理时会给这个字段增加默认值；如果是升级了proto文件而没有升级对应的代码，则新的字段会被忽略；</li>
<li>可以删除字段，但是对应的Tag不应该再被使用，否则对于之前的二进制序列化消息处理时对应关系出现问题；</li>
<li>int32、uint32、int64、uint64和bool类型是相互兼容的，这意味着你可以在他们之间修改类型而不会有兼容性问题；</li>
</ul>
<h3 id="Any消息类型"><a href="#Any消息类型" class="headerlink" title="Any消息类型"></a>Any消息类型</h3><ul>
<li>protobuf内置了一些通用的消息类型，Any就是其他的一种，通过查看它的proto文件可以看到它包含了一个URL标识符和一个byte数组；</li>
<li>在使用Any消息类型之前，需要通过<strong>import “google/protobuf/any.proto”;</strong>导入proto文件定义；</li>
</ul>
<h3 id="Oneof关键字"><a href="#Oneof关键字" class="headerlink" title="Oneof关键字"></a>Oneof关键字</h3><ul>
<li>oneof关键字用于声明一组字段中，必须要有一个字段被赋值；通常比如我们在登陆的时候，可以用手机号、邮箱和用户名登陆，这种时候就可以使用oneof来定义；</li>
<li>当我们对oneof其中一个字段赋值时，其他字段的值将会被清空；所以只有最后一次赋值是有效的；</li>
<li>下面是一个oneof的示例：</li>
</ul>
<pre><code>message LoginMessage {
  oneof user_identifier {
    string user_name = 4;
    string phone_num = 5;
    string user_email = 6;
  }

  string password = 10;
}
</code></pre><h3 id="定义服务"><a href="#定义服务" class="headerlink" title="定义服务"></a>定义服务</h3><ul>
<li>在proto文件中还允许定义RPC服务，以下是一个示例：</li>
</ul>
<pre><code>service SearchService {
  rpc Search (SearchRequest) returns (SearchResponse);
}
</code></pre><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><ul>
<li>随着微服务架构的流行，RPC框架渐渐地成为服务框架的一个重要部分。在很多RPC的设计中，都采用了高性能的编解码技术，protobuf就属于其中的佼佼者；</li>
<li>protobuf相对于其他编解码框架，有着非常惊人的性能表现；</li>
<li>通过一个简单的实例，我们了解如果使用protobuf进行序列化和数据交互；</li>
<li>最后，我们列举了一些重要的特性和配置说明，这些在我们使用protobuf中都会给频繁使用；</li>
<li>后续学习：后面我会根据所学的Netty和protobuf知识，开发一个简单的RPC框架。</li>
</ul>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a href="https://developers.google.com/protocol-buffers/docs/proto3" target="_blank" rel="external">Language Guide (proto3)</a></li>
<li><a href="https://developers.google.com/protocol-buffers/docs/javatutorial" target="_blank" rel="external">Protocol Buffer Basics: Java</a></li>
<li><a href="https://developers.google.com/protocol-buffers/docs/reference/java-generated" target="_blank" rel="external">Java Generated Code</a></li>
<li><a href="https://solicomo.com/network-dev/protobuf-proto3-vs-proto2.html" target="_blank" rel="external">Protobuf 的 proto3 与 proto2 的区别</a></li>
<li><a href="http://agapple.iteye.com/blog/859052" target="_blank" rel="external">几种序列化协议(protobuf,xstream,jackjson,jdk,hessian)相关数据对比</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;随着微服务架构的流行，RPC框架渐渐地成为服务框架的一个重要部分。在很多RPC的设计中，都采用了高性能的编解码技术，Protocol Buffers就属于其中的佼佼者。&lt;a href=&quot;https://github.com/google/protobuf&quot;&gt;Protocol Buffers&lt;/a&gt;是Google开源的一个语言无关、平台无关的通信协议，其小巧、高效和友好的兼容性设计，使其被广泛使用。&lt;br&gt;
    
    </summary>
    
      <category term="OpenSource" scheme="http://ginobefunny.com/categories/OpenSource/"/>
    
    
      <category term="Google" scheme="http://ginobefunny.com/tags/Google/"/>
    
      <category term="入门" scheme="http://ginobefunny.com/tags/%E5%85%A5%E9%97%A8/"/>
    
      <category term="教程" scheme="http://ginobefunny.com/tags/%E6%95%99%E7%A8%8B/"/>
    
      <category term="实例" scheme="http://ginobefunny.com/tags/%E5%AE%9E%E4%BE%8B/"/>
    
      <category term="Protocol Buffers" scheme="http://ginobefunny.com/tags/Protocol-Buffers/"/>
    
      <category term="protobuf" scheme="http://ginobefunny.com/tags/protobuf/"/>
    
  </entry>
  
  <entry>
    <title>代码之外的生存指南</title>
    <link href="http://ginobefunny.com/post/soft_skills/"/>
    <id>http://ginobefunny.com/post/soft_skills/</id>
    <published>2017-02-04T11:54:18.000Z</published>
    <updated>2017-02-04T06:02:02.051Z</updated>
    
    <content type="html"><![CDATA[<p>阅读<a href="https://book.douban.com/subject/26835090/" target="_blank" rel="external">《软技能》</a>一书的笔记和随想。<br><a id="more"></a></p>
<h1 id="笔记整理"><a href="#笔记整理" class="headerlink" title="笔记整理"></a>笔记整理</h1><h2 id="职业篇"><a href="#职业篇" class="headerlink" title="职业篇"></a>职业篇</h2><ul>
<li>你所能犯的最大错误就是相信自己是在为别人工作。这样一来你对工作的安全感依然尽失。职业发展的驱动力一定是来自个体本身。记住：工作是属于公司的，而职业生涯却是属于你自己的。</li>
<li>大多数软件开发人员从职业生涯一开始就犯了几个严重的错误，最大的错误就是没有把自己的软件开发事业当作一桩生意来看待。你应该把自己当作一个企业去思考，把雇主当作是你的软件开发企业的一个客户。</li>
<li>通常软件开发人员售卖的就是他们把一个想法变成一个数字化的现实产品的能力。因此你需要做到：专注于你正在提供怎么样的服务以及如何营销；想法设法提升你的服务；集中精力称为一位专家；</li>
<li>如何设定目标：起步阶段最简单的就是在心中树立一个大目标，然后再建立能帮你达成这个大目标的小目标。较小的目标可以让你航行在自己的轨道上，激励你保持航向朝着更大的目标前进。</li>
<li>学会与人打交道：每个人都希望感到自己重要；永远不要批评；换位思考；避免争吵；</li>
<li>关于面试：让面试官对你怀有好感（比如阅读过你的博客，比如有员工推荐你）；集中精力证明自己无需督促也能自动自发做好事情以及在技术上你确实胜任工作；坚持阅读技术书籍和博客文章，提升自己的技能；扩展自己的社交网络；</li>
<li>专业化很重要：虽然会把你关在一些机会的大门之外，但与此同时它将打开的机会大门要比你用其他方式打开的多得多。专业化的规则是程序越深，潜在的机会越少，但获得这些机会的可能性越大。</li>
<li>不同规模的公司选择：小公司（承担更多职责但稳定性差）、中等规模公司（工作稳定但变化很慢）、大公司（完备的流程和规范但负责一小部分且可能充斥着官僚作风）；</li>
<li>关于晋升：承担责任（负责不受重视的项目、帮助新人成长、负责文档更新等）；引入注目（记录活动日志、提供演讲、发表意见）；自学（提升技能并分享）；成为问题的解决者；</li>
<li>成为专业人士是一种心态。如果我们总是与恐惧。自毁。拖延和自我怀疑作斗争，那么问题就是我们正在像外行那么思考问题。外行毫不起眼，外行人废话连篇，外行屈从于逆境。专业人士可不这么想。不管怎样，他引人注目，他恪尽职守，他始终如一。</li>
<li>对技术虔诚的一大问题是，我们中的大多数崇拜某项特定的技术，只是因为自己熟悉这种技术，我们很自然地会相信自己选择的是最好的，然而这会让我们经常忽略任何反对意见。</li>
</ul>
<h2 id="自我营销篇"><a href="#自我营销篇" class="headerlink" title="自我营销篇"></a>自我营销篇</h2><ul>
<li>自我营销的关键在于：如果想让别人喜欢你，想和你一起工作，你必须要为他们提供价值。自我营销无非就是学习如何控制好自己要传达的信息，塑造好自己的形象，扩展信息送达的人群；</li>
<li>尽管有多种媒介可供你使用，但对于软件开发人员，最突出也是我个人推荐的还是博客。我认为博客就是你在互联网上的大本营，这是一个你完全能够控制信息的地方。</li>
<li>自我营销的基本机制是，要想让人们追随你、倾听你，你就要带给他们价值：你能为他们的问题提供答案，甚至是给他们带去欢乐。</li>
<li>打造成功博客的最大秘诀有且仅有一个 – 持之以恒。定好计划，然后坚持不懈，另外还需要重视博客内容品质。</li>
</ul>
<h2 id="学习篇"><a href="#学习篇" class="headerlink" title="学习篇"></a>学习篇</h2><ul>
<li>通过动手实践和教会他人，我们能学得更好。与其他的学习方式相比，主动学习是效率更高的方式。</li>
<li>十步学习法：要对自己要学的内容有个基本的了解然后利用这些信息勾勒出学习的范围，依靠这些知识找出各种资源来帮助自己学习，最后创建自己的学习计划、列出要学习的相关课程、筛选学习材料，再通过“学习-实践-掌握-教授”的过程假设理解。下面是十步学习法的示意图，第1步到第6步只做一次，集中精力完成足够多的前期调研，确保自己明确知道要学哪些内容，以及如何确认自己已达成目标，另外还需要挑选最好的资源、制定学习计划；第7步到第10步通过LDLT的方式真正领会知识。</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/12_soft_skills/learning_method.png" alt="十步学习法"></p>
<ul>
<li>寻找导师与做一名导师；</li>
<li>发现自己的知识短板：消除短板的关键就是定位短板，然后通过十步学习法用心掌握它。</li>
</ul>
<h2 id="生产力篇"><a href="#生产力篇" class="headerlink" title="生产力篇"></a>生产力篇</h2><ul>
<li>如何专注：挑选短时间片专注于单一任务（番茄工作法）、克服集中于单一任务的痛感（学会享受任务和奖励）、屏蔽打扰；</li>
<li>生产力提升计划：季度计划 -&gt; 月计划 -&gt; 周计划 -&gt; 日计划及执行，使用看板实时关注进展；</li>
<li>番茄工作法：每25分钟一个番茄，专注于当前优先级最高的任务，拥抱变化并诚实地记录中断；</li>
<li>定额工作法：给自己在确定的期限确立一个明确的目标；挑选一些需要重复去做的事情，设定一个定额，如每周写一篇博客；</li>
<li>批量处理生产效率更高，比如处理电子邮件、开短会，避免多任务同时处理；</li>
<li>职业倦怠：穿多那堵墙（很多倦怠是自然而然产生，但是如果咬牙坚持或许就是不一样的风景）；</li>
<li>追踪你的时间：了解自己每天时间的使用情况（比如RescueTime工具），避免浪费；</li>
<li>习惯主要由三个要素构成：暗示、惯例和奖励。找出坏习惯，改掉！养成好习惯。</li>
<li>任何行动往往都比没有行动好，特别是当你一直停滞在不愉快的情势下很长时间的时候。如果这是一个错误，至少你学到了一些东西。这样一来，它就不再是一个错误。如果你仍然选择停滞不前，那么你就学不到任何东西。</li>
</ul>
<h2 id="理财篇"><a href="#理财篇" class="headerlink" title="理财篇"></a>理财篇</h2><ul>
<li>金钱只是一种工具，它会带你去往任何你想去的地方，但不会取代你成为司机；</li>
<li>是成为百万富翁还是一生都靠薪水过活，选择权在你自己，而且在很大程度上取决于你在财务管理方面的知识，以及世界金额系统运行方面的知识。</li>
<li>怎样支配你的薪水：拒绝短期思维（更长远地看待薪水的分配而不仅仅是当前）；资产与负债（通过成本和价值来考虑的理财思维）；</li>
<li>怎样进行薪酬谈判：薪酬水平受声望的影响（自我营销）；先出价者输（先出价的人处于明显的劣势）；被要求先出价怎么办（先要求了解预算范围、不透露当前薪酬、了解自己值什么价钱）；</li>
<li>期权：赋予你再未来某个日期之前以固定价格购买一定数量股票的选择权。</li>
<li>规划退休计划的关键就是利用逆向思维，计算退休目标。</li>
</ul>
<h2 id="健身篇"><a href="#健身篇" class="headerlink" title="健身篇"></a>健身篇</h2><ul>
<li>人的身体就是人的灵魂的最好写照。</li>
<li>健身不仅是保持健康体魄的关键要素之一，也是灵活的、具有创造性的脑力活动的基础。健身可以增强自信心、提高创造力、减少对疾病的恐惧。</li>
<li>设置你的健身标准：挑选一个具体的目标（比如增长肌肉）、创建里程碑、对进展进行可视化。最后保持健康的生活方式。</li>
</ul>
<h2 id="精神篇"><a href="#精神篇" class="headerlink" title="精神篇"></a>精神篇</h2><ul>
<li>信念决定思想，思想决定言语，言语决定行动，行动决定习惯，习惯决定价值，价值决定命运。（by 甘地）</li>
<li>拥有正确的心态：重新启动。积极思考问题的根源是这样一种信念 – 你比你所处的环境更伟大。这种信念让你总能先看到事物好的一面，因为无论身处何处，你都有能力改变自己的未来。这是人类成就的最高信念，是世界上最强大的力量。</li>
</ul>
<h1 id="随想"><a href="#随想" class="headerlink" title="随想"></a>随想</h1><ul>
<li>转变心态，从被卖身契束缚的工人转变为一个自主管理的商人；</li>
<li>设定一个大目标，比如称为一个卓越的高效的工程师；但是这样不够清晰，那么我希望在十年后能成为一个软件开发的自由工作者，让自己依然能高效而简洁地解决编程问题，但是不受企业低效的管理束缚且有能满足高质量生活的收入水平；</li>
<li>对于2017年，希望自己能巩固好编程基础、Java语言核心特性，同时学习微服务的关键设计和实现，搭建自己熟悉的快速开发框架。每个月定时的跟踪和调整这个小目标，激励自己前行；</li>
<li>关于专业化，我想目前给自己比较好的定位还是一个企业级基础平台或中间件的工程师，因为比较喜欢深研技术而不大喜欢具体的业务实现。那么我就应该用很多时间投入到这一块的学习中，掌握常见的中间件技术，并深入理解其中的设计、原理和实现；</li>
<li>对于公司的选择，我最关注的几点：1.团队或导师是否优秀（是否能得到成长）；2.开放的技术氛围（是否高效）；3.开放的时间安排（是否能自主选择和安排）；4.项目有挑战（有挑战才有成长）；</li>
<li>关于博客，我想第一个目的还是写给自己看的（总结和记录，并经常回看和更新）；当然如果自己用心写，肯定能吸引到志同道合者一起讨论学习，从而扩展自己的社交圈和影响力。关于博客的建立，尝试过很多平台之后，我选择了Github Pages + Hexo的方式，这种方式即最大可能地减少对服务器的依赖和搭建过程的繁琐，又不缺少灵活性，使得我们能将更多精力放在写作上而不是博客的维护上；关于博客写作的频率，我希望自己一年下来最少能有60篇博文，每个月至少6篇，当然因为我经常写一些读书笔记，因此这个数量不会成为很大的挑战。另外，对于原创类的博文也要更加注重质量、结构和行文。</li>
<li>关于学习，我现在比较喜欢的就是完整的阅读书籍，相比于作者提供的十步学习法，存在几个地方存在不足：一个是关于目标的设定，在学习的过程中没有给自己明确的学习目标，这样在学习之后无法验证，所以在后续的月度计划时我会增加这一部分的内容；二是缺少全局的掌握，比如学习Java网络，我应该先了解下关于Java网络的知识以及我自己所欠缺的，然后有针对性地去学习；三是关于LDLT，现在主要的就是通过笔记来记录和分享学习的过程，后面会要求自己每次阅读结束都需要写书评或读书笔记。</li>
<li>对于生产力的提升，我现在是通过“年度计划 -&gt; 月计划 -&gt; 周计划 -&gt; 日计划”来规划的，对于年度计划和月计划，通过<a href="http://ginobefunny.com/my2017/">“小目标”</a>定期维护，而对于周计划和日计划，通过任务清单和番茄钟来管理和展示，这样可以确保自己沿着自己的目标方向前进。</li>
<li>关于习惯，现在我上班第一件事就是打开任务清单和番茄计时器，找到今天优先级最高的任务录入番茄计时器，这相当于给自己一个暗示，我今天有这么多的任务需要完成。慢慢地这就成为了一个惯例，我每次休息后第一件事就是看看番茄计时器的完全情况，如果今天完成的不错，会给自己一些奖励，比如下楼喝个奶茶、允许查看网页或者看半个小时手机。</li>
<li>关于理财，我现在比较关注的就是ETF投资，一个是因为本身积累的财富有限，而ETF的定投对于投资的额度限制比较小，另外自己对这一块也比较感兴趣。</li>
<li>关于健身，目前我对自己熬夜方面控制得还算满意。在阅读本书的过程中，我忽然有个想法，我应该增强一些力量、增长一些肌肉，好吧，加一个番茄钟，这个周末研究研究。</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;阅读&lt;a href=&quot;https://book.douban.com/subject/26835090/&quot;&gt;《软技能》&lt;/a&gt;一书的笔记和随想。&lt;br&gt;
    
    </summary>
    
      <category term="CodingLife" scheme="http://ginobefunny.com/categories/CodingLife/"/>
    
    
      <category term="读书笔记" scheme="http://ginobefunny.com/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
      <category term="自我完善" scheme="http://ginobefunny.com/tags/%E8%87%AA%E6%88%91%E5%AE%8C%E5%96%84/"/>
    
      <category term="职场" scheme="http://ginobefunny.com/tags/%E8%81%8C%E5%9C%BA/"/>
    
  </entry>
  
  <entry>
    <title>Elasticsearch Reference阅读笔记</title>
    <link href="http://ginobefunny.com/post/elasticsearch_reference_notes/"/>
    <id>http://ginobefunny.com/post/elasticsearch_reference_notes/</id>
    <published>2017-02-03T03:28:01.000Z</published>
    <updated>2017-02-04T09:38:26.258Z</updated>
    
    <content type="html"><![CDATA[<p>花了几天把<a href="https://www.elastic.co/guide/en/elasticsearch/reference/2.3/index.html" target="_blank" rel="external">Elasticsearch的官方文档</a>读了一遍，随手记一些关键的笔记。</p>
<a id="more"></a>
<h1 id="1-Getting-Started"><a href="#1-Getting-Started" class="headerlink" title="1. Getting Started"></a>1. Getting Started</h1><h2 id="1-1-Elasticsearch"><a href="#1-1-Elasticsearch" class="headerlink" title="1.1 Elasticsearch"></a>1.1 Elasticsearch</h2><p>Elasticsearch is a highly scalable open-source full-text search and analytics engine. It allows you to store, search, and analyze big volumes of data quickly and in near real time. It is generally used as the underlying engine/technology that powers applications that have complex search features and requirements.</p>
<h2 id="1-2-Sharding-is-important-for-two-primary-reasons"><a href="#1-2-Sharding-is-important-for-two-primary-reasons" class="headerlink" title="1.2 Sharding is important for two primary reasons:"></a>1.2 Sharding is important for two primary reasons:</h2><p>It allows you to horizontally split/scale your content volume<br>It allows you to distribute and parallelize operations across shards (potentially on multiple nodes) thus increasing performance/throughput </p>
<h2 id="1-3-Replication-is-important-for-two-primary-reasons"><a href="#1-3-Replication-is-important-for-two-primary-reasons" class="headerlink" title="1.3 Replication is important for two primary reasons:"></a>1.3 Replication is important for two primary reasons:</h2><p>It provides high availability in case a shard/node fails. For this reason, it is important to note that a replica shard is never allocated on the same node as the original/primary shard that it was copied from.<br>It allows you to scale out your search volume/throughput since searches can be executed on all replicas in parallel.</p>
<h2 id="1-4-Cluster-Health"><a href="#1-4-Cluster-Health" class="headerlink" title="1.4 Cluster Health"></a>1.4 Cluster Health</h2><pre><code>curl &apos;localhost:9200/_cat/health?v&apos;

curl &apos;localhost:9200/_cat/nodes?v&apos;
</code></pre><h2 id="1-5-List-All-Indices"><a href="#1-5-List-All-Indices" class="headerlink" title="1.5 List All Indices"></a>1.5 List All Indices</h2><pre><code>curl &apos;localhost:9200/_cat/indices?v&apos;
</code></pre><h2 id="1-6-Create-an-Index"><a href="#1-6-Create-an-Index" class="headerlink" title="1.6 Create an Index"></a>1.6 Create an Index</h2><pre><code>curl -XPUT &apos;localhost:9200/customer?pretty&apos;
</code></pre><h2 id="1-7-Index-and-Query-a-Document"><a href="#1-7-Index-and-Query-a-Document" class="headerlink" title="1.7 Index and Query a Document"></a>1.7 Index and Query a Document</h2><pre><code>curl -XPUT &apos;localhost:9200/customer/external/1?pretty&apos; -d &apos;
{
  &quot;name&quot;: &quot;John Doe&quot;
}&apos;

curl -XGET &apos;localhost:9200/customer/external/1?pretty&apos;
</code></pre><h2 id="1-8-Delete-an-Index"><a href="#1-8-Delete-an-Index" class="headerlink" title="1.8 Delete an Index"></a>1.8 Delete an Index</h2><pre><code>curl -XDELETE &apos;localhost:9200/customer?pretty&apos;
</code></pre><h2 id="1-9-Updating-Documents"><a href="#1-9-Updating-Documents" class="headerlink" title="1.9 Updating Documents"></a>1.9 Updating Documents</h2><p>Note though that Elasticsearch does not actually do in-place updates under the hood. Whenever we do an update, Elasticsearch deletes the old document and then indexes a new document with the update applied to it in one shot.</p>
<h2 id="1-10-Deleting-Documents"><a href="#1-10-Deleting-Documents" class="headerlink" title="1.10 Deleting Documents"></a>1.10 Deleting Documents</h2><pre><code>curl -XDELETE &apos;localhost:9200/customer/external/2?pretty&apos;
</code></pre><h2 id="1-11-Batch-Processing"><a href="#1-11-Batch-Processing" class="headerlink" title="1.11 Batch Processing"></a>1.11 Batch Processing</h2><p>In addition to being able to index, update, and delete individual documents, Elasticsearch also provides the ability to perform any of the above operations in batches using the _bulk API. This functionality is important in that it provides a very efficient mechanism to do multiple operations as fast as possible with as little network roundtrips as possible.</p>
<p>The bulk API executes all the actions sequentially and in order. If a single action fails for whatever reason, it will continue to process the remainder of the actions after it. When the bulk API returns, it will provide a status for each action (in the same order it was sent in) so that you can check if a specific action failed or not.</p>
<h2 id="1-12-The-Search-API"><a href="#1-12-The-Search-API" class="headerlink" title="1.12 The Search API"></a>1.12 The Search API</h2><p>It is important to understand that once you get your search results back, Elasticsearch is completely done with the request and does not maintain any kind of server-side resources or open cursors into your results. This is in stark contrast to many other platforms such as SQL wherein you may initially get a partial subset of your query results up-front and then you have to continuously go back to the server if you want to fetch (or page through) the rest of the results using some kind of stateful server-side cursor.</p>
<h2 id="1-13-Executing-Filters"><a href="#1-13-Executing-Filters" class="headerlink" title="1.13 Executing Filters"></a>1.13 Executing Filters</h2><p>In the previous section, we skipped over a little detail called the document score (_score field in the search results). The score is a numeric value that is a relative measure of how well the document matches the search query that we specified. The higher the score, the more relevant the document is, the lower the score, the less relevant the document is.</p>
<p>But queries do not always need to produce scores, in particular when they are only used for “filtering” the document set. Elasticsearch detects these situations and automatically optimizes query execution in order not to compute useless scores.</p>
<h2 id="1-14-Executing-Aggregations"><a href="#1-14-Executing-Aggregations" class="headerlink" title="1.14 Executing Aggregations"></a>1.14 Executing Aggregations</h2><p>Aggregations provide the ability to group and extract statistics from your data. The easiest way to think about aggregations is by roughly equating it to the SQL GROUP BY and the SQL aggregate functions. In Elasticsearch, you have the ability to execute searches returning hits and at the same time return aggregated results separate from the hits all in one response. This is very powerful and efficient in the sense that you can run queries and multiple aggregations and get the results back of both (or either) operations in one shot avoiding network roundtrips using a concise and simplified API.</p>
<h1 id="2-Setup"><a href="#2-Setup" class="headerlink" title="2. Setup"></a>2. Setup</h1><h2 id="2-1-Environment-Variables"><a href="#2-1-Environment-Variables" class="headerlink" title="2.1 Environment Variables"></a>2.1 Environment Variables</h2><p>Most times it is better to leave the default JAVA_OPTS as they are, and use the ES_JAVA_OPTS environment variable in order to set / change JVM settings or arguments.</p>
<p>The ES_HEAP_SIZE environment variable allows to set the heap memory that will be allocated to elasticsearch java process. It will allocate the same value to both min and max values, though those can be set explicitly (not recommended) by setting ES_MIN_MEM (defaults to 256m), and ES_MAX_MEM (defaults to 1g).</p>
<p>It is recommended to set the min and max memory to the same value, and enable mlockall.</p>
<h2 id="2-2-File-Descriptors"><a href="#2-2-File-Descriptors" class="headerlink" title="2.2 File Descriptors"></a>2.2 File Descriptors</h2><p>Make sure to increase the number of open files descriptors on the machine (or for the user running elasticsearch). Setting it to 32k or even 64k is recommended.</p>
<p>In order to test how many open files the process can open, start it with -Des.max-open-files set to true. This will print the number of open files the process can open on startup.</p>
<p>Alternatively, you can retrieve the max_file_descriptors for each node using the Nodes Info API, with:</p>
<pre><code>curl localhost:9200/_nodes/stats/process?pretty
</code></pre><h2 id="2-3-Virtual-memory"><a href="#2-3-Virtual-memory" class="headerlink" title="2.3 Virtual memory"></a>2.3 Virtual memory</h2><p>Elasticsearch uses a hybrid mmapfs / niofs directory by default to store its indices. The default operating system limits on mmap counts is likely to be too low, which may result in out of memory exceptions. On Linux, you can increase the limits by running the following command as root:</p>
<pre><code>sysctl -w vm.max_map_count=262144
</code></pre><p>To set this value permanently, update the vm.max_map_count setting in /etc/sysctl.conf.</p>
<h2 id="2-4-Memory-Settings"><a href="#2-4-Memory-Settings" class="headerlink" title="2.4 Memory Settings"></a>2.4 Memory Settings</h2><p>Most operating systems try to use as much memory as possible for file system caches and eagerly swap out unused application memory, possibly resulting in the elasticsearch process being swapped. Swapping is very bad for performance and for node stability, so it should be avoided at all costs.</p>
<p>There are three options: Disable swap、Configure swappiness、mlockall</p>
<h2 id="2-5-Elasticsearch-Settings"><a href="#2-5-Elasticsearch-Settings" class="headerlink" title="2.5 Elasticsearch Settings"></a>2.5 Elasticsearch Settings</h2><p>elasticsearch configuration files can be found under ES_HOME/config folder. The folder comes with two files, the elasticsearch.yml for configuring Elasticsearch different modules, and logging.yml for configuring the Elasticsearch logging.</p>
<p>The configuration format is YAML.</p>
<h2 id="2-6-Directory-Layout"><a href="#2-6-Directory-Layout" class="headerlink" title="2.6 Directory Layout"></a>2.6 Directory Layout</h2><p>zip and tar.gz<br>|Type |    Description| Location |<br>|:— |:———–|:———|<br>home  |Home of elasticsearch installation|{extract.path}<br>bin   |Binary scripts including elasticsearch to start a node|{extract.path}/bin<br>conf  |Configuration files elasticsearch.yml and logging.yml|{extract.path}/config<br>data  |The location of the data files of each index / shard allocated on the node|{extract.path}/data<br>logs  |Log files location|{extract.path}/logs<br>plugins|Plugin files location. Each plugin will be contained in a subdirectory|{extract.path}/plugins<br>repo  |Shared file system repository locations.|Not configured<br>script|Location of script files.|{extract.path}/config/scripts</p>
<h1 id="3-Breaking-changes-skipped"><a href="#3-Breaking-changes-skipped" class="headerlink" title="3 Breaking changes (skipped)"></a>3 Breaking changes (skipped)</h1><h1 id="4-API-Conventions-skipped"><a href="#4-API-Conventions-skipped" class="headerlink" title="4 API Conventions (skipped)"></a>4 API Conventions (skipped)</h1><h1 id="5-Document-APIs"><a href="#5-Document-APIs" class="headerlink" title="5 Document APIs"></a>5 Document APIs</h1><h2 id="5-1-Index-API"><a href="#5-1-Index-API" class="headerlink" title="5.1 Index API"></a>5.1 Index API</h2><p>The index API adds or updates a typed JSON document in a specific index, making it searchable. The following example inserts the JSON document into the “twitter” index, under a type called “tweet” with an id of 1:</p>
<pre><code>curl -XPUT &apos;http://localhost:9200/twitter/tweet/1&apos; -d &apos;{
    &quot;user&quot; : &quot;kimchy&quot;,
    &quot;post_date&quot; : &quot;2009-11-15T14:12:12&quot;,
    &quot;message&quot; : &quot;trying out Elasticsearch&quot;
}&apos;
</code></pre><h2 id="5-2-Automatic-Index-Creation"><a href="#5-2-Automatic-Index-Creation" class="headerlink" title="5.2 Automatic Index Creation"></a>5.2 Automatic Index Creation</h2><p>The index operation automatically creates an index if it has not been created before (check out the create index API for manually creating an index), and also automatically creates a dynamic type mapping for the specific type if one has not yet been created (check out the put mapping API for manually creating a type mapping).</p>
<h2 id="5-3-Versioning"><a href="#5-3-Versioning" class="headerlink" title="5.3 Versioning"></a>5.3 Versioning</h2><p>Each indexed document is given a version number. The associated version number is returned as part of the response to the index API request. The index API optionally allows for optimistic concurrency control when the version parameter is specified. This will control the version of the document the operation is intended to be executed against. A good example of a use case for versioning is performing a transactional read-then-update. Specifying a version from the document initially read ensures no changes have happened in the meantime (when reading in order to update, it is recommended to set preference to _primary). For example:</p>
<pre><code>curl -XPUT &apos;localhost:9200/twitter/tweet/1?version=2&apos; -d &apos;{
    &quot;message&quot; : &quot;elasticsearch now has versioning support, double cool!&quot;
}&apos;
</code></pre><h2 id="5-4-Operation-Type"><a href="#5-4-Operation-Type" class="headerlink" title="5.4 Operation Type"></a>5.4 Operation Type</h2><p>The index operation also accepts an op_type that can be used to force a create operation, allowing for “put-if-absent” behavior. When create is used, the index operation will fail if a document by that id already exists in the index.</p>
<p>Here is an example of using the op_type parameter:</p>
<pre><code>curl -XPUT &apos;http://localhost:9200/twitter/tweet/1?op_type=create&apos; -d &apos;{
    &quot;user&quot; : &quot;kimchy&quot;,
    &quot;post_date&quot; : &quot;2009-11-15T14:12:12&quot;,
    &quot;message&quot; : &quot;trying out Elasticsearch&quot;
}&apos;
</code></pre><h2 id="5-5-Routing"><a href="#5-5-Routing" class="headerlink" title="5.5 Routing"></a>5.5 Routing</h2><p>By default, shard placement — or routing — is controlled by using a hash of the document’s id value. For more explicit control, the value fed into the hash function used by the router can be directly specified on a per-operation basis using the routing parameter. For example:</p>
<pre><code>curl -XPOST &apos;http://localhost:9200/twitter/tweet?routing=kimchy&apos; -d &apos;{
    &quot;user&quot; : &quot;kimchy&quot;,
    &quot;post_date&quot; : &quot;2009-11-15T14:12:12&quot;,
    &quot;message&quot; : &quot;trying out Elasticsearch&quot;
}&apos;
</code></pre><h2 id="5-6-Parents-amp-Children-（-适合什么场景-）"><a href="#5-6-Parents-amp-Children-（-适合什么场景-）" class="headerlink" title="5.6 Parents &amp; Children （**适合什么场景?）"></a>5.6 Parents &amp; Children （<strong>**</strong>适合什么场景?）</h2><p>A child document can be indexed by specifying its parent when indexing. For example:</p>
<pre><code>curl -XPUT localhost:9200/blogs/blog_tag/1122?parent=1111 -d &apos;{
    &quot;tag&quot; : &quot;something&quot;
}&apos;
</code></pre><p>When indexing a child document, the routing value is automatically set to be the same as its parent, unless the routing value is explicitly specified using the routing parameter.</p>
<h2 id="5-7-Distributed"><a href="#5-7-Distributed" class="headerlink" title="5.7 Distributed"></a>5.7 Distributed</h2><p>The index operation is directed to the primary shard based on its route (see the Routing section above) and performed on the actual node containing this shard. After the primary shard completes the operation, if needed, the update is distributed to applicable replicas.</p>
<h2 id="5-8-Write-Consistency"><a href="#5-8-Write-Consistency" class="headerlink" title="5.8 Write Consistency"></a>5.8 Write Consistency</h2><p>To prevent writes from taking place on the “wrong” side of a network partition, by default, index operations only succeed if a quorum (&gt;replicas/2+1) of active shards are available. </p>
<h2 id="5-9-Write-Consistency-如果index设置为1个主分片，两个复制分片。当两个复制分片都不可用的时候index在主分片是否成功？复制分片可用了是否能复制？"><a href="#5-9-Write-Consistency-如果index设置为1个主分片，两个复制分片。当两个复制分片都不可用的时候index在主分片是否成功？复制分片可用了是否能复制？" class="headerlink" title="5.9 Write Consistency (**如果index设置为1个主分片，两个复制分片。当两个复制分片都不可用的时候index在主分片是否成功？复制分片可用了是否能复制？)"></a>5.9 Write Consistency (<strong>**</strong>如果index设置为1个主分片，两个复制分片。当两个复制分片都不可用的时候index在主分片是否成功？复制分片可用了是否能复制？)</h2><p>To prevent writes from taking place on the “wrong” side of a network partition, by default, index operations only succeed if a quorum (&gt;replicas/2+1) of active shards are available. </p>
<p>The index operation only returns after all active shards within the replication group have indexed the document (sync replication).</p>
<h2 id="5-10-Refresh"><a href="#5-10-Refresh" class="headerlink" title="5.10 Refresh"></a>5.10 Refresh</h2><p>To refresh the shard (not the whole index) immediately after the operation occurs, so that the document appears in search results immediately, the refresh parameter can be set to true. Setting this option to true should ONLY be done after careful thought and verification that it does not lead to poor performance, both from an indexing and a search standpoint. Note, getting a document using the get API is completely realtime and doesn’t require a refresh.</p>
<h2 id="5-11-Get-API"><a href="#5-11-Get-API" class="headerlink" title="5.11 Get API"></a>5.11 Get API</h2><p>The get API allows to get a typed JSON document from the index based on its id. The following example gets a JSON document from an index called twitter, under a type called tweet, with id valued 1:</p>
<pre><code>curl -XGET &apos;http://localhost:9200/twitter/tweet/1&apos;
</code></pre><h2 id="5-12-Preference"><a href="#5-12-Preference" class="headerlink" title="5.12 Preference"></a>5.12 Preference</h2><p>Controls a preference of which shard replicas to execute the get request on. By default, the operation is randomized between the shard replicas.</p>
<p>The preference can be set to:</p>
<ul>
<li>_primary: The operation will go and be executed only on the primary shards.</li>
<li>_local: The operation will prefer to be executed on a local allocated shard if possible.</li>
<li>Custom (string) value: A custom value will be used to guarantee that the same shards will be used for the same custom value. This can help with “jumping values” when hitting different shards in different refresh states. A sample value can be something like the web session id, or the user name.</li>
</ul>
<h2 id="5-13-Delete-API"><a href="#5-13-Delete-API" class="headerlink" title="5.13 Delete API"></a>5.13 Delete API</h2><p>The delete API allows to delete a typed JSON document from a specific index based on its id. The following example deletes the JSON document from an index called twitter, under a type called tweet, with id valued 1:</p>
<pre><code>curl -XDELETE &apos;http://localhost:9200/twitter/tweet/1&apos;
</code></pre><p>The delete operation gets hashed into a specific shard id. It then gets redirected into the primary shard within that id group, and replicated (if needed) to shard replicas within that id group.</p>
<h2 id="5-14-Update-API"><a href="#5-14-Update-API" class="headerlink" title="5.14 Update API"></a>5.14 Update API</h2><p>The update API allows to update a document based on a script provided. The operation gets the document (collocated with the shard) from the index, runs the script (with optional script language and parameters), and index back the result (also allows to delete, or ignore the operation). It uses versioning to make sure no updates have happened during the “get” and “reindex”.</p>
<p>Note, this operation still means full reindex of the document, it just removes some network roundtrips and reduces chances of version conflicts between the get and the index. The _source field needs to be enabled for this feature to work.</p>
<h2 id="5-15-Update-By-Query-API-new-and-should-still-be-considered-experimental"><a href="#5-15-Update-By-Query-API-new-and-should-still-be-considered-experimental" class="headerlink" title="5.15 Update By Query API (new and should still be considered experimental)"></a>5.15 Update By Query API (new and should still be considered experimental)</h2><p>The simplest usage of _update_by_query just performs an update on every document in the index without changing the source. This is useful to pick up a new property or some other online mapping change. Here is the API:</p>
<pre><code>curl -XPOST &apos;localhost:9200/twitter/_update_by_query?conflicts=proceed&apos;
</code></pre><p>All update and query failures cause the _update_by_query to abort and are returned in the failures of the response. The updates that have been performed still stick. In other words, the process is not rolled back, only aborted.</p>
<h2 id="5-16-Multi-Get-API"><a href="#5-16-Multi-Get-API" class="headerlink" title="5.16 Multi Get API"></a>5.16 Multi Get API</h2><p>Multi GET API allows to get multiple documents based on an index, type (optional) and id (and possibly routing). The response includes a docs array with all the fetched documents, each element similar in structure to a document provided by the get API. Here is an example:</p>
<pre><code>curl &apos;localhost:9200/_mget&apos; -d &apos;{
    &quot;docs&quot; : [
        {
            &quot;_index&quot; : &quot;test&quot;,
            &quot;_type&quot; : &quot;type&quot;,
            &quot;_id&quot; : &quot;1&quot;
        },
        {
            &quot;_index&quot; : &quot;test&quot;,
            &quot;_type&quot; : &quot;type&quot;,
            &quot;_id&quot; : &quot;2&quot;
        }
    ]
}&apos;
</code></pre><h2 id="5-17-Bulk-API"><a href="#5-17-Bulk-API" class="headerlink" title="5.17 Bulk API"></a>5.17 Bulk API</h2><p>The bulk API makes it possible to perform many index/delete operations in a single API call. This can greatly increase the indexing speed.</p>
<p>The REST API endpoint is /_bulk, and it expects the following JSON structure:</p>
<pre><code>action_and_meta_data\n
optional_source\n
action_and_meta_data\n
optional_source\n
....
action_and_meta_data\n
optional_source\n
</code></pre><p>NOTE: the final line of data must end with a newline character \n.</p>
<p>The possible actions are index, create, delete and update. index and create expect a source on the next line, and have the same semantics as the op_type parameter to the standard index API (i.e. create will fail if a document with the same index and type exists already, whereas index will add or replace a document as necessary). delete does not expect a source on the following line, and has the same semantics as the standard delete API. update expects that the partial doc, upsert and script and its options are specified on the next line.</p>
<p>Here is an example of a correct sequence of bulk commands:</p>
<pre><code>{ &quot;index&quot; : { &quot;_index&quot; : &quot;test&quot;, &quot;_type&quot; : &quot;type1&quot;, &quot;_id&quot; : &quot;1&quot; } }
{ &quot;field1&quot; : &quot;value1&quot; }
{ &quot;delete&quot; : { &quot;_index&quot; : &quot;test&quot;, &quot;_type&quot; : &quot;type1&quot;, &quot;_id&quot; : &quot;2&quot; } }
{ &quot;create&quot; : { &quot;_index&quot; : &quot;test&quot;, &quot;_type&quot; : &quot;type1&quot;, &quot;_id&quot; : &quot;3&quot; } }
{ &quot;field1&quot; : &quot;value3&quot; }
{ &quot;update&quot; : {&quot;_id&quot; : &quot;1&quot;, &quot;_type&quot; : &quot;type1&quot;, &quot;_index&quot; : &quot;index1&quot;} }
{ &quot;doc&quot; : {&quot;field2&quot; : &quot;value2&quot;} }
</code></pre><p>The endpoints are /_bulk, /{index}/_bulk, and {index}/{type}/_bulk. When the index or the index/type are provided, they will be used by default on bulk items that don’t provide them explicitly.</p>
<p>A note on the format. The idea here is to make processing of this as fast as possible. As some of the actions will be redirected to other shards on other nodes, only action_meta_data is parsed on the receiving node side.</p>
<h2 id="5-18-Reindex-API-new-and-should-still-be-considered-experimental"><a href="#5-18-Reindex-API-new-and-should-still-be-considered-experimental" class="headerlink" title="5.18 Reindex API (new and should still be considered experimental)"></a>5.18 Reindex API (new and should still be considered experimental)</h2><p>The most basic form of _reindex just copies documents from one index to another. This will copy documents from the twitter index into the new_twitter index:</p>
<pre><code>POST /_reindex
{
  &quot;source&quot;: {
    &quot;index&quot;: &quot;twitter&quot;
  },
  &quot;dest&quot;: {
    &quot;index&quot;: &quot;new_twitter&quot;
  }
}
</code></pre><p>You can limit the documents by adding a type to the source or by adding a query. This will only copy tweet’s made by kimchy into new_twitter:</p>
<pre><code>POST /_reindex
{
  &quot;source&quot;: {
    &quot;index&quot;: &quot;twitter&quot;,
    &quot;type&quot;: &quot;tweet&quot;,
    &quot;query&quot;: {
      &quot;term&quot;: {
        &quot;user&quot;: &quot;kimchy&quot;
      }
    }
  },
  &quot;dest&quot;: {
    &quot;index&quot;: &quot;new_twitter&quot;
  }
}
</code></pre><h2 id="5-19-Term-Vectors"><a href="#5-19-Term-Vectors" class="headerlink" title="5.19 Term Vectors"></a>5.19 Term Vectors</h2><p>Returns information and statistics on terms in the fields of a particular document. The document could be stored in the index or artificially provided by the user. Term vectors are realtime by default, not near realtime. This can be changed by setting realtime parameter to false.</p>
<pre><code>curl -XGET &apos;http://localhost:9200/twitter/tweet/1/_termvectors?pretty=true&apos;
</code></pre><p>Three types of values can be requested: term information, term statistics and field statistics. By default, all term information and field statistics are returned for all fields but no term statistics.</p>
<p>Term information</p>
<ul>
<li>term frequency in the field (always returned)</li>
<li>term positions (positions : true)</li>
<li>start and end offsets (offsets : true)</li>
<li>term payloads (payloads : true), as base64 encoded bytes</li>
</ul>
<p>Term statistics</p>
<ul>
<li>total term frequency (how often a term occurs in all documents)</li>
<li>document frequency (the number of documents containing the current term)</li>
</ul>
<blockquote>
<p>Setting term_statistics to true (default is false) will return term statistics. By default these values are not returned since term statistics can have a serious performance impact.</p>
</blockquote>
<p>Field statistics</p>
<ul>
<li>document count (how many documents contain this field)</li>
<li>sum of document frequencies (the sum of document frequencies for all terms in this field)</li>
<li>sum of total term frequencies (the sum of total term frequencies of each term in this field)</li>
</ul>
<p>The term and field statistics are not accurate. Deleted documents are not taken into account. The information is only retrieved for the shard the requested document resides in, unless dfs is set to true. The term and field statistics are therefore only useful as relative measures whereas the absolute numbers have no meaning in this context. By default, when requesting term vectors of artificial documents, a shard to get the statistics from is randomly selected.</p>
<p>See more examples: <a href="https://www.elastic.co/guide/en/elasticsearch/reference/2.3/docs-termvectors.html#_behaviour" target="_blank" rel="external">https://www.elastic.co/guide/en/elasticsearch/reference/2.3/docs-termvectors.html#_behaviour</a></p>
<h1 id="6-Search-APIs"><a href="#6-Search-APIs" class="headerlink" title="6 Search APIs"></a>6 Search APIs</h1><h2 id="6-1-Search"><a href="#6-1-Search" class="headerlink" title="6.1 Search"></a>6.1 Search</h2><p>The search API allows you to execute a search query and get back search hits that match the query. The query can either be provided using a simple query string as a parameter, or using a request body.</p>
<p>All search APIs can be applied across multiple types within an index, and across multiple indices with support for the multi index syntax. </p>
<h2 id="6-2-URI-Search"><a href="#6-2-URI-Search" class="headerlink" title="6.2 URI Search"></a>6.2 URI Search</h2><p>A search request can be executed purely using a URI by providing request parameters. Not all search options are exposed when executing a search using this mode, but it can be handy for quick “curl tests”. Here is an example:</p>
<pre><code>curl -XGET &apos;http://localhost:9200/twitter/tweet/_search?q=user:kimchy&apos;
</code></pre><h2 id="6-3-Request-Body-Search"><a href="#6-3-Request-Body-Search" class="headerlink" title="6.3 Request Body Search"></a>6.3 Request Body Search</h2><p>The search request can be executed with a search DSL, which includes the Query DSL, within its body. Here is an example:</p>
<pre><code>curl -XGET &apos;http://localhost:9200/twitter/tweet/_search&apos; -d &apos;{
    &quot;query&quot; : {
        &quot;term&quot; : { &quot;user&quot; : &quot;kimchy&quot; }
    }
}&apos;
</code></pre><h2 id="6-4-Query"><a href="#6-4-Query" class="headerlink" title="6.4 Query"></a>6.4 Query</h2><p>The query element within the search request body allows to define a query using the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/2.3/query-dsl.html" target="_blank" rel="external">Query DSL</a>.</p>
<pre><code>{
    &quot;query&quot; : {
        &quot;term&quot; : { &quot;user&quot; : &quot;kimchy&quot; }
    }
}
</code></pre><h2 id="6-5-From-Size"><a href="#6-5-From-Size" class="headerlink" title="6.5 From / Size"></a>6.5 From / Size</h2><p>Pagination of results can be done by using the from and size parameters.</p>
<pre><code>{
    &quot;from&quot; : 0, &quot;size&quot; : 10,
    &quot;query&quot; : {
        &quot;term&quot; : { &quot;user&quot; : &quot;kimchy&quot; }
    }
}
</code></pre><p>Note that from + size can not be more than the index.max_result_window index setting which defaults to 10,000. See the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/2.3/search-request-scroll.html" target="_blank" rel="external">Scroll</a> API for more efficient ways to do deep scrolling.</p>
<h2 id="6-6-Sort-多个字段的排序规则是怎么样的？"><a href="#6-6-Sort-多个字段的排序规则是怎么样的？" class="headerlink" title="6.6 Sort (**多个字段的排序规则是怎么样的？)"></a>6.6 Sort (<strong>**</strong>多个字段的排序规则是怎么样的？)</h2><p>Allows to add one or more sort on specific fields. Each sort can be reversed as well. The sort is defined on a per field level, with special field name for _score to sort by score, and _doc to sort by index order.</p>
<pre><code>{
    &quot;sort&quot; : [
        { &quot;post_date&quot; : {&quot;order&quot; : &quot;asc&quot;}},
        &quot;user&quot;,
        { &quot;name&quot; : &quot;desc&quot; },
        { &quot;age&quot; : &quot;desc&quot; },
        &quot;_score&quot;
    ],
    &quot;query&quot; : {
        &quot;term&quot; : { &quot;user&quot; : &quot;kimchy&quot; }
    }
}
</code></pre><p>The sort values for each document returned are also returned as part of the response.<br>The order option can have the following values:</p>
<ul>
<li>asc: Sort in ascending order</li>
<li>desc: Sort in descending order</li>
</ul>
<p>The order defaults to desc when sorting on the _score, and defaults to asc when sorting on anything else.</p>
<h2 id="6-7-Sort-mode-option"><a href="#6-7-Sort-mode-option" class="headerlink" title="6.7 Sort mode option"></a>6.7 Sort mode option</h2><p>Elasticsearch supports sorting by array or multi-valued fields. The mode option controls what array value is picked for sorting the document it belongs to. The mode option can have the following values:</p>
<ul>
<li>min: Pick the lowest value.</li>
<li>max: Pick the highest value.</li>
<li>sum: Use the sum of all values as sort value. Only applicable for number based array fields.</li>
<li>avg: Use the average of all values as sort value. Only applicable for number based array fields.</li>
<li>median: Use the median of all values as sort value. Only applicable for number based array fields.</li>
</ul>
<h2 id="6-8-Missing-Values"><a href="#6-8-Missing-Values" class="headerlink" title="6.8 Missing Values"></a>6.8 Missing Values</h2><p>The missing parameter specifies how docs which are missing the field should be treated: The missing value can be set to _last, _first, or a custom value (that will be used for missing docs as the sort value). For example:</p>
<pre><code>{
    &quot;sort&quot; : [
        { &quot;price&quot; : {&quot;missing&quot; : &quot;_last&quot;} },
    ],
    &quot;query&quot; : {
        &quot;term&quot; : { &quot;user&quot; : &quot;kimchy&quot; }
    }
}
</code></pre><h2 id="6-9-Script-Based-Sorting"><a href="#6-9-Script-Based-Sorting" class="headerlink" title="6.9 Script Based Sorting"></a>6.9 Script Based Sorting</h2><p>Allow to sort based on custom scripts, here is an example:</p>
<pre><code>{
    &quot;query&quot; : {
        ....
    },
    &quot;sort&quot; : {
        &quot;_script&quot; : {
            &quot;type&quot; : &quot;number&quot;,
            &quot;script&quot; : {
                &quot;inline&quot;: &quot;doc[&apos;field_name&apos;].value * factor&quot;,
                &quot;params&quot; : {
                    &quot;factor&quot; : 1.1
                }
            },
            &quot;order&quot; : &quot;asc&quot;
        }
    }
}
</code></pre><h2 id="6-10-Memory-Considerations"><a href="#6-10-Memory-Considerations" class="headerlink" title="6.10 Memory Considerations"></a>6.10 Memory Considerations</h2><p>When sorting, the relevant sorted field values are loaded into memory. This means that per shard, there should be enough memory to contain them. For string based types, the field sorted on should not be analyzed / tokenized. For numeric types, if possible, it is recommended to explicitly set the type to narrower types (like short, integer and float).</p>
<h2 id="6-11-Source-filtering"><a href="#6-11-Source-filtering" class="headerlink" title="6.11 Source filtering"></a>6.11 Source filtering</h2><p>Allows to control how the _source field is returned with every hit.</p>
<p>By default operations return the contents of the _source field unless you have used the fields parameter or if the _source field is disabled.</p>
<ul>
<li>To disable _source retrieval set to false.</li>
<li>The _source also accepts one or more wildcard patterns to control what parts of the _source should be returned.</li>
<li>Finally, for complete control, you can specify both include and exclude patterns.</li>
</ul>
<h2 id="6-12-Fields"><a href="#6-12-Fields" class="headerlink" title="6.12 Fields"></a>6.12 Fields</h2><blockquote>
<p>The fields parameter is about fields that are explicitly marked as stored in the mapping, which is off by default and generally not recommended. Use source filtering instead to select subsets of the original source document to be returned.</p>
</blockquote>
<h2 id="6-13-Script-Fields"><a href="#6-13-Script-Fields" class="headerlink" title="6.13 Script Fields"></a>6.13 Script Fields</h2><p>Allows to return a script evaluation (based on different fields) for each hit, for example:</p>
<pre><code>{
    &quot;query&quot; : {
        ...
    },
    &quot;script_fields&quot; : {
        &quot;test1&quot; : {
            &quot;script&quot; : &quot;_source.obj1.obj2&quot;
        },
        &quot;test2&quot; : {
            &quot;script&quot; : {
                &quot;inline&quot;: &quot;doc[&apos;my_field_name&apos;].value * factor&quot;,
                &quot;params&quot; : {
                    &quot;factor&quot;  : 2.0
                }
            }
        }
    }
}
</code></pre><p>Note the _source keyword here to navigate the json-like model.</p>
<p>It’s important to understand the difference between doc[‘my_field’].value and _source.my_field. The first, using the doc keyword, will cause the terms for that field to be loaded to memory (cached), which will result in faster execution, but more memory consumption. Also, the doc[…] notation only allows for simple valued fields (can’t return a json object from it) and make sense only on non-analyzed or single term based fields.</p>
<p>The _source on the other hand causes the source to be loaded, parsed, and then only the relevant part of the json is returned.</p>
<h2 id="6-14-Field-Data-Fields-需要了解stored和fielddata的概念"><a href="#6-14-Field-Data-Fields-需要了解stored和fielddata的概念" class="headerlink" title="6.14 Field Data Fields (**需要了解stored和fielddata的概念)"></a>6.14 Field Data Fields (<strong>**</strong>需要了解stored和fielddata的概念)</h2><p>Allows to return the field data representation of a field for each hit, for example:</p>
<pre><code>{
    &quot;query&quot; : {
        ...
    },
    &quot;fielddata_fields&quot; : [&quot;test1&quot;, &quot;test2&quot;]
}
</code></pre><p>Field data fields can work on fields that are not stored.</p>
<p>It’s important to understand that using the fielddata_fields parameter will cause the terms for that field to be loaded to memory (cached), which will result in more memory consumption.</p>
<h2 id="6-15-Post-filter"><a href="#6-15-Post-filter" class="headerlink" title="6.15 Post filter"></a>6.15 Post filter</h2><p>The post_filter is applied to the search hits at the very end of a search request, after aggregations have already been calculated.</p>
<pre><code>{
  &quot;query&quot;: {
    &quot;bool&quot;: {
      &quot;filter&quot;: {
        &quot;term&quot;: {
          &quot;brandName&quot;: &quot;vans&quot;
        }
      }
    }
  },
  &quot;aggs&quot;: {
    &quot;colors&quot;: {
      &quot;terms&quot;: {
        &quot;field&quot;: &quot;colorNames&quot;
      }
    },
    &quot;color_red&quot;: {
      &quot;filter&quot;: {
        &quot;term&quot;: {
          &quot;colorNames&quot;: &quot;红色&quot;
        }
      },
      &quot;aggs&quot;: {
        &quot;smallSorts&quot;: {
          &quot;terms&quot;: {
            &quot;field&quot;: &quot;smallSort&quot;
          }
        }
      }
    }
  },
  &quot;post_filter&quot;: {
    &quot;term&quot;: {
      &quot;colorNames&quot;: &quot;红色&quot;
    }
  }
}
</code></pre><ul>
<li>The main query now finds all products by vans, regardless of color.</li>
<li>The colors agg returns popular colors by vans.</li>
<li>The color_red agg limits the small sort sub-aggregation to red vans products.</li>
<li>Finally, the post_filter removes colors other than red from the search hits.</li>
</ul>
<h2 id="6-16-Highlighting"><a href="#6-16-Highlighting" class="headerlink" title="6.16 Highlighting"></a>6.16 Highlighting</h2><p>Allows to highlight search results on one or more fields. The implementation uses either the lucene highlighter, fast-vector-highlighter or postings-highlighter. The following is an example of the search request body:</p>
<pre><code>{
    &quot;query&quot; : {...},
    &quot;highlight&quot; : {
        &quot;fields&quot; : {
            &quot;content&quot; : {}
        }
    }
}
</code></pre><h3 id="6-16-1-Plain-highlighter"><a href="#6-16-1-Plain-highlighter" class="headerlink" title="6.16.1 Plain highlighter"></a>6.16.1 Plain highlighter</h3><p>The default choice of highlighter is of type plain and uses the Lucene highlighter. It tries hard to reflect the query matching logic in terms of understanding word importance and any word positioning criteria in phrase queries.</p>
<h3 id="6-16-2-Postings-highlighter"><a href="#6-16-2-Postings-highlighter" class="headerlink" title="6.16.2 Postings highlighter"></a>6.16.2 Postings highlighter</h3><p>If index_options is set to offsets in the mapping the postings highlighter will be used instead of the plain highlighter. The postings highlighter:</p>
<ul>
<li>Is faster since it doesn’t require to reanalyze the text to be highlighted: the larger the documents the better the performance gain should be</li>
<li>Requires less disk space than term_vectors, needed for the fast vector highlighter</li>
<li>Breaks the text into sentences and highlights them. Plays really well with natural languages, not as well with - fields containing for instance html markup</li>
<li>Treats the document as the whole corpus, and scores individual sentences as if they were documents in this corpus, using the BM25 algorithm</li>
</ul>
<h3 id="6-16-3-Fast-vector-highlighter"><a href="#6-16-3-Fast-vector-highlighter" class="headerlink" title="6.16.3 Fast vector highlighter"></a>6.16.3 Fast vector highlighter</h3><p>If term_vector information is provided by setting term_vector to with_positions_offsets in the mapping then the fast vector highlighter will be used instead of the plain highlighter. The fast vector highlighter:</p>
<ul>
<li>Is faster especially for large fields (&gt; 1MB)</li>
<li>Can be customized with boundary_chars, boundary_max_scan, and fragment_offset (see below)</li>
<li>Requires setting term_vector to with_positions_offsets which increases the size of the index</li>
<li>Can combine matches from multiple fields into one result. See matched_fields</li>
<li>Can assign different weights to matches at different positions allowing for things like phrase matches being - sorted above term matches when highlighting a Boosting Query that boosts phrase matches over term matches</li>
</ul>
<h2 id="6-17-Rescoring"><a href="#6-17-Rescoring" class="headerlink" title="6.17 Rescoring"></a>6.17 Rescoring</h2><p>Rescoring can help to improve precision by reordering just the top (eg 100 - 500) documents returned by the query and post_filter phases, using a secondary (usually more costly) algorithm, instead of applying the costly algorithm to all documents in the index.</p>
<p>A rescore request is executed on each shard before it returns its results to be sorted by the node handling the overall search request.</p>
<p>Currently the rescore API has only one implementation: the query rescorer, which uses a query to tweak the scoring. In the future, alternative rescorers may be made available, for example, a pair-wise rescorer.</p>
<p>By default the scores from the original query and the rescore query are combined linearly to produce the final _score for each document. The relative importance of the original query and of the rescore query can be controlled with the query_weight and rescore_query_weight respectively. Both default to 1.</p>
<pre><code>{
  &quot;query&quot;: {
    &quot;match&quot;: {
      &quot;productName.productName_ansj&quot;: {
        &quot;operator&quot;: &quot;or&quot;,
        &quot;query&quot;: &quot;连帽 套装&quot;,
        &quot;type&quot;: &quot;boolean&quot;
      }
    }
  },
  &quot;_source&quot;: [
    &quot;productName&quot;
  ],
  &quot;rescore&quot;: {
    &quot;window_size&quot;: 50,
    &quot;query&quot;: {
      &quot;rescore_query&quot;: {
        &quot;match&quot;: {
          &quot;productName.productName_ansj&quot;: {
            &quot;query&quot;: &quot;连帽 套装&quot;,
            &quot;type&quot;: &quot;phrase&quot;,
            &quot;slop&quot;: 2
          }
        }
      },
      &quot;query_weight&quot;: 0.7,
      &quot;rescore_query_weight&quot;: 1.2
    }
  }
}
</code></pre><p>Score Mode</p>
<ul>
<li>total:Add the original score and the rescore query score. The default.</li>
<li>multiply: Multiply the original score by the rescore query score. Useful for function query rescores.</li>
<li>avg: Average the original score and the rescore query score.</li>
<li>max: Take the max of original score and the rescore query score.</li>
<li>min: Take the min of the original score and the rescore query score.</li>
</ul>
<h2 id="6-18-Search-Type"><a href="#6-18-Search-Type" class="headerlink" title="6.18 Search Type"></a>6.18 Search Type</h2><p>There are different execution paths that can be done when executing a distributed search. The distributed search operation needs to be scattered to all the relevant shards and then all the results are gathered back. When doing scatter/gather type execution, there are several ways to do that, specifically with search engines.</p>
<p>One of the questions when executing a distributed search is how many results to retrieve from each shard. For example, if we have 10 shards, the 1st shard might hold the most relevant results from 0 till 10, with other shards results ranking below it. For this reason, when executing a request, we will need to get results from 0 till 10 from all shards, sort them, and then return the results if we want to ensure correct results.</p>
<p>Another question, which relates to the search engine, is the fact that each shard stands on its own. When a query is executed on a specific shard, it does not take into account term frequencies and other search engine information from the other shards. If we want to support accurate ranking, we would need to first gather the term frequencies from all shards to calculate global term frequencies, then execute the query on each shard using these global frequencies.</p>
<p>Also, because of the need to sort the results, getting back a large document set, or even scrolling it, while maintaining the correct sorting behavior can be a very expensive operation. For large result set scrolling, it is best to sort by _doc if the order in which documents are returned is not important.</p>
<p>Elasticsearch is very flexible and allows to control the type of search to execute on a per search request basis. The type can be configured by setting the search_type parameter in the query string. The types are:</p>
<h3 id="6-18-1-Query-Then-Fetch-query-then-fetch"><a href="#6-18-1-Query-Then-Fetch-query-then-fetch" class="headerlink" title="6.18.1 Query Then Fetch(query_then_fetch)"></a>6.18.1 Query Then Fetch(query_then_fetch)</h3><p>The request is processed in two phases. In the first phase, the query is forwarded to all involved shards. Each shard executes the search and generates a sorted list of results, local to that shard. Each shard returns just enough information to the coordinating node to allow it merge and re-sort the shard level results into a globally sorted set of results, of maximum length size.</p>
<p>During the second phase, the coordinating node requests the document content (and highlighted snippets, if any) from only the relevant shards.</p>
<p>Note: This is the default setting, if you do not specify a search_type in your request.</p>
<h3 id="6-18-2-Dfs-Query-Then-Fetch-dfs-query-then-fetch"><a href="#6-18-2-Dfs-Query-Then-Fetch-dfs-query-then-fetch" class="headerlink" title="6.18.2 Dfs, Query Then Fetch(dfs_query_then_fetch)"></a>6.18.2 Dfs, Query Then Fetch(dfs_query_then_fetch)</h3><p>Same as “Query Then Fetch”, except for an initial scatter phase which goes and computes the distributed term frequencies for more accurate scoring.</p>
<h3 id="6-18-3-Count-Deprecated-in-2-0-0-beta1"><a href="#6-18-3-Count-Deprecated-in-2-0-0-beta1" class="headerlink" title="6.18.3 Count (Deprecated in 2.0.0-beta1)"></a>6.18.3 Count (Deprecated in 2.0.0-beta1)</h3><h3 id="6-18-4-Scan-Deprecated-in-2-1-0"><a href="#6-18-4-Scan-Deprecated-in-2-1-0" class="headerlink" title="6.18.4 Scan (Deprecated in 2.1.0)"></a>6.18.4 Scan (Deprecated in 2.1.0)</h3><h2 id="6-19-Scroll"><a href="#6-19-Scroll" class="headerlink" title="6.19 Scroll"></a>6.19 Scroll</h2><p>While a search request returns a single “page” of results, the scroll API can be used to retrieve large numbers of results (or even all results) from a single search request, in much the same way as you would use a cursor on a traditional database.</p>
<p>Scrolling is not intended for real time user requests, but rather for processing large amounts of data, e.g. in order to reindex the contents of one index into a new index with a different configuration.</p>
<h2 id="6-20-Preference"><a href="#6-20-Preference" class="headerlink" title="6.20 Preference"></a>6.20 Preference</h2><p>Controls a preference of which shard replicas to execute the search request on. By default, the operation is randomized between the shard replicas.</p>
<p>The preference is a query string parameter which can be set to:</p>
<ul>
<li>_primary: The operation will go and be executed only on the primary shards.</li>
<li>_primary_first: The operation will go and be executed on the primary shard, and if not available (failover), will execute on other shards.</li>
<li>_replica: The operation will go and be executed only on a replica shard.</li>
<li>_replica_first: The operation will go and be executed only on a replica shard, and if not available (failover), will execute on other shards.</li>
<li>_local: The operation will prefer to be executed on a local allocated shard if possible.</li>
<li>_only_node:xyz: Restricts the search to execute only on a node with the provided node id (xyz in this case).</li>
<li>_prefer_node:xyz: Prefers execution on the node with the provided node id (xyz in this case) if applicable.</li>
<li>_shards:2,3: Restricts the operation to the specified shards. (2 and 3 in this case). This preference can be combined with other preferences but it has to appear first: _shards:2,3;_primary</li>
<li>_only_nodes: Restricts the operation to nodes specified in node specification <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster.html" target="_blank" rel="external">https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster.html</a></li>
<li>Custom (string) value: A custom value will be used to guarantee that the same shards will be used for the same custom value. This can help with “jumping values” when hitting different shards in different refresh states. A sample value can be something like the web session id, or the user name.</li>
</ul>
<h2 id="6-21-Explain"><a href="#6-21-Explain" class="headerlink" title="6.21 Explain"></a>6.21 Explain</h2><p>Enables explanation for each hit on how its score was computed.</p>
<pre><code>{
    &quot;explain&quot;: true,
    &quot;query&quot; : {
        &quot;term&quot; : { &quot;user&quot; : &quot;kimchy&quot; }
    }
}
</code></pre><h2 id="6-22-Version"><a href="#6-22-Version" class="headerlink" title="6.22 Version"></a>6.22 Version</h2><p>Returns a version for each search hit.</p>
<pre><code>{
    &quot;version&quot;: true,
    &quot;query&quot; : {
        &quot;term&quot; : { &quot;user&quot; : &quot;kimchy&quot; }
    }
}
</code></pre><h2 id="6-23-Index-Boost"><a href="#6-23-Index-Boost" class="headerlink" title="6.23 Index Boost"></a>6.23 Index Boost</h2><p>Allows to configure different boost level per index when searching across more than one indices. This is very handy when hits coming from one index matter more than hits coming from another index (think social graph where each user has an index).</p>
<pre><code>{
    &quot;indices_boost&quot; : {
        &quot;index1&quot; : 1.4,
        &quot;index2&quot; : 1.3
    }
}
</code></pre><h2 id="6-24-Inner-hits-Skipped-和parent-child有关系，后续一起学习"><a href="#6-24-Inner-hits-Skipped-和parent-child有关系，后续一起学习" class="headerlink" title="6.24 Inner hits (**Skipped: 和parent/child有关系，后续一起学习)"></a>6.24 Inner hits (<strong>**</strong>Skipped: 和parent/child有关系，后续一起学习)</h2><h2 id="6-25-Search-Template"><a href="#6-25-Search-Template" class="headerlink" title="6.25 Search Template"></a>6.25 Search Template</h2><p>The /_search/template endpoint allows to use the mustache language to pre render search requests, before they are executed and fill existing templates with template parameters.</p>
<pre><code>GET /_search/template
{
    &quot;inline&quot; : {
      &quot;query&quot;: { &quot;match&quot; : { &quot;{{my_field}}&quot; : &quot;{{my_value}}&quot; } },
      &quot;size&quot; : &quot;{{my_size}}&quot;
    },
    &quot;params&quot; : {
        &quot;my_field&quot; : &quot;foo&quot;,
        &quot;my_value&quot; : &quot;bar&quot;,
        &quot;my_size&quot; : 5
    }
}
</code></pre><h2 id="6-26-Search-Shards-API"><a href="#6-26-Search-Shards-API" class="headerlink" title="6.26 Search Shards API"></a>6.26 Search Shards API</h2><p>The search shards api returns the indices and shards that a search request would be executed against. This can give useful feedback for working out issues or planning optimizations with routing and shard preferences.</p>
<pre><code>curl -XGET &apos;localhost:9200/twitter/_search_shards&apos;
</code></pre><h2 id="6-27-Suggesters-Skipped"><a href="#6-27-Suggesters-Skipped" class="headerlink" title="6.27 Suggesters (Skipped)"></a>6.27 Suggesters (Skipped)</h2><p>The suggest feature suggests similar looking terms based on a provided text by using a suggester. Parts of the suggest feature are still under development.</p>
<h2 id="6-28-Count-API"><a href="#6-28-Count-API" class="headerlink" title="6.28 Count API"></a>6.28 Count API</h2><p>The count API allows to easily execute a query and get the number of matches for that query. It can be executed across one or more indices and across one or more types. The query can either be provided using a simple query string as a parameter, or using the Query DSL defined within the request body. Here is an example:</p>
<pre><code>curl -XGET &apos;http://localhost:9200/twitter/tweet/_count?q=user:kimchy&apos;

curl -XGET &apos;http://localhost:9200/twitter/tweet/_count&apos; -d &apos;
{
    &quot;query&quot; : {
        &quot;term&quot; : { &quot;user&quot; : &quot;kimchy&quot; }
    }
}&apos;
</code></pre><h2 id="6-29-Validate-API"><a href="#6-29-Validate-API" class="headerlink" title="6.29 Validate API"></a>6.29 Validate API</h2><p>The validate API allows a user to validate a potentially expensive query without executing it. The following example shows how it can be used:</p>
<pre><code>curl -XPUT &apos;http://localhost:9200/twitter/tweet/1&apos; -d &apos;{
    &quot;user&quot; : &quot;kimchy&quot;,
    &quot;post_date&quot; : &quot;2009-11-15T14:12:12&quot;,
    &quot;message&quot; : &quot;trying out Elasticsearch&quot;
}&apos;
</code></pre><p>When the query is valid, the response contains valid:true:</p>
<pre><code>curl -XGET &apos;http://localhost:9200/twitter/_validate/query?q=user:foo&apos;

{&quot;valid&quot;:true,&quot;_shards&quot;:{&quot;total&quot;:1,&quot;successful&quot;:1,&quot;failed&quot;:0}}
</code></pre><h2 id="6-30-Explain-API"><a href="#6-30-Explain-API" class="headerlink" title="6.30 Explain API"></a>6.30 Explain API</h2><p>The explain api computes a score explanation for a query and a specific document. This can give useful feedback whether a document matches or didn’t match a specific query.</p>
<pre><code>curl -XGET &apos;localhost:9200/twitter/tweet/1/_explain&apos; -d &apos;{
      &quot;query&quot; : {
        &quot;term&quot; : { &quot;message&quot; : &quot;search&quot; }
      }
}&apos;
</code></pre><h2 id="6-30-Profile-API-experimental-and-may-be-changed-or-removed"><a href="#6-30-Profile-API-experimental-and-may-be-changed-or-removed" class="headerlink" title="6.30 Profile API (experimental and may be changed or removed)"></a>6.30 Profile API (experimental and may be changed or removed)</h2><p>The Profile API provides detailed timing information about the execution of individual components in a query. It gives the user insight into how queries are executed at a low level so that the user can understand why certain queries are slow, and take steps to improve their slow queries.</p>
<p>The output from the Profile API is very verbose, especially for complicated queries executed across many shards. Pretty-printing the response is recommended to help understand the output.</p>
<pre><code>curl -XGET &apos;localhost:9200/_search&apos; -d &apos;{
  &quot;profile&quot;: true,
  &quot;query&quot; : {
    &quot;match&quot; : { &quot;message&quot; : &quot;search test&quot; }
  }
}
</code></pre><h2 id="6-31-Field-stats-API-experimental-and-may-be-changed-or-removed"><a href="#6-31-Field-stats-API-experimental-and-may-be-changed-or-removed" class="headerlink" title="6.31 Field stats API (experimental and may be changed or removed)"></a>6.31 Field stats API (experimental and may be changed or removed)</h2><p>The field stats api allows one to find statistical properties of a field without executing a search, but looking up measurements that are natively available in the Lucene index. This can be useful to explore a dataset which you don’t know much about. For example, this allows creating a histogram aggregation with meaningful intervals based on the min/max range of values.</p>
<p>The field stats api by defaults executes on all indices, but can execute on specific indices too.</p>
<p>All indices:</p>
<pre><code>curl -XGET &quot;http://localhost:9200/_field_stats?fields=rating&quot;
</code></pre><p>Specific indices:</p>
<pre><code>curl -XGET &quot;http://localhost:9200/index1,index2/_field_stats?fields=rating&quot;
</code></pre><h1 id="7-Aggregations"><a href="#7-Aggregations" class="headerlink" title="7 Aggregations"></a>7 Aggregations</h1><h2 id="7-1-Aggregations"><a href="#7-1-Aggregations" class="headerlink" title="7.1 Aggregations"></a>7.1 Aggregations</h2><p>The aggregations framework helps provide aggregated data based on a search query. It is based on simple building blocks called aggregations, that can be composed in order to build complex summaries of the data.</p>
<p>An aggregation can be seen as a unit-of-work that builds analytic information over a set of documents. The context of the execution defines what this document set is (e.g. a top-level aggregation executes within the context of the executed query/filters of the search request).</p>
<p>There are many different types of aggregations, each with its own purpose and output. To better understand these types, it is often easier to break them into three main families:</p>
<ul>
<li>Bucketing: A family of aggregations that build buckets, where each bucket is associated with a key and a document criterion. When the aggregation is executed, all the buckets criteria are evaluated on every document in the context and when a criterion matches, the document is considered to “fall in” the relevant bucket. By the end of the aggregation process, we’ll end up with a list of buckets - each one with a set of documents that “belong” to it.</li>
<li>Metric: Aggregations that keep track and compute metrics over a set of documents.</li>
<li>Pipeline: Aggregations that aggregate the output of other aggregations and their associated metrics</li>
</ul>
<p>The interesting part comes next. Since each bucket effectively defines a document set (all documents belonging to the bucket), one can potentially associate aggregations on the bucket level, and those will execute within the context of that bucket. This is where the real power of aggregations kicks in: aggregations can be nested!</p>
<h2 id="7-2-Structuring-Aggregations"><a href="#7-2-Structuring-Aggregations" class="headerlink" title="7.2 Structuring Aggregations"></a>7.2 Structuring Aggregations</h2><p>The following snippet captures the basic structure of aggregations:</p>
<pre><code>&quot;aggregations&quot; : {
    &quot;&lt;aggregation_name&gt;&quot; : {
        &quot;&lt;aggregation_type&gt;&quot; : {
            &lt;aggregation_body&gt;
        }
        [,&quot;meta&quot; : {  [&lt;meta_data_body&gt;] } ]?
        [,&quot;aggregations&quot; : { [&lt;sub_aggregation&gt;]+ } ]?
    }
    [,&quot;&lt;aggregation_name_2&gt;&quot; : { ... } ]*
}
</code></pre><h2 id="7-3-Metrics-Aggregations"><a href="#7-3-Metrics-Aggregations" class="headerlink" title="7.3 Metrics Aggregations"></a>7.3 Metrics Aggregations</h2><p>The aggregations in this family compute metrics based on values extracted in one way or another from the documents that are being aggregated. The values are typically extracted from the fields of the document (using the field data), but can also be generated using scripts.</p>
<p>Numeric metrics aggregations are a special type of metrics aggregation which output numeric values. Some aggregations output a single numeric metric (e.g. avg) and are called single-value numeric metrics aggregation, others generate multiple metrics (e.g. stats) and are called multi-value numeric metrics aggregation. The distinction between single-value and multi-value numeric metrics aggregations plays a role when these aggregations serve as direct sub-aggregations of some bucket aggregations (some bucket aggregations enable you to sort the returned buckets based on the numeric metrics in each bucket).</p>
<h3 id="7-3-1-Avg-Aggregation"><a href="#7-3-1-Avg-Aggregation" class="headerlink" title="7.3.1 Avg Aggregation"></a>7.3.1 Avg Aggregation</h3><p>A single-value metrics aggregation that computes the average of numeric values that are extracted from the aggregated documents. These values can be extracted either from specific numeric fields in the documents, or be generated by a provided script.</p>
<pre><code>{
  &quot;size&quot;: 0,
  &quot;query&quot;: {
    &quot;match_all&quot;: {}
  },
  &quot;aggs&quot;: {
    &quot;avg_price&quot;: {
      &quot;avg&quot;: {
        &quot;field&quot;: &quot;salesPrice&quot;
      }
    }
  }
}

&quot;aggregations&quot;: {
    &quot;avg_price&quot;: {
        &quot;value&quot;: 428.51063644785825
    }
}
</code></pre><h3 id="7-3-2-Cardinality-Aggregation"><a href="#7-3-2-Cardinality-Aggregation" class="headerlink" title="7.3.2 Cardinality Aggregation"></a>7.3.2 Cardinality Aggregation</h3><p>A single-value metrics aggregation that calculates an approximate count of distinct values. Values can be extracted either from specific fields in the document or generated by a script.</p>
<pre><code>{
  &quot;size&quot;: 0,
  &quot;query&quot;: {
    &quot;match_all&quot;: {}
  },
  &quot;aggs&quot;: {
    &quot;brand_count&quot;: {
      &quot;cardinality&quot;: {
        &quot;field&quot;: &quot;brandId&quot;
      }
    }
  }
}

&quot;aggregations&quot;: {
    &quot;brand_count&quot;: {
        &quot;value&quot;: 1186
    }
}
</code></pre><h3 id="7-3-3-Stats-Aggregation"><a href="#7-3-3-Stats-Aggregation" class="headerlink" title="7.3.3 Stats Aggregation"></a>7.3.3 Stats Aggregation</h3><p>A multi-value metrics aggregation that computes stats over numeric values extracted from the aggregated documents. These values can be extracted either from specific numeric fields in the documents, or be generated by a provided script.</p>
<p>The stats that are returned consist of: min, max, sum, count and avg.</p>
<pre><code>{
  &quot;size&quot;: 0,
  &quot;query&quot;: {
    &quot;match_all&quot;: {}
  },
  &quot;aggs&quot;: {
    &quot;price_stat&quot;: {
      &quot;stats&quot;: {
        &quot;field&quot;: &quot;salesPrice&quot;
      }
    }
  }
}

&quot;aggregations&quot;: {
    &quot;price_stat&quot;: {
        &quot;count&quot;: 221275,
        &quot;min&quot;: 0,
        &quot;max&quot;: 131231,
        &quot;avg&quot;: 428.51063644785825,
        &quot;sum&quot;: 94818691.07999983
    }
}
</code></pre><h3 id="7-3-4-Extended-Stats-Aggregation"><a href="#7-3-4-Extended-Stats-Aggregation" class="headerlink" title="7.3.4 Extended Stats Aggregation"></a>7.3.4 Extended Stats Aggregation</h3><p>A multi-value metrics aggregation that computes stats over numeric values extracted from the aggregated documents. These values can be extracted either from specific numeric fields in the documents, or be generated by a provided script.</p>
<p>The extended_stats aggregations is an extended version of the stats aggregation, where additional metrics are added such as sum_of_squares, variance, std_deviation and std_deviation_bounds.</p>
<pre><code>{
  &quot;size&quot;: 0,
  &quot;query&quot;: {
    &quot;match_all&quot;: {}
  },
  &quot;aggs&quot;: {
    &quot;price_stat&quot;: {
      &quot;extended_stats&quot;: {
        &quot;field&quot;: &quot;salesPrice&quot;
      }
    }
  }
}

&quot;aggregations&quot;: {
    &quot;price_stat&quot;: {
        &quot;count&quot;: 221275,
        &quot;min&quot;: 0,
        &quot;max&quot;: 131231,
        &quot;avg&quot;: 428.51063644785825,
        &quot;sum&quot;: 94818691.07999983,
        &quot;sum_of_squares&quot;: 118950750156.63016,
        &quot;variance&quot;: 353948.4012870255,
        &quot;std_deviation&quot;: 594.9356278514723,
        &quot;std_deviation_bounds&quot;: {
        &quot;upper&quot;: 1618.3818921508027,
        &quot;lower&quot;: -761.3606192550864
        }
    }
}
</code></pre><h3 id="7-3-5-Geo-Bounds-Aggregation-Skipped"><a href="#7-3-5-Geo-Bounds-Aggregation-Skipped" class="headerlink" title="7.3.5 Geo Bounds Aggregation (Skipped)"></a>7.3.5 Geo Bounds Aggregation (Skipped)</h3><h3 id="7-3-6-Geo-Centroid-Aggregation-Skipped"><a href="#7-3-6-Geo-Centroid-Aggregation-Skipped" class="headerlink" title="7.3.6 Geo Centroid Aggregation (Skipped)"></a>7.3.6 Geo Centroid Aggregation (Skipped)</h3><h3 id="7-3-7-Max-Aggregation"><a href="#7-3-7-Max-Aggregation" class="headerlink" title="7.3.7 Max Aggregation"></a>7.3.7 Max Aggregation</h3><p>A single-value metrics aggregation that keeps track and returns the maximum value among the numeric values extracted from the aggregated documents.</p>
<pre><code>&quot;aggs&quot;: {
    &quot;max_price&quot;: {
        &quot;max&quot;: {
            &quot;field&quot;: &quot;salesPrice&quot;
        }
    }
}
</code></pre><h3 id="7-3-8-Min-Aggregation"><a href="#7-3-8-Min-Aggregation" class="headerlink" title="7.3.8 Min Aggregation"></a>7.3.8 Min Aggregation</h3><p>A single-value metrics aggregation that keeps track and returns the minimum value among numeric values extracted from the aggregated documents.</p>
<pre><code>&quot;aggs&quot;: {
    &quot;min_price&quot;: {
      &quot;min&quot;: {
        &quot;field&quot;: &quot;salesPrice&quot;
      }
    }
  }
</code></pre><h3 id="7-3-9-Percentiles-Aggregation"><a href="#7-3-9-Percentiles-Aggregation" class="headerlink" title="7.3.9 Percentiles Aggregation"></a>7.3.9 Percentiles Aggregation</h3><p>A multi-value metrics aggregation that calculates one or more percentiles over numeric values extracted from the aggregated documents. </p>
<p>Percentiles show the point at which a certain percentage of observed values occur. For example, the 95th percentile is the value which is greater than 95% of the observed values.</p>
<p>When a range of percentiles are retrieved, they can be used to estimate the data distribution and determine if the data is skewed, bimodal, etc.</p>
<pre><code>&quot;aggs&quot;: {
    &quot;price_outlier&quot;: {
      &quot;percentiles&quot;: {
        &quot;field&quot;: &quot;salesPrice&quot;
      }
    }
  }

&quot;aggregations&quot;: {
    &quot;price_outlier&quot;: {
        &quot;values&quot;: {
            &quot;1.0&quot;: 19,
            &quot;5.0&quot;: 49.049088235742005,
            &quot;25.0&quot;: 148.8903318997934,
            &quot;50.0&quot;: 288.33201291736634,
            &quot;75.0&quot;: 521.2972145384141,
            &quot;95.0&quot;: 1286.9096656603726,
            &quot;99.0&quot;: 2497.931283641535
        }
    }
}
</code></pre><h3 id="7-3-10-Percentile-Ranks-Aggregation"><a href="#7-3-10-Percentile-Ranks-Aggregation" class="headerlink" title="7.3.10 Percentile Ranks Aggregation"></a>7.3.10 Percentile Ranks Aggregation</h3><p>A multi-value metrics aggregation that calculates one or more percentile ranks over numeric values extracted from the aggregated documents.</p>
<p>Percentile rank show the percentage of observed values which are below certain value. For example, if a value is greater than or equal to 95% of the observed values it is said to be at the 95th percentile rank.</p>
<pre><code>&quot;aggs&quot;: {
    &quot;price_outlier&quot;: {
      &quot;percentile_ranks&quot;: {
        &quot;field&quot;: &quot;salesPrice&quot;,
        &quot;values&quot;: [
          200,
          500
        ]
      }
    }
  }

&quot;aggregations&quot;: {
    &quot;price_outlier&quot;: {
        &quot;values&quot;: {
            &quot;200.0&quot;: 37.906112721751086,
            &quot;500.0&quot;: 74.407593883831
        }
    }
}
</code></pre><h3 id="7-3-11-Scripted-Metric-Aggregation-experimental-and-may-be-changed-or-removed"><a href="#7-3-11-Scripted-Metric-Aggregation-experimental-and-may-be-changed-or-removed" class="headerlink" title="7.3.11 Scripted Metric Aggregation (experimental and may be changed or removed)"></a>7.3.11 Scripted Metric Aggregation (experimental and may be changed or removed)</h3><p>A metric aggregation that executes using scripts to provide a metric output.</p>
<p>See a detailed example: <a href="https://www.elastic.co/guide/en/elasticsearch/reference/2.3/search-aggregations-metrics-scripted-metric-aggregation.html" target="_blank" rel="external">https://www.elastic.co/guide/en/elasticsearch/reference/2.3/search-aggregations-metrics-scripted-metric-aggregation.html</a></p>
<h3 id="7-3-12-Sum-Aggregation"><a href="#7-3-12-Sum-Aggregation" class="headerlink" title="7.3.12 Sum Aggregation"></a>7.3.12 Sum Aggregation</h3><p>A single-value metrics aggregation that sums up numeric values that are extracted from the aggregated documents.</p>
<pre><code>{
  &quot;query&quot;: {
    &quot;term&quot;: {
      &quot;brandName&quot;: &quot;vans&quot;
    }
  },
  &quot;aggs&quot;: {
    &quot;salesNum_total&quot;: {
      &quot;sum&quot;: {
        &quot;field&quot;: &quot;salesNum&quot;
      }
    }
  }
}

&quot;aggregations&quot;: {
    &quot;salesNum_total&quot;: {
        &quot;value&quot;: 253365
    }
}
</code></pre><h3 id="7-3-13-Top-hits-Aggregation"><a href="#7-3-13-Top-hits-Aggregation" class="headerlink" title="7.3.13 Top hits Aggregation"></a>7.3.13 Top hits Aggregation</h3><p>A top_hits metric aggregator keeps track of the most relevant document being aggregated. This aggregator is intended to be used as a sub aggregator, so that the top matching documents can be aggregated per bucket.</p>
<p>The top_hits aggregator can effectively be used to group result sets by certain fields via a bucket aggregator. One or more bucket aggregators determines by which properties a result set get sliced into.</p>
<p>Options:</p>
<ul>
<li>from: The offset from the first result you want to fetch.</li>
<li>size: The maximum number of top matching hits to return per bucket. By default the top three matching hits are returned.</li>
<li>sort: How the top matching hits should be sorted. By default the hits are sorted by the score of the main query.   </li>
</ul>
<h3 id="7-3-14-Value-Count-Aggregation"><a href="#7-3-14-Value-Count-Aggregation" class="headerlink" title="7.3.14 Value Count Aggregation"></a>7.3.14 Value Count Aggregation</h3><p>A single-value metrics aggregation that counts the number of values that are extracted from the aggregated documents. Typically, this aggregator will be used in conjunction with other single-value aggregations. For example, when computing the avg one might be interested in the number of values the average is computed over.</p>
<h2 id="7-4-Bucket-Aggregations"><a href="#7-4-Bucket-Aggregations" class="headerlink" title="7.4 Bucket Aggregations"></a>7.4 Bucket Aggregations</h2><p>Bucket aggregations don’t calculate metrics over fields like the metrics aggregations do, but instead, they create buckets of documents. Each bucket is associated with a criterion (depending on the aggregation type) which determines whether or not a document in the current context “falls” into it. In other words, the buckets effectively define document sets. In addition to the buckets themselves, the bucket aggregations also compute and return the number of documents that “fell into” each bucket.</p>
<p>Bucket aggregations, as opposed to metrics aggregations, can hold sub-aggregations. These sub-aggregations will be aggregated for the buckets created by their “parent” bucket aggregation.</p>
<p>There are different bucket aggregators, each with a different “bucketing” strategy. Some define a single bucket, some define fixed number of multiple buckets, and others dynamically create the buckets during the aggregation process.</p>
<h3 id="7-4-1-Children-Aggregation"><a href="#7-4-1-Children-Aggregation" class="headerlink" title="7.4.1 Children Aggregation"></a>7.4.1 Children Aggregation</h3><p>A special single bucket aggregation that enables aggregating from buckets on parent document types to buckets on child documents.</p>
<h3 id="7-4-2-Histogram-Aggregation"><a href="#7-4-2-Histogram-Aggregation" class="headerlink" title="7.4.2 Histogram Aggregation"></a>7.4.2 Histogram Aggregation</h3><p>A multi-bucket values source based aggregation that can be applied on numeric values extracted from the documents. It dynamically builds fixed size (a.k.a. interval) buckets over the values. For example, if the documents have a field that holds a price (numeric), we can configure this aggregation to dynamically build buckets with interval 5 (in case of price it may represent $5). When the aggregation executes, the price field of every document will be evaluated and will be rounded down to its closest bucket - for example, if the price is 32 and the bucket size is 5 then the rounding will yield 30 and thus the document will “fall” into the bucket that is associated with the key 30.</p>
<p>From the rounding function above it can be seen that the intervals themselves must be integers.</p>
<pre><code>{
  &quot;size&quot;: 0,
  &quot;query&quot;: {
    &quot;term&quot;: {
      &quot;brandName&quot;: &quot;vans&quot;
    }
  },
  &quot;aggs&quot;: {
    &quot;prices&quot;: {
      &quot;histogram&quot;: {
        &quot;field&quot;: &quot;salesPrice&quot;,
        &quot;interval&quot;: 200
      }
    }
  }
}

&quot;aggregations&quot;: {
    &quot;prices&quot;: {
    &quot;buckets&quot;: [
        {
        &quot;key&quot;: 0,
        &quot;doc_count&quot;: 838
        }
        ,
        {
        &quot;key&quot;: 200,
        &quot;doc_count&quot;: 1123
        }
        ,
        {
        &quot;key&quot;: 400,
        &quot;doc_count&quot;: 804
        }
        ,
        {
        &quot;key&quot;: 600,
        &quot;doc_count&quot;: 283
        }
        ,
        {
        &quot;key&quot;: 800,
        &quot;doc_count&quot;: 64
        }
        ,
        {
        &quot;key&quot;: 1000,
        &quot;doc_count&quot;: 16
        }
        ,
        {
        &quot;key&quot;: 1200,
        &quot;doc_count&quot;: 18
        }
        ,
        {
        &quot;key&quot;: 1400,
        &quot;doc_count&quot;: 8
        }
        ,
        {
        &quot;key&quot;: 1600,
        &quot;doc_count&quot;: 7
        }
        ]
    }
}
</code></pre><h3 id="7-4-3-Date-Histogram-Aggregation"><a href="#7-4-3-Date-Histogram-Aggregation" class="headerlink" title="7.4.3 Date Histogram Aggregation"></a>7.4.3 Date Histogram Aggregation</h3><p>A multi-bucket aggregation similar to the histogram except it can only be applied on date values. Since dates are represented in elasticsearch internally as long values, it is possible to use the normal histogram on dates as well, though accuracy will be compromised. The reason for this is in the fact that time based intervals are not fixed (think of leap years and on the number of days in a month). For this reason, we need special support for time based data. From a functionality perspective, this histogram supports the same features as the normal histogram. The main difference is that the interval can be specified by date/time expressions.</p>
<p>Requesting bucket intervals of a month.</p>
<pre><code>{
    &quot;aggs&quot; : {
        &quot;articles_over_time&quot; : {
            &quot;date_histogram&quot; : {
                &quot;field&quot; : &quot;date&quot;,
                &quot;interval&quot; : &quot;month&quot;
            }
        }
    }
}
</code></pre><h3 id="7-4-4-Range-Aggregation"><a href="#7-4-4-Range-Aggregation" class="headerlink" title="7.4.4 Range Aggregation"></a>7.4.4 Range Aggregation</h3><p>A multi-bucket value source based aggregation that enables the user to define a set of ranges - each representing a bucket. During the aggregation process, the values extracted from each document will be checked against each bucket range and “bucket” the relevant/matching document. Note that this aggregation includes the from value and excludes the to value for each range.</p>
<pre><code>{
  &quot;size&quot;: 0,
  &quot;query&quot;: {
    &quot;term&quot;: {
      &quot;brandName&quot;: &quot;vans&quot;
    }
  },
  &quot;aggs&quot;: {
    &quot;price_ranges&quot;: {
      &quot;range&quot;: {
        &quot;field&quot;: &quot;salesPrice&quot;,
        &quot;ranges&quot;: [
          {
            &quot;to&quot;: 200
          },
          {
            &quot;from&quot;: 200,
            &quot;to&quot;: 500
          },
          {
            &quot;from&quot;: 500
          }
        ]
      }
    }
  }
}

&quot;aggregations&quot;: {
    &quot;price_ranges&quot;: {
        &quot;buckets&quot;: [
            {
                &quot;key&quot;: &quot;*-200.0&quot;,
                &quot;to&quot;: 200,
                &quot;to_as_string&quot;: &quot;200.0&quot;,
                &quot;doc_count&quot;: 838
            }
            ,
            {
                &quot;key&quot;: &quot;200.0-500.0&quot;,
                &quot;from&quot;: 200,
                &quot;from_as_string&quot;: &quot;200.0&quot;,
                &quot;to&quot;: 500,
                &quot;to_as_string&quot;: &quot;500.0&quot;,
                &quot;doc_count&quot;: 1594
            }
            ,
            {
                &quot;key&quot;: &quot;500.0-*&quot;,
                &quot;from&quot;: 500,
                &quot;from_as_string&quot;: &quot;500.0&quot;,
                &quot;doc_count&quot;: 729
            }
        ]
    }
}
</code></pre><h3 id="7-4-5-Date-Range-Aggregation"><a href="#7-4-5-Date-Range-Aggregation" class="headerlink" title="7.4.5 Date Range Aggregation"></a>7.4.5 Date Range Aggregation</h3><p>A range aggregation that is dedicated for date values. The main difference between this aggregation and the normal range aggregation is that the from and to values can be expressed in Date Math expressions, and it is also possible to specify a date format by which the from and to response fields will be returned. Note that this aggregation includes the from value and excludes the to value for each range.</p>
<pre><code>{
    &quot;aggs&quot;: {
        &quot;range&quot;: {
            &quot;date_range&quot;: {
                &quot;field&quot;: &quot;date&quot;,
                &quot;format&quot;: &quot;MM-yyy&quot;,
                &quot;ranges&quot;: [
                    { &quot;to&quot;: &quot;now-10M/M&quot; }, 
                    { &quot;from&quot;: &quot;now-10M/M&quot; } 
                ]
            }
        }
    }
}
</code></pre><h3 id="7-4-6-Filter-Aggregation"><a href="#7-4-6-Filter-Aggregation" class="headerlink" title="7.4.6 Filter Aggregation"></a>7.4.6 Filter Aggregation</h3><p>Defines a single bucket of all the documents in the current document set context that match a specified filter. Often this will be used to narrow down the current aggregation context to a specific set of documents.</p>
<pre><code>{
  &quot;size&quot;: 0,
  &quot;query&quot;: {
    &quot;term&quot;: {
      &quot;brandName&quot;: &quot;vans&quot;
    }
  },
  &quot;aggs&quot;: {
    &quot;red_products&quot;: {
      &quot;filter&quot;: {
        &quot;term&quot;: {
          &quot;colorNames&quot;: &quot;红色&quot;
        }
      },
      &quot;aggs&quot;: {
        &quot;avg_price&quot;: {
          &quot;avg&quot;: {
            &quot;field&quot;: &quot;salesPrice&quot;
          }
        }
      }
    }
  }
}
</code></pre><h3 id="7-4-7-Filters-Aggregation"><a href="#7-4-7-Filters-Aggregation" class="headerlink" title="7.4.7 Filters Aggregation"></a>7.4.7 Filters Aggregation</h3><p>Defines a multi bucket aggregation where each bucket is associated with a filter. Each bucket will collect all documents that match its associated filter.</p>
<pre><code>{
  &quot;aggs&quot; : {
    &quot;messages&quot; : {
      &quot;filters&quot; : {
        &quot;filters&quot; : {
          &quot;errors&quot; :   { &quot;term&quot; : { &quot;body&quot; : &quot;error&quot;   }},
          &quot;warnings&quot; : { &quot;term&quot; : { &quot;body&quot; : &quot;warning&quot; }}
        }
      },
      &quot;aggs&quot; : {
        &quot;monthly&quot; : {
          &quot;histogram&quot; : {
            &quot;field&quot; : &quot;timestamp&quot;,
            &quot;interval&quot; : &quot;1M&quot;
          }
        }
      }
    }
  }
}
</code></pre><p>In the above example, we analyze log messages. The aggregation will build two collection (buckets) of log messages - one for all those containing an error, and another for all those containing a warning. And for each of these buckets it will break them down by month.Response:</p>
<pre><code>&quot;aggs&quot; : {
  &quot;messages&quot; : {
    &quot;buckets&quot; : {
      &quot;errors&quot; : {
        &quot;doc_count&quot; : 34,
        &quot;monthly&quot; : {
          &quot;buckets&quot; : [
            ... // the histogram monthly breakdown
          ]
        }
      },
      &quot;warnings&quot; : {
        &quot;doc_count&quot; : 439,
        &quot;monthly&quot; : {
          &quot;buckets&quot; : [
             ... // the histogram monthly breakdown
          ]
        }
      }
    }
  }
}
</code></pre><h3 id="7-4-8-Geo-Distance-Aggregation-Skipped"><a href="#7-4-8-Geo-Distance-Aggregation-Skipped" class="headerlink" title="7.4.8 Geo Distance Aggregation (Skipped)"></a>7.4.8 Geo Distance Aggregation (Skipped)</h3><h3 id="7-4-9-GeoHash-grid-Aggregation-Skipped"><a href="#7-4-9-GeoHash-grid-Aggregation-Skipped" class="headerlink" title="7.4.9 GeoHash grid Aggregation (Skipped)"></a>7.4.9 GeoHash grid Aggregation (Skipped)</h3><h3 id="7-4-10-Global-Aggregation"><a href="#7-4-10-Global-Aggregation" class="headerlink" title="7.4.10 Global Aggregation"></a>7.4.10 Global Aggregation</h3><p>Defines a single bucket of all the documents within the search execution context. This context is defined by the indices and the document types you’re searching on, but is not influenced by the search query itself.</p>
<h3 id="7-4-11-IPv4-Range-Aggregation"><a href="#7-4-11-IPv4-Range-Aggregation" class="headerlink" title="7.4.11 IPv4 Range Aggregation"></a>7.4.11 IPv4 Range Aggregation</h3><p>Just like the dedicated date range aggregation, there is also a dedicated range aggregation for IPv4 typed fields:</p>
<h3 id="7-4-12-Missing-Aggregation"><a href="#7-4-12-Missing-Aggregation" class="headerlink" title="7.4.12 Missing Aggregation"></a>7.4.12 Missing Aggregation</h3><p>A field data based single bucket aggregation, that creates a bucket of all documents in the current document set context that are missing a field value (effectively, missing a field or having the configured NULL value set). This aggregator will often be used in conjunction with other field data bucket aggregators (such as ranges) to return information for all the documents that could not be placed in any of the other buckets due to missing field data values.</p>
<h3 id="7-4-13-Nested-Aggregation"><a href="#7-4-13-Nested-Aggregation" class="headerlink" title="7.4.13 Nested Aggregation"></a>7.4.13 Nested Aggregation</h3><p>A special single bucket aggregation that enables aggregating nested documents.</p>
<h3 id="7-4-14-Reverse-nested-Aggregation"><a href="#7-4-14-Reverse-nested-Aggregation" class="headerlink" title="7.4.14 Reverse nested Aggregation"></a>7.4.14 Reverse nested Aggregation</h3><p>A special single bucket aggregation that enables aggregating on parent docs from nested documents. Effectively this aggregation can break out of the nested block structure and link to other nested structures or the root document, which allows nesting other aggregations that aren’t part of the nested object in a nested aggregation.</p>
<h3 id="7-4-15-Significant-Terms-Aggregation"><a href="#7-4-15-Significant-Terms-Aggregation" class="headerlink" title="7.4.15 Significant Terms Aggregation"></a>7.4.15 Significant Terms Aggregation</h3><p>An aggregation that returns interesting or unusual occurrences of terms in a set.</p>
<blockquote>
<p>Warning: The significant_terms aggregation can be very heavy when run on large indices. Work is in progress to provide more lightweight sampling techniques. As a result, the API for this feature may change in backwards incompatible ways.</p>
</blockquote>
<p>Example use cases:</p>
<ul>
<li>Suggesting “H5N1” when users search for “bird flu” in text</li>
<li>Identifying the merchant that is the “common point of compromise” from the transaction history of credit card owners reporting loss</li>
<li>Suggesting keywords relating to stock symbol $ATI for an automated news classifier</li>
<li>Spotting the fraudulent doctor who is diagnosing more than his fair share of whiplash injuries</li>
<li><p>Spotting the tire manufacturer who has a disproportionate number of blow-outs</p>
<p>  {</p>
<pre><code>&quot;query&quot;: {
  &quot;terms&quot;: {
    &quot;smallSort&quot;: [
      &quot;牛仔裤&quot;
    ]
  }
},
&quot;aggregations&quot;: {
  &quot;significantColors&quot;: {
    &quot;significant_terms&quot;: {
      &quot;field&quot;: &quot;colorNames&quot;
    }
  }
}
</code></pre><p>  }</p>
<p>  “aggregations”: {</p>
<pre><code>&quot;significantColors&quot;: {
    &quot;doc_count&quot;: 7365,
    &quot;buckets&quot;: [
        {
            &quot;key&quot;: &quot;蓝色&quot;,
            &quot;doc_count&quot;: 4750,
            &quot;score&quot;: 1.9775784118031037,
            &quot;bg_count&quot;: 35656
        }
        ,
        {
            &quot;key&quot;: &quot;蓝&quot;,
            &quot;doc_count&quot;: 1287,
            &quot;score&quot;: 0.35538801144606,
            &quot;bg_count&quot;: 12949
        }
        ,
        {
            &quot;key&quot;: &quot;原色&quot;,
            &quot;doc_count&quot;: 39,
            &quot;score&quot;: 0.09168423890725523,
            &quot;bg_count&quot;: 65
        }
        ,
        {
            &quot;key&quot;: &quot;浅蓝色&quot;,
            &quot;doc_count&quot;: 79,
            &quot;score&quot;: 0.031059308568117887,
            &quot;bg_count&quot;: 619
        }
        ,
        {
            &quot;key&quot;: &quot;水洗&quot;,
            &quot;doc_count&quot;: 10,
            &quot;score&quot;: 0.030522422208204166,
            &quot;bg_count&quot;: 13
        }
        ,
        {
            &quot;key&quot;: &quot;深蓝色&quot;,
            &quot;doc_count&quot;: 131,
            &quot;score&quot;: 0.024955048079471253,
            &quot;bg_count&quot;: 1664
        }
    ]
}
</code></pre><p>  }</p>
</li>
</ul>
<p>GINO: 为什么深蓝色排在最后？<br>在所有的商品(总数为224778)中，共有1664件商品为深蓝色；但是对于牛仔裤(总数为7365)，只有131件牛仔裤为深蓝色，因此认为他们之间的关联度很低，就不是很推荐深蓝色的牛仔裤。再试试看品牌推荐的效果：</p>
<pre><code>{
  &quot;query&quot;: {
    &quot;terms&quot;: {
      &quot;smallSort&quot;: [
        &quot;牛仔裤&quot;
      ]
    }
  },
  &quot;aggregations&quot;: {
    &quot;significantBrands&quot;: {
      &quot;significant_terms&quot;: {
        &quot;field&quot;: &quot;brandNameEn&quot;
      }
    }
  }
}

&quot;aggregations&quot;: {
    &quot;significantBrands&quot;: {
        &quot;doc_count&quot;: 7365,
        &quot;buckets&quot;: [
        {
            &quot;key&quot;: &quot;xinfeiyang&quot;,
            &quot;doc_count&quot;: 1061,
            &quot;score&quot;: 4.179762739096525,
            &quot;bg_count&quot;: 1079
        }
        ,
        {
            &quot;key&quot;: &quot;lee&quot;,
            &quot;doc_count&quot;: 432,
            &quot;score&quot;: 0.5581216486055066,
            &quot;bg_count&quot;: 1254
        }
        ,
        {
            &quot;key&quot;: &quot;levi&apos;s&quot;,
            &quot;doc_count&quot;: 473,
            &quot;score&quot;: 0.5004641576199044,
            &quot;bg_count&quot;: 1642
        }
        ,
        {
            &quot;key&quot;: &quot;jasonwood&quot;,
            &quot;doc_count&quot;: 495,
            &quot;score&quot;: 0.3789564299098067,
            &quot;bg_count&quot;: 2276
        }
        ,
        {
            &quot;key&quot;: &quot;able&quot;,
            &quot;doc_count&quot;: 304,
            &quot;score&quot;: 0.36273857444325336,
            &quot;bg_count&quot;: 948
        }
        ,
        {
            &quot;key&quot;: &quot;jeans&quot;,
            &quot;doc_count&quot;: 307,
            &quot;score&quot;: 0.3581144552023549,
            &quot;bg_count&quot;: 977
        }
        ,
        {
            &quot;key&quot;: &quot;agamemnon&quot;,
            &quot;doc_count&quot;: 119,
            &quot;score&quot;: 0.3311112854845905,
            &quot;bg_count&quot;: 169
        }
        ,
        {
            &quot;key&quot;: &quot;krbl/korakublue&quot;,
            &quot;doc_count&quot;: 183,
            &quot;score&quot;: 0.31616211163218183,
            &quot;bg_count&quot;: 407
        }
        ,
        {
            &quot;key&quot;: &quot;wrangler&quot;,
            &quot;doc_count&quot;: 198,
            &quot;score&quot;: 0.264814268306479,
            &quot;bg_count&quot;: 557
        }
        ,
        {
            &quot;key&quot;: &quot;evisu&quot;,
            &quot;doc_count&quot;: 143,
            &quot;score&quot;: 0.19733121908058618,
            &quot;bg_count&quot;: 391
        }
        ]
    }
}
</code></pre><h3 id="7-4-16-Sampler-Aggregation-experimental-and-may-be-changed-or-removed"><a href="#7-4-16-Sampler-Aggregation-experimental-and-may-be-changed-or-removed" class="headerlink" title="7.4.16 Sampler Aggregation (experimental and may be changed or removed)"></a>7.4.16 Sampler Aggregation (experimental and may be changed or removed)</h3><p>A filtering aggregation used to limit any sub aggregations’ processing to a sample of the top-scoring documents. Optionally, diversity settings can be used to limit the number of matches that share a common value such as an “author”.</p>
<p>Example use cases:</p>
<ul>
<li>Tightening the focus of analytics to high-relevance matches rather than the potentially very long tail of low-quality matches</li>
<li>Removing bias from analytics by ensuring fair representation of content from different sources</li>
<li>Reducing the running cost of aggregations that can produce useful results using only samples e.g. significant_terms</li>
</ul>
<h3 id="7-4-17-Terms-Aggregation"><a href="#7-4-17-Terms-Aggregation" class="headerlink" title="7.4.17 Terms Aggregation"></a>7.4.17 Terms Aggregation</h3><p>A multi-bucket value source based aggregation where buckets are dynamically built - one per unique value.</p>
<pre><code>{
  &quot;query&quot;: {
    &quot;terms&quot;: {
      &quot;smallSort&quot;: [
        &quot;牛仔裤&quot;
      ]
    }
  },
  &quot;aggs&quot;: {
    &quot;genders&quot;: {
      &quot;terms&quot;: {
        &quot;field&quot;: &quot;genderS&quot;
      }
    }
  }
}

&quot;aggregations&quot;: {
    &quot;genders&quot;: {
        &quot;doc_count_error_upper_bound&quot;: 0,
        &quot;sum_other_doc_count&quot;: 0,
        &quot;buckets&quot;: [
        {
            &quot;key&quot;: &quot;男&quot;,
            &quot;doc_count&quot;: 5271
        }
        ,
        {
            &quot;key&quot;: &quot;女&quot;,
            &quot;doc_count&quot;: 2255
        }
        ]
    }
}
</code></pre><p>The size parameter can be set to define how many term buckets should be returned out of the overall terms list. By default, the node coordinating the search process will request each shard to provide its own top size term buckets and once all shards respond, it will reduce the results to the final list that will then be returned to the client. This means that if the number of unique terms is greater than size, the returned list is slightly off and not accurate (it could be that the term counts are slightly off and it could even be that a term that should have been in the top size buckets was not returned). If set to 0, the size will be set to Integer.MAX_VALUE.</p>
<h2 id="7-5-Pipeline-Aggregations-experimental-and-may-be-changed-or-removed-Skipped）"><a href="#7-5-Pipeline-Aggregations-experimental-and-may-be-changed-or-removed-Skipped）" class="headerlink" title="7.5 Pipeline Aggregations (experimental and may be changed or removed, Skipped）"></a>7.5 Pipeline Aggregations (experimental and may be changed or removed, Skipped）</h2><h2 id="7-6-Caching-heavy-aggregations"><a href="#7-6-Caching-heavy-aggregations" class="headerlink" title="7.6 Caching heavy aggregations"></a>7.6 Caching heavy aggregations</h2><p>Frequently used aggregations (e.g. for display on the home page of a website) can be cached for faster responses. These cached results are the same results that would be returned by an uncached aggregation – you will never get stale results.</p>
<p>See <a href="https://www.elastic.co/guide/en/elasticsearch/reference/2.3/shard-request-cache.html" target="_blank" rel="external">Shard request cache</a> for more details.</p>
<h2 id="7-7-Returning-only-aggregation-results"><a href="#7-7-Returning-only-aggregation-results" class="headerlink" title="7.7 Returning only aggregation results"></a>7.7 Returning only aggregation results</h2><p>There are many occasions when aggregations are required but search hits are not. For these cases the hits can be ignored by setting size=0. For example:</p>
<pre><code>curl -XGET &apos;http://localhost:9200/twitter/tweet/_search&apos; -d &apos;{
  &quot;size&quot;: 0,
  &quot;aggregations&quot;: {
    &quot;my_agg&quot;: {
      &quot;terms&quot;: {
        &quot;field&quot;: &quot;text&quot;
      }
    }
  }
}
&apos;
</code></pre><p>Setting size to 0 avoids executing the fetch phase of the search making the request more efficient.</p>
<h1 id="8-Indices-APIs"><a href="#8-Indices-APIs" class="headerlink" title="8 Indices APIs"></a>8 Indices APIs</h1><p>The indices APIs are used to manage individual indices, index settings, aliases, mappings, index templates and warmers.</p>
<h2 id="8-1-Create-Index"><a href="#8-1-Create-Index" class="headerlink" title="8.1 Create Index"></a>8.1 Create Index</h2><p>The create index API allows to instantiate an index. Elasticsearch provides support for multiple indices, including executing operations across several indices.</p>
<pre><code>curl -XPUT &apos;http://localhost:9200/twitter/&apos; -d &apos;{
    &quot;settings&quot; : {
        &quot;index&quot; : {
            &quot;number_of_shards&quot; : 3, 
            &quot;number_of_replicas&quot; : 2 
        }
    }
}&apos;
</code></pre><p>The create index API allows to provide a set of one or more mappings:</p>
<pre><code>curl -XPOST localhost:9200/test -d &apos;{
    &quot;settings&quot; : {
        &quot;number_of_shards&quot; : 1
    },
    &quot;mappings&quot; : {
        &quot;type1&quot; : {
            &quot;properties&quot; : {
                &quot;field1&quot; : { &quot;type&quot; : &quot;string&quot;, &quot;index&quot; : &quot;not_analyzed&quot; }
            }
        }
    }
}&apos;
</code></pre><h2 id="8-2-Delete-Index"><a href="#8-2-Delete-Index" class="headerlink" title="8.2 Delete Index"></a>8.2 Delete Index</h2><p>The delete index API allows to delete an existing index.</p>
<pre><code>curl -XDELETE &apos;http://localhost:9200/twitter/&apos;
</code></pre><h2 id="8-3-Get-Index"><a href="#8-3-Get-Index" class="headerlink" title="8.3 Get Index"></a>8.3 Get Index</h2><p>The get index API allows to retrieve information about one or more indexes.</p>
<pre><code>curl -XGET &apos;http://localhost:9200/twitter/&apos;
</code></pre><h2 id="8-4-Indices-Exists"><a href="#8-4-Indices-Exists" class="headerlink" title="8.4 Indices Exists"></a>8.4 Indices Exists</h2><p>Used to check if the index (indices) exists or not. For example:</p>
<pre><code>curl -XHEAD -i &apos;http://localhost:9200/twitter&apos;
</code></pre><p>The HTTP status code indicates if the index exists or not. A 404 means it does not exist, and 200 means it does.</p>
<h2 id="8-5-Open-Close-Index-API"><a href="#8-5-Open-Close-Index-API" class="headerlink" title="8.5 Open / Close Index API"></a>8.5 Open / Close Index API</h2><p>The open and close index APIs allow to close an index, and later on opening it. A closed index has almost no overhead on the cluster (except for maintaining its metadata), and is blocked for read/write operations. A closed index can be opened which will then go through the normal recovery process.</p>
<p>The REST endpoint is /{index}/_close and /{index}/_open. For example:</p>
<pre><code>curl -XPOST &apos;localhost:9200/my_index/_close&apos;
curl -XPOST &apos;localhost:9200/my_index/_open&apos;
</code></pre><h2 id="8-6-Put-Mapping"><a href="#8-6-Put-Mapping" class="headerlink" title="8.6 Put Mapping"></a>8.6 Put Mapping</h2><p>The PUT mapping API allows you to provide type mappings while creating a new index, add a new type to an existing index, or add new fields to an existing type:</p>
<pre><code>PUT twitter 
{
  &quot;mappings&quot;: {
    &quot;tweet&quot;: {
      &quot;properties&quot;: {
        &quot;message&quot;: {
          &quot;type&quot;: &quot;string&quot;
        }
      }
    }
  }
}

PUT twitter/_mapping/user 
{
  &quot;properties&quot;: {
    &quot;name&quot;: {
      &quot;type&quot;: &quot;string&quot;
    }
  }
}

PUT twitter/_mapping/tweet 
{
  &quot;properties&quot;: {
    &quot;user_name&quot;: {
      &quot;type&quot;: &quot;string&quot;
    }
  }
}
</code></pre><ul>
<li>Creates an index called twitter with the message field in the tweet mapping type.</li>
<li>Uses the PUT mapping API to add a new mapping type called user.</li>
<li>Uses the PUT mapping API to add a new field called user_name to the tweet mapping type.</li>
</ul>
<h2 id="8-7-Get-Mapping"><a href="#8-7-Get-Mapping" class="headerlink" title="8.7 Get Mapping"></a>8.7 Get Mapping</h2><p>The get mapping API allows to retrieve mapping definitions for an index or index/type.</p>
<pre><code>curl -XGET &apos;http://localhost:9200/twitter/_mapping/tweet&apos;
</code></pre><h2 id="8-8-Get-Field-Mapping"><a href="#8-8-Get-Field-Mapping" class="headerlink" title="8.8 Get Field Mapping"></a>8.8 Get Field Mapping</h2><p>The get field mapping API allows you to retrieve mapping definitions for one or more fields. This is useful when you do not need the complete type mapping returned by the Get Mapping API.</p>
<p>The following returns the mapping of the field text only:</p>
<pre><code>curl -XGET &apos;http://localhost:9200/twitter/_mapping/tweet/field/text&apos;
</code></pre><h2 id="8-9-Types-Exists"><a href="#8-9-Types-Exists" class="headerlink" title="8.9 Types Exists"></a>8.9 Types Exists</h2><p>Used to check if a type/types exists in an index/indices.</p>
<pre><code>curl -XHEAD -i &apos;http://localhost:9200/twitter/tweet&apos;
</code></pre><h2 id="8-10-Index-Aliases"><a href="#8-10-Index-Aliases" class="headerlink" title="8.10 Index Aliases"></a>8.10 Index Aliases</h2><p>APIs in elasticsearch accept an index name when working against a specific index, and several indices when applicable. The index aliases API allow to alias an index with a name, with all APIs automatically converting the alias name to the actual index name. An alias can also be mapped to more than one index, and when specifying it, the alias will automatically expand to the aliases indices. An alias can also be associated with a filter that will automatically be applied when searching, and routing values. An alias cannot have the same name as an index.</p>
<p>Here is a sample of associating the alias alias1 with index test1:</p>
<pre><code>curl -XPOST &apos;http://localhost:9200/_aliases&apos; -d &apos;
{
    &quot;actions&quot; : [
        { &quot;add&quot; : { &quot;index&quot; : &quot;test1&quot;, &quot;alias&quot; : &quot;alias1&quot; } }
    ]
}&apos;
</code></pre><h3 id="Filtered-Aliases"><a href="#Filtered-Aliases" class="headerlink" title="Filtered Aliases"></a>Filtered Aliases</h3><p>Aliases with filters provide an easy way to create different “views” of the same index. The filter can be defined using Query DSL and is applied to all Search, Count, Delete By Query and More Like This operations with this alias.</p>
<p>To create a filtered alias, first we need to ensure that the fields already exist in the mapping:</p>
<pre><code>curl -XPUT &apos;http://localhost:9200/test1&apos; -d &apos;{
  &quot;mappings&quot;: {
    &quot;type1&quot;: {
      &quot;properties&quot;: {
        &quot;user&quot; : {
          &quot;type&quot;: &quot;string&quot;,
          &quot;index&quot;: &quot;not_analyzed&quot;
        }
      }
    }
  }
}&apos;
</code></pre><p>Now we can create an alias that uses a filter on field user:</p>
<pre><code>curl -XPOST &apos;http://localhost:9200/_aliases&apos; -d &apos;{
    &quot;actions&quot; : [
        {
            &quot;add&quot; : {
                 &quot;index&quot; : &quot;test1&quot;,
                 &quot;alias&quot; : &quot;alias2&quot;,
                 &quot;filter&quot; : { &quot;term&quot; : { &quot;user&quot; : &quot;kimchy&quot; } }
            }
        }
    ]
}&apos;
</code></pre><p>An alias can also be added or deleted with the endpoint</p>
<pre><code>curl -XPUT &apos;localhost:9200/logs_201305/_alias/2013&apos;
curl -XDELETE &apos;localhost:9200/logs_201305/_alias/2013&apos;
curl -XGET &apos;localhost:9200/_alias/2013&apos;
curl -XHEAD -i &apos;localhost:9200/_alias/2013&apos;
</code></pre><h2 id="8-11-Update-Indices-Settings"><a href="#8-11-Update-Indices-Settings" class="headerlink" title="8.11 Update Indices Settings"></a>8.11 Update Indices Settings</h2><p>Change specific index level settings in real time.</p>
<p>The REST endpoint is /_settings (to update all indices) or {index}/_settings to update one (or more) indices settings.</p>
<pre><code>curl -XPUT &apos;localhost:9200/my_index/_settings&apos; -d &apos;
{
    &quot;index&quot; : {
        &quot;number_of_replicas&quot; : 4
    }
}&apos;
</code></pre><h3 id="Bulk-Indexing-Usage"><a href="#Bulk-Indexing-Usage" class="headerlink" title="Bulk Indexing Usage"></a>Bulk Indexing Usage</h3><p>For example, the update settings API can be used to dynamically change the index from being more performant for bulk indexing, and then move it to more real time indexing state. Before the bulk indexing is started, use:</p>
<pre><code>curl -XPUT localhost:9200/test/_settings -d &apos;{
    &quot;index&quot; : {
        &quot;refresh_interval&quot; : &quot;-1&quot;
    } }&apos;
</code></pre><p>(Another optimization option is to start the index without any replicas, and only later adding them, but that really depends on the use case).</p>
<p>Then, once bulk indexing is done, the settings can be updated (back to the defaults for example):</p>
<pre><code>curl -XPUT localhost:9200/test/_settings -d &apos;{
    &quot;index&quot; : {
        &quot;refresh_interval&quot; : &quot;1s&quot;
    } }&apos;
</code></pre><p>And, a force merge should be called:</p>
<pre><code>curl -XPOST &apos;http://localhost:9200/test/_forcemerge?max_num_segments=5&apos;
</code></pre><h2 id="8-12-Get-Settings"><a href="#8-12-Get-Settings" class="headerlink" title="8.12 Get Settings"></a>8.12 Get Settings</h2><p>The get settings API allows to retrieve settings of index/indices:</p>
<pre><code>curl -XGET &apos;http://localhost:9200/twitter/_settings&apos;
</code></pre><h2 id="8-13-Analyze"><a href="#8-13-Analyze" class="headerlink" title="8.13 Analyze"></a>8.13 Analyze</h2><p>Performs the analysis process on a text and return the tokens breakdown of the text.</p>
<p>Can be used without specifying an index against one of the many built in analyzers:</p>
<pre><code>curl -XGET &apos;localhost:9200/twitter/_analyze&apos; -d &apos;
{
  &quot;analyzer&quot; : &quot;standard&quot;,
  &quot;text&quot; : &quot;this is a test&quot;
}&apos;
</code></pre><p>All parameters can also supplied as request parameters. For example:</p>
<pre><code>curl -XGET &apos;localhost:9200/_analyze?tokenizer=keyword&amp;filter=lowercase&amp;text=this+is+a+test&apos;
</code></pre><h3 id="Explain-Analyze"><a href="#Explain-Analyze" class="headerlink" title="Explain Analyze"></a>Explain Analyze</h3><p>If you want to get more advanced details, set explain to true (defaults to false). It will output all token attributes for each token. You can filter token attributes you want to output by setting attributes option.</p>
<pre><code>POST productindex/_analyze
{
  &quot;field&quot;: &quot;productName.productName_ansj&quot;,
  &quot;text&quot;: &quot;S.T.A.M.P.S./诗坦表 时尚PU皮表带&quot;,
  &quot;explain&quot;: true
}
</code></pre><h2 id="8-14-Index-Templates"><a href="#8-14-Index-Templates" class="headerlink" title="8.14 Index Templates"></a>8.14 Index Templates</h2><p>Index templates allow you to define templates that will automatically be applied when new indices are created. The templates include both settings and mappings, and a simple pattern template that controls whether the template should be applied to the new index.</p>
<h2 id="8-15-Shadow-replica-indices-experimental-and-may-be-changed-or-removed"><a href="#8-15-Shadow-replica-indices-experimental-and-may-be-changed-or-removed" class="headerlink" title="8.15 Shadow replica indices(experimental and may be changed or removed)"></a>8.15 Shadow replica indices(experimental and may be changed or removed)</h2><p>If you would like to use a shared filesystem, you can use the shadow replicas settings to choose where on disk the data for an index should be kept, as well as how Elasticsearch should replay operations on all the replica shards of an index.</p>
<h2 id="8-16-Indices-Stats"><a href="#8-16-Indices-Stats" class="headerlink" title="8.16 Indices Stats"></a>8.16 Indices Stats</h2><p>Indices level stats provide statistics on different operations happening on an index. The API provides statistics on the index level scope (though most stats can also be retrieved using node level scope).</p>
<p>The following returns high level aggregation and index level stats for all indices:</p>
<pre><code>curl localhost:9200/_stats
</code></pre><p>Specific index stats can be retrieved using:</p>
<pre><code>curl localhost:9200/index1,index2/_stats
</code></pre><p>By default, all stats are returned, returning only specific stats can be specified as well in the URI. Those stats can be any of:</p>
<ul>
<li>docs:The number of docs / deleted docs (docs not yet merged out). Note, affected by refreshing the index.</li>
<li>store: The size of the index.</li>
<li>indexing: Indexing statistics, can be combined with a comma separated list of types to provide document type level stats.</li>
<li>get: Get statistics, including missing stats.</li>
<li>search: Search statistics. You can include statistics for custom groups by adding an extra groups parameter (search operations can be associated with one or more groups). The groups parameter accepts a comma separated list of group names. Use _all to return statistics for all groups.</li>
<li>completion: Completion suggest statistics.</li>
<li>fielddata: Fielddata statistics.</li>
<li>flush:Flush statistics.</li>
<li>merge:Merge statistics.</li>
<li>request_cache: Shard request cache statistics.</li>
<li>refresh: Refresh statistics.</li>
<li>suggest: Suggest statistics.</li>
<li>warmer: Warmer statistics.</li>
<li>translog: Translog statistics.</li>
</ul>
<h2 id="8-17-Indices-Segments"><a href="#8-17-Indices-Segments" class="headerlink" title="8.17 Indices Segments"></a>8.17 Indices Segments</h2><p>Provide low level segments information that a Lucene index (shard level) is built with. Allows to be used to provide more information on the state of a shard and an index, possibly optimization information, data “wasted” on deletes, and so on.</p>
<p>Endpoints include segments for a specific index, several indices, or all:</p>
<pre><code>curl -XGET &apos;http://localhost:9200/test/_segments&apos;
curl -XGET &apos;http://localhost:9200/test1,test2/_segments&apos;
curl -XGET &apos;http://localhost:9200/_segments&apos;
</code></pre><ul>
<li>_0: The key of the JSON document is the name of the segment. This name is used to generate file names: all files starting with this segment name in the directory of the shard belong to this segment.</li>
<li>generation: A generation number that is basically incremented when needing to write a new segment. The segment name is derived from this generation number.</li>
<li>num_docs: The number of non-deleted documents that are stored in this segment.</li>
<li>deleted_docs: The number of deleted documents that are stored in this segment. It is perfectly fine if this number is greater than 0, space is going to be reclaimed when this segment gets merged.</li>
<li>size_in_bytes: The amount of disk space that this segment uses, in bytes.</li>
<li>memory_in_bytes: Segments need to store some data into memory in order to be searchable efficiently. This number returns the number of bytes that are used for that purpose. A value of -1 indicates that Elasticsearch was not able to compute this number.</li>
<li>committed: Whether the segment has been sync’ed on disk. Segments that are committed would survive a hard reboot. No need to worry in case of false, the data from uncommitted segments is also stored in the transaction log so that Elasticsearch is able to replay changes on the next start.</li>
<li>search: Whether the segment is searchable. A value of false would most likely mean that the segment has been written to disk but no refresh occurred since then to make it searchable.</li>
<li>version: The version of Lucene that has been used to write this segment.</li>
<li>compound: Whether the segment is stored in a compound file. When true, this means that Lucene merged all files from the segment in a single one in order to save file descriptors.</li>
</ul>
<h2 id="8-18-Indices-Recovery-Advanced-Topic"><a href="#8-18-Indices-Recovery-Advanced-Topic" class="headerlink" title="8.18 Indices Recovery(Advanced Topic)"></a>8.18 Indices Recovery(Advanced Topic)</h2><p>The indices recovery API provides insight into on-going index shard recoveries. Recovery status may be reported for specific indices, or cluster-wide.</p>
<h2 id="8-19-Indices-Shard-Stores"><a href="#8-19-Indices-Shard-Stores" class="headerlink" title="8.19 Indices Shard Stores"></a>8.19 Indices Shard Stores</h2><p>Provides store information for shard copies of indices. Store information reports on which nodes shard copies exist, the shard copy version, indicating how recent they are, and any exceptions encountered while opening the shard index or from earlier engine failure.</p>
<p>By default, only lists store information for shards that have at least one unallocated copy. When the cluster health status is yellow, this will list store information for shards that have at least one unassigned replica. When the cluster health status is red, this will list store information for shards, which has unassigned primaries.</p>
<p>Endpoints include shard stores information for a specific index, several indices, or all:</p>
<pre><code>curl -XGET &apos;http://localhost:9200/test/_shard_stores&apos;
curl -XGET &apos;http://localhost:9200/test1,test2/_shard_stores&apos;
curl -XGET &apos;http://localhost:9200/_shard_stores&apos;
</code></pre><h2 id="8-20-Clear-Cache"><a href="#8-20-Clear-Cache" class="headerlink" title="8.20 Clear Cache"></a>8.20 Clear Cache</h2><p>The clear cache API allows to clear either all caches or specific cached associated with one or more indices.</p>
<pre><code>curl -XPOST &apos;http://localhost:9200/twitter/_cache/clear&apos;
</code></pre><p>The API, by default, will clear all caches. Specific caches can be cleaned explicitly by setting query, fielddata or request.</p>
<p>All caches relating to a specific field(s) can also be cleared by specifying fields parameter with a comma delimited list of the relevant fields.</p>
<h2 id="8-21-Flush"><a href="#8-21-Flush" class="headerlink" title="8.21 Flush"></a>8.21 Flush</h2><p>The flush API allows to flush one or more indices through an API. The flush process of an index basically frees memory from the index by flushing data to the index storage and clearing the internal transaction log. By default, Elasticsearch uses memory heuristics in order to automatically trigger flush operations as required in order to clear memory.</p>
<pre><code>POST /twitter/_flush
</code></pre><p>The flush API accepts the following request parameters:</p>
<ul>
<li>wait_if_ongoing: If set to true the flush operation will block until the flush can be executed if another flush operation is already executing. The default is false and will cause an exception to be thrown on the shard level if another flush operation is already running.</li>
<li>force: Whether a flush should be forced even if it is not necessarily needed ie. if no changes will be committed to the index. This is useful if transaction log IDs should be incremented even if no uncommitted changes are present. (This setting can be considered as internal)</li>
</ul>
<h2 id="8-22-Synced-Flush-Advanced-Topic"><a href="#8-22-Synced-Flush-Advanced-Topic" class="headerlink" title="8.22 Synced Flush(Advanced Topic)"></a>8.22 Synced Flush(Advanced Topic)</h2><p>Elasticsearch tracks the indexing activity of each shard. Shards that have not received any indexing operations for 5 minutes are automatically marked as inactive. This presents an opportunity for Elasticsearch to reduce shard resources and also perform a special kind of flush, called synced flush. A synced flush performs a normal flush, then adds a generated unique marker (sync_id) to all shards.</p>
<h2 id="8-23-Refresh"><a href="#8-23-Refresh" class="headerlink" title="8.23 Refresh"></a>8.23 Refresh</h2><p>The refresh API allows to explicitly refresh one or more index, making all operations performed since the last refresh available for search. The (near) real-time capabilities depend on the index engine used. For example, the internal one requires refresh to be called, but by default a refresh is scheduled periodically.</p>
<pre><code>curl -XPOST &apos;http://localhost:9200/twitter/_refresh&apos;
</code></pre><h2 id="8-24-Force-Merge"><a href="#8-24-Force-Merge" class="headerlink" title="8.24 Force Merge"></a>8.24 Force Merge</h2><p>The force merge API allows to force merging of one or more indices through an API. The merge relates to the number of segments a Lucene index holds within each shard. The force merge operation allows to reduce the number of segments by merging them.</p>
<p>This call will block until the merge is complete. If the http connection is lost, the request will continue in the background, and any new requests will block until the previous force merge is complete.</p>
<pre><code>curl -XPOST &apos;http://localhost:9200/twitter/_forcemerge&apos;
</code></pre><p>The force merge API accepts the following request parameters:</p>
<ul>
<li>max_num_segments: The number of segments to merge to. To fully merge the index, set it to 1. Defaults to simply checking if a merge needs to execute, and if so, executes it.</li>
<li>only_expunge_deletes: Should the merge process only expunge segments with deletes in it. In Lucene, a document is not deleted from a segment, just marked as deleted. During a merge process of segments, a new segment is created that does not have those deletes. This flag allows to only merge segments that have deletes. Defaults to false. Note that this won’t override the index.merge.policy.expunge_deletes_allowed threshold.</li>
<li>flush: Should a flush be performed after the forced merge. Defaults to true.</li>
</ul>
<h1 id="9-cat-APIs"><a href="#9-cat-APIs" class="headerlink" title="9 cat APIs"></a>9 cat APIs</h1><p>JSON is great… for computers. Even if it’s pretty-printed, trying to find relationships in the data is tedious. Human eyes, especially when looking at an ssh terminal, need compact and aligned text. The cat API aims to meet this need.</p>
<p>All the cat commands accept a query string parameter help to see all the headers and info they provide, and the /_cat command alone lists all the available commands.</p>
<h2 id="9-1-cat-aliases"><a href="#9-1-cat-aliases" class="headerlink" title="9.1 cat aliases"></a>9.1 cat aliases</h2><p>aliases shows information about currently configured aliases to indices including filter and routing infos.</p>
<pre><code>curl &apos;localhost:9200/_cat/aliases?v&apos;
</code></pre><h2 id="9-2-cat-allocation"><a href="#9-2-cat-allocation" class="headerlink" title="9.2 cat allocation"></a>9.2 cat allocation</h2><p>allocation provides a snapshot of how many shards are allocated to each data node and how much disk space they are using.</p>
<pre><code>curl &apos;localhost:9200/_cat/allocation?v&apos;
</code></pre><h2 id="9-3-cat-count"><a href="#9-3-cat-count" class="headerlink" title="9.3 cat count"></a>9.3 cat count</h2><p>count provides quick access to the document count of the entire cluster, or individual indices.</p>
<pre><code>curl &apos;localhost:9200/_cat/indices?v&apos;
green wiki1 3 0 10000 331 168.5mb 168.5mb
green wiki2 3 0   428   0     8mb     8mb
curl &apos;localhost:9200/_cat/count?v&apos;
1384314124582 19:42:04 10428
curl &apos;localhost:9200/_cat/count/wiki2?v&apos;
1384314139815 19:42:19 428
</code></pre><h2 id="9-4-cat-fielddata"><a href="#9-4-cat-fielddata" class="headerlink" title="9.4 cat fielddata"></a>9.4 cat fielddata</h2><p>fielddata shows how much heap memory is currently being used by fielddata on every data node in the cluster.</p>
<pre><code>curl &apos;localhost:9200/_cat/fielddata?v&apos;
id                     host    ip            node          total   body    text
c223lARiSGeezlbrcugAYQ myhost1 10.20.100.200 Jessica Jones 385.6kb 159.8kb 225.7kb
waPCbitNQaCL6xC8VxjAwg myhost2 10.20.100.201 Adversary     435.2kb 159.8kb 275.3kb
yaDkp-G3R0q1AJ-HUEvkSQ myhost3 10.20.100.202 Microchip     284.6kb 109.2kb 175.3kb
</code></pre><p>Fields can be specified either as a query parameter, or in the URL path.</p>
<h2 id="9-5-cat-health"><a href="#9-5-cat-health" class="headerlink" title="9.5 cat health"></a>9.5 cat health</h2><p>health is a terse, one-line representation of the same information from /_cluster/health. It has one option ts to disable the timestamping.</p>
<pre><code>curl &apos;localhost:9200/_cat/health?v&amp;ts=0&apos;
cluster status nodeTotal nodeData shards pri relo init unassign tasks
foo     green          3        3      3   3    0    0        0     0
</code></pre><h2 id="9-6-cat-indices"><a href="#9-6-cat-indices" class="headerlink" title="9.6 cat indices"></a>9.6 cat indices</h2><p>The indices command provides a cross-section of each index. This information spans nodes.</p>
<pre><code>curl &apos;localhost:9200/_cat/indices?v&apos;
</code></pre><h2 id="9-7-cat-master"><a href="#9-7-cat-master" class="headerlink" title="9.7 cat master"></a>9.7 cat master</h2><p>master doesn’t have any extra options. It simply displays the master’s node ID, bound IP address, and node name.</p>
<pre><code>curl &apos;localhost:9200/_cat/master?v&apos;
id                     ip            node
Ntgn2DcuTjGuXlhKDUD4vA 192.168.56.30 Solarr
</code></pre><h2 id="9-8-cat-nodeattrs"><a href="#9-8-cat-nodeattrs" class="headerlink" title="9.8 cat nodeattrs"></a>9.8 cat nodeattrs</h2><p>The nodeattrs command shows custom node attributes.</p>
<pre><code>curl &apos;localhost:9200/_cat/nodeattrs?v&apos;
node       host    ip          attr  value
Black Bolt epsilon 192.168.1.8 rack  rack314
Black Bolt epsilon 192.168.1.8 azone us-east-1    
</code></pre><h2 id="9-9-cat-nodes"><a href="#9-9-cat-nodes" class="headerlink" title="9.9 cat nodes"></a>9.9 cat nodes</h2><p>The nodes command shows the cluster topology.</p>
<pre><code>curl &apos;localhost:9200/_cat/nodes?v&apos;
SP4H 4727 192.168.56.30 9300 2.3.4 1.8.0_73 72.1gb 35.4 93.9mb 79 239.1mb 0.45 3.4h d m Boneyard
_uhJ 5134 192.168.56.10 9300 2.3.4 1.8.0_73 72.1gb 33.3 93.9mb 85 239.1mb 0.06 3.4h d * Athena
HfDp 4562 192.168.56.20 9300 2.3.4 1.8.0_73 72.2gb 74.5 93.9mb 83 239.1mb 0.12 3.4h d m Zarek
</code></pre><h2 id="9-10-cat-pending-tasks"><a href="#9-10-cat-pending-tasks" class="headerlink" title="9.10 cat pending tasks"></a>9.10 cat pending tasks</h2><p>pending_tasks provides the same information as the /_cluster/pending_tasks API in a convenient tabular format.</p>
<pre><code>curl &apos;localhost:9200/_cat/pending_tasks?v&apos;
</code></pre><h2 id="9-11-cat-plugins"><a href="#9-11-cat-plugins" class="headerlink" title="9.11 cat plugins"></a>9.11 cat plugins</h2><p>The plugins command provides a view per node of running plugins. This information spans nodes.</p>
<pre><code>curl &apos;localhost:9200/_cat/plugins?v&apos;
</code></pre><h2 id="9-12-cat-recovery"><a href="#9-12-cat-recovery" class="headerlink" title="9.12 cat recovery"></a>9.12 cat recovery</h2><p>The recovery command is a view of index shard recoveries, both on-going and previously completed. It is a more compact view of the JSON recovery API.</p>
<pre><code>curl &apos;localhost:9200/_cat/recovery?v&apos;
</code></pre><h2 id="9-13-cat-repositories"><a href="#9-13-cat-repositories" class="headerlink" title="9.13 cat repositories"></a>9.13 cat repositories</h2><p>The repositories command shows the snapshot repositories registered in the cluster.</p>
<pre><code>curl &apos;localhost:9200/_cat/repositories?v&apos;
</code></pre><h2 id="9-14-cat-thread-pool"><a href="#9-14-cat-thread-pool" class="headerlink" title="9.14 cat thread pool"></a>9.14 cat thread pool</h2><p>The thread_pool command shows cluster wide thread pool statistics per node. By default the active, queue and rejected statistics are returned for the bulk, index and search thread pools.</p>
<pre><code>curl &apos;localhost:9200/_cat/thread_pool?v&apos;
</code></pre><h2 id="9-15-cat-shards"><a href="#9-15-cat-shards" class="headerlink" title="9.15 cat shards"></a>9.15 cat shards</h2><p>The shards command is the detailed view of what nodes contain which shards. It will tell you if it’s a primary or replica, the number of docs, the bytes it takes on disk, and the node where it’s located.</p>
<p>Here we see a single index, with three primary shards and no replicas:</p>
<pre><code>curl &apos;localhost:9200/_cat/shards?v&apos;
</code></pre><h2 id="9-16-cat-segments"><a href="#9-16-cat-segments" class="headerlink" title="9.16 cat segments"></a>9.16 cat segments</h2><p>The segments command provides low level information about the segments in the shards of an index. It provides information similar to the _segments endpoint.</p>
<pre><code>curl &apos;http://localhost:9200/_cat/segments?v&apos;
</code></pre><h2 id="9-17-cat-snapshots"><a href="#9-17-cat-snapshots" class="headerlink" title="9.17 cat snapshots"></a>9.17 cat snapshots</h2><p>The snapshots command shows all snapshots that belong to a specific repository. To find a list of available repositories to query, the command /_cat/repositories can be used. Querying the snapshots of a repository named repo1 then looks as follows.</p>
<pre><code>curl &apos;localhost:9200/_cat/snapshots/repo1?v&apos;
</code></pre><h1 id="10-Cluster-APIs"><a href="#10-Cluster-APIs" class="headerlink" title="10. Cluster APIs"></a>10. Cluster APIs</h1><h2 id="10-1-Cluster-Health"><a href="#10-1-Cluster-Health" class="headerlink" title="10.1 Cluster Health"></a>10.1 Cluster Health</h2><p>The cluster health API allows to get a very simple status on the health of the cluster.</p>
<pre><code>curl &apos;http://localhost:9200/_cluster/health?pretty=true&apos;
{
  &quot;cluster_name&quot; : &quot;testcluster&quot;,
  &quot;status&quot; : &quot;green&quot;,
  &quot;timed_out&quot; : false,
  &quot;number_of_nodes&quot; : 2,
  &quot;number_of_data_nodes&quot; : 2,
  &quot;active_primary_shards&quot; : 5,
  &quot;active_shards&quot; : 10,
  &quot;relocating_shards&quot; : 0,
  &quot;initializing_shards&quot; : 0,
  &quot;unassigned_shards&quot; : 0,
  &quot;delayed_unassigned_shards&quot;: 0,
  &quot;number_of_pending_tasks&quot; : 0,
  &quot;number_of_in_flight_fetch&quot;: 0,
  &quot;task_max_waiting_in_queue_millis&quot;: 0,
  &quot;active_shards_percent_as_number&quot;: 100
}
</code></pre><h2 id="10-2-Cluster-State"><a href="#10-2-Cluster-State" class="headerlink" title="10.2 Cluster State"></a>10.2 Cluster State</h2><p>The cluster state API allows to get a comprehensive state information of the whole cluster.</p>
<pre><code>curl &apos;http://localhost:9200/_cluster/state&apos;
</code></pre><h2 id="10-3-Cluster-Stats"><a href="#10-3-Cluster-Stats" class="headerlink" title="10.3 Cluster Stats"></a>10.3 Cluster Stats</h2><p>The Cluster Stats API allows to retrieve statistics from a cluster wide perspective. The API returns basic index metrics (shard numbers, store size, memory usage) and information about the current nodes that form the cluster (number, roles, os, jvm versions, memory usage, cpu and installed plugins).</p>
<pre><code>curl -XGET &apos;http://localhost:9200/_cluster/stats?human&amp;pretty&apos;
</code></pre><h2 id="10-4-Pending-cluster-tasks"><a href="#10-4-Pending-cluster-tasks" class="headerlink" title="10.4 Pending cluster tasks"></a>10.4 Pending cluster tasks</h2><p>The pending cluster tasks API returns a list of any cluster-level changes (e.g. create index, update mapping, allocate or fail shard) which have not yet been executed.</p>
<pre><code>curl -XGET &apos;http://localhost:9200/_cluster/pending_tasks&apos;
</code></pre><h2 id="10-5-Cluster-Reroute-Advanced-Topic"><a href="#10-5-Cluster-Reroute-Advanced-Topic" class="headerlink" title="10.5 Cluster Reroute(Advanced Topic)"></a>10.5 Cluster Reroute(Advanced Topic)</h2><p>The reroute command allows to explicitly execute a cluster reroute allocation command including specific commands. For example, a shard can be moved from one node to another explicitly, an allocation can be canceled, or an unassigned shard can be explicitly allocated on a specific node.</p>
<p>Here is a short example of how a simple reroute API call:</p>
<pre><code>curl -XPOST &apos;localhost:9200/_cluster/reroute&apos; -d &apos;{
    &quot;commands&quot; : [ {
        &quot;move&quot; :
            {
              &quot;index&quot; : &quot;test&quot;, &quot;shard&quot; : 0,
              &quot;from_node&quot; : &quot;node1&quot;, &quot;to_node&quot; : &quot;node2&quot;
            }
        },
        {
          &quot;allocate&quot; : {
              &quot;index&quot; : &quot;test&quot;, &quot;shard&quot; : 1, &quot;node&quot; : &quot;node3&quot;
          }
        }
    ]
}&apos;
</code></pre><h2 id="10-6-Cluster-Update-Settings"><a href="#10-6-Cluster-Update-Settings" class="headerlink" title="10.6 Cluster Update Settings"></a>10.6 Cluster Update Settings</h2><p>Allows to update cluster wide specific settings. Settings updated can either be persistent (applied cross restarts) or transient (will not survive a full cluster restart). Here is an example:</p>
<pre><code>curl -XPUT localhost:9200/_cluster/settings -d &apos;{
    &quot;persistent&quot; : {
        &quot;discovery.zen.minimum_master_nodes&quot; : 2
    }
}&apos;
</code></pre><p>Or:</p>
<pre><code>curl -XPUT localhost:9200/_cluster/settings -d &apos;{
    &quot;transient&quot; : {
        &quot;discovery.zen.minimum_master_nodes&quot; : 2
    }
}&apos;
</code></pre><p>The cluster responds with the settings updated. So the response for the last example will be:</p>
<pre><code>{
    &quot;persistent&quot; : {},
    &quot;transient&quot; : {
        &quot;discovery.zen.minimum_master_nodes&quot; : &quot;2&quot;
    }
}
</code></pre><p>Cluster wide settings can be returned using:</p>
<pre><code>curl -XGET localhost:9200/_cluster/settings 
</code></pre><h2 id="10-7-Nodes-statistics"><a href="#10-7-Nodes-statistics" class="headerlink" title="10.7 Nodes statistics"></a>10.7 Nodes statistics</h2><p>The cluster nodes stats API allows to retrieve one or more (or all) of the cluster nodes statistics.</p>
<pre><code>curl -XGET &apos;http://localhost:9200/_nodes/stats&apos;
curl -XGET &apos;http://localhost:9200/_nodes/nodeId1,nodeId2/stats&apos;
</code></pre><p>By default, all stats are returned. You can limit this by combining any of indices, os, process, jvm, transport, http, fs, breaker and thread_pool. For example:</p>
<ul>
<li>indices: Indices stats about size, document count, indexing and deletion times, search times, field cache size, merges and flushes</li>
<li>fs: File system information, data path, free disk space, read/write stats (see FS information)</li>
<li>http: HTTP connection information</li>
<li>jvm: JVM stats, memory pool information, garbage collection, buffer pools, number of loaded/unloaded classes</li>
<li>os: Operating system stats, load average, mem, swap (see OS statistics)</li>
<li>process: Process statistics, memory consumption, cpu usage, open file descriptors (see Process statistics)</li>
<li>thread_pool: Statistics about each thread pool, including current size, queue and rejected tasks</li>
<li>transport: Transport statistics about sent and received bytes in cluster communication</li>
<li>breaker: Statistics about the field data circuit breaker</li>
</ul>
<h2 id="10-8-Nodes-Info"><a href="#10-8-Nodes-Info" class="headerlink" title="10.8 Nodes Info"></a>10.8 Nodes Info</h2><p>The cluster nodes info API allows to retrieve one or more (or all) of the cluster nodes information.</p>
<pre><code>curl -XGET &apos;http://localhost:9200/_nodes&apos;
curl -XGET &apos;http://localhost:9200/_nodes/nodeId1,nodeId2&apos;
</code></pre><p>By default, it just returns all attributes and core settings for a node. It also allows to get only information on settings, os, process, jvm, thread_pool, transport, http and plugins.</p>
<h2 id="10-9-Nodes-hot-threads"><a href="#10-9-Nodes-hot-threads" class="headerlink" title="10.9 Nodes hot_threads"></a>10.9 Nodes hot_threads</h2><p>An API allowing to get the current hot threads on each node in the cluster. Endpoints are /_nodes/hot_threads, and /_nodes/{nodesIds}/hot_threads.</p>
<pre><code>curl -XGET &apos;http://localhost:9200/_nodes/hot_threads&apos;
</code></pre><p>The output is plain text with a breakdown of each node’s top hot threads. Parameters allowed are:</p>
<ul>
<li>threads: number of hot threads to provide, defaults to 3.</li>
<li>interval: the interval to do the second sampling of threads. Defaults to 500ms.</li>
<li>type: The type to sample, defaults to cpu, but supports wait and block to see hot threads that are in wait or block state.</li>
<li>ignore_idle_threads: If true, known idle threads (e.g. waiting in a socket select, or to get a task from an empty queue) are filtered out. Defaults to true.</li>
</ul>
<h1 id="11-Query-DSL"><a href="#11-Query-DSL" class="headerlink" title="11. Query DSL"></a>11. Query DSL</h1><p>Elasticsearch provides a full Query DSL based on JSON to define queries. Think of the Query DSL as an AST of queries, consisting of two types of clauses:</p>
<ul>
<li>Leaf query clauses look for a particular value in a particular field, such as the match, term or range queries. These queries can be used by themselves.</li>
<li>Compound query clauses wrap other leaf or compound queries and are used to combine multiple queries in a logical fashion (such as the bool or dis_max query), or to alter their behaviour (such as the not or constant_score query).</li>
</ul>
<p>Query clauses behave differently depending on whether they are used in query context or filter context.</p>
<p><strong>Query context</strong><br>A query clause used in query context answers the question “How well does this document match this query clause?” Besides deciding whether or not the document matches, the query clause also calculates a _score representing how well the document matches, relative to other documents.</p>
<p>Query context is in effect whenever a query clause is passed to a query parameter, such as the query parameter in the search API.</p>
<p><strong>Filter context</strong><br>In filter context, a query clause answers the question “Does this document match this query clause?” The answer is a simple Yes or No – no scores are calculated. Filter context is mostly used for filtering structured data, e.g.</p>
<pre><code>Does this timestamp fall into the range 2015 to 2016?
Is the status field set to &quot;published&quot;?
</code></pre><p>Frequently used filters will be cached automatically by Elasticsearch, to speed up performance.</p>
<p>Filter context is in effect whenever a query clause is passed to a filter parameter, such as the filter or must_not parameters in the bool query, the filter parameter in the constant_score query, or the filter aggregation.</p>
<h2 id="11-1-Match-All-Query"><a href="#11-1-Match-All-Query" class="headerlink" title="11.1 Match All Query"></a>11.1 Match All Query</h2><p>The most simple query, which matches all documents, giving them all a _score of 1.0.</p>
<pre><code>{ &quot;match_all&quot;: {} }
</code></pre><h2 id="11-2-Full-text-queries"><a href="#11-2-Full-text-queries" class="headerlink" title="11.2 Full text queries"></a>11.2 Full text queries</h2><p>The high-level full text queries are usually used for running full text queries on full text fields like the body of an email. They understand how the field being queried is analyzed and will apply each field’s analyzer (or search_analyzer) to the query string before executing.</p>
<p>The queries in this group are:</p>
<ul>
<li>match query:The standard query for performing full text queries, including fuzzy matching and phrase or proximity queries.</li>
<li>multi_match query: The multi-field version of the match query.</li>
<li>common_terms query: A more specialized query which gives more preference to uncommon words.</li>
<li>query_string query: Supports the compact Lucene query string syntax, allowing you to specify AND|OR|NOT conditions and multi-field search within a single query string. For expert users only.</li>
<li>simple_query_string: A simpler, more robust version of the query_string syntax suitable for exposing directly to users.</li>
</ul>
<h3 id="11-2-1-Match-Query"><a href="#11-2-1-Match-Query" class="headerlink" title="11.2.1 Match Query"></a>11.2.1 Match Query</h3><p>A family of match queries that accepts text/numerics/dates, analyzes them, and constructs a query. For example:</p>
<pre><code>POST /productindex/_search
{
  &quot;query&quot;: {
    &quot;match&quot;: {
      &quot;productName.productName_ansj&quot;: &quot;vans鞋&quot;
    }
  }
}
</code></pre><p>There are three types of match query: boolean, phrase, and phrase_prefix:</p>
<p><strong>boolean</strong><br>The default match query is of type boolean. It means that the text provided is analyzed and the analysis process constructs a boolean query from the provided text. The <em>operator</em> flag can be set to or or and to control the boolean clauses (defaults to or). The minimum number of optional should clauses to match can be set using the <em>minimum_should_match</em> parameter.</p>
<p>The <em>analyzer</em> can be set to control which analyzer will perform the analysis process on the text. It defaults to the field explicit mapping definition, or the default search analyzer.</p>
<p>The <em>lenient</em> parameter can be set to true to ignore exceptions caused by data-type mismatches, such as trying to query a numeric field with a text query string. Defaults to false.</p>
<pre><code>POST /productindex/_search
{
  &quot;query&quot;: {
    &quot;match&quot;: {
      &quot;productName.productName_ansj&quot;: {
        &quot;query&quot;: &quot;vans鞋&quot;,
        &quot;operator&quot;: &quot;and&quot;
      }
    }
  }
}
</code></pre><p><strong>phrase</strong><br>The match_phrase query analyzes the text and creates a phrase query out of the analyzed text. For example:</p>
<pre><code>{
    &quot;match_phrase&quot; : {
        &quot;message&quot; : &quot;this is a test&quot;
    }
}
</code></pre><p>Since match_phrase is only a type of a match query, it can also be used in the following manner:</p>
<pre><code>{
    &quot;match&quot; : {
        &quot;message&quot; : {
            &quot;query&quot; : &quot;this is a test&quot;,
            &quot;type&quot; : &quot;phrase&quot;
        }
    }
}
</code></pre><p><strong>match_phrase_prefix</strong><br>The match_phrase_prefix is the same as match_phrase, except that it allows for prefix matches on the last term in the text. For example:</p>
<pre><code>{
    &quot;match_phrase_prefix&quot; : {
        &quot;message&quot; : &quot;quick brown f&quot;
    }
}
</code></pre><h3 id="11-2-2-Multi-Match-Query"><a href="#11-2-2-Multi-Match-Query" class="headerlink" title="11.2.2 Multi Match Query"></a>11.2.2 Multi Match Query</h3><p>The multi_match query builds on the match query to allow multi-field queries:</p>
<pre><code>{
  &quot;multi_match&quot; : {
    &quot;query&quot;:    &quot;this is a test&quot;, 
    &quot;fields&quot;: [ &quot;subject^3&quot;, &quot;message&quot; ] 
  }
}
</code></pre><p>Individual fields can be boosted with the caret (^) notation.</p>
<p><strong>Types of multi_match query:</strong></p>
<ul>
<li>best_fields: (default) Finds documents which match any field, but uses the _score from the best field. </li>
<li>most_fields: Finds documents which match any field and combines the _score from each field. </li>
<li>cross_fields: Treats fields with the same analyzer as though they were one big field. Looks for each word in any field. </li>
<li>phrase: Runs a match_phrase query on each field and combines the _score from each field. </li>
<li>phrase_prefix: Runs a match_phrase_prefix query on each field and combines the _score from each field. </li>
</ul>
<p><strong>best_fields</strong></p>
<pre><code>{
  &quot;multi_match&quot; : {
    &quot;query&quot;:      &quot;brown fox&quot;,
    &quot;type&quot;:       &quot;best_fields&quot;,
    &quot;fields&quot;:     [ &quot;subject^3&quot;, &quot;message&quot; ],
    &quot;tie_breaker&quot;: 0.3
  }
}
</code></pre><p>Normally the best_fields type uses the score of the single best matching field, but if tie_breaker is specified, then it calculates the score as follows:</p>
<ul>
<li>the score from the best matching field</li>
<li>plus tie_breaker * _score for all other matching fields</li>
</ul>
<p>Also, accepts analyzer, boost, operator, minimum_should_match, fuzziness, prefix_length, max_expansions, rewrite, zero_terms_query and cutoff_frequency, as explained in match query.</p>
<p>The best_fields and most_fields types are field-centric – they generate a match query per field. This means that the operator and minimum_should_match parameters are applied to each field individually, which is probably not what you want.In other words, <strong>all terms</strong> must be present <strong>in a single field</strong> for a document to match.</p>
<p><strong>cross_fields</strong><br>The cross_fields type is particularly useful with structured documents where multiple fields should match. For instance, when querying the first_name and last_name fields for “Will Smith”, the best match is likely to have “Will” in one field and “Smith” in the other.</p>
<p>This sounds like a job for most_fields but there are two problems with that approach：</p>
<ul>
<li>The first problem is that operator and minimum_should_match are applied per-field, instead of per-term (see explanation above).</li>
<li>The second problem is to do with relevance: the different term frequencies in the first_name and last_name fields can produce unexpected results.</li>
</ul>
<p>In other words, <strong>all terms</strong> must be present <strong>in at least one field</strong> for a document to match. (Compare this to the logic used for best_fields and most_fields.)</p>
<p>One way of dealing with these types of queries is simply to index the first_name and last_name fields into a single full_name field. Of course, this can only be done at index time.</p>
<p><strong>most_fields</strong><br>The most_fields type is most useful when querying multiple fields that contain the same text analyzed in different ways. For instance, the main field may contain synonyms, stemming and terms without diacritics. A second field may contain the original terms, and a third field might contain shingles. By combining scores from all three fields we can match as many documents as possible with the main field, but use the second and third fields to push the most similar results to the top of the list.</p>
<p><strong>phrase and phrase_prefix</strong><br>The phrase and phrase_prefix types behave just like best_fields, but they use a match_phrase or match_phrase_prefix query instead of a match query.</p>
<h3 id="11-2-3-Common-Terms-Query-Advanced-Topic"><a href="#11-2-3-Common-Terms-Query-Advanced-Topic" class="headerlink" title="11.2.3 Common Terms Query (Advanced Topic)"></a>11.2.3 Common Terms Query (Advanced Topic)</h3><p>The common terms query is a modern alternative to stopwords which improves the precision and recall of search results (by taking stopwords into account), without sacrificing performance.</p>
<p>The common terms query divides the query terms into two groups: more important (ie low frequency terms) and less important (ie high frequency terms which would previously have been stopwords).</p>
<p>First it searches for documents which match the more important terms. These are the terms which appear in fewer documents and have a greater impact on relevance.</p>
<p>Then, it executes a second query for the less important terms – terms which appear frequently and have a low impact on relevance. But instead of calculating the relevance score for all matching documents, it only calculates the _score for documents already matched by the first query. In this way the high frequency terms can improve the relevance calculation without paying the cost of poor performance.</p>
<h3 id="11-2-4-Query-String-Query-Advanced-Topic"><a href="#11-2-4-Query-String-Query-Advanced-Topic" class="headerlink" title="11.2.4 Query String Query (Advanced Topic)"></a>11.2.4 Query String Query (Advanced Topic)</h3><p>A query that uses a query parser in order to parse its content. Here is an example:</p>
<pre><code>{
    &quot;query_string&quot; : {
        &quot;default_field&quot; : &quot;content&quot;,
        &quot;query&quot; : &quot;this AND that OR thus&quot;
    }
}
</code></pre><h3 id="11-2-5-Simple-Query-String-Query-Advanced-Topic"><a href="#11-2-5-Simple-Query-String-Query-Advanced-Topic" class="headerlink" title="11.2.5 Simple Query String Query (Advanced Topic)"></a>11.2.5 Simple Query String Query (Advanced Topic)</h3><p>A query that uses the SimpleQueryParser to parse its context. Unlike the regular query_string query, the simple_query_string query will never throw an exception, and discards invalid parts of the query. Here is an example:</p>
<pre><code>{
    &quot;simple_query_string&quot; : {
        &quot;query&quot;: &quot;\&quot;fried eggs\&quot; +(eggplant | potato) -frittata&quot;,
        &quot;analyzer&quot;: &quot;snowball&quot;,
        &quot;fields&quot;: [&quot;body^5&quot;,&quot;_all&quot;],
        &quot;default_operator&quot;: &quot;and&quot;
    }
}
</code></pre><h2 id="11-3-Term-level-queries"><a href="#11-3-Term-level-queries" class="headerlink" title="11.3 Term level queries"></a>11.3 Term level queries</h2><p>While the full text queries will analyze the query string before executing, the term-level queries operate on the exact terms that are stored in the inverted index.</p>
<p>These queries are usually used for structured data like numbers, dates, and enums, rather than full text fields. Alternatively, they allow you to craft low-level queries, foregoing the analysis process.</p>
<p>The queries in this group are:</p>
<ul>
<li>term query: Find documents which contain the exact term specified in the field specified.</li>
<li>terms query: Find documents which contain any of the exact terms specified in the field specified.</li>
<li>range query: Find documents where the field specified contains values (dates, numbers, or strings) in the range specified.</li>
<li>exists query: Find documents where the field specified contains any non-null value.</li>
<li>missing query: Find documents where the field specified does is missing or contains only null values.</li>
<li>prefix query: Find documents where the field specified contains terms which begin with the exact prefix specified.</li>
<li>wildcard query: Find documents where the field specified contains terms which match the pattern specified, where the pattern supports single character wildcards (?) and multi-character wildcards (*)</li>
<li>regexp query: Find documents where the field specified contains terms which match the regular expression specified.</li>
<li>fuzzy query: Find documents where the field specified contains terms which are fuzzily similar to the specified term. Fuzziness is measured as a Levenshtein edit distance of 1 or 2.</li>
<li>type query: Find documents of the specified type.</li>
<li>ids query: Find documents with the specified type and IDs.</li>
</ul>
<h3 id="11-3-1-Term-Query"><a href="#11-3-1-Term-Query" class="headerlink" title="11.3.1 Term Query"></a>11.3.1 Term Query</h3><pre><code>{
  &quot;query&quot;: {
    &quot;term&quot;: {
      &quot;productSkn&quot;: &quot;51022624&quot;
    }
  }
}
</code></pre><p>A boost parameter can be specified to give this term query a higher relevance score than another query, for instance:</p>
<pre><code>GET /_search
{
  &quot;query&quot;: {
    &quot;bool&quot;: {
      &quot;should&quot;: [
        {
          &quot;term&quot;: {
            &quot;status&quot;: {
              &quot;value&quot;: &quot;urgent&quot;,
              &quot;boost&quot;: 2.0 
            }
          }
        },
        {
          &quot;term&quot;: {
            &quot;status&quot;: &quot;normal&quot; 
          }
        }
      ]
    }
  }
}
</code></pre><h3 id="11-3-2-Terms-Query"><a href="#11-3-2-Terms-Query" class="headerlink" title="11.3.2 Terms Query"></a>11.3.2 Terms Query</h3><p>Filters documents that have fields that match any of the provided terms (not analyzed). For example:</p>
<pre><code>{
  &quot;query&quot;: {
    &quot;terms&quot;: {
      &quot;brandId&quot;: [
        &quot;144&quot;,
        &quot;248&quot;
      ]
    }
  }
}

更高效的方法是使用constant_score.filter
{
    &quot;constant_score.&quot; : {
        &quot;filter&quot; : {
            &quot;terms&quot; : { &quot;brandId&quot; : [&quot;144&quot;, &quot;248&quot;]}
        }
    }
}

search on all the tweets that match the followers of user 2
curl -XGET localhost:9200/tweets/_search -d &apos;{
  &quot;query&quot; : {
    &quot;terms&quot; : {
      &quot;user&quot; : {
        &quot;index&quot; : &quot;users&quot;,
        &quot;type&quot; : &quot;user&quot;,
        &quot;id&quot; : &quot;2&quot;,
        &quot;path&quot; : &quot;followers&quot;
      }
    }
  }
}&apos;
</code></pre><h3 id="11-3-3-Range-Query"><a href="#11-3-3-Range-Query" class="headerlink" title="11.3.3 Range Query"></a>11.3.3 Range Query</h3><p>Matches documents with fields that have terms within a certain range. The type of the Lucene query depends on the field type, for string fields, the TermRangeQuery, while for number/date fields, the query is a NumericRangeQuery. </p>
<pre><code>{
  &quot;query&quot;: {
    &quot;range&quot;: {
      &quot;salesNum&quot;: {
        &quot;gte&quot;: &quot;5&quot;,
        &quot;lte&quot;: &quot;100&quot;
      }
    }
  }
}
</code></pre><p>The range query accepts the following parameters:</p>
<ul>
<li>gte： Greater-than or equal to</li>
<li>gt： Greater-than</li>
<li>lte： Less-than or equal to</li>
<li>lt： Less-than</li>
<li>boost： Sets the boost value of the query, defaults to 1.0</li>
</ul>
<h3 id="11-3-4-Exists-Query"><a href="#11-3-4-Exists-Query" class="headerlink" title="11.3.4 Exists Query"></a>11.3.4 Exists Query</h3><p>Returns documents that have at least one non-null value in the original field:</p>
<pre><code>{
    &quot;exists&quot; : { &quot;field&quot; : &quot;user&quot; }
}
</code></pre><p>The exists query can advantageously replace the missing query (Missing Query Deprecated in 2.2.0) when used inside a must_not clause as follows:</p>
<pre><code>&quot;bool&quot;: {
    &quot;must_not&quot;: {
        &quot;exists&quot;: {
            &quot;field&quot;: &quot;user&quot;
        }
    }
}
</code></pre><p>This query returns documents that have no value in the user field.</p>
<h3 id="11-3-5-Prefix-Query"><a href="#11-3-5-Prefix-Query" class="headerlink" title="11.3.5 Prefix Query"></a>11.3.5 Prefix Query</h3><p>Matches documents that have fields containing terms with a specified prefix (not analyzed). The prefix query maps to Lucene PrefixQuery. The following matches documents where the user field contains a term that starts with ki:</p>
<pre><code>{
  &quot;query&quot;: {
    &quot;prefix&quot;: {
      &quot;productName&quot;: &quot;VA&quot;
    }
  }
}
</code></pre><h3 id="11-3-6-Wildcard-Query"><a href="#11-3-6-Wildcard-Query" class="headerlink" title="11.3.6 Wildcard Query"></a>11.3.6 Wildcard Query</h3><p>Matches documents that have fields matching a wildcard expression (not analyzed). Supported wildcards are <em>, which matches any character sequence (including the empty one), and ?, which matches any single character. Note that this query can be slow, as it needs to iterate over many terms. In order to prevent extremely slow wildcard queries, a wildcard term should not start with one of the wildcards </em> or ?. The wildcard query maps to Lucene WildcardQuery.</p>
<pre><code>{
  &quot;query&quot;: {
    &quot;wildcard&quot;: {
      &quot;productName&quot;: &quot;VA*鞋*&quot;
    }
  }
}
</code></pre><h3 id="11-3-7-Regexp-Query"><a href="#11-3-7-Regexp-Query" class="headerlink" title="11.3.7 Regexp Query"></a>11.3.7 Regexp Query</h3><p>The regexp query allows you to use regular expression term queries. See Regular expression syntax for details of the supported regular expression language. The “term queries” in that first sentence means that Elasticsearch will apply the regexp to the terms produced by the tokenizer for that field, and not to the original text of the field.</p>
<p>Note: The performance of a regexp query heavily depends on the regular expression chosen. Matching everything like .<em> is very slow as well as using lookaround regular expressions. If possible, you should try to use a long prefix before your regular expression starts. Wildcard matchers like .</em>?+ will mostly lower performance.</p>
<pre><code>{
    &quot;regexp&quot;:{
        &quot;name.first&quot;: &quot;s.*y&quot;
    }
}
</code></pre><h3 id="11-3-8-Fuzzy-Query"><a href="#11-3-8-Fuzzy-Query" class="headerlink" title="11.3.8 Fuzzy Query"></a>11.3.8 Fuzzy Query</h3><p>The fuzzy query uses similarity based on Levenshtein edit distance for string fields, and a +/- margin on numeric and date fields.</p>
<p>The fuzzy query generates all possible matching terms that are within the maximum edit distance specified in fuzziness and then checks the term dictionary to find out which of those generated terms actually exist in the index.</p>
<p>Here is a simple example:</p>
<pre><code>{
    &quot;fuzzy&quot; : { &quot;user&quot; : &quot;ki&quot; }
}
</code></pre><h3 id="11-3-9-Type-Query"><a href="#11-3-9-Type-Query" class="headerlink" title="11.3.9 Type Query"></a>11.3.9 Type Query</h3><p>Filters documents matching the provided document / mapping type.</p>
<pre><code>{
    &quot;type&quot; : {
        &quot;value&quot; : &quot;my_type&quot;
    }
}
</code></pre><h3 id="11-3-10-Ids-Query"><a href="#11-3-10-Ids-Query" class="headerlink" title="11.3.10 Ids Query"></a>11.3.10 Ids Query</h3><p>Filters documents that only have the provided ids. Note, this query uses the _uid field.</p>
<pre><code>{
    &quot;ids&quot; : {
        &quot;type&quot; : &quot;my_type&quot;,
        &quot;values&quot; : [&quot;1&quot;, &quot;4&quot;, &quot;100&quot;]
    }
}
</code></pre><p>The type is optional and can be omitted, and can also accept an array of values. If no type is specified, all types defined in the index mapping are tried.</p>
<h2 id="11-4-Compound-queries"><a href="#11-4-Compound-queries" class="headerlink" title="11.4 Compound queries"></a>11.4 Compound queries</h2><p>Compound queries wrap other compound or leaf queries, either to combine their results and scores, to change their behaviour, or to switch from query to filter context.</p>
<p>The queries in this group are:</p>
<ul>
<li>constant_score query: A query which wraps another query, but executes it in filter context. All matching documents are given the same “constant” _score.</li>
<li>bool query: The default query for combining multiple leaf or compound query clauses, as must, should, must_not, or filter clauses. The must and should clauses have their scores combined – the more matching clauses, the better – while the must_not and filter clauses are executed in filter context.</li>
<li>dis_max query: A query which accepts multiple queries, and returns any documents which match any of the query clauses. While the bool query combines the scores from all matching queries, the dis_max query uses the score of the single best- matching query clause.</li>
<li>function_score query: Modify the scores returned by the main query with functions to take into account factors like popularity, recency, distance, or custom algorithms implemented with scripting.</li>
<li>boosting query: Return documents which match a positive query, but reduce the score of documents which also match a negative query.</li>
<li>indices query: Execute one query for the specified indices, and another for other indices.</li>
</ul>
<h3 id="11-4-1-Constant-Score-Query"><a href="#11-4-1-Constant-Score-Query" class="headerlink" title="11.4.1 Constant Score Query"></a>11.4.1 Constant Score Query</h3><p>A query that wraps another query and simply returns a constant score equal to the query boost for every document in the filter. Maps to Lucene ConstantScoreQuery.</p>
<pre><code>{
    &quot;constant_score&quot; : {
        &quot;filter&quot; : {
            &quot;term&quot; : { &quot;user&quot; : &quot;kimchy&quot;}
        },
        &quot;boost&quot; : 1.2
    }
}
</code></pre><h3 id="11-4-2-Bool-Query"><a href="#11-4-2-Bool-Query" class="headerlink" title="11.4.2 Bool Query"></a>11.4.2 Bool Query</h3><p>A query that matches documents matching boolean combinations of other queries. The bool query maps to Lucene BooleanQuery. It is built using one or more boolean clauses, each clause with a typed occurrence. The occurrence types are:</p>
<table>
<thead>
<tr>
<th>Occur</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>must</td>
<td>The clause (query) must appear in matching documents and will contribute to the score.</td>
</tr>
<tr>
<td>filter</td>
<td>The clause (query) must appear in matching documents. However unlike must the score of the query will be ignored.</td>
</tr>
<tr>
<td>should</td>
<td>The clause (query) should appear in the matching document. In a boolean query with no must or filter clauses, one or more should clauses must match a document. The minimum number of should clauses to match can be set using the minimum_should_match parameter.</td>
</tr>
<tr>
<td>must_not</td>
<td>The clause (query) must not appear in the matching documents.</td>
</tr>
</tbody>
</table>
<p><strong>If this query is used in a filter context and it has should clauses then at least one should clause is required to match.</strong></p>
<p>The bool query takes a more-matches-is-better approach, so the score from each matching <em>must</em> or <em>should</em> clause will be added together to provide the final _score for each document.</p>
<pre><code>{
    &quot;bool&quot; : {
        &quot;must&quot; : {
            &quot;term&quot; : { &quot;user&quot; : &quot;kimchy&quot; }
        },
        &quot;filter&quot;: {
            &quot;term&quot; : { &quot;tag&quot; : &quot;tech&quot; }
        },
        &quot;must_not&quot; : {
            &quot;range&quot; : {
                &quot;age&quot; : { &quot;from&quot; : 10, &quot;to&quot; : 20 }
            }
        },
        &quot;should&quot; : [
            {
                &quot;term&quot; : { &quot;tag&quot; : &quot;wow&quot; }
            },
            {
                &quot;term&quot; : { &quot;tag&quot; : &quot;elasticsearch&quot; }
            }
        ],
        &quot;minimum_should_match&quot; : 1,
        &quot;boost&quot; : 1.0
    }
}
</code></pre><h3 id="11-4-3-Dis-Max-Query"><a href="#11-4-3-Dis-Max-Query" class="headerlink" title="11.4.3 Dis Max Query"></a>11.4.3 Dis Max Query</h3><p>A query that generates the union of documents produced by its subqueries, and that scores each document with the maximum score for that document as produced by any subquery, plus a tie breaking increment for any additional matching subqueries.</p>
<p>This is useful when searching for a word in multiple fields with different boost factors (so that the fields cannot be combined equivalently into a single search field). We want the primary score to be the one associated with the highest boost, not the sum of the field scores (as Boolean Query would give). If the query is “albino elephant” this ensures that “albino” matching one field and “elephant” matching another gets a higher score than “albino” matching both fields. To get this result, use both Boolean Query and DisjunctionMax Query: for each term a DisjunctionMaxQuery searches for it in each field, while the set of these DisjunctionMaxQuery’s is combined into a BooleanQuery.</p>
<p>The tie breaker capability allows results that include the same term in multiple fields to be judged better than results that include this term in only the best of those multiple fields, without confusing this with the better case of two different terms in the multiple fields.The default tie_breaker is 0.0.</p>
<p>This query maps to Lucene DisjunctionMaxQuery.</p>
<pre><code>{
    &quot;dis_max&quot; : {
        &quot;tie_breaker&quot; : 0.7,
        &quot;boost&quot; : 1.2,
        &quot;queries&quot; : [
            {
                &quot;term&quot; : { &quot;age&quot; : 34 }
            },
            {
                &quot;term&quot; : { &quot;age&quot; : 35 }
            }
        ]
    }
}
</code></pre><h3 id="11-4-4-Function-Score-Query"><a href="#11-4-4-Function-Score-Query" class="headerlink" title="11.4.4 Function Score Query"></a>11.4.4 Function Score Query</h3><p>The function_score allows you to modify the score of documents that are retrieved by a query. This can be useful if, for example, a score function is computationally expensive and it is sufficient to compute the score on a filtered set of documents.</p>
<p>To use function_score, the user has to define a query and one or more functions, that compute a new score for each document returned by the query.</p>
<pre><code>&quot;function_score&quot;: {
    &quot;query&quot;: {},
    &quot;boost&quot;: &quot;boost for the whole query&quot;,
    &quot;functions&quot;: [
        {
            &quot;filter&quot;: {},
            &quot;FUNCTION&quot;: {}, 
            &quot;weight&quot;: number
        },
        {
            &quot;FUNCTION&quot;: {} 
        },
        {
            &quot;filter&quot;: {},
            &quot;weight&quot;: number
        }
    ],
    &quot;max_boost&quot;: number,
    &quot;score_mode&quot;: &quot;(multiply|max|...)&quot;,
    &quot;boost_mode&quot;: &quot;(multiply|replace|...)&quot;,
    &quot;min_score&quot; : number
}
</code></pre><p>++<em>The</em> scores produced by the filtering query of each function do not matter.++</p>
<p>First, each document is scored by the defined functions. The parameter score_mode specifies how the computed scores are combined:</p>
<ul>
<li>multiply: scores are multiplied (default)</li>
<li>sum: scores are summed</li>
<li>avg: scores are averaged</li>
<li>first: the first function that has a matching filter is applied</li>
<li>max: maximum score is used</li>
<li>min: minimum score is used</li>
</ul>
<p>The newly computed score is combined with the score of the query. The parameter boost_mode defines how:</p>
<ul>
<li>multiply: query score and function score is multiplied (default)</li>
<li>replace: only function score is used, the query score is ignored</li>
<li>sum: query score and function score are added</li>
<li>avg: average</li>
<li>max: max of query score and function score</li>
<li>min: min of query score and function score</li>
</ul>
<p>The function_score query provides several types of score functions: </p>
<ul>
<li>script_score</li>
<li>weight</li>
<li>random_score</li>
<li>field_value_factor</li>
<li>decay functions: gauss, linear, exp</li>
</ul>
<p><strong>Field Value factor</strong><br>The field_value_factor function allows you to use a field from a document to influence the score. It’s similar to using the script_score function, however, it avoids the overhead of scripting. If used on a multi-valued field, only the first value of the field is used in calculations.</p>
<p>As an example, imagine you have a document indexed with a numeric popularity field and wish to influence the score of a document with this field, an example doing so would look like:</p>
<pre><code>&quot;field_value_factor&quot;: {
  &quot;field&quot;: &quot;popularity&quot;,
  &quot;factor&quot;: 1.2,
  &quot;modifier&quot;: &quot;sqrt&quot;,
  &quot;missing&quot;: 1
}
</code></pre><p>Which will translate into the following formula for scoring:</p>
<pre><code>sqrt(1.2 * doc[&apos;popularity&apos;].value)
</code></pre><p>There are a number of options for the field_value_factor function:</p>
<ul>
<li>field: Field to be extracted from the document.</li>
<li>factor: Optional factor to multiply the field value with, defaults to 1.</li>
<li>modifier: Modifier to apply to the field value, can be one of: none, log, log1p, log2p, ln, ln1p, ln2p, square, sqrt, or reciprocal. Defaults to none.</li>
</ul>
<table>
<thead>
<tr>
<th>Modifier</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>none</td>
<td>Do not apply any multiplier to the field value</td>
</tr>
<tr>
<td>log</td>
<td>Take the logarithm of the field value</td>
</tr>
<tr>
<td>log1p</td>
<td>Add 1 to the field value and take the logarithm</td>
</tr>
<tr>
<td>log2p</td>
<td>Add 2 to the field value and take the logarithm</td>
</tr>
<tr>
<td>ln</td>
<td>Take the natural logarithm of the field value</td>
</tr>
<tr>
<td>ln1p</td>
<td>Add 1 to the field value and take the natural logarithm</td>
</tr>
<tr>
<td>ln2p</td>
<td>Add 2 to the field value and take the natural logarithm</td>
</tr>
<tr>
<td>square</td>
<td>Square the field value (multiply it by itself)</td>
</tr>
<tr>
<td>sqrt</td>
<td>Take the square root of the field value</td>
</tr>
<tr>
<td>reciprocal</td>
<td>Reciprocate the field value, same as 1/x where x is the field’s value</td>
</tr>
</tbody>
</table>
<h3 id="11-4-5-Boosting-Query"><a href="#11-4-5-Boosting-Query" class="headerlink" title="11.4.5 Boosting Query"></a>11.4.5 Boosting Query</h3><p>The boosting query can be used to effectively demote results that match a given query. Unlike the “NOT” clause in bool query, this still selects documents that contain undesirable terms, but reduces their overall score.</p>
<pre><code>{
    &quot;boosting&quot; : {
        &quot;positive&quot; : {
            &quot;term&quot; : {
                &quot;field1&quot; : &quot;value1&quot;
            }
        },
        &quot;negative&quot; : {
            &quot;term&quot; : {
                &quot;field2&quot; : &quot;value2&quot;
            }
        },
        &quot;negative_boost&quot; : 0.2
    }
}
</code></pre><h3 id="11-4-6-Indices-Query"><a href="#11-4-6-Indices-Query" class="headerlink" title="11.4.6 Indices Query"></a>11.4.6 Indices Query</h3><p>The indices query is useful in cases where a search is executed across multiple indices. It allows to specify a list of index names and an inner query that is only executed for indices matching names on that list. For other indices that are searched, but that don’t match entries on the list, the alternative no_match_query is executed.</p>
<pre><code>{
    &quot;indices&quot; : {
        &quot;indices&quot; : [&quot;index1&quot;, &quot;index2&quot;],
        &quot;query&quot; : {
            &quot;term&quot; : { &quot;tag&quot; : &quot;wow&quot; }
        },
        &quot;no_match_query&quot; : {
            &quot;term&quot; : { &quot;tag&quot; : &quot;kow&quot; }
        }
    }
}
</code></pre><h2 id="11-5-Joining-queries"><a href="#11-5-Joining-queries" class="headerlink" title="11.5 Joining queries"></a>11.5 Joining queries</h2><p>Performing full SQL-style joins in a distributed system like Elasticsearch is prohibitively expensive. Instead, Elasticsearch offers two forms of join which are designed to scale horizontally.</p>
<ul>
<li>nested query: Documents may contains fields of type nested. These fields are used to index arrays of objects, where each object can be queried (with the nested query) as an independent document.</li>
<li>has_child and has_parent queries: A parent-child relationship can exist between two document types within a single index. The has_child query returns parent documents whose child documents match the specified query, while the has_parent query returns child documents whose parent document matches the specified query.</li>
</ul>
<p>Also see the terms-lookup mechanism in the terms query, which allows you to build a terms query from values contained in another document.</p>
<h2 id="11-6-Geo-queries-Advanced-Topic"><a href="#11-6-Geo-queries-Advanced-Topic" class="headerlink" title="11.6 Geo queries  (Advanced Topic)"></a>11.6 Geo queries  (Advanced Topic)</h2><h2 id="11-7-Specialized-queries-Advanced-Topic"><a href="#11-7-Specialized-queries-Advanced-Topic" class="headerlink" title="11.7 Specialized queries (Advanced Topic)"></a>11.7 Specialized queries (Advanced Topic)</h2><p>This group contains queries which do not fit into the other groups:</p>
<ul>
<li>more_like_this query: This query finds documents which are similar to the specified text, document, or collection of documents.</li>
<li>template query: The template query accepts a Mustache template (either inline, indexed, or from a file), and a map of parameters, and combines the two to generate the final query to execute.</li>
<li>script query: This query allows a script to act as a filter. Also see the function_score query.</li>
</ul>
<h2 id="11-8-Span-queries-Advanced-Topic"><a href="#11-8-Span-queries-Advanced-Topic" class="headerlink" title="11.8 Span queries (Advanced Topic)"></a>11.8 Span queries (Advanced Topic)</h2><p>Span queries are low-level positional queries which provide expert control over the order and proximity of the specified terms. These are typically used to implement very specific queries on legal documents or patents.</p>
<p>Span queries cannot be mixed with non-span queries (with the exception of the span_multi query).</p>
<p>The queries in this group are:</p>
<ul>
<li>span_term query: The equivalent of the term query but for use with other span queries.</li>
<li>span_multi query: Wraps a term, range, prefix, wildcard, regexp, or fuzzy query.</li>
<li>span_first query: Accepts another span query whose matches must appear within the first N positions of the field.</li>
<li>span_near query: Accepts multiple span queries whose matches must be within the specified distance of each other, and possibly in the same order.</li>
<li>span_or query: Combines multiple span queries – returns documents which match any of the specified queries.</li>
<li>span_not query: Wraps another span query, and excludes any documents which match that query.</li>
<li>span_containing query: Accepts a list of span queries, but only returns those spans which also match a second span query.</li>
<li>span_within query: The result from a single span query is returned as long is its span falls within the spans returned by a list of other span queries.</li>
</ul>
<h1 id="12-Mapping"><a href="#12-Mapping" class="headerlink" title="12 Mapping"></a>12 Mapping</h1><p>Mapping is the process of defining how a document, and the fields it contains, are stored and indexed. For instance, use mappings to define:</p>
<pre><code>which string fields should be treated as full text fields.
which fields contain numbers, dates, or geolocations.
whether the values of all fields in the document should be indexed into the catch-all _all field.
the format of date values.
custom rules to control the mapping for dynamically added fields.
</code></pre><p><strong>Mapping Types</strong><br>Each index has one or more mapping types, which are used to divide the documents in an index into logical groups. User documents might be stored in a user type, and blog posts in a blogpost type.</p>
<p>Each mapping type has:</p>
<ul>
<li>Meta-fields: Meta-fields are used to customize how a document’s metadata associated is treated. Examples of meta-fields include the document’s _index, _type, _id, and _source fields.</li>
<li>Fields or properties: Each mapping type contains a list of fields or properties pertinent to that type. A user type might contain title, name, and age fields, while a blogpost type might contain title, body, user_id and created fields. Fields with the same name in different mapping types in the same index must have the same mapping.</li>
</ul>
<p><strong>Field datatypes</strong><br>Each field has a data type which can be:</p>
<pre><code>a simple type like string, date, long, double, boolean or ip.
a type which supports the hierarchical nature of JSON such as object or nested.
or a specialised type like geo_point, geo_shape, or completion.
</code></pre><p>It is often useful to index the same field in different ways for different purposes. For instance, a string field could be indexed as an analyzed field for full-text search, and as a not_analyzed field for sorting or aggregations. Alternatively, you could index a string field with the standard analyzer, the english analyzer, and the french analyzer.</p>
<p>This is the purpose of multi-fields. Most datatypes support multi-fields via the fields parameter.</p>
<p><strong>Dynamic mapping</strong><br>Fields and mapping types do not need to be defined before being used. Thanks to dynamic mapping, new mapping types and new field names will be added automatically, just by indexing a document. New fields can be added both to the top-level mapping type, and to inner object and nested fields.</p>
<p>The dynamic mapping rules can be configured to customise the mapping that is used for new types and new fields.</p>
<p><strong>Explicit mappings</strong><br>You know more about your data than Elasticsearch can guess, so while dynamic mapping can be useful to get started, at some point you will want to specify your own explicit mappings.</p>
<p>You can create mapping types and field mappings when you create an index, and you can add mapping types and fields to an existing index with the PUT mapping API.</p>
<p><strong>Updating existing mappings</strong><br>Other than where documented, <strong>existing type and field mappings cannot be updated</strong>. Changing the mapping would mean invalidating already indexed documents. Instead, you should create a new index with the correct mappings and reindex your data into that index.</p>
<p><strong>Fields are shared across mapping types</strong><br>Mapping types are used to group fields, but the fields in each mapping type are not independent of each other. Fields with:</p>
<pre><code>the same name
in the same index
in different mapping types
map to the same field internally,
and must have the same mapping.
</code></pre><p>If a title field exists in both the user and blogpost mapping types, the title fields must have exactly the same mapping in each type. The only exceptions to this rule are the copy_to, dynamic, enabled, ignore_above, include_in_all, and properties parameters, which may have different settings per field.</p>
<p>Usually, fields with the same name also contain the same type of data, so having the same mapping is not a problem. When conflicts do arise, these can be solved by choosing more descriptive names, such as user_title and blog_title.</p>
<p><strong>Example mapping</strong><br>A mapping for the example described above could be specified when creating the index, as follows:</p>
<pre><code>PUT my_index 
{
  &quot;mappings&quot;: {
    &quot;user&quot;: { 
      &quot;_all&quot;:       { &quot;enabled&quot;: false  }, 
      &quot;properties&quot;: { 
        &quot;title&quot;:    { &quot;type&quot;: &quot;string&quot;  }, 
        &quot;name&quot;:     { &quot;type&quot;: &quot;string&quot;  }, 
        &quot;age&quot;:      { &quot;type&quot;: &quot;integer&quot; }  
      }
    },
    &quot;blogpost&quot;: { 
      &quot;properties&quot;: { 
        &quot;title&quot;:    { &quot;type&quot;: &quot;string&quot;  }, 
        &quot;body&quot;:     { &quot;type&quot;: &quot;string&quot;  }, 
        &quot;user_id&quot;:  {
          &quot;type&quot;:   &quot;string&quot;, 
          &quot;index&quot;:  &quot;not_analyzed&quot;
        },
        &quot;created&quot;:  {
          &quot;type&quot;:   &quot;date&quot;, 
          &quot;format&quot;: &quot;strict_date_optional_time||epoch_millis&quot;
        }
      }
    }
  }
}
</code></pre><h2 id="12-1-Field-datatypes"><a href="#12-1-Field-datatypes" class="headerlink" title="12.1 Field datatypes"></a>12.1 Field datatypes</h2><p>Elasticsearch supports a number of different datatypes for the fields in a document:</p>
<h3 id="Core-datatypes"><a href="#Core-datatypes" class="headerlink" title="Core datatypes"></a>Core datatypes</h3><p><strong>String datatype: string</strong><br>The following parameters are accepted by string fields:</p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>analyzer</td>
<td>The analyzer which should be used for analyzed string fields, both at index-time and at search-time (unless overridden by the search_analyzer). Defaults to the default index analyzer, or the standard analyzer.</td>
</tr>
<tr>
<td>boost</td>
<td>Field-level index time boosting. Accepts a floating point number, defaults to 1.0.</td>
</tr>
<tr>
<td>doc_values</td>
<td>Should the field be stored on disk in a column-stride fashion, so that it can later be used for sorting, aggregations, or scripting? Accepts true or false. Defaults to true for not_analyzed fields. Analyzed fields do not support doc values.</td>
</tr>
<tr>
<td>fielddata</td>
<td>Can the field use in-memory fielddata for sorting, aggregations, or scripting? Accepts disabled or paged_bytes (default). Not analyzed fields will use doc values in preference to fielddata.</td>
</tr>
<tr>
<td>fields</td>
<td>Multi-fields allow the same string value to be indexed in multiple ways for different purposes, such as one field for search and a multi-field for sorting and aggregations, or the same string value analyzed by different analyzers.</td>
</tr>
<tr>
<td>ignore_above</td>
<td>Do not index or analyze any string longer than this value. Defaults to 0 (disabled).</td>
</tr>
<tr>
<td>include_in_all</td>
<td>Whether or not the field value should be included in the _all field? Accepts true or false. Defaults to false if index is set to no, or if a parent object field sets include_in_all to false. Otherwise defaults to true.</td>
</tr>
<tr>
<td>index</td>
<td>Should the field be searchable? Accepts analyzed (default, treat as full-text field), not_analyzed (treat as keyword field) and no.</td>
</tr>
<tr>
<td>index_options</td>
<td>What information should be stored in the index, for search and highlighting purposes. Defaults to positions for analyzed fields, and to docs for not_analyzed fields.</td>
</tr>
<tr>
<td>norms</td>
<td>Whether field-length should be taken into account when scoring queries. Defaults depend on the index setting: analyzed fields default to { “enabled”: true, “loading”: “lazy” }; not_analyzed fields default to { “enabled”: false }.</td>
</tr>
<tr>
<td>null_value</td>
<td>Accepts a string value which is substituted for any explicit null values. Defaults to null, which means the field is treated as missing. If the field is analyzed, the null_value will also be analyzed.</td>
</tr>
<tr>
<td>position_increment_gap</td>
<td>The number of fake term position which should be inserted between each element of an array of strings. Defaults to the position_increment_gap configured on the analyzer which defaults to 100. 100 was chosen because it prevents phrase queries with reasonably large slops (less than 100) from matching terms across field values.</td>
</tr>
<tr>
<td>store</td>
<td>Whether the field value should be stored and retrievable separately from the _source field. Accepts true or false (default).</td>
</tr>
<tr>
<td>search_analyzer</td>
<td>The analyzer that should be used at search time on analyzed fields. Defaults to the analyzer setting.</td>
</tr>
<tr>
<td>search_quote_analyzer</td>
<td>The analyzer that should be used at search time when a phrase is encountered. Defaults to the search_analyzer setting.</td>
</tr>
<tr>
<td>similarity</td>
<td>Which scoring algorithm or similarity should be used. Defaults to default, which uses TF/IDF.</td>
</tr>
<tr>
<td>term_vector</td>
<td>Whether term vectors should be stored for an analyzed field. Defaults to no.</td>
</tr>
</tbody>
</table>
<p><strong>Numeric datatypes: long, integer, short, byte, double, float</strong></p>
<p><strong>Date datatype: date </strong><br>JSON doesn’t have a date datatype, so dates in Elasticsearch can either be:</p>
<ul>
<li>strings containing formatted dates, e.g. “2015-01-01” or “2015/01/01 12:10:30”</li>
<li>a long number representing milliseconds-since-the-epoch.</li>
<li>an integer representing seconds-since-the-epoch.)</li>
</ul>
<p><strong>Boolean datatype: boolean</strong><br><strong>Binary datatype: binary </strong><br>The binary type accepts a binary value as a Base64 encoded string. The field is not stored by default and is not searchable.</p>
<h3 id="Complex-datatypes"><a href="#Complex-datatypes" class="headerlink" title="Complex datatypes"></a>Complex datatypes</h3><p><strong>Array datatype</strong><br>Array support does not require a dedicated type (In Elasticsearch, there is no dedicated array type. Any field can contain zero or more values by default, however, all values in the array must be of the same datatype.)</p>
<p><strong>Object datatype: object for single JSON objects</strong><br><strong>Nested datatype: nested for arrays of JSON objects</strong></p>
<h3 id="Geo-datatypes-Advanced-Topic"><a href="#Geo-datatypes-Advanced-Topic" class="headerlink" title="Geo datatypes (Advanced Topic)"></a>Geo datatypes (Advanced Topic)</h3><ul>
<li>Geo-point datatype: geo_point for lat/lon points</li>
<li>Geo-Shape datatype: geo_shape for complex shapes like polygons</li>
</ul>
<p>Specialised datatypes</p>
<ul>
<li>IPv4 datatype： ip for IPv4 addresses</li>
<li>Completion datatype： completion to provide auto-complete suggestions</li>
<li>Token count datatype： token_count to count the number of tokens in a string</li>
<li>mapper-murmur3： murmur3 to compute hashes of values at index-time and store them in the index</li>
<li>Attachment datatype： See the mapper-attachments plugin which supports indexing attachments like Microsoft Office formats, Open Document formats, ePub, HTML, etc. into an attachment datatype.</li>
</ul>
<h3 id="Multi-fields"><a href="#Multi-fields" class="headerlink" title="Multi-fields"></a>Multi-fields</h3><p>It is often useful to index the same field in different ways for different purposes. For instance, a string field could be indexed as an analyzed field for full-text search, and as a not_analyzed field for sorting or aggregations. Alternatively, you could index a string field with the standard analyzer, the english analyzer, and the french analyzer.</p>
<p>This is the purpose of multi-fields. Most datatypes support multi-fields via the fields parameter.</p>
<h2 id="12-2-Meta-Fields"><a href="#12-2-Meta-Fields" class="headerlink" title="12.2 Meta-Fields"></a>12.2 Meta-Fields</h2><p>Each document has metadata associated with it, such as the _index, mapping _type, and _id meta-fields. The behaviour of some of these meta-fields can be customised when a mapping type is created.</p>
<h3 id="Identity-meta-fields"><a href="#Identity-meta-fields" class="headerlink" title="Identity meta-fields"></a>Identity meta-fields</h3><ul>
<li>_index: The index to which the document belongs.</li>
<li>_uid: A composite field consisting of the _type and the _id.</li>
<li>_type: The document’s mapping type.</li>
<li>_id: The document’s ID.</li>
</ul>
<h3 id="Document-source-meta-fields"><a href="#Document-source-meta-fields" class="headerlink" title="Document source meta-fields"></a>Document source meta-fields</h3><ul>
<li>_source: The original JSON representing the body of the document.</li>
<li>_size: The size of the _source field in bytes, provided by the mapper-size plugin.</li>
</ul>
<h3 id="Indexing-meta-fields"><a href="#Indexing-meta-fields" class="headerlink" title="Indexing meta-fields"></a>Indexing meta-fields</h3><ul>
<li><p>_all: The _all field is a special catch-all field which concatenates the values of all of the other fields into one big string, using space as a delimiter, which is then analyzed and indexed, but not stored. This means that it can be searched, but not retrieved; The _all field can be completely disabled per-type by setting enabled to false; While there is only a single _all field per index, the copy_to parameter allows the creation of multiple custom _all fields. For instance, first_name and last_name fields can be combined together into the full_name field.</p>
</li>
<li><p>_field_names: The _field_names field indexes the names of every field in a document that contains any value other than null. This field is used by the exists and missing queries to find documents that either have or don’t have any non-null value for a particular field; The value of the _field_name field is accessible in queries, aggregations, and scripts.</p>
</li>
</ul>
<h3 id="Routing-meta-fields"><a href="#Routing-meta-fields" class="headerlink" title="Routing meta-fields"></a>Routing meta-fields</h3><ul>
<li>_parent: Used to create a parent-child relationship between two mapping types.</li>
<li>_routing: A custom routing value which routes a document to a particular shard.</li>
</ul>
<h3 id="Other-meta-field"><a href="#Other-meta-field" class="headerlink" title="Other meta-field"></a>Other meta-field</h3><ul>
<li>_meta: Application specific metadata.</li>
</ul>
<h2 id="12-3-Mapping-parameters"><a href="#12-3-Mapping-parameters" class="headerlink" title="12.3 Mapping parameters"></a>12.3 Mapping parameters</h2><p>The following mapping parameters are common to some or all field datatypes:</p>
<ul>
<li>analyzer</li>
<li>boost</li>
<li>coerce</li>
<li>copy_to: The copy_to parameter allows you to create custom _all fields. In other words, the values of multiple fields can be copied into a group field, which can then be queried as a single field. For instance, the first_name and last_name fields can be copied to the full_name field.</li>
<li>doc_values: Doc values are the on-disk data structure, built at document index time, which makes this data access pattern possible. They store the same values as the _source but in a column-oriented fashion that is way more efficient for sorting and aggregations. Doc values are supported on almost all field types, with the notable exception of analyzed string fields.</li>
<li>dynamic</li>
<li>enabled</li>
<li>fielddata: Most fields can use index-time, on-disk doc_values to support this type of data access pattern, but analyzed string fields do not support doc_values; Instead, analyzed strings use a query-time data structure called fielddata. This data structure is built on demand the first time that a field is used for aggregations, sorting, or is accessed in a script. It is built by reading the entire inverted index for each segment from disk, inverting the term ↔︎ document relationship, and storing the result in memory, in the JVM heap; Loading fielddata is an expensive process so, once it has been loaded, it remains in memory for the lifetime of the segment.</li>
<li>geohash</li>
<li>geohash_precision</li>
<li>geohash_prefix</li>
<li>format</li>
<li>ignore_above</li>
<li>ignore_malformed</li>
<li>include_in_all</li>
<li>index_options</li>
<li>lat_lon</li>
<li>index</li>
<li>fields: It is often useful to index the same field in different ways for different purposes. This is the purpose of multi-fields. For instance, a string field could be indexed as an analyzed field for full-text search, and as a not_analyzed field for sorting or aggregations. </li>
<li>norms: Norms store various normalization factors – a number to represent the relative field length and the index time boost setting – that are later used at query time in order to compute the score of a document relatively to a query; Although useful for scoring, norms also require quite a lot of memory (typically in the order of one byte per document per field in your index, even for documents that don’t have this specific field). As a consequence, if you don’t need scoring on a specific field, you should disable norms on that field. In particular, this is the case for fields that are used solely for filtering or aggregations.</li>
<li>null_value</li>
<li>position_increment_gap</li>
<li>properties: Type mappings, object fields and nested fields contain sub-fields, called properties. These properties may be of any datatype, including object and nested. </li>
<li>search_analyzer</li>
<li>similarity</li>
<li>store: By default, field values are indexed to make them searchable, but they are not stored. This means that the field can be queried, but the original field value cannot be retrieved; Usually this doesn’t matter. The field value is already part of the _source field, which is stored by default. If you only want to retrieve the value of a single field or of a few fields, instead of the whole _source, then this can be achieved with source filtering; In certain situations it can make sense to store a field. For instance, if you have a document with a title, a date, and a very large content field, you may want to retrieve just the title and the date without having to extract those fields from a large _source field.</li>
<li>term_vector: Term vectors contain information about the terms produced by the analysis process, including: a list of terms/the position (or order) of each term/the start and end character offsets mapping the term to its origin in the original string; These term vectors can be stored so that they can be retrieved for a particular document.</li>
</ul>
<p>The term_vector setting accepts:</p>
<ul>
<li>no: No term vectors are stored. (default)</li>
<li>yes: Just the terms in the field are stored.</li>
<li>with_positions: Terms and positions are stored.</li>
<li>with_offsets: Terms and character offsets are stored.</li>
<li>with_positions_offsets: Terms, positions, and character offsets are stored.</li>
</ul>
<h2 id="12-4-Dynamic-Mapping"><a href="#12-4-Dynamic-Mapping" class="headerlink" title="12.4 Dynamic Mapping"></a>12.4 Dynamic Mapping</h2><p>One of the most important features of Elasticsearch is that it tries to get out of your way and let you start exploring your data as quickly as possible. To index a document, you don’t have to first create an index, define a mapping type, and define your fields – you can just index a document and the index, type, and fields will spring to life automatically:</p>
<pre><code>PUT data/counters/1 
{ &quot;count&quot;: 5 }
</code></pre><p>Creates the data index, the counters mapping type, and a field called count with datatype long.</p>
<p>The automatic detection and addition of new types and fields is called dynamic mapping. The dynamic mapping rules can be customised to suit your purposes with:</p>
<ul>
<li><em>default</em> mapping: Configure the base mapping to be used for new mapping types.</li>
<li>Dynamic field mappings: The rules governing dynamic field detection.</li>
<li>Dynamic templates: Custom rules to configure the mapping for dynamically added fields.</li>
</ul>
<h1 id="13-Analysis-See-another-documents"><a href="#13-Analysis-See-another-documents" class="headerlink" title="13 Analysis (See another documents)"></a>13 Analysis (See another documents)</h1><h1 id="14-Modules-Skipped"><a href="#14-Modules-Skipped" class="headerlink" title="14 Modules (Skipped)"></a>14 Modules (Skipped)</h1><h1 id="15-Index-Modules-Skipped"><a href="#15-Index-Modules-Skipped" class="headerlink" title="15 Index Modules (Skipped)"></a>15 Index Modules (Skipped)</h1><h1 id="16-Testing-Skipped"><a href="#16-Testing-Skipped" class="headerlink" title="16 Testing (Skipped)"></a>16 Testing (Skipped)</h1><h1 id="17-Glossary-of-terms-Skipped"><a href="#17-Glossary-of-terms-Skipped" class="headerlink" title="17 Glossary of terms (Skipped)"></a>17 Glossary of terms (Skipped)</h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;花了几天把&lt;a href=&quot;https://www.elastic.co/guide/en/elasticsearch/reference/2.3/index.html&quot;&gt;Elasticsearch的官方文档&lt;/a&gt;读了一遍，随手记一些关键的笔记。&lt;/p&gt;
    
    </summary>
    
      <category term="Elasticsearch" scheme="http://ginobefunny.com/categories/Elasticsearch/"/>
    
    
      <category term="Elasticsearch" scheme="http://ginobefunny.com/tags/Elasticsearch/"/>
    
      <category term="一起读文档" scheme="http://ginobefunny.com/tags/%E4%B8%80%E8%B5%B7%E8%AF%BB%E6%96%87%E6%A1%A3/"/>
    
  </entry>
  
  <entry>
    <title>《深入理解Java虚拟机》读书笔记7：高效并发</title>
    <link href="http://ginobefunny.com/post/deep_in_jvm_notes_part7/"/>
    <id>http://ginobefunny.com/post/deep_in_jvm_notes_part7/</id>
    <published>2017-01-29T07:31:25.000Z</published>
    <updated>2017-01-31T09:50:33.579Z</updated>
    
    <content type="html"><![CDATA[<p>国内JVM相关书籍NO.1，Java程序员必读。读书笔记第七部分对应原书的第十二章和第十三章，主要介绍Java内存模型、先行发生原则、线程安全和虚拟机的锁优化细节。<br><a id="more"></a></p>
<h1 id="第五部分-高效并发"><a href="#第五部分-高效并发" class="headerlink" title="第五部分 高效并发"></a>第五部分 高效并发</h1><h2 id="第十二章-Java内存模型与线程"><a href="#第十二章-Java内存模型与线程" class="headerlink" title="第十二章 Java内存模型与线程"></a>第十二章 Java内存模型与线程</h2><p>并发处理的广泛应用是使得Amdahl定律代替摩尔定律成为计算机性能发展源动力的根本原因，也是人类“压榨”计算机运算能力的最有力武器。</p>
<h3 id="12-1-概述"><a href="#12-1-概述" class="headerlink" title="12.1 概述"></a>12.1 概述</h3><ul>
<li>多任务处理在现代计算机操作系统中几乎已是一项必备的功能了；</li>
<li>除了充分利用计算机处理器的能力外，一个服务端同时对多个客户端提供服务则是另一个更具体的并发应用场景；</li>
<li>服务端是Java语言最擅长的领域之一，不过如何写好并发应用程序却又是服务端程序开发的难点之一，处理好并发方面的问题通常需要更多的编码经验来支持，幸好Java语言和虚拟机提供了许多工具，把并发编码的门槛降低了不少；</li>
</ul>
<h3 id="12-2-硬件的效率与一致性"><a href="#12-2-硬件的效率与一致性" class="headerlink" title="12.2 硬件的效率与一致性"></a>12.2 硬件的效率与一致性</h3><ul>
<li>绝大多数的运算任务不可能只靠处理器计算就能完成，处理器至少要与内存交互，所以现代计算机系统都不得不加入一层读写速度尽可能接近处理器运算速度的高速缓存来作为内存与处理器之间的缓冲：将运算需要使用到的数据复制到缓存中，让运算能快速运行，当运算结束后再从缓存同步回内存之中，这样处理器就无须等待缓慢的内存读写了；</li>
<li>基于高速缓存的存储交互很好地解决了处理器与内存的速度矛盾，但是也为计算机系统带来更高的复杂度，因为它引入了一个新的问题：缓存一致性；为了解决一致性的问题，需要各个处理器访问缓存时都遵循一些协议，在读写时要根据协议来进行操作，这类协议有MSI、MESI、MOSI、Synapse、Firefly及Dragon Protocol等；</li>
<li>本章将会多次提到内存模型一词，可以理解在特定的操作协议下，对特定的内存或高速缓存进行读写访问的过程抽象；不同架构的物理机器可以拥有不一样的内存模型，而Java虚拟机也有自己的内存模型，并且这里介绍的内存访问操作与硬件的缓存访问具有很高的可比性；</li>
<li>除了增加高速缓存之外，为了使得处理器内部的运算单元能尽量被充分利用，处理器可能会对输入代码进行乱序执行优化，处理器会在计算之后将乱序执行的结果重组，保证该结果与顺序执行的结果是一致的；</li>
</ul>
<h3 id="12-3-Java内存模型"><a href="#12-3-Java内存模型" class="headerlink" title="12.3 Java内存模型"></a>12.3 Java内存模型</h3><p>Java虚拟机规范中视图定义一种Java内存模型（JMM）来屏蔽掉各种硬件和操作系统的内存访问差异，以实现让Java程序在各种平台下都能达到一致的内存访问效果。</p>
<h4 id="12-3-1-主内存与工作内存"><a href="#12-3-1-主内存与工作内存" class="headerlink" title="12.3.1 主内存与工作内存"></a>12.3.1 主内存与工作内存</h4><ul>
<li>Java内存模型的主要目标是定义程序中各个变量的访问规则，即在虚拟机中将变量存储到内存和从内存中取出变量这样的底层细节；此处的变量与Java编程中所说的变量有所区别，它包括了实例字段、静态字段和构成数组对象的元素，但不包括局部变量与方法参数，因为后者是线程私有的，不会被共享；</li>
<li>Java内存模型规定了所有的变量都存储在主内存中，每个线程还有自己的工作内存，线程的工作内存中保存了被该线程使用到的变量的主内存副本拷贝，线程对变量的所有操作都必须在工作内存中进行，而不能直接读写主内存中的变量；</li>
<li>这里所讲的主内存、工作内存与第二章所讲的Java内存区域中的Java堆、栈、方法区等并不是同一个层次的内存划分，这两者基本上是没有关系的；线程、主内存和工作内存的关系如下所示：</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/jmm_relation.png" alt="线程、主内存和工作内存的关系"></p>
<h4 id="12-3-2-内存间交互操作"><a href="#12-3-2-内存间交互操作" class="headerlink" title="12.3.2 内存间交互操作"></a>12.3.2 内存间交互操作</h4><p>关于主内存与工作内存之间具体的交互协议，即一个变量如何从主内存拷贝到工作内存、如何从工作内存同步回主内存之类的实现细节，Java内存模型中定义了以下八种操作来完成，虚拟机实现时必须保证下面提及的每一种操作都是原子的、不可再分的（对于double和long类型的变量的某些操作在某些平台允许有例外）：</p>
<ul>
<li>lock</li>
<li>unlock</li>
<li>read</li>
<li>load</li>
<li>use</li>
<li>assign</li>
<li>store</li>
<li>write</li>
</ul>
<p>基于理解难度和严谨性考虑，最新的JSR-133文档中，已经放弃采用这八种操作去定义Java内存模型的访问协议了，后面将会介绍一个等效判断原则 – 先行发生原则，用来确定一个访问在并发环境下是否安全；</p>
<h4 id="12-3-3-对于volatile型变量的特殊规则"><a href="#12-3-3-对于volatile型变量的特殊规则" class="headerlink" title="12.3.3 对于volatile型变量的特殊规则"></a>12.3.3 对于volatile型变量的特殊规则</h4><ul>
<li>关键字volatile可以说是Java虚拟机提供的最轻量级的同步机制；</li>
<li>当一个变量定义为volatile之后，它将具备两种特性：第一是保证此变量对所有线程的可见性，这里的可见性是指当一个线程修改了这个变量的值，新的值对于其他线程来说是可以立即得知的，而普通的变量的值在线程间传递均需要通过主内存来完成；另外一个是禁止指令重排序优化，普通的变量仅仅会保证在该方法的执行过程中所有依赖赋值结果的地方都能获取到正确的结果，而不能保证变量赋值操作的顺序与程序代码中的执行顺序一致；</li>
<li>volatile变量在各个线程的工作内存中不存在一致性问题，但是Java里面的运算并非原子操作，导致volatile变量的运算在并发下一样是不安全的；</li>
<li>在不符合以下两条规则的运算场景中，我们仍然要通过加锁来保证原子性：运算结果并不依赖变量的当前值或者能够确保只有单一的线程修改变量的值、变量不需要与其他的状态变量共同参与不变约束；</li>
<li>volatile变量读操作的性能消耗与普通变量几乎没有任何差别，但是写操作则可能会慢一些；不过大多数场景下volatile的总开销仍然要比锁低，我们在volatile与锁之中选择的唯一依据仅仅是volatile的语义能否满足使用场景的需求；</li>
</ul>
<h4 id="12-3-4-对于long和double型变量的特殊规则"><a href="#12-3-4-对于long和double型变量的特殊规则" class="headerlink" title="12.3.4 对于long和double型变量的特殊规则"></a>12.3.4 对于long和double型变量的特殊规则</h4><ul>
<li>允许虚拟机将没有被volatile修饰的64位数据的读写操作划分为两次32位的操作来进行，即允许虚拟机实现选择可以不保证64位数据类型的load、store、read和write这4个操作的原子性，这点就是所谓的long和double的非原子性协定；</li>
<li>但允许虚拟机选择把这些操作实现为具有原子性的操作，目前各种平台下的商用虚拟机几乎都选择把64位数据的读写操作作为原子操作来对待；</li>
</ul>
<h4 id="12-3-5-原子性、可见性与有序性"><a href="#12-3-5-原子性、可见性与有序性" class="headerlink" title="12.3.5 原子性、可见性与有序性"></a>12.3.5 原子性、可见性与有序性</h4><ul>
<li>原子性（Atomicity）：由Java内存模型来直接保证的原子性变量操作包括read、load、assign、use、store和write；在synchronized块之间的操作也具备原子性；</li>
<li>可见性（Visibility）：是指当一个线程修改了共享变量的值，其他线程能够立即得知这个修改；除了volatile之外，Java还有synchronized和final关键字能实现可见性；</li>
<li>有序性（Ordering）：如果在本线程内观察，所有的操作都是有序的；如果在一个线程中观察另一个线程，所有的操作都是无序的；Java语言提供了volatile和synchronized两个关键字来保证线程之间操作的有序性；</li>
</ul>
<h4 id="12-3-6-先行发生原则"><a href="#12-3-6-先行发生原则" class="headerlink" title="12.3.6 先行发生原则"></a>12.3.6 先行发生原则</h4><ul>
<li>先行发生是Java内存模型中定义的两项操作之间的偏序关系，如果说操作A先行发生于操作B，其实就是说在发生操作B之前，操作A产生的影响能被操作B观察到，影响包括了修改了内存中共享变量的值、发送了消息、调用了方法等；</li>
<li>下面是Java内存模型下一些天然的先行发生关系：程序次序规则、管程锁定规则、volatile变量规则、线程启动规则、线程终止规则、线程中断规则、对象终结规则、传递性；</li>
<li>时间先后顺序与先行发生原则之间基本没有太大的关系，所以我们衡量并发安全问题的时候不要受到时间顺序的干扰，一切必须以先行发生原则为准；</li>
</ul>
<h3 id="12-4-Java与线程"><a href="#12-4-Java与线程" class="headerlink" title="12.4 Java与线程"></a>12.4 Java与线程</h3><h4 id="12-4-1-线程的实现"><a href="#12-4-1-线程的实现" class="headerlink" title="12.4.1 线程的实现"></a>12.4.1 线程的实现</h4><ul>
<li>线程是比进程更轻量级的调度执行单位，线程的引入可以把一个进程的资源分配和执行调度分开，各个线程既可以共享进程资源又可以独立调度；</li>
<li>Thread类与大部分的Java API有显著的差别，它的所有关键方法都是声明为Native的；</li>
<li>实现线程主要有三种方式：使用内核线程实现（系统调用代价相对较高、一个系统支持轻量级进程的数量是有限的）、使用用户线程实现（优势在于不需要系统内核支援，劣势在于所有线程操作都需要用户程序自己处理）和使用用户线程加轻量级进程混合实现（用户线程是完全建立在用户空间中，因此用户线程的创建、切换等操作依然廉价，并且可以支持大规模的用户线程并发；而操作系统提供支持的轻量级进程则作为用户线程和内核线程之间的桥梁，这样可以使用内核提供的线程调度功能及处理器映射，并且用户线程的系统调用要通过轻量级线程来完成，大大降低了整个进程被完全阻塞的风险）；</li>
<li>对于Sun JDK来说，它的Windows版与Linux版都是使用一对一的线程模型实现的，一条Java线程就映射到一条轻量级进程之中，因为Windows和Linux系统提供的线程模式就是一对一的；</li>
</ul>
<h4 id="12-4-2-Java线程调度"><a href="#12-4-2-Java线程调度" class="headerlink" title="12.4.2 Java线程调度"></a>12.4.2 Java线程调度</h4><ul>
<li>线程调度是指系统为线程分配处理器使用权的过程，主要调度方式有两种，分别是协同式线程调度（线程的执行时间由线程本身来控制）和抢占式线程调度（线程由系统来分配执行时间，线程的切换不由线程本身来决定）；</li>
<li>Java语言一共设置了10个级别的线程优先级，不过线程优先级并不是太靠谱，原因就是操作系统的线程优先级不见得总是与Java线程的优先级一一对应，另外优先级还可能被系统自行改变；</li>
</ul>
<h4 id="12-4-3-状态转换"><a href="#12-4-3-状态转换" class="headerlink" title="12.4.3 状态转换"></a>12.4.3 状态转换</h4><ul>
<li>Java语言定义了五种线程状态，在任意一个时间点，一个线程只能有且只有其中一种状态，分别是新建（New）、运行（Runnable）、无限期等待（Waiting）、限期等待（Timed Waiting）、阻塞（Blocled）、结束（Terminated）。它们之间相互的转换关系如下所示：</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/thread_status.png" alt="线程状态转换关系"></p>
<h3 id="12-5-本章小结"><a href="#12-5-本章小结" class="headerlink" title="12.5 本章小结"></a>12.5 本章小结</h3><p>本章我们首先了解了虚拟机Java内存模型的结构及操作，然后讲解了原子性、可见性、有序性在Java内存模型中的体现，最后介绍了先行发生原则的规则及使用。另外，我们还了解了线程在Java语言之中是如何实现的。</p>
<p>在本章主要介绍了虚拟机如何实现并发，而在下一章我们主要关注点将是虚拟机如何实现高效，以及虚拟机对我们编写的并发代码提供了什么样的优化手段。</p>
<h2 id="第十三章-线程安全与锁优化"><a href="#第十三章-线程安全与锁优化" class="headerlink" title="第十三章 线程安全与锁优化"></a>第十三章 线程安全与锁优化</h2><h3 id="13-1-概述"><a href="#13-1-概述" class="headerlink" title="13.1 概述"></a>13.1 概述</h3><ul>
<li>首先需要保证并发的正确性，然后在此基础上实现高效；</li>
</ul>
<h3 id="13-2-线程安全"><a href="#13-2-线程安全" class="headerlink" title="13.2 线程安全"></a>13.2 线程安全</h3><p>Brian Goetz对线程安全有一个比较恰当的定义：当多个线程访问一个对象时，如果不用考虑这些线程在运行时环境下的调度和交替执行，也不需要进行额外的同步，或者在调用方进行任何其他的协调操作，调用这个对象的行为都可以获得正确的结果，那这个对象是线程安全的。</p>
<h4 id="13-2-1-Java语言中的线程安全"><a href="#13-2-1-Java语言中的线程安全" class="headerlink" title="13.2.1 Java语言中的线程安全"></a>13.2.1 Java语言中的线程安全</h4><ul>
<li>我们可以将Java语言中各个操作共享的数据分为以下五类：不可变、绝对线程安全、相对线程安全、线程兼容和线程对立；</li>
<li>不可变：不可变带来的安全性是最简单和最纯粹的，如final的基本数据类型；如果共享的数据是一个对象，那就需要保证对象的行为不会对其状态产生任何影响才行，比如String类的substring、replace方法；Number类型的大部分子类都符合不可变要求的类型，但是AtomicInteger和AtomicLong则并非不可变的；</li>
<li>线程绝对安全：Java API中标注自己是线程安全的类，大多数都不是绝对的线程安全；比如java.util.Vector，不意味着调用它的是时候永远都不再需要同步手段了；</li>
<li>线程相对安全：是我们通常意义上所讲的线程安全，在Java语言中，大部分的线程安全类都属于这种类型；</li>
<li>线程兼容：指对象本身并不是线程安全的，但是可以通过在调用端正确地使用同步手段来保证对象在并发环境中可以安全地使用；我们说一个类不是线程安全的，绝大多数时候指的是这一种情况；</li>
<li>线程对立：无论调用端是否采取了同步措施，都无法在多线程环境中并发使用的代码，Java语言中很少出现；</li>
</ul>
<h4 id="13-2-2-线程安全的实现方法"><a href="#13-2-2-线程安全的实现方法" class="headerlink" title="13.2.2 线程安全的实现方法"></a>13.2.2 线程安全的实现方法</h4><ul>
<li>互斥同步：同步是指在多个线程并发访问共享数据时，保证共享数据在同一个时刻只被一个线程使用，而互斥是实现同步的一种手段，临界区、互斥量和信号量都是主要的互斥实现方式；Java中最基本的互斥同步手段就是synchronized关键字，它对同一个线程来说是可重入的且会阻塞后面其他线程的进入；另外还可以使用java.util.concurrent包中的重入锁（ReentrantLock）来实现同步，相比synchronized关键字ReentrantLock增加了一些高级功能：等待可中断、可实现公平锁以及锁可以绑定多个条件；</li>
<li>非阻塞同步：互斥同步最主要的问题就是进行线程阻塞和唤醒带来的性能问题，其属于一种悲观的并发策略；随着硬件指令集的发展，我们有了另外一个选择即基于冲突检测的乐观并发策略，就是先进行操作，如果没有其他线程争用共享数据那就操作成功了，如果有争用产生了冲突，那就再采取其他的补偿措施（最常见的就是不断重试直至成功），这种同步操作称为非阻塞同步；Java并发包的整数原子类，其中的compareAndSet和getAndIncrement等方法都使用了Unsafe类的CAS操作；</li>
<li>无同步方案：要保证线程安全，并不是一定就要进行同步；有一些代码天生就是线程安全的，比如可重入代码和线程本地存储的代码；</li>
</ul>
<h3 id="13-3-锁优化"><a href="#13-3-锁优化" class="headerlink" title="13.3 锁优化"></a>13.3 锁优化</h3><h4 id="13-3-1-自旋锁与自适应自旋"><a href="#13-3-1-自旋锁与自适应自旋" class="headerlink" title="13.3.1 自旋锁与自适应自旋"></a>13.3.1 自旋锁与自适应自旋</h4><ul>
<li>互斥同步对性能最大的影响是阻塞的实现，挂起线程和恢复线程的操作都需要转入内核态中完成，这些操作给系统的并发性能带来了很大的压力；另外在共享数据的锁定状态只会持续很短的一段时间，为了这段时间去挂起和恢复线程并不值得，如果让两个或以上的线程同时并行执行，让后面请求锁的那个线程稍等一下，但不放弃处理器的执行时间，看看持有锁的线程是否很快就会释放锁；为了让线程等待，我们只需让线程执行一个忙循环，这些技术就是所谓的自旋锁；</li>
<li>在JDK 1.6已经默认开启自旋锁；如果锁被占用的时间很短自旋等待的效果就会非常好，反之则会白白消耗处理器资源；</li>
<li>在JDK 1.6中引入了自适应的自旋锁，这意味着自旋的时间不再固定，而是由前一次在同一个锁上的自旋时间及锁的拥有者的状态来决定；</li>
</ul>
<h4 id="13-3-2-锁消除"><a href="#13-3-2-锁消除" class="headerlink" title="13.3.2 锁消除"></a>13.3.2 锁消除</h4><ul>
<li>锁消除是指虚拟机即时编译器在运行时，对一些代码上要求同步，但是被检测到不可能存在共享数据竞争的锁进行消除；</li>
<li>锁消除的主要判断依据来源于逃逸分析的数据支持；</li>
</ul>
<h4 id="13-3-3-锁粗化"><a href="#13-3-3-锁粗化" class="headerlink" title="13.3.3 锁粗化"></a>13.3.3 锁粗化</h4><ul>
<li>原则上总是推荐将同步块的作用范围限制得尽量小 – 只有在共享数据的实际作用域中才进行同步，这样是为了使得需要同步的操作数量尽可能变小，如果存在锁竞争，那等待锁的线程也能尽快拿到锁；</li>
<li>但是如果一系列的连续操作都对同一个对象反复加锁和解锁，甚至加锁操作是出现在循环体中的，那即使没有线程竞争，频繁地进行互斥同步操作也会导致不必要的性能损耗；  </li>
</ul>
<h4 id="13-3-4-轻量级锁"><a href="#13-3-4-轻量级锁" class="headerlink" title="13.3.4 轻量级锁"></a>13.3.4 轻量级锁</h4><ul>
<li>轻量级锁是JDK 1.6之中加入的新型锁机制，它是相对于使用操作系统互斥量来实现的传统锁而言的；它并不是用来代替重量级锁的，它的本意是在没有多线程竞争的前提下，减少传统的重量级锁使用操作系统互斥量产生的性能消耗；</li>
<li>要理解轻量级锁，以及后面会讲到的偏向锁的原理和运作过程，必须从HotSpot虚拟机的对象的内存布局开始介绍；HotSpot虚拟机的对象头分为两部分信息：第一部分用于存储对象自身的运行时数据，如哈希码、GC分代年龄等，这部分官方称之为Mark Word，是实现轻量级锁和偏向锁的关键，另外一部分用于存储指向方法区对象类型数据的指针； Mark Word被设计成一个非固定的数据结构以便在极小的空间存储尽量多的信息，在32位的HotSpot虚拟机中对象未被锁定的状态下，25bit用于存储对象哈希码，4bit用于存储对象分代年龄，2bit用于存储锁标志位，1bit固定为0；在其他状态（轻量级锁定、重量级锁定、GC标志、可偏向）下对象的存储内容如下：</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/mark_word_object_header.png" alt="HotSpot虚拟机对象头"></p>
<ul>
<li>在代码进入同步块的时候，如果此同步对象没有被锁定，虚拟机首先将在当前线程的栈帧中建立一个名为锁记录（Lock Record）的空间，用于存储对象目前的Mark Word的拷贝（官方称之为Displaced Mark Word）；然后虚拟机将使用CAS操作尝试将对象的Mark Word更新为指向Lock Record的指针，如果更新成功了那么这个线程就拥有了该对象的锁，并且对象Mark Word的锁标志位将转变为“00”，即表示此对象处于轻量级锁定状态；如果这个更新操作失败了，虚拟机首先会检查对象的Mark Word是否指向当前线程的栈帧，如果是就可以直接进入同步块继续执行，否则说明这个锁对象已经被其他线程抢占了；如果有两条以上的线程争用同一个锁，那轻量级锁就不再有效，要膨胀为重量级锁，锁标志的状态值变为“10”，Mark Word中存储的就是指向重量级锁的指针，后面等待锁的线程也要进行阻塞状态；</li>
<li>轻量级锁能提升程序同步性能的依据是“对于绝大部分的锁，在整个同步周期内都是不存在竞争的”，这是一个经验数据；</li>
</ul>
<h4 id="13-3-5-偏向锁"><a href="#13-3-5-偏向锁" class="headerlink" title="13.3.5 偏向锁"></a>13.3.5 偏向锁</h4><ul>
<li>偏向锁也是JDK 1.6中引入的一项锁优化，它的目的是消除数据在无竞争情况下的同步原语，进一步提高程序的运行性能；如果说轻量级锁是在无竞争的情况下使用CAS操作去消除同步使用的互斥量，那偏向锁就是在无竞争的情况下把整个同步都消除掉，连CAS操作都不做了；</li>
<li>偏向锁会偏向于第一个获得它的线程，如果在接下来的执行过程中，该锁没有被其他的线程获取，则持有偏向锁的线程将永远不需要再进行同步；</li>
<li>假设当前虚拟机启动了偏向锁，那么当锁对象第一次被线程获取的时候，虚拟机将会把对象头中的标志位设为“01”，即偏向模式；同时使用CAS操作把获取到这个锁的线程ID记录在对象的Mark Word之中；如果CAS操作成功，持有偏向锁的线程以后每次进入这个锁相关的同步块时，虚拟机都可以不再进行任何同步操作；当有另外一个线程去尝试获取这个锁时，偏向模式就宣告结束，根据锁对象目前是否被锁定的状态，撤销偏向后恢复到未锁定或轻量级锁定的状态，后续的同步操作就如上面介绍的轻量级锁那样执行；偏向锁、轻量级锁的状态转化以及对象Mark Work的关系如下图所示：</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/mark_word_for_lock.png" alt="偏向锁、轻量级锁的状态转化"></p>
<ul>
<li>偏向锁可以提高带有同步但无竞争的程序性能，它同样是一个带有效益权衡性质的优化；</li>
</ul>
<h3 id="本章小结"><a href="#本章小结" class="headerlink" title="本章小结"></a>本章小结</h3><p>本章介绍了线程安全所涉及的概念和分类、同步实现的方式及虚拟机的底层运行原理，并且介绍了虚拟机为了实现高效并发所采取的一系列锁优化措施。</p>
<p><strong>系列读书笔记</strong></p>
<ul>
<li><a href="http://ginobefunny.com/post/deep_in_jvm_notes_part1">《深入理解Java虚拟机》读书笔记1：Java技术体系、Java内存区域和内存溢出异常</a></li>
<li><a href="http://ginobefunny.com/post/deep_in_jvm_notes_part2">《深入理解Java虚拟机》读书笔记2：垃圾收集器与内存分配策略</a></li>
<li><a href="http://ginobefunny.com/post/deep_in_jvm_notes_part3">《深入理解Java虚拟机》读书笔记3：虚拟机性能监控与调优实战</a></li>
<li><a href="http://ginobefunny.com/post/deep_in_jvm_notes_part4">《深入理解Java虚拟机》读书笔记4：类文件结构</a></li>
<li><a href="http://ginobefunny.com/post/deep_in_jvm_notes_part5">《深入理解Java虚拟机》读书笔记5：类加载机制与字节码执行引擎</a></li>
<li><a href="http://ginobefunny.com/post/deep_in_jvm_notes_part6">《深入理解Java虚拟机》读书笔记6：程序编译与代码优化</a></li>
<li><a href="http://ginobefunny.com/post/deep_in_jvm_notes_part7">《深入理解Java虚拟机》读书笔记7：高效并发</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;国内JVM相关书籍NO.1，Java程序员必读。读书笔记第七部分对应原书的第十二章和第十三章，主要介绍Java内存模型、先行发生原则、线程安全和虚拟机的锁优化细节。&lt;br&gt;
    
    </summary>
    
      <category term="JVM" scheme="http://ginobefunny.com/categories/JVM/"/>
    
    
      <category term="Java" scheme="http://ginobefunny.com/tags/Java/"/>
    
      <category term="读书笔记" scheme="http://ginobefunny.com/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
      <category term="JVM" scheme="http://ginobefunny.com/tags/JVM/"/>
    
      <category term="虚拟机" scheme="http://ginobefunny.com/tags/%E8%99%9A%E6%8B%9F%E6%9C%BA/"/>
    
      <category term="HotSpot" scheme="http://ginobefunny.com/tags/HotSpot/"/>
    
      <category term="内存模型" scheme="http://ginobefunny.com/tags/%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/"/>
    
      <category term="volatile" scheme="http://ginobefunny.com/tags/volatile/"/>
    
      <category term="synchronized" scheme="http://ginobefunny.com/tags/synchronized/"/>
    
      <category term="锁" scheme="http://ginobefunny.com/tags/%E9%94%81/"/>
    
      <category term="同步" scheme="http://ginobefunny.com/tags/%E5%90%8C%E6%AD%A5/"/>
    
      <category term="并发" scheme="http://ginobefunny.com/tags/%E5%B9%B6%E5%8F%91/"/>
    
      <category term="多线程" scheme="http://ginobefunny.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"/>
    
      <category term="先行发生原则" scheme="http://ginobefunny.com/tags/%E5%85%88%E8%A1%8C%E5%8F%91%E7%94%9F%E5%8E%9F%E5%88%99/"/>
    
  </entry>
  
  <entry>
    <title>《深入理解Java虚拟机》读书笔记6：程序编译与代码优化</title>
    <link href="http://ginobefunny.com/post/deep_in_jvm_notes_part6/"/>
    <id>http://ginobefunny.com/post/deep_in_jvm_notes_part6/</id>
    <published>2017-01-26T09:30:57.000Z</published>
    <updated>2017-01-31T09:50:48.931Z</updated>
    
    <content type="html"><![CDATA[<p>国内JVM相关书籍NO.1，Java程序员必读。读书笔记第六部分对应原书的第十章和第十一章，主要介绍javac编译过程、HotSpot的即时编译器以及常见的编译优化技术，通过了解这部分的内容有利于我们更好的编码。<br><a id="more"></a></p>
<h1 id="第四部分-程序编译与代码优化"><a href="#第四部分-程序编译与代码优化" class="headerlink" title="第四部分 程序编译与代码优化"></a>第四部分 程序编译与代码优化</h1><h2 id="第十章-早期（编译器）优化"><a href="#第十章-早期（编译器）优化" class="headerlink" title="第十章 早期（编译器）优化"></a>第十章 早期（编译器）优化</h2><h3 id="10-1-概述"><a href="#10-1-概述" class="headerlink" title="10.1 概述"></a>10.1 概述</h3><ul>
<li>前端编译器（或叫编译器前端）：把<em>.java文件转变为</em>.class文件的过程，比如Sun的javac、Eclipse JDT中的ECJ；</li>
<li>后端运行编译器（JIT编译器）：把字节码转变为机器码的过程，比如HotSpot VM的C1、C2编译器；</li>
<li>静态提前编译器（AOT编译器）：直接把*.java文件编译成本地机器代码的过程，比如GNU Compiler for the Java；</li>
<li>本章主要针对第一类，把第二类的编译过程留到下一章讨论；</li>
<li>javac这类编译器对代码运行效率几乎没有任何优化措施，虚拟机设计团队把对性能的优化集中到了后端的即时编译器中，这样那些不是由javac产生的Class文件也同样能享受到编译器优化所带来的好处；</li>
<li>javac做了许多针对Java语言编码过程的优化措施来改善程序员的编码风格和提高编码效率；可以说，Java中即时编译器在运行期的优化过程对于程序运行来说更重要，而前端编译器在编译器的优化过程对于程序编码来说关系更加密切；</li>
</ul>
<h3 id="10-2-javac编译器"><a href="#10-2-javac编译器" class="headerlink" title="10.2 javac编译器"></a>10.2 javac编译器</h3><p>javac编译器本身就是一个由Java语言编写的程序，这为纯Java的程序员了解它的编译过程带来了很大的便利。</p>
<h4 id="10-2-1-javac的源码与调试"><a href="#10-2-1-javac的源码与调试" class="headerlink" title="10.2.1 javac的源码与调试"></a>10.2.1 javac的源码与调试</h4><ul>
<li>javac的源码存放在JDK_SRC_HOME/langtools/src/share/classes/com/sun/tools/javac，除了JDK自身的API外，就只引用了JDK_SRC_HOME/langtools/src/share/classes/com/sun/*里面的代码；</li>
<li>导入javac的源码后就可以运行com.sun.tools.javac.Main的main方法来执行编译了；</li>
<li>javac编译过程大概可以分为3个过程：解析与填充符号表过程、插入式注解处理器的注解处理过程、分析与字节码生成过程；</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/javac_compiler.png" alt="javac编译过程的主体代码"></p>
<h4 id="10-2-2-解析与填充符号表"><a href="#10-2-2-解析与填充符号表" class="headerlink" title="10.2.2 解析与填充符号表"></a>10.2.2 解析与填充符号表</h4><ul>
<li>解析步骤由parseFiles方法完成；</li>
<li>词法分析将源代码的字符流转变为标记（Token）集合，由com.sun.tools.javac.parser.Scanner类完成；</li>
<li>语法分析是根据Token序列构造抽象语法树（AST，一种用来描述程序代码语法结构的树形表示方式）的过程，由com.sun.tools.javac.parser.Parser类实现，AST由com.sun.tools.javac.tree.JCTree类表示；</li>
<li>填充符号表：由enterTrees方法完成；符号表是由一组符号地址和符号信息构成的表格，所登记的信息在编译的不同阶段都要用到，在语义分析中用于语义检查，在目标代码生成时用于地址分配；由com.sun.tools.javac.comp.Enter类实现；</li>
</ul>
<h4 id="10-2-3-注解处理器"><a href="#10-2-3-注解处理器" class="headerlink" title="10.2.3 注解处理器"></a>10.2.3 注解处理器</h4><ul>
<li>在JDK 1.6中实现了JSR-269规范，提供了一组插入式注解处理器的标准API在编译期间对注解进行处理，可以读取、修改、添加抽象语法树中的任意元素；</li>
<li>通过插入式注解处理器实现的插件在功能上有很大的发挥空间，程序员可以使用插入式注解处理器来实现许多原本只能在编码中完成的事情；</li>
<li>javac中，在initProcessAnnotations初始化，在processAnnotations执行，如果有新的注解处理器，通过com.sun.tools.javac.processing.JavacProcessingEnviroment类的doProcessing方法生成一个新的JavaCompiler对象对编译的后续步骤进行处理；</li>
</ul>
<h4 id="10-2-4-语义分析与字节码生成"><a href="#10-2-4-语义分析与字节码生成" class="headerlink" title="10.2.4 语义分析与字节码生成"></a>10.2.4 语义分析与字节码生成</h4><ul>
<li>语义分析的主要任务是对结构上正确的源程序进行上下文有关性质的审查，主要包括标注检查、数据及控制流分析两个步骤；</li>
<li>解语法糖（Syntactic Sugar，添加的某种对语言功能没有影响但方便程序员使用的语法）：Java中最常用的语法糖主要是泛型、变长参数、自动装箱等，他们在编译阶段还原回简单的基础语法结构；在com.sun.tools.javac.comp.TransTypes类和com.sun.tools.javac.comp.Lower类中完成；</li>
<li>字节码生成：javac编译的最后一个阶段，不仅仅是把前面各个步骤所生成的信息转化为字节码写入到磁盘中，编译器还进行了少量的代码添加和转换工作（如实例构造器<init>方法和类构造器<clinit>方法）；由com.sun.tools.javac.jvm.ClassWriter类的writeClass方法输出字节码，生成最终的Class文件；</clinit></init></li>
</ul>
<h3 id="10-3-Java语法糖的味道"><a href="#10-3-Java语法糖的味道" class="headerlink" title="10.3 Java语法糖的味道"></a>10.3 Java语法糖的味道</h3><h4 id="10-3-1-泛型与类型擦除"><a href="#10-3-1-泛型与类型擦除" class="headerlink" title="10.3.1 泛型与类型擦除"></a>10.3.1 泛型与类型擦除</h4><ul>
<li>Java语言的泛型只在程序源码中存在，在编译后的字节码文件中，就已经替换为原来的原生类型了，并且在相应的地方插入了强制转换，这种基于类型擦除的泛型实现是一种伪泛型；</li>
<li>JCP组织引入了Signature属性，它的作用就是存储一个方法在字节码层面的特征签名，这个属性中保存的参数类型并不是原生类型，而是包括了参数化类型的信息，这样我们就可以通过反射手段获取参数化类型；</li>
</ul>
<h4 id="10-3-2-自动装箱、拆箱与遍历循环"><a href="#10-3-2-自动装箱、拆箱与遍历循环" class="headerlink" title="10.3.2 自动装箱、拆箱与遍历循环"></a>10.3.2 自动装箱、拆箱与遍历循环</h4><ul>
<li>它们的实现比较简单，但却是Java语言里使用最多的语法糖；</li>
</ul>
<h4 id="10-3-3-条件编译"><a href="#10-3-3-条件编译" class="headerlink" title="10.3.3 条件编译"></a>10.3.3 条件编译</h4><ul>
<li>Java语言之中并没有使用预处理器，因为Java编译器并非一个个地编译Java文件，而是将所有编译单元的语法树顶级节点输入到待处理列表后再进行编译；</li>
<li>Java语言可以使用条件为常量的if语句进行条件编译；编译器将会把分支中不成立的代码块消除掉；</li>
</ul>
<h3 id="10-4-实战：插入式注解处理器"><a href="#10-4-实战：插入式注解处理器" class="headerlink" title="10.4 实战：插入式注解处理器"></a>10.4 实战：插入式注解处理器</h3><ul>
<li>实战目标：使用注解处理器API来编写一款拥有自己编码风格的校验工具；</li>
<li>代码实现：继承javax.annotation.processing.AbstractProcessor，实现process方法，从第一个参数annotations获取此注解处理器所要处理的注解集合，从第二个参数roundEnv中访问到当前这个Round中的语法树节点；另外还有一个很常用的实例变量processingEnv，它代表了注解处理器框架提供的一个上下文环境；可以配合使用的@SupportedAnnotationTypes和@SupportedSourceVersion注解；</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="meta">@SupportedAnnotationTypes</span>(<span class="string">"*"</span>)</div><div class="line"><span class="meta">@SupportedSourceVersion</span>(SourceVersion.RELEASE_6)</div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">NameCheckProcessor</span> <span class="keyword">extends</span> <span class="title">AbstractProcessor</span></span>&#123;</div><div class="line">    </div><div class="line">    <span class="keyword">private</span> NameChecker nameChecker;</div><div class="line">    </div><div class="line">    <span class="meta">@Override</span></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">init</span><span class="params">(ProcessingEnviroment processingEnv)</span></span>&#123;</div><div class="line">        <span class="keyword">super</span>.init(processingEnv);</div><div class="line">        nameChecker = <span class="keyword">new</span> NameChecker(processingEnv);</div><div class="line">    &#125;</div><div class="line">    </div><div class="line">    <span class="meta">@Override</span></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">process</span><span class="params">(Set&lt;? extends TypeElement&gt; annotations, RoundEnviroment roundEnv)</span></span>&#123;</div><div class="line">        <span class="keyword">if</span>(!roundEnv.processingOver)&#123;</div><div class="line">            <span class="keyword">for</span>(Element element : roundEnv.getRootElements())&#123;</div><div class="line">                nameChecker.checkNames(element);</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">        </div><div class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</div><div class="line">    &#125;</div><div class="line">    </div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="10-5-本章小结"><a href="#10-5-本章小结" class="headerlink" title="10.5 本章小结"></a>10.5 本章小结</h3><p>本章我们从编译器源码实现的层次上了解了javac源代码编译为字节码的过程，分析了Java语言中多种语法糖的前因后果，并实战实习了如何使用插入式注解处理器来完成一个检查程序命名规范的编译器插件。下一章我们将会介绍即时编译器的运作和优化过程。</p>
<h2 id="第十一章-晚期（运行期）优化"><a href="#第十一章-晚期（运行期）优化" class="headerlink" title="第十一章 晚期（运行期）优化"></a>第十一章 晚期（运行期）优化</h2><h3 id="11-1-概述"><a href="#11-1-概述" class="headerlink" title="11.1 概述"></a>11.1 概述</h3><ul>
<li>为了提高热点代码的执行效率，在运行时虚拟机将会把这些代码编译成与本地平台相关的机器码，并进行各种层次的优化，完成这个任务的编译器称为即时编译器（JIT）；</li>
<li>JIT不是虚拟机必需的，但是其编译性能的好坏、代码优化程度的高低却是衡量一款商用虚拟机优秀与否的最关键的指标之一，它也是虚拟机中最核心且最能体现虚拟机技术水平的部分；</li>
</ul>
<h3 id="11-2-HotSpot虚拟机内的即时编译器"><a href="#11-2-HotSpot虚拟机内的即时编译器" class="headerlink" title="11.2 HotSpot虚拟机内的即时编译器"></a>11.2 HotSpot虚拟机内的即时编译器</h3><h4 id="11-2-1-解释器与编译器"><a href="#11-2-1-解释器与编译器" class="headerlink" title="11.2.1 解释器与编译器"></a>11.2.1 解释器与编译器</h4><ul>
<li>当程序需要迅速启动和执行的时候，解释器可以先发挥作用，省去编译的时间立即执行；在程序运行后，随着时间的推移，编译器把越来越多的代码编译成本地代码提升执行效率；</li>
<li>HotSpot虚拟机中内置了两个即时编译器，分别为Client Compiler和Server Compiler，或简称为C1编译器和C2编译器；虚拟机会根据自身版本与宿主机器的硬件性能自动选择运行模式，也可以使用“-client”或“-server”参数去强制指定运行模式；</li>
<li>想要编译出优化程度更高的代码，解释器可能还要替编译器收集性能监控信息，为了在程序启动响应速度与运行效率之间达到最佳平衡，HotSpot虚拟机还会逐渐启动分层编译的策略：第0层，程序解释运行；第1层，C1编译；第2层，C2编译；</li>
<li>实施分层编译后，Client Compiler和Server Compiler将会同时工作，许多代码都可能会被多次编译，用Client Compiler获取更高的编译速度，用Server Compiler来获取更好的编译质量，在解释执行的时候也无须再承担性能收集监控信息的任务；</li>
</ul>
<h4 id="11-2-2-编译对象与触发条件"><a href="#11-2-2-编译对象与触发条件" class="headerlink" title="11.2.2 编译对象与触发条件"></a>11.2.2 编译对象与触发条件</h4><ul>
<li>被JIT编译的热点代码有两类：被多次调用的方法、被多次执行的循环体；对于前者编译器会以整个方法作为编译对象，属于标准的JIT编译方式；对于后者尽管编译动作是由循环体所触发的，但编译器依然会以整个方法作为编译对象，这种编译方式称之为栈上替换（OSR编译）；</li>
<li>热点探测：基于采样的热点探测和基于计数器的热点探测，在HotSpot虚拟机中使用的是第二种，通过方法计数器和回边计数器进行热点探测。方法调用计数器触发的即时编译交互过程如下图所示：</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/javac_jit.png" alt="调用计数器触发的即时编译"></p>
<h4 id="11-2-3-编译过程"><a href="#11-2-3-编译过程" class="headerlink" title="11.2.3 编译过程"></a>11.2.3 编译过程</h4><ul>
<li>对于Client Compiler来说，它是一个简单快速的三段式编译器，主要的关注点在于局部性的优化，而放弃了很多耗时较长的全局优化手段；第一阶段一个平台独立的前端将字节码构造成一个高级中间代码表示（HIR），第二阶段一个平台相关的后端从HIR中产生低级中间代码表示（LIR），最后阶段是在平台相关的后端使用线性扫描算法在LIR上分配寄存器，并在LIR上做窥孔优化，然后产生机器代码。其大致过程如下所示：</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/javac_client_compiler.png" alt="Client Compiler架构"></p>
<ul>
<li>Server Compiler是专门面向服务端的典型应用并为服务端的性能配置特别调整过的编译器，也是一个充分优化过的高级编译器，几乎能达到GNU C++编译器使用-02参数时的优化强大，它会执行所有经典的优化动作，如无用代码消除、循环展开、循环表达式外提、消除公共子表达式、常量传播、基本块重排序等，还会实现如范围检查消除、空值检查消除等Java语言特性密切相关的优化技术；</li>
</ul>
<h4 id="11-2-4-查看及分析即时编译结果"><a href="#11-2-4-查看及分析即时编译结果" class="headerlink" title="11.2.4 查看及分析即时编译结果"></a>11.2.4 查看及分析即时编译结果</h4><ul>
<li>本节的运行参数有一部分需要Debug或FastDebug版虚拟机的支持；</li>
<li>要知道某个方法是否被编译过，可以使用参数-XX:+PrintCompilation要求虚拟机在即时编译时将被编译成本地代码的方法名称打印出来；</li>
<li>还可以加上参数-XX:+PrintInlining要求虚拟机输出方法内联信息，输出内容如下：</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/javac_jit_viewer.png" alt="编译结果"></p>
<ul>
<li>除了查看那些方法被编译之外，还可以进一步查看即时编译器生成的机器码内容，这个需要结合虚拟机提供的反汇编接口来阅读；</li>
</ul>
<h3 id="11-3-编译优化技术"><a href="#11-3-编译优化技术" class="headerlink" title="11.3 编译优化技术"></a>11.3 编译优化技术</h3><h4 id="11-3-1-优化技术概览"><a href="#11-3-1-优化技术概览" class="headerlink" title="11.3.1 优化技术概览"></a>11.3.1 优化技术概览</h4><p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/javac_jit_tech1.png" alt="优化技术概览1"><br><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/javac_jit_tech2.png" alt="优化技术概览2"></p>
<h4 id="11-3-2-公共子表达式消除"><a href="#11-3-2-公共子表达式消除" class="headerlink" title="11.3.2 公共子表达式消除"></a>11.3.2 公共子表达式消除</h4><ul>
<li>如果一个表达式E已经计算过了，并且从先前的计算到现在E中所有变量的值都没有发生变化，那么E的这次出现就成为了公共子表达式，只需要直接用前面计算过的表达式结果代替E就可以了；</li>
</ul>
<h4 id="11-3-3-数组边界检查消除"><a href="#11-3-3-数组边界检查消除" class="headerlink" title="11.3.3 数组边界检查消除"></a>11.3.3 数组边界检查消除</h4><ul>
<li>对于虚拟机的执行子系统来说，每次数组元素的读写都带有一次隐含的条件判断，对于拥有大量数组访问的程序代码无疑是一种性能负担；</li>
</ul>
<h4 id="11-3-4-方法内联"><a href="#11-3-4-方法内联" class="headerlink" title="11.3.4 方法内联"></a>11.3.4 方法内联</h4><ul>
<li>除了消除方法调用的成本外更重要的意义是为其他优化手段建立良好的基础；</li>
<li>为了解决虚方法的内联问题，引入了类型继承关系分析（CHA）技术和内联缓存（Inline Cache）来完成方法内联；</li>
</ul>
<h4 id="11-3-5-逃逸分析"><a href="#11-3-5-逃逸分析" class="headerlink" title="11.3.5 逃逸分析"></a>11.3.5 逃逸分析</h4><ul>
<li>逃逸分析的基本行为就是分析对象动态作用域，当一个对象在方法中被定义后，它可能被外部方法所引用（方法逃逸），甚至还可能被外部线程所访问到（线程逃逸）；如果能证明一个对象不会逃逸到方法或线程之外，则可能为这个变量进行一些高效的优化，比如栈上分配（减轻垃圾收集的压力）、同步消除（读写不会有竞争）、标量替换；</li>
</ul>
<h3 id="11-4-Java与C-C-的编译器对比"><a href="#11-4-Java与C-C-的编译器对比" class="headerlink" title="11.4 Java与C/C++的编译器对比"></a>11.4 Java与C/C++的编译器对比</h3><ul>
<li>Java虚拟机的即时编译器与C/C++的静态优化编译器相比，可能会由于下列这些原因而导致输出的本地代码有一些劣势：即时编译器运行占用用户程序运行时间、动态类型安全语言导致的频繁检查、运行时对方法接收者进行多态选择的频率大、可以动态扩展导致很多全局的优化难以运行、大部分对象在堆上分配导致垃圾收集机制的效率低；</li>
<li>Java语言的特性换取了开发效率的提升、还有许多优化是静态优化编译器不好做的，比如别名分析、还有一些以运行期性能监控为基础的优化措施如调用频率预测等；</li>
</ul>
<h3 id="11-5-本章小结"><a href="#11-5-本章小结" class="headerlink" title="11.5 本章小结"></a>11.5 本章小结</h3><p>本章我们着重了解了虚拟机的热点探测方法、HotSpot的即时编译器、编译触发条件以及如何从虚拟机外部观察和分析JIT编译的数据和结果，还选择了集中场景的编译期优化技术进行讲解。对Java编译器的深入了解，有助于在工作中分辨哪些代码是编译器可以帮我们处理的，哪些代码需要自己调节以便更适合编译器的优化。</p>
<p><strong>系列读书笔记</strong></p>
<ul>
<li><a href="http://ginobefunny.com/post/deep_in_jvm_notes_part1">《深入理解Java虚拟机》读书笔记1：Java技术体系、Java内存区域和内存溢出异常</a></li>
<li><a href="http://ginobefunny.com/post/deep_in_jvm_notes_part2">《深入理解Java虚拟机》读书笔记2：垃圾收集器与内存分配策略</a></li>
<li><a href="http://ginobefunny.com/post/deep_in_jvm_notes_part3">《深入理解Java虚拟机》读书笔记3：虚拟机性能监控与调优实战</a></li>
<li><a href="http://ginobefunny.com/post/deep_in_jvm_notes_part4">《深入理解Java虚拟机》读书笔记4：类文件结构</a></li>
<li><a href="http://ginobefunny.com/post/deep_in_jvm_notes_part5">《深入理解Java虚拟机》读书笔记5：类加载机制与字节码执行引擎</a></li>
<li><a href="http://ginobefunny.com/post/deep_in_jvm_notes_part6">《深入理解Java虚拟机》读书笔记6：程序编译与代码优化</a></li>
<li><a href="http://ginobefunny.com/post/deep_in_jvm_notes_part7">《深入理解Java虚拟机》读书笔记7：高效并发</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;国内JVM相关书籍NO.1，Java程序员必读。读书笔记第六部分对应原书的第十章和第十一章，主要介绍javac编译过程、HotSpot的即时编译器以及常见的编译优化技术，通过了解这部分的内容有利于我们更好的编码。&lt;br&gt;
    
    </summary>
    
      <category term="JVM" scheme="http://ginobefunny.com/categories/JVM/"/>
    
    
      <category term="Java" scheme="http://ginobefunny.com/tags/Java/"/>
    
      <category term="读书笔记" scheme="http://ginobefunny.com/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
      <category term="JVM" scheme="http://ginobefunny.com/tags/JVM/"/>
    
      <category term="虚拟机" scheme="http://ginobefunny.com/tags/%E8%99%9A%E6%8B%9F%E6%9C%BA/"/>
    
      <category term="字节码" scheme="http://ginobefunny.com/tags/%E5%AD%97%E8%8A%82%E7%A0%81/"/>
    
      <category term="编译器" scheme="http://ginobefunny.com/tags/%E7%BC%96%E8%AF%91%E5%99%A8/"/>
    
      <category term="解释器" scheme="http://ginobefunny.com/tags/%E8%A7%A3%E9%87%8A%E5%99%A8/"/>
    
      <category term="HotSpot" scheme="http://ginobefunny.com/tags/HotSpot/"/>
    
      <category term="即时编译器" scheme="http://ginobefunny.com/tags/%E5%8D%B3%E6%97%B6%E7%BC%96%E8%AF%91%E5%99%A8/"/>
    
  </entry>
  
  <entry>
    <title>基于Elasticsearch实现搜索建议</title>
    <link href="http://ginobefunny.com/post/search_suggestion_implemention_based_elasticsearch/"/>
    <id>http://ginobefunny.com/post/search_suggestion_implemention_based_elasticsearch/</id>
    <published>2017-01-23T09:14:44.000Z</published>
    <updated>2017-01-23T12:30:46.686Z</updated>
    
    <content type="html"><![CDATA[<p>搜索建议是搜索的一个重要组成部分，一个搜索建议的实现通常需要考虑建议词的来源、匹配、排序、聚合、关联的文档数和拼写纠错等，本文介绍一个基于Elasticsearch实现的搜索建议。</p>
<a id="more"></a>
<h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>电商网站的搜索是最基础最重要的功能之一，搜索框上面的良好体验能为电商带来更高的收益，我们先来看看淘宝、京东、亚马逊网站的搜索建议。</p>
<p>在淘宝的搜索框输入【卫衣】时，下方的搜索建议包括建议词以及相关的标签：<br><img src="http://oi46mo3on.bkt.clouddn.com/11_es_suggestion/suggestion_taobao.png" alt="淘宝的搜索建议"></p>
<p>在京东的搜索框输入【卫衣】时，下方搜索建议右方显示建议词关联的商品数量：<br><img src="http://oi46mo3on.bkt.clouddn.com/11_es_suggestion/suggestion_jindong.png" alt="京东的搜索建议"></p>
<p>在亚马逊的搜索框输入【卫衣】时，搜索建议上部分能支持在特定的分类下进行搜索：<br><img src="http://oi46mo3on.bkt.clouddn.com/11_es_suggestion/suggestion_amazon.png" alt="亚马逊的搜索建议"></p>
<p>通过上述对比可以看出，不同的电商对于搜索建议的侧重点略有不同，但核心的问题包括：</p>
<ul>
<li>匹配：能够通过用户的输入进行前缀匹配；</li>
<li>排序：根据建议词的优先级进行排序；</li>
<li>聚合：能够根据建议词关联的商品进行聚合，比如聚合分类、聚合标签等；</li>
<li>纠错：能够对用户的输入进行拼写纠错；</li>
</ul>
<h2 id="搜索建议实现"><a href="#搜索建议实现" class="headerlink" title="搜索建议实现"></a>搜索建议实现</h2><p>在我们的搜索建议实现里，主要考虑了建议词的来源、匹配、排序、关联的商品数量和拼写纠错。</p>
<h3 id="SuggestionDiscovery"><a href="#SuggestionDiscovery" class="headerlink" title="SuggestionDiscovery"></a>SuggestionDiscovery</h3><ul>
<li>SuggestionDiscovery的职责是发现建议词；</li>
<li>建议词的来源可以是商品的分类名称、品牌名称、商品标签、商品名称的高频词、热搜词，也可以是一些组合词，比如“分类 + 性别”和“分类 + 标签”，还可以是一些自定义添加的词；</li>
<li>建议词维护的时候需要考虑去重，比如“卫衣男”和“卫衣 男”应该是相同的，“Nike”和“nike”也应该是相同的；</li>
<li>由于建议词的来源通常比较稳定，所以执行的周期可以比较长一点，比如每周一次；</li>
</ul>
<h3 id="SuggestionCounter"><a href="#SuggestionCounter" class="headerlink" title="SuggestionCounter"></a>SuggestionCounter</h3><ul>
<li>SuggestionCounter的职责是获取建议词关联的商品数量，如果需要可以进行一些聚合操作，比如聚合分类和标签；</li>
<li>SuggestionCounter的实现的时候由于要真正地调用搜索接口，应该尽量避免对用户搜索的影响，比如在凌晨执行并且使用单线程调用；</li>
<li>为了提升效率，应该使用Elasticsearch的Multi Search接口批量进行count，同时批量更新数据库里建议词的count值；</li>
<li>由于SuggestionCounter是比较耗资源的，可以考虑延长执行的周期，但是这可能会带来count值与实际搜索时误差较大的问题，这个需要根据实际情况考虑；</li>
</ul>
<h3 id="SuggestionIndexRebuiler"><a href="#SuggestionIndexRebuiler" class="headerlink" title="SuggestionIndexRebuiler"></a>SuggestionIndexRebuiler</h3><ul>
<li>SuggestionIndexRebuiler的职责是负责重建索引；</li>
<li>考虑到用户的搜索习惯，可以使用<a href="https://www.elastic.co/guide/en/elasticsearch/reference/2.3/multi-fields.html#_multi_fields_with_multiple_analyzers" target="_blank" rel="external">Multi-fields</a>来给建议词增加多个分析器。比如对于【卫衣 套头】的建议词使用Multi-fields增加不分词字段、拼音分词字段、拼音首字母分词字段、IK分词字段，这样输入【weiyi】和【套头】都可以匹配到该建议词；</li>
<li>重建索引时通过是通过bulk批量添加到临时索引中，然后通过别名来更新；</li>
<li>重建索引的数据依赖于SuggestionCounter，因此其执行的周期应该与SuggestionCounter保持一致；</li>
</ul>
<h3 id="SuggestionService"><a href="#SuggestionService" class="headerlink" title="SuggestionService"></a>SuggestionService</h3><ul>
<li>SuggestionService是真正处于用户搜索建议的服务类；</li>
<li>通常的实现是先到缓存中查询是否能匹配到缓存记录，如果能匹配到则直接返回；否则的话调用Elasticsearch的<a href="https://www.elastic.co/guide/en/elasticsearch/reference/2.3/query-dsl-prefix-query.html" target="_blank" rel="external">Prefix Query</a>进行搜索，由于我们在重建索引的时候定义了Multi-fields，在搜索的时候应该用boolQuery来处理；如果此时Elasticsearch返回不为空的结果数据，那么加入缓存并返回即可；</li>
</ul>
<pre><code>POST /suggestion/_search
{
  &quot;from&quot; : 0,
  &quot;size&quot; : 10,
  &quot;query&quot; : {
    &quot;bool&quot; : {
      &quot;must&quot; : {
        &quot;bool&quot; : {
          &quot;should&quot; : [ {
            &quot;prefix&quot; : {
              &quot;keyword&quot; : &quot;卫衣&quot;
            }
          }, {
            &quot;prefix&quot; : {
              &quot;keyword.keyword_ik&quot; : &quot;卫衣&quot;
            }
          }, {
            &quot;prefix&quot; : {
              &quot;keyword.keyword_pinyin&quot; : &quot;卫衣&quot;
            }
          }, {
            &quot;prefix&quot; : {
              &quot;keyword.keyword_first_py&quot; : &quot;卫衣&quot;
            }
          } ]
        }
      },
      &quot;filter&quot; : {
        &quot;range&quot; : {
          &quot;count&quot; : {
            &quot;from&quot; : 5,
            &quot;to&quot; : null,
            &quot;include_lower&quot; : true,
            &quot;include_upper&quot; : true
          }
        }
      }
    }
  },
  &quot;sort&quot; : [ {
    &quot;weight&quot; : {
      &quot;order&quot; : &quot;desc&quot;
    }
  }, {
    &quot;count&quot; : {
      &quot;order&quot; : &quot;desc&quot;
    }
  } ]
}
</code></pre><ul>
<li>如果Elasticsearch返回的是空结果，此时应该需要增加拼写纠错的处理（拼写纠错也可以在调用Elasticsearch搜索的时候带上，但是通常情况下用户并没有拼写错误，所以建议还是在后面单独调用suggester）；如果返回的suggest不为空，则根据新的词调用建议词服务；比如用户输入了【adidss】，调用Elasticsearch的suggester获取到的结果是【adidas】，则再根据adidas进行搜索建议词处理。</li>
</ul>
<pre><code>POST /suggestion/_search
{
  &quot;size&quot; : 0,
  &quot;suggest&quot; : {
    &quot;keyword_suggestion&quot; : {
      &quot;text&quot; : &quot;adidss&quot;,
      &quot;term&quot; : {
        &quot;field&quot; : &quot;keyword&quot;,
        &quot;size&quot; : 1
      }
    }
  }
}
</code></pre><ul>
<li>关于排序：在我们的实现里面是通过weight和count进行排序的，weight目前只考虑了建议词的类型（比如分类 &gt; 品牌 &gt; 标签）；</li>
</ul>
<h2 id="实现效果和后续改进"><a href="#实现效果和后续改进" class="headerlink" title="实现效果和后续改进"></a>实现效果和后续改进</h2><ul>
<li>通过上面的实现，我们已经能实现一个比较强大的搜索建议词了，实际的效果如下所示：</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/11_es_suggestion/suggestion_yoho.png" alt="最终效果"></p>
<ul>
<li>后续可以考虑的改进：参考亚马逊增加分类的聚合展示、增加用户个性化的处理支持更好的建议词排序、基于用户的搜索历史支持更好的建议词推荐；</li>
</ul>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/2.3/query-dsl-prefix-query.html" target="_blank" rel="external">Elasticsearch Prefix Query</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/2.3/search-suggesters.html" target="_blank" rel="external">Elasticsearch Suggester</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;搜索建议是搜索的一个重要组成部分，一个搜索建议的实现通常需要考虑建议词的来源、匹配、排序、聚合、关联的文档数和拼写纠错等，本文介绍一个基于Elasticsearch实现的搜索建议。&lt;/p&gt;
    
    </summary>
    
      <category term="Elasticsearch" scheme="http://ginobefunny.com/categories/Elasticsearch/"/>
    
    
      <category term="Elasticsearch" scheme="http://ginobefunny.com/tags/Elasticsearch/"/>
    
      <category term="搜索建议" scheme="http://ginobefunny.com/tags/%E6%90%9C%E7%B4%A2%E5%BB%BA%E8%AE%AE/"/>
    
  </entry>
  
  <entry>
    <title>《深入理解Java虚拟机》读书笔记5：类加载机制与字节码执行引擎</title>
    <link href="http://ginobefunny.com/post/deep_in_jvm_notes_part5/"/>
    <id>http://ginobefunny.com/post/deep_in_jvm_notes_part5/</id>
    <published>2017-01-22T06:57:08.000Z</published>
    <updated>2017-01-31T09:49:55.824Z</updated>
    
    <content type="html"><![CDATA[<p>国内JVM相关书籍NO.1，Java程序员必读。读书笔记第五部分对应原书的第七章至第九章，主要介绍虚拟机的类加载机制、字节码执行引擎，并通过实例和实战加深对虚拟机执行子系统这一部分的理解。<br><a id="more"></a></p>
<h2 id="第七章-虚拟机类加载机制"><a href="#第七章-虚拟机类加载机制" class="headerlink" title="第七章 虚拟机类加载机制"></a>第七章 虚拟机类加载机制</h2><h3 id="7-1-概述"><a href="#7-1-概述" class="headerlink" title="7.1 概述"></a>7.1 概述</h3><ul>
<li>虚拟机把描述类的数据从Class文件加载到内存，并对数据进行校验、转换解析和初始化，最终形成可以被虚拟机直接使用的Java类型，这就是虚拟机的类加载机制。</li>
<li>在Java语言里面，类型的加载、连接和初始化过程都是在程序运行期间完成，这虽然增量一些性能开销，但是会为Java应用程序提供高度的灵活性。</li>
</ul>
<h3 id="7-2-类加载的时机"><a href="#7-2-类加载的时机" class="headerlink" title="7.2 类加载的时机"></a>7.2 类加载的时机</h3><ul>
<li>类的整个生命周期：加载、验证、准备、解析、初始化、使用和卸载；其中验证、准备和解析统称为连接；</li>
<li>虚拟机规范没有强制约束类加载的时机，但严格规定了有且只有5种情况必须立即对类进行初始化：遇到new、getstatic、putstatic和invokestatic指令；对类进行反射调用时如果类没有进行过初始化；初始化时发现父类还没有进行初始化；虚拟机启动指定的主类；动态语言中MethodHandle实例最后解析结果REF_getStatic等的方法句柄对应的类没有初始化时；</li>
</ul>
<h3 id="7-3-类加载的过程"><a href="#7-3-类加载的过程" class="headerlink" title="7.3 类加载的过程"></a>7.3 类加载的过程</h3><h4 id="7-3-1-加载"><a href="#7-3-1-加载" class="headerlink" title="7.3.1 加载"></a>7.3.1 加载</h4><ul>
<li>通过一个类的全限定名来获取定义此类的二进制字节流；</li>
<li>将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构；</li>
<li>在内存中生成一个代表这个类的java.lang.Class对象，作为方法区这个类的各种数据的访问入口；</li>
</ul>
<h4 id="7-3-2-验证"><a href="#7-3-2-验证" class="headerlink" title="7.3.2 验证"></a>7.3.2 验证</h4><ul>
<li>验证是连接阶段的第一步，其目的是确保Class文件的字节流中包含的信息符合当前虚拟机的要求，并且不会危害虚拟机自身的安全；</li>
<li>验证阶段是非常重要的，这个阶段是否严谨决定了Java虚拟机是否能承受恶意代码的攻击；</li>
<li>校验动作：文件格式验证（基于二进制字节流）、元数据验证（对类的元数据语义分析）、字节码验证（对方法体语义分析）、符号引用验证（对类自身以外的信息进行匹配性校验）；</li>
</ul>
<h4 id="7-3-3-准备"><a href="#7-3-3-准备" class="headerlink" title="7.3.3 准备"></a>7.3.3 准备</h4><ul>
<li>正式为变量分配内存并设置类变量初始值的阶段，这些变量所使用的内存都将在这个方法区中进行分配；</li>
<li>需要强调两点：这时候内存分配的仅包括类变量，而不包括类实例变量；这里所说的初始化通常情况下是数据类型的零值，真正的赋值是在初始化阶段，如果是static final的则是直接赋值；</li>
</ul>
<h4 id="7-3-4-解析"><a href="#7-3-4-解析" class="headerlink" title="7.3.4 解析"></a>7.3.4 解析</h4><ul>
<li>解析阶段是虚拟机将常量池内的符号引用（如CONSTANT_Class_info、CONSTANT_Fieldref_info、CONSTANT_Methodref_info等7种）替换为直接引用的过程；</li>
<li>符号引用可以是任何形式的字面量，与虚拟机实现的内存布局无关，引用的目标并不一定已经加载到内存中；而直接引用是直接指向目标的指针、相对偏移量或是一个能间接定位到目标的句柄，它和虚拟机实现的内存布局相关，引用的目标必定以及在内存中存在；</li>
<li>对同一个符号引用进行多次解析请求是很常见的事情，虚拟机实现可以对第一次解析的结果进行缓存；</li>
</ul>
<h4 id="7-3-5-初始化"><a href="#7-3-5-初始化" class="headerlink" title="7.3.5 初始化"></a>7.3.5 初始化</h4><ul>
<li>是类加载过程的最后一步，真正开始执行类中定义的Java程序代码（或者说是字节码）；</li>
<li>初始化阶段是执行类构造器<clinit>方法的过程，该方法是由编译器自动收集类中的所有类变量的赋值动作和静态语句块中的语句合并产生的；</clinit></li>
<li><clinit>方法与类的构造函数（或者说是实例构造器<init>方法）不同，它不需要显式地调用父类构造器，虚拟机会保证在子类的<clinit>方法执行之前，父类的<clinit>方法已执行完毕；</clinit></clinit></init></clinit></li>
<li>执行接口的<clinit>方法不需要先执行父接口的<clinit>方法，只有当父接口中定义的变量使用时父接口才会初始化，接口的实现类在初始化时也一样不会执行接口的<clinit>方法；</clinit></clinit></clinit></li>
<li><clinit>方法初始化是加锁阻塞等待的，应当避免在<clinit>方法中有耗时很长的操作；</clinit></clinit></li>
</ul>
<h3 id="7-4-类加载器"><a href="#7-4-类加载器" class="headerlink" title="7.4 类加载器"></a>7.4 类加载器</h3><ul>
<li>虚拟机设计团队把类加载阶段的“通过一个类的全限定名来获取描述此类的二进制字节流”这个动作放到虚拟机外部去实现，实现这个动作的代码模块称为类加载器；</li>
<li>这时Java语言的一项创新，也是Java语言流行的重要原因，在类层次划分、OSGI、热部署、代码加密等领域大放异彩；</li>
</ul>
<h4 id="7-4-1-类与类加载器"><a href="#7-4-1-类与类加载器" class="headerlink" title="7.4.1 类与类加载器"></a>7.4.1 类与类加载器</h4><ul>
<li>对于任意一个类，都需要由加载它的类加载器和这个类本身一同确立其在Java虚拟机的唯一性，每一个类加载器都拥有一个独立的类名称空间；</li>
<li>比较两个类是否相等（如Class对象的equals方法、isAssignableFrom方法、isInstance方法），只有在这两个类是由同一个类加载器加载的前提下才有意义；</li>
</ul>
<h4 id="7-4-2-双亲委派模型"><a href="#7-4-2-双亲委派模型" class="headerlink" title="7.4.2 双亲委派模型"></a>7.4.2 双亲委派模型</h4><p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/classloader_extend.png" alt="双亲委派模型"></p>
<ul>
<li>三种系统提供的类加载器：启动类加载器（Bootstrap ClassLoader）、扩展类加载器（Extension ClassLoader）、应用程序类加载器（Application ClassLoader）；</li>
<li>双亲委派模型要求除了顶层的启动类加载器外，其他的类加载器都应当有自己的父类加载器，这里一般不会以继承的关系来实现，而是使用组合的关系来复用父加载器的代码；</li>
<li>其工作过程是：如果一个类加载器收到了类加载的请求，它首先不会自己去尝试加载这个类，而是把这个请求委派给父类加载器去完成，只有父类加载器反馈自己无法完成这个加载请求时（它的搜索范围中没有找到所需的类），子加载器才会尝试自己去加载；</li>
<li>这样的好处是Java类随着它的类加载器具备了一种带有优先级的层次关系，对保证Java程序的稳定运作很重要；</li>
<li>实现双亲委派的代码都集中在java.lang.ClassLoader的loadClass方法中，逻辑清晰易懂；</li>
</ul>
<h4 id="7-4-3-破坏双亲委派模型"><a href="#7-4-3-破坏双亲委派模型" class="headerlink" title="7.4.3 破坏双亲委派模型"></a>7.4.3 破坏双亲委派模型</h4><ul>
<li>上一小节的双亲委派模型是Java设计者推荐给开发者的类加载器实现方法，但不是一个强制性的约束模型；</li>
<li>典型的两种情况：为了解决JNI接口提供者（SPI）引入的线程上下文类加载器；为了程序动态性加强的OSGI的Bundle类加载器；</li>
</ul>
<h3 id="7-5-本章小结"><a href="#7-5-本章小结" class="headerlink" title="7.5 本章小结"></a>7.5 本章小结</h3><p>本章介绍了类加载过程的加载、验证、准备、解析和初始化五个阶段中虚拟机进行了哪些动作，还介绍了类加载器的工作原理及其对虚拟机的意义。下一章将一起看看虚拟机如果执行定义在Class文件里的字节码。</p>
<h2 id="第八章-虚拟机字节码执行引擎"><a href="#第八章-虚拟机字节码执行引擎" class="headerlink" title="第八章 虚拟机字节码执行引擎"></a>第八章 虚拟机字节码执行引擎</h2><h3 id="8-1-概述"><a href="#8-1-概述" class="headerlink" title="8.1 概述"></a>8.1 概述</h3><ul>
<li>执行引擎是Java虚拟机最核心的组成部分之一，区别于物理机的执行引擎是直接建立在处理器、硬件、指令集和操作系统层面上的，虚拟机的执行引擎是自己实现的，可以自行制定指令集与执行引擎的结构体系，并且能够执行哪些不被硬件直接支持的指令集格式；</li>
<li>在虚拟机规范中制定了虚拟机字节码执行引擎的概念模型，该模型成为各种虚拟机执行引擎的统一外观；</li>
<li>在不同的虚拟机实现里面，执行引擎在执行Java代码时可能会有解释执行和编译执行两种选择，也可能两者兼备，甚至还可能会包含几个不同级别的编译器执行引擎，但从外观来说是一致的：输入的都是字节码文件，处理过程是字节码解析的等效过程，输出的是执行结果。</li>
</ul>
<h3 id="8-2-运行时栈帧结构"><a href="#8-2-运行时栈帧结构" class="headerlink" title="8.2 运行时栈帧结构"></a>8.2 运行时栈帧结构</h3><p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/execution_stackframe.png" alt="运行时栈帧结构"></p>
<ul>
<li>栈帧是用于支持虚拟机进行方法调用和方法执行的数据结构，它是虚拟机运行时数据区中的虚拟机栈的栈元素；</li>
<li>栈帧存储了方法的局部变量表、操作数栈、动态连接和方法返回地址等信息，每一个方法从调用开始至执行完成的过程，都对应着一个栈帧在虚拟机里面从入栈到出栈的过程；</li>
<li>栈帧需要分配多少内存在编译时就完全确定并写入到方法表的Code属性之中了，不会受到程序运行期变量数据的影响；</li>
<li>对于执行引擎来说，在活动线程中只有位于栈顶的栈帧才算有效的，称为当前栈帧，与这个栈帧相关联的方法称为当前方法，执行引擎运行的所有字节码指令都只针对当前栈帧进行操作。</li>
</ul>
<h4 id="8-2-1-局部变量表"><a href="#8-2-1-局部变量表" class="headerlink" title="8.2.1 局部变量表"></a>8.2.1 局部变量表</h4><ul>
<li>是一组变量值存储空间，用于存放方法参数和方法内部定义的局部变量，Code属性的max_locals确定了该方法所需要分配的局部变量表的最大容量；</li>
<li>其容量以变量槽（Variable Slot）为最小单位，虚拟机规范允许Slot的长度随处理器、操作系统或虚拟机的不同而发生变化；</li>
<li>一个Slot可以存放一个32位以内的数据类型，包括boolean、byte、char。short、int、float、reference和returnAddress这八种类型；对于64位的数据类型（long和double），虚拟机会以高位对齐的方式为其分配两个连续的Slot空间；</li>
</ul>
<h4 id="8-2-2-操作数栈"><a href="#8-2-2-操作数栈" class="headerlink" title="8.2.2 操作数栈"></a>8.2.2 操作数栈</h4><ul>
<li>也常称为操作栈，它是一个后入先出栈；Code属性的max_stacks确定了其最大深度；</li>
<li>比如整数加法的字节码指令iadd在运行的时候操作数栈中最接近栈顶的两个元素已经存入了两个int型的数值，当执行这个指令时，会将这两个int值出栈并相加，然后将相加的结果入栈；</li>
<li>操作数栈中元素的类型必须与字节码指令的序列严格匹配；</li>
<li>Java虚拟机的解释执行引擎称为“基于栈的执行引擎”，其中所指的栈就是操作数栈；</li>
</ul>
<h4 id="8-2-3-动态连接"><a href="#8-2-3-动态连接" class="headerlink" title="8.2.3 动态连接"></a>8.2.3 动态连接</h4><ul>
<li>每个栈帧都包含一个执行运行时常量池中该栈帧所属方法引用，持有这个引用是为了支持方法调用过程中的动态连接（Dynamic Linking）；</li>
<li>Class文件的常量池的符号引用，有一部分在类加载阶段或者第一次使用时就转换为直接引用，这种称为静态解析，而另外一部分在每一次运行期间转换为直接引用，这部分称为动态连接；</li>
</ul>
<h4 id="8-2-4-方法返回地址"><a href="#8-2-4-方法返回地址" class="headerlink" title="8.2.4 方法返回地址"></a>8.2.4 方法返回地址</h4><ul>
<li>退出方法的方式：正常完成出口和异常完成出口；</li>
<li>方法退出的过程实际上就等同于把当前栈帧出栈，因此退出时可能只需的操作有：恢复上层方法的局部变量表和操作数栈，把返回值压入调用者栈帧的操作数中，调整PC计数器的值以只需方法调用指令后面的一套指令等；</li>
</ul>
<h4 id="8-2-5-附加信息"><a href="#8-2-5-附加信息" class="headerlink" title="8.2.5 附加信息"></a>8.2.5 附加信息</h4><ul>
<li>虚拟机规范允许具体的虚拟机实现增加一些规范里没有描述的信息到栈帧中，例如与调试相关的信息，这部分完成取决于具体的虚拟机实现；</li>
</ul>
<h3 id="方法调用"><a href="#方法调用" class="headerlink" title="方法调用"></a>方法调用</h3><ul>
<li>方法调用并不等同于方法执行，方法调用阶段唯一的任务就是确定被调用方法的版本即调用哪一个方法，暂时还不涉及方法内部的具体运行过程；</li>
<li>Class文件的编译过程中不报警传统编译的连接步骤，一切方法调用在Class文件里面存储的都只是符号引用，而不是方法在实际运行时内存布局的入口地址。这个特性给Java带来了更强大的动态扩展能力，但也使得Java方法调用过程变得相对复杂；</li>
</ul>
<h4 id="8-3-1-解析"><a href="#8-3-1-解析" class="headerlink" title="8.3.1 解析"></a>8.3.1 解析</h4><ul>
<li>方法在程序真正运行之前就有一个可确定的调用版本，并且这个方法的调用版本在运行期是不可改变的，这类方法的调用称为解析；</li>
<li>在Java语言中符合编译器可知、运行期不可变这个要求的方法，主要包括静态方法和私有方法两大类；</li>
<li>五条方法调用字节码指令：invokestatic、invokespecial、invokevirtual、invokeinterface、invokedynamic；</li>
<li>解析调用是一个静态的过程，在编译期间就完全确定，在类加载的解析阶段就会把涉及的符号引用全部转变为可确定的直接引用；而分派调用则可能是静态的也可能是动态的；</li>
</ul>
<h4 id="8-3-2-分派"><a href="#8-3-2-分派" class="headerlink" title="8.3.2 分派"></a>8.3.2 分派</h4><ul>
<li>静态分派：“Human man = new Man();”语句中Human称为变量的静态类型，后面的Man称为变量的实际类型；静态类型和实际类型在程序中都可以发生一些变化，区别是静态类型的变化仅仅在使用时发生，变量本身的静态类型不会被改变，并且最终的静态类型是在编译器可知的；而实际类型的变化在运行期才确定，编译器在编译程序的时候并不知道一个对象的实际类型是什么；编译器在重载时是通过参数的静态类型而不是实际类型作为判定依据的；所有根据静态类型来定位方法执行版本的分派动作称为静态分派，其典型应用是方法重载；</li>
<li>动态分派：invokevirtual指令执行的第一步就是在运行期间确定接收者的实际类型，所以两次调用中invokevirtual指令把常量池中的类方法符号引用解析到了不同的直接引用上，这个过程就是Java语言中方法重写的本质；我们把这种在运行期根据实际类型确定方法执行版本的分派过程称为动态分派；</li>
<li>单分派与多分派：方法的接收者与方法的参数统称为方法的宗量，根据分派基于多少种宗量，可以将分派分为单分派（根据一个宗量对目标方法进行选择）与多分派（根据多于一个宗量对目标方法进行选择）两种；今天的Java语言是一门静态多分派、动态单分派的语言；</li>
<li>虚拟机动态分派的实现：在方法区中建立一个虚方法表（Virtual Method Table），使用虚方法表索引来代替元数据查找以提高性能；方法表一般在类加载的连接阶段进行初始化，准备了类的变量初始化值后，虚拟机会把该类的方法表也初始化完毕；</li>
</ul>
<h4 id="8-3-3-动态类型语言支持"><a href="#8-3-3-动态类型语言支持" class="headerlink" title="8.3.3 动态类型语言支持"></a>8.3.3 动态类型语言支持</h4><ul>
<li>JDK 1.7发布增加的invokedynamic指令实现了“动态类型语言”支持，也是为JDK 1.8顺利实现Lambda表达式做技术准备；</li>
<li>动态类型语言的关键特征是它的类型检查的主体过程是在运行期而不是编译器，比如JavaScript、Python等；</li>
<li>Java语言在编译期间就将方法完整的符号引用生成出来，作为方法调用指令的参数存储到Class文件中；这个符号引用包含了此方法定义在哪个具体类型之中、方法的名字以及参数顺序、参数类型和方法返回值等信息；而在ECMAScript等动态语言中，变量本身是没有类型的，变量的值才具有类型，编译时最多只能确定方法名称、参数、返回值这些信息，而不会去确定方法所在的具体类型；变量无类型而变量值才有类型，这个特点也是动态类型语言的一个重要特征；</li>
<li>JDK 1.7实现了JSR-292，新加入的java.lang.invoke包的主要目的是在之前单纯依靠符号引用来确定调用的目标方法外，提供一种新的动态确定目标方法的机制，称为MethodHandle；</li>
<li>从本质上讲，Reflection（反射）和MethodHandle机制都是在模拟方法调用，但Reflection是在模拟Java代码层次的方法调用，而MethodHandle是在模拟字节码层次的方法调用，前者是重量级，而后者是轻量级；另外前者只为Java语言服务，后者可服务于所有Java虚拟机之上的语言；</li>
<li>每一处含有invokedynamic指令的位置都称为“动态调用点(Dynamic Call Site)”，这条指令的第一个参数不再是代表符号引用的CONSTANT_Methodref_info常量，而是CONSTANT_InvokeDynamic_info常量（可以得到引导方法、方法类型和名称）；</li>
<li>invokedynamic指令与其他invoke指令的最大差别就是它的分派逻辑不是由虚拟机决定的，而是由程序员决定的；</li>
</ul>
<h3 id="8-4-基于栈的字节码解释执行引擎"><a href="#8-4-基于栈的字节码解释执行引擎" class="headerlink" title="8.4 基于栈的字节码解释执行引擎"></a>8.4 基于栈的字节码解释执行引擎</h3><p>上节主要讲虚拟机是如何调用方法的，这节探讨虚拟机是如何执行方法中的字节码指令的。</p>
<h4 id="8-4-1-解释执行"><a href="#8-4-1-解释执行" class="headerlink" title="8.4.1 解释执行"></a>8.4.1 解释执行</h4><ul>
<li>只有确定了谈论对象是某种具体的Java实现版本和执行引擎运行模式时，谈解释执行还是编译执行才比较确切；</li>
<li>Java语言中，javac编译器完成了程序代码经过词法分析、语法分析到抽象语法树，再遍历语法树生成线性的字节码指令流的过程；因为这一部分动作是在Java虚拟机之外进行的，而解释器在虚拟机的内部，所以Java程序的编译就是半独立的实现；</li>
</ul>
<h4 id="8-4-2-基于栈的指令集与基于寄存器的指令集"><a href="#8-4-2-基于栈的指令集与基于寄存器的指令集" class="headerlink" title="8.4.2 基于栈的指令集与基于寄存器的指令集"></a>8.4.2 基于栈的指令集与基于寄存器的指令集</h4><ul>
<li>Java编译器输出的指令集，基本上是一种基于栈的指令集架构，指令流中的指令大部分是零地址指令，它们依赖操作数栈进行工作；</li>
<li>基于栈的指令集主要的优点是可移植性，寄存器由硬件直接提供，程序直接依赖这些硬件寄存器则不可避免地要受到硬件的约束；主要缺点是执行速度相对来说会稍慢一点；</li>
</ul>
<h4 id="8-4-3-基于栈的解释器执行过程"><a href="#8-4-3-基于栈的解释器执行过程" class="headerlink" title="8.4.3 基于栈的解释器执行过程"></a>8.4.3 基于栈的解释器执行过程</h4><p>一段简单的算法代码</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">calc</span><span class="params">()</span></span>&#123;</div><div class="line">    <span class="keyword">int</span> a = <span class="number">100</span>;</div><div class="line">    <span class="keyword">int</span> b = <span class="number">200</span>;</div><div class="line">    <span class="keyword">int</span> c = <span class="number">300</span>;</div><div class="line">    <span class="keyword">return</span> (a + b) * c;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>上述代码的字节码表示</p>
<pre><code>public int calc();
Code:
Stack=2, Locals=4, Args_size=1
0:bipush 100
2:istore_1
3:sipush 200
6:istore_2
7:sipush 300
10:istore_3
11:iload_1
12:iload_2
13:iadd
14:iload_3
15:imul
16:ireturn
</code></pre><p>javap提示这段代码需要深度为2的操作数栈和4个Slot的局部变量空间，作者根据这些信息画了示意图来说明执行过程中的变化情况：</p>
<p>执行偏移地址为0的指令<br><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/execution_1.png" alt="执行偏移地址为0的指令"></p>
<p>执行偏移地址为2的指令<br><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/execution_2.png" alt="执行偏移地址为2的指令"></p>
<p>执行偏移地址为11的指令<br><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/execution_3.png" alt="执行偏移地址为11的指令"></p>
<p>执行偏移地址为12的指令<br><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/execution_4.png" alt="执行偏移地址为12的指令"></p>
<p>执行偏移地址为13的指令<br><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/execution_5.png" alt="执行偏移地址为13的指令"></p>
<p>执行偏移地址为14的指令<br><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/execution_6.png" alt="执行偏移地址为14的指令"></p>
<p>执行偏移地址为16的指令<br><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/execution_7.png" alt="执行偏移地址为16的指令"></p>
<p>注：上面的执行过程仅仅是一种概念模型，虚拟机中解析器和即时编译器会对输入的字节码进行优化。</p>
<h3 id="8-5-本章小结"><a href="#8-5-本章小结" class="headerlink" title="8.5 本章小结"></a>8.5 本章小结</h3><p>本章分析了虚拟机在执行代码时，如何找到正确的方法、如何执行方法内的字节码以及执行代码时涉及的内存结构。这第六、七、八三章中，我们针对Java程序是如何存储的、如何载入的以及如何执行的问题进行了讲解，下一章一起看看这些理论知识在具体开发中的经典应用。</p>
<h2 id="第九章-类加载及执行子系统的案例与实战"><a href="#第九章-类加载及执行子系统的案例与实战" class="headerlink" title="第九章 类加载及执行子系统的案例与实战"></a>第九章 类加载及执行子系统的案例与实战</h2><h3 id="9-1-概述"><a href="#9-1-概述" class="headerlink" title="9.1 概述"></a>9.1 概述</h3><ul>
<li>在Class文件格式与执行引擎这部分中，用户的程序能直接影响的内容并不多；</li>
<li>能通过程序进行操作的，主要是字节码生成与类加载器这两部分的功能，但仅仅在如何处理这两点上，就已经出现了许多值得欣赏和借鉴的思路；</li>
</ul>
<h3 id="9-2-案例分析"><a href="#9-2-案例分析" class="headerlink" title="9.2 案例分析"></a>9.2 案例分析</h3><h4 id="9-2-1-Tomcat：正统的类加载器架构"><a href="#9-2-1-Tomcat：正统的类加载器架构" class="headerlink" title="9.2.1 Tomcat：正统的类加载器架构"></a>9.2.1 Tomcat：正统的类加载器架构</h4><p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/execution_tomcat_classloader.png" alt="Tomcat服务器的类加载架构"></p>
<ul>
<li>Java Web服务器：部署在同一个服务器上的两个Web应用程序所使用的Java类库可以实现相互隔离又要可以互相共享；尽可能保证自身的安全不受部署的Web应用程序影响；要支持JSP生成类的热替换；</li>
<li>上图中，灰色背景的三个类加载器是JDK默认提供的类加载器，而CommonClassLoader、CatalinaClassLoader、SharedClassLoader和WebappClassLoader是Tomcat自己定义的类加载器，分别加载/common/<em>（可被Tomcat和Web应用共用）、/server/</em>（可被Tomcat使用）、/shared/<em>（可被Web应用使用）和/WebApp/WEB-INF/</em>（可被当前Web应用使用）中的Java类库，Tomcat 6.x把前面三个目录默认合并到一起变成一个/lib目录（作用同原先的common目录）；</li>
</ul>
<h4 id="9-2-2-OSGI：灵活的类加载架构"><a href="#9-2-2-OSGI：灵活的类加载架构" class="headerlink" title="9.2.2 OSGI：灵活的类加载架构"></a>9.2.2 OSGI：灵活的类加载架构</h4><p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/execution_osgi_classloader.png" alt="OSGI的类加载架构"></p>
<ul>
<li>OSGI的每个模块称为Bundle，可以声明它所依赖的Java Package（通过Import-Package描述），也可以声明它允许导出发布的Java Package（通过Export-Package描述）；</li>
<li>除了更精确的模块划分和可见性控制外，引入OSGI的另外一个重要理由是基于OSGI的程序很可能可以实现模块级的热插拔功能；</li>
<li>OSGI的类加载器之间只有规则，没有固定的委派关系；加载器之间的关系更为复杂、运行时才能确定的网状结构，提供灵活性的同时，可能会产生许多的隐患；</li>
</ul>
<h4 id="9-2-3-字节码生成技术与动态代理的实现"><a href="#9-2-3-字节码生成技术与动态代理的实现" class="headerlink" title="9.2.3 字节码生成技术与动态代理的实现"></a>9.2.3 字节码生成技术与动态代理的实现</h4><ul>
<li>在Java里面除了javac和字节码类库外，使用字节码生成的例子还有Web服务器中的JSP编译器、编译时植入的AOP框架和很常用的动态代理技术等，这里选择其中相对简单的动态代理来看看字节码生成技术是如何影响程序运作的；</li>
<li>动态代理的优势在于实现了在原始类和接口还未知的时候就确定类的代理行为，可以很灵活地重用于不同的应用场景之中；</li>
<li>以下的例子中生成的代理类“$Proxy0.class”文件可以看到代理为传入接口的每一个方法统一调用了InvocationHandler对象的invoke方法；其生成代理类的字节码大致过程其实就是根据Class文件的格式规范去拼接字节码；</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DynamicProxyTest</span> </span>&#123;</div><div class="line"></div><div class="line">    <span class="class"><span class="keyword">interface</span> <span class="title">IHello</span> </span>&#123;</div><div class="line">        <span class="function"><span class="keyword">void</span> <span class="title">sayHello</span><span class="params">()</span></span>;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Hello</span> <span class="keyword">implements</span> <span class="title">IHello</span> </span>&#123;</div><div class="line">        <span class="meta">@Override</span></div><div class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">sayHello</span><span class="params">()</span> </span>&#123;</div><div class="line">            System.out.println(<span class="string">"Hello world"</span>);</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">DynamicProxy</span> <span class="keyword">implements</span> <span class="title">InvocationHandler</span> </span>&#123;</div><div class="line">        Object originalObj;</div><div class="line"></div><div class="line">        <span class="function">Object <span class="title">bind</span><span class="params">(Object originalObj)</span> </span>&#123;</div><div class="line">            <span class="keyword">this</span>.originalObj = originalObj;</div><div class="line">            <span class="keyword">return</span> Proxy.newProxyInstance(originalObj.getClass().getClassLoader(), originalObj.getClass().getInterfaces(), <span class="keyword">this</span>);</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="meta">@Override</span></div><div class="line">        <span class="function"><span class="keyword">public</span> Object <span class="title">invoke</span><span class="params">(Object proxy, Method method, Object[] args)</span> <span class="keyword">throws</span> Throwable </span>&#123;</div><div class="line">            System.out.println(<span class="string">"Welcome"</span>);</div><div class="line">            <span class="keyword">return</span> method.invoke(originalObj, args);</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</div><div class="line">        <span class="comment">// add this property to generate proxy class file</span></div><div class="line">        System.getProperties().put(<span class="string">"sun.misc.ProxyGenerator.saveGeneratedFiles"</span>, <span class="string">"true"</span>);</div><div class="line"></div><div class="line">        IHello hello = (IHello) <span class="keyword">new</span> DynamicProxy().bind(<span class="keyword">new</span> Hello());</div><div class="line">        hello.sayHello();</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h4 id="9-2-4-Retrotranslator：跨越JDK版本"><a href="#9-2-4-Retrotranslator：跨越JDK版本" class="headerlink" title="9.2.4 Retrotranslator：跨越JDK版本"></a>9.2.4 Retrotranslator：跨越JDK版本</h4><ul>
<li>Retrotranslator的作用是将JDK 1.5编译出来的Class文件转变为可以在JDK 1.4或JDK 1.3部署的版本，它可以很好地支持自动装箱、泛型、动态注解、枚举、变长参数、遍历循环、静态导入这些语法特性，甚至还可以支持JDK 1.5中新增的集合改进、并发包以及对泛型、注解等的反射操作；</li>
<li>JDK升级通常包括四种类型：编译器层面的做的改进、Java API的代码增强、需要再字节码中进行支持的活动以及虚拟机内部的改进，Retrotranslator只能模拟前两类，第二类通过独立类库实现，第一类则通过ASM框架直接对字节码进行处理；</li>
</ul>
<h3 id="9-3-实战：自己动手实现远程执行功能"><a href="#9-3-实战：自己动手实现远程执行功能" class="headerlink" title="9.3 实战：自己动手实现远程执行功能"></a>9.3 实战：自己动手实现远程执行功能</h3><ul>
<li>目标：不依赖JDK版本、不改变原有服务端程序的部署，不依赖任何第三方类库、不侵入原有程序、临时代码的执行结果能返回到客户端；</li>
<li>思路：如何编译提交到服务器的Java代码（客户端编译好上传Class文件而不是Java代码）、如何执行编译之后的Java代码（要能访问其他类库，要能卸载）、如何收集Java代码的执行结果（在执行的类中把System.out的符号引用替换为我们准备的PrintStream的符号引用）；</li>
<li>具体实现：HotSwapClassLoader用于实现同一个类的代码可以被多次加载，通过公开父类ClassLoader的defineClass实现；HackSystem是为了替换java.lang.System，它直接修改Class文件格式的byte[]数组中的常量池部分，将常量池中指定内容的CONSTANT_Utf8_info常量替换为新的字符串；ClassModifier涉及对byte[]数组操作的部分，主要是将byte[]与int和String互相转换，以及把对byte[]数据的替换操作封装在ByteUtils类中；经过ClassModifier处理过的byte[]数组才会传给HotSwapClassLoader.loadByte方法进行类加载；而JavaClassExecutor是提供给外部调用的入口；</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JavaClassExecutor</span> </span>&#123;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> String <span class="title">execute</span><span class="params">(<span class="keyword">byte</span>[] classByte)</span> </span>&#123;</div><div class="line">        HackSystem.clearBuffer();</div><div class="line">        ClassModifier cm = <span class="keyword">new</span> ClassModifier(classByte);</div><div class="line">        <span class="keyword">byte</span>[] modifiedBytes = cm.modifyUTF8Constant(<span class="string">"java/lang/System"</span>, <span class="string">"org/fenixsoft/classloading/execute/HackSystem"</span>);</div><div class="line">        HotSwapClassLoader hotSwapClassLoader = <span class="keyword">new</span> HotSwapClassLoader();</div><div class="line">        Class clazz = hotSwapClassLoader.loadByte(modifiedBytes);</div><div class="line">        <span class="keyword">try</span> &#123;</div><div class="line">            Method method = clazz.getMethod(<span class="string">"main"</span>, <span class="keyword">new</span> Class[]&#123;String[].class&#125;);</div><div class="line">            method.invoke(<span class="keyword">null</span>, <span class="keyword">new</span> String[]&#123;<span class="keyword">null</span>&#125;);</div><div class="line">        &#125; <span class="keyword">catch</span> (Throwable t) &#123;</div><div class="line">            t.printStackTrace(HackSystem.out);</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="keyword">return</span> HackSystem.getBufferString();</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>用于测试的JSP</p>
<figure class="highlight jsp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">&lt;%<span class="meta">@page</span> <span class="keyword">import</span>=<span class="string">"java.lang.*"</span> %&gt;</div><div class="line">&lt;%<span class="meta">@page</span> <span class="keyword">import</span>=<span class="string">"java.io.*"</span> %&gt;</div><div class="line">&lt;%<span class="meta">@page</span> <span class="keyword">import</span>=<span class="string">"org.fenixsoft.classloading.execute.*"</span> %&gt;</div><div class="line"></div><div class="line">&lt;%</div><div class="line">InputStream is = <span class="keyword">new</span> FileInputStream(<span class="string">"c:/TestClass.class"</span>);</div><div class="line"><span class="keyword">byte</span>[] b = <span class="keyword">new</span> <span class="keyword">byte</span>[is.available()];</div><div class="line">is.read(b);</div><div class="line">is.close();</div><div class="line"></div><div class="line">out.println(JavaClassExecutor.execute(b));</div><div class="line"></div><div class="line">%&gt;</div></pre></td></tr></table></figure>
<h3 id="9-4-本章小结"><a href="#9-4-本章小结" class="headerlink" title="9.4 本章小结"></a>9.4 本章小结</h3><p>只有了解虚拟机如何执行程序，才能更好地理解怎样写出优秀的代码。</p>
<p><strong>系列读书笔记</strong></p>
<ul>
<li><a href="http://ginobefunny.com/post/deep_in_jvm_notes_part1">《深入理解Java虚拟机》读书笔记1：Java技术体系、Java内存区域和内存溢出异常</a></li>
<li><a href="http://ginobefunny.com/post/deep_in_jvm_notes_part2">《深入理解Java虚拟机》读书笔记2：垃圾收集器与内存分配策略</a></li>
<li><a href="http://ginobefunny.com/post/deep_in_jvm_notes_part3">《深入理解Java虚拟机》读书笔记3：虚拟机性能监控与调优实战</a></li>
<li><a href="http://ginobefunny.com/post/deep_in_jvm_notes_part4">《深入理解Java虚拟机》读书笔记4：类文件结构</a></li>
<li><a href="http://ginobefunny.com/post/deep_in_jvm_notes_part5">《深入理解Java虚拟机》读书笔记5：类加载机制与字节码执行引擎</a></li>
<li><a href="http://ginobefunny.com/post/deep_in_jvm_notes_part6">《深入理解Java虚拟机》读书笔记6：程序编译与代码优化</a></li>
<li><a href="http://ginobefunny.com/post/deep_in_jvm_notes_part7">《深入理解Java虚拟机》读书笔记7：高效并发</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;国内JVM相关书籍NO.1，Java程序员必读。读书笔记第五部分对应原书的第七章至第九章，主要介绍虚拟机的类加载机制、字节码执行引擎，并通过实例和实战加深对虚拟机执行子系统这一部分的理解。&lt;br&gt;
    
    </summary>
    
      <category term="JVM" scheme="http://ginobefunny.com/categories/JVM/"/>
    
    
      <category term="Java" scheme="http://ginobefunny.com/tags/Java/"/>
    
      <category term="读书笔记" scheme="http://ginobefunny.com/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
      <category term="JVM" scheme="http://ginobefunny.com/tags/JVM/"/>
    
      <category term="虚拟机" scheme="http://ginobefunny.com/tags/%E8%99%9A%E6%8B%9F%E6%9C%BA/"/>
    
      <category term="类加载器" scheme="http://ginobefunny.com/tags/%E7%B1%BB%E5%8A%A0%E8%BD%BD%E5%99%A8/"/>
    
      <category term="双亲委派" scheme="http://ginobefunny.com/tags/%E5%8F%8C%E4%BA%B2%E5%A7%94%E6%B4%BE/"/>
    
      <category term="字节码" scheme="http://ginobefunny.com/tags/%E5%AD%97%E8%8A%82%E7%A0%81/"/>
    
      <category term="执行引擎" scheme="http://ginobefunny.com/tags/%E6%89%A7%E8%A1%8C%E5%BC%95%E6%93%8E/"/>
    
      <category term="编译器" scheme="http://ginobefunny.com/tags/%E7%BC%96%E8%AF%91%E5%99%A8/"/>
    
      <category term="解释器" scheme="http://ginobefunny.com/tags/%E8%A7%A3%E9%87%8A%E5%99%A8/"/>
    
  </entry>
  
  <entry>
    <title>《深入理解Java虚拟机》读书笔记4：类文件结构</title>
    <link href="http://ginobefunny.com/post/deep_in_jvm_notes_part4/"/>
    <id>http://ginobefunny.com/post/deep_in_jvm_notes_part4/</id>
    <published>2017-01-18T01:24:01.000Z</published>
    <updated>2017-01-31T09:51:04.486Z</updated>
    
    <content type="html"><![CDATA[<p>国内JVM相关书籍NO.1，Java程序员必读。读书笔记第四部分对应原书的第六章，主要介绍类文件结构的组成，并通过一个实例一步步了解各个部分的结构。<br><a id="more"></a></p>
<h1 id="第三部分-虚拟机执行子系统"><a href="#第三部分-虚拟机执行子系统" class="headerlink" title="第三部分 虚拟机执行子系统"></a>第三部分 虚拟机执行子系统</h1><h2 id="第六章-类文件结构"><a href="#第六章-类文件结构" class="headerlink" title="第六章 类文件结构"></a>第六章 类文件结构</h2><p>代码编译的结果从本地机器码转变为字节码，是存储格式发展的一小步，却是编程语言发展的一大步。</p>
<h3 id="6-1-概述"><a href="#6-1-概述" class="headerlink" title="6.1 概述"></a>6.1 概述</h3><p>由于最近十年内虚拟机以及大量建立在虚拟机之上的程序语言如雨后春笋般出现并蓬勃发展，将我们编写的程序编译成二进制本地机器码（Native Code）已不再是唯一的选择，越来越多的程序语言选择了操作系统和机器指令集无关的、平台中立的格式作为程序编译后的存储格式。</p>
<h3 id="6-2-无关性的基石"><a href="#6-2-无关性的基石" class="headerlink" title="6.2 无关性的基石"></a>6.2 无关性的基石</h3><ul>
<li>Java刚诞生的宣传口号：一次编写，到处运行（Write Once, Run Anywhere）。其最终实现在操作系统的应用层：Sun公司以及其他虚拟机提供商发布了许多可以运行在各种不同平台的虚拟机，这些虚拟机都可以载入和执行同一种平台无关的字节码。</li>
<li>字节码（ByteCode）是构成平台无关的基石；</li>
<li>另外虚拟机的语言无关性也越来越被开发者所重视，JVM设计者在最初就考虑过实现让其他语言运行在Java虚拟机之上的可能性，如今已发展出一大批在JVM上运行的语言，比如Clojure、Groovy、JRuby、Jython、Scala；</li>
<li>实现语言无关性的基础仍是虚拟机和字节码存储格式，Java虚拟机不和包括Java在内的任何语言绑定，它只与Class文件这种特定的二进制文件格式所关联，这使得任何语言的都可以使用特定的编译器将其源码编译成Class文件，从而在虚拟机上运行。</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/class_for_other_languages.png" alt="Java虚拟机提供的语言无关性"></p>
<h3 id="6-3-Class类文件的结构"><a href="#6-3-Class类文件的结构" class="headerlink" title="6.3 Class类文件的结构"></a>6.3 Class类文件的结构</h3><ul>
<li>Class文件是一组以8个字节为基础单位的二进制流（可能是磁盘文件，也可能是类加载器直接生成的），各个数据项目严格按照顺序紧凑地排列，中间没有任何分隔符；</li>
<li>Class文件格式采用一种类似于C语言结构体的伪结构来存储数据，其中只有两种数据类型：无符号数和表；</li>
<li>无符号数属于基本的数据类型，以u1、u2、u4和u8来分别代表1个字节、2个字节、4个字节和8个字节的无符号数，可以用来描述数字、索引引用、数量值或者按照UTF-8编码构成字符串值；</li>
<li>表是由多个无符号数获取其他表作为数据项构成的复合数据类型，习惯以“_info”结尾；</li>
<li>无论是无符号数还是表，当需要描述同一个类型但数量不定的多个数据时，经常会使用一个前置的容量计数器加若干个连续的数据项的形式，这时称这一系列连续的某一类型的数据未某一类型的集合。</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/class_file_format.png" alt="Class文件格式"></p>
<p>下面我以自己本机写的一个简单的Java文件来学习其中各个部分的含义：<br><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/class_example1.png" alt="TestClass.java"></p>
<p>使用javac编译成TestClass.class文件，使用16进制打开：<br><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/class_example2.png" alt="TestClass.class"></p>
<p>使用javap命令输出Class文件信息：<br><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/class_example3.png" alt="javap TestClass"></p>
<h4 id="6-3-1-魔数和版本"><a href="#6-3-1-魔数和版本" class="headerlink" title="6.3.1 魔数和版本"></a>6.3.1 魔数和版本</h4><ul>
<li>Class文件的头4个字节，唯一作用是确定文件是否为一个可被虚拟机接受的Class文件，固定为“0xCAFEBABE”。</li>
<li>第5和第6个字节是次版本号，第7和第8个字节是主版本号（0x0034为52，对应JDK版本1.8）；能向下兼容之前的版本，无法运行后续的版本；</li>
</ul>
<h4 id="6-3-2-常量池"><a href="#6-3-2-常量池" class="headerlink" title="6.3.2 常量池"></a>6.3.2 常量池</h4><ul>
<li>常量池可以理解为Class文件之中的资源仓库，是Class文件结构中与其他项目关联最多的数据类型，也是占用Class文件空间最大的数据项之一；</li>
<li>由于常量池中的常量数量不固定，因此需要在常量池前放置一项u2类型的数据来表示容量，该值是从1开始的，上图的0x0013为十进制的19，代表常量池中有18项常量，索引值范围为1~18；</li>
<li>常量池主要存放两大类常量：字面量（Literal，笔记接近Java的常量概念，比如文本字符串和final常量等）和符号引用（Symbolic References，主要包括类和接口的全限定名、字段的名称和描述符、方法的名称和描述符）；</li>
<li>Java代码在javac编译时不会有“连接”这一步骤，而是在虚拟机加载Class文件的时候进行动态连接；所以在Class文件不会保存各个方法、字段和最终内存布局信息；当虚拟机运行时需要从常量池获取对应的符号引用，再在类创建时或运行时解析、翻译到具体的内存地址中；</li>
<li>JDK 1.7中常量池共有14种不同的表结构数据，这些表结构开始的第一位是一个u1类型的标志位，代表当前常量的类型，具体如下图所示：</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/class_file_constant_pool.png" alt="常量池的项目类型"></p>
<ul>
<li>之所以说常量池是最繁琐的数据就是因为这14种常量类型都有自己的结结构。可以结合下图中各个表结构的说明和之前使用javap解析的文件内容一起看。</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/class_file_constant_pool_detail1.png" alt="常量池中14种常量项结构说明1"><br><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/class_file_constant_pool_detail2.png" alt="常量池中14种常量项结构说明2"></p>
<ul>
<li>第1项：0x0A（15标志为方法句柄），0x0004（指向第4项的类描述符），0x000F（指向第15项的名称及类型描述符）；</li>
<li>第2项：0x09（9标志为字段符号引用），0x0003（指向第3项类描述符），0x0010（指向第16项的名称及类型描述符）；</li>
<li>第3项：0x07（7标志为类符号引用），0x0011（指向第17项全限定名常量项）；</li>
<li>第4项：0x07（7标志为类符号引用），0x0012（指向第18项全限定名常量项）；</li>
<li>第5项：0x01（1标志为UTF-字符串常量），0x0001（字符串占用1个字节），6D（字符“m”）；</li>
<li>第6项：0x01（1标志为UTF-字符串常量），0x0001（字符串占用1个字节），49（字符“I”）；</li>
<li>第7项：0x01（1标志为UTF-字符串常量），0x0006（字符串占用6个字节），3C 69 6E 69 74 3E（字符“<init>”）；</init></li>
<li>第8项：0x01（1标志为UTF-字符串常量），0x0003（字符串占用3个字节），28 29 56（字符“()V”）；</li>
<li>第9项：0x01（1标志为UTF-字符串常量），0x0004（字符串占用4个字节），43 6F 64 65（字符“Code”）；</li>
<li>第10项：0x01（1标志为UTF-字符串常量），0x000F（字符串占用15个字节），4C 69 6E 65 4E 75 6D 62 65 72 54 61 62 6C 65（字符“LineNumberTable”）；</li>
<li>第11项：0x01（1标志为UTF-字符串常量），0x0003（字符串占用3个字节），69 6E 63（字符“inc”）；</li>
<li>第12项：0x01（1标志为UTF-字符串常量），0x0003（字符串占用3个字节），28 29 49（字符“()I”）；</li>
<li>第13项：0x01（1标志为UTF-字符串常量），0x000A（字符串占用10个字节），53 6F 75 72 63 65 46 69 6C 65（字符“SourceFile”）；</li>
<li>第14项：0x01（1标志为UTF-字符串常量），0x000E（字符串占用14个字节），54 65 73 74 43 6C 61 73 73 2E 6A 61 76 61（字符“TestClass.java”）；</li>
<li>第15项：0x0C（12标志为名称和类型符号引用），0x0007（指向第7项名称常量项）， 0x0008（指向第8项描述符常量项）；</li>
<li>第16项：0x0C（12标志为名称和类型符号引用），0x0005（指向第5项名称常量项）， 0x0006（指向第6项描述符常量项）；</li>
<li>第17项：0x01（1标志为UTF-字符串常量），0x001F（字符串占用31个字节），63 6F 6D 2F 67 69 6E 6F 62 65 66 75 6E 6E 79 2F 63 6C 61 7A 7A 2F 54 65 73 74 43 6C 61 73 73（字符“com/ginobefunny/clazz/TestClas”）；</li>
<li>第18项：0x01（1标志为UTF-字符串常量），0x0010（字符串占用16个字节），6A 61 76 61 2F 6C 61 6E 67 2F 4F 62 6A 65 63 74（字符“java/lang/Object”）；</li>
</ul>
<h4 id="6-3-3-访问标志"><a href="#6-3-3-访问标志" class="headerlink" title="6.3.3 访问标志"></a>6.3.3 访问标志</h4><ul>
<li>紧接在常量池后面的是两个字节的访问标志，用于标识类或接口的访问信息；</li>
<li>访问标志一个有16个标志位，但目前只采用了其中8位，本例子中的0x0021标识为一个public的普通类；</li>
</ul>
<h4 id="6-3-4-类索引、父类索引与接口索引集合"><a href="#6-3-4-类索引、父类索引与接口索引集合" class="headerlink" title="6.3.4 类索引、父类索引与接口索引集合"></a>6.3.4 类索引、父类索引与接口索引集合</h4><ul>
<li>类索引：u2类型的数据，用于确定类的全限定名。本例子中为0x0003，指向常量池中第3项；</li>
<li>父类索引：u2类型的数据，用于确定父类的全限定名。本例子中为0x0004，指向常量池中第4项；</li>
<li>接口索引计算器：u2类型的数据，用于表示索引集合的容量。本例子中为0x0000，说明没有实现接口；</li>
<li>接口索引集合：一组u2类型的数据的集合，用于确定实现的接口（对于接口来说就是extend的接口）。本例子不存在。</li>
</ul>
<h4 id="6-3-5-字段表集合"><a href="#6-3-5-字段表集合" class="headerlink" title="6.3.5 字段表集合"></a>6.3.5 字段表集合</h4><p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/class_file_fields.png" alt="字段表结构"></p>
<ul>
<li>用于描述接口或者类中声明的变量，包括类级变量和实例级变量，但不包括方法内部声明的局部变量；它不会列出从父类和超类继承而来的字段；</li>
<li>0x0001表示这个类只有一个字段表数据；</li>
<li>字段修饰符放在access_flag中，是一个u2的数据类型，0x0002表示为private的属性；</li>
<li>字段名称name_index，是一个u2的数据类型，0x0005表示该属性的名称为常量池的第5项；</li>
<li>字段描述符descriptor_index，是一个u2的数据类型，0x0006表示该属性的描述符为常量池的第6项，其值“I”表示类型为整形；</li>
<li>字段属性计算器和属性集合：0x0000表示该例子中不存在；</li>
</ul>
<h4 id="6-3-6-方法表集合"><a href="#6-3-6-方法表集合" class="headerlink" title="6.3.6 方法表集合"></a>6.3.6 方法表集合</h4><ul>
<li>和字段表集合的方式几乎一样；</li>
<li>方法里面的代码经过编译器编译成字节码指令后，存放在方法属性表集合中一个名为Code的属性里面；</li>
<li>0x0002表示这个类有两个方法表数据，分别是编译器添加的实例构造器<init>和源码中的方式inc()；</init></li>
<li>第一个方法的访问标志是0x0001（public方法），名称索引值为0x0007（常量池第7项，“<init>”），描述符索引值为0x0008（常量池第8项，“()V”），属性表计算器为0x0001（有一项属性），属性名称索引为0x0009（常量池第9项，“Code”）；</init></li>
<li>根据“6.3.7.1 Code属性”说明，属性值的长度为23（0x0000001D表示29，但需要减去属性名称索引和属性长度固定的6个字节长度），操作数栈深度的最大值为1（0x0001，虚拟机运行时根据这个值来分配栈帧中操作栈深度），局部变量表所需要的存储空间为1个Slot（0x0001，Slot是内存分配的最小单位），字节码长度为5（0x00000005），分别为2A（aload_0，将第0个Slot中为reference类型的本地变量推送到操作数栈顶）、B7（invokespecial，以栈顶的reference类型的数据所指向的对象作为方法接收者，调用此对象的实例构造器方法、private方法或者它父类的方法，后面接着一个u2的参数指向常量池的方法引用）、0x0001（表示常量池的第1项，即Object类的<init>方法）、B1（对应的指令为return，返回值为void）；显式异常表为空（0x0000，计数器为0）；该Code属性还内嵌1个属性（0x0001），属性的名称索引为0x000A（即“LineNumberTable”属性，用于记录对应的代码行数），该内嵌属性的长度为6（0x00000006），对应的行数信息为源码的第3行（0x000100000003）；</init></li>
<li>第二个方法的访问标志是0x0001（public方法），名称索引值为0x000B（常量池第11项，“inc”），描述符索引值为0x000C（常量池第12项，“()I”），属性表计算器为0x0001（有一项属性），属性名称索引为0x0009（常量池第9项，“Code”）；</li>
<li>根据“6.3.7.1 Code属性”说明，属性值的长度为25（0x0000001F表示31，但需要减去属性名称索引和属性长度固定的6个字节长度），操作数栈深度的最大值为2（0x0002），局部变量表所需要的存储空间为1个Slot（0x0001），字节码长度为7（0x00000007），分别为2A（aload_0）、B4（getfield，后面接着一个u2的参数指向常量池的属性引用）、0x0002（表示常量池的第2项，即TestClass类的m属性）、04（对应的指令为iconst_1）、60（对应的指令为iadd，整形求和）、AC（对应的指令为ireturn，返回值为整形）；显式异常表为空（0x0000，计数器为0）；该Code属性还内嵌1个属性（0x0001），属性的名称索引为0x000A（即“LineNumberTable”属性，用于记录对应的代码行数），该内嵌属性的长度为6（0x00000006），对应的行数信息为源码的第8行（0x000100000008）；</li>
</ul>
<h4 id="6-3-7-属性表集合"><a href="#6-3-7-属性表集合" class="headerlink" title="6.3.7 属性表集合"></a>6.3.7 属性表集合</h4><ul>
<li>在Class文件、字段表、方法表都可以携带自己的属性表集合；</li>
<li>属性表集合的限制较为宽松，不再要求严格的顺序，只要属性名不重复即可；</li>
<li>以下是Java虚拟机规范里预定义的虚拟机实现应当能识别的属性：</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/class_file_attrs1.png" alt="虚拟机规范预定义的属性1"><br><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/class_file_attrs2.png" alt="虚拟机规范预定义的属性2"></p>
<ul>
<li>接着我们的例子的Class文件还有最后一段：0x0001表示该Class有一个属性，0x000D表示属性名索引为第13项（对应“SourceFile”），0x00000002表示该属性长度为2，0x000E表示该类的SourceFile名称为第14项（对应“TestClass.java”）。</li>
</ul>
<h5 id="6-3-7-1-Code属性"><a href="#6-3-7-1-Code属性" class="headerlink" title="6.3.7.1 Code属性"></a>6.3.7.1 Code属性</h5><p>Java程序方法体中的代码经过javac编译后，字节码指令存放在Code属性，其属性表结构如下：</p>
<p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/class_file_attrs_code.png" alt="Code属性表的结构"></p>
<h5 id="6-3-7-2-Exceptions属性"><a href="#6-3-7-2-Exceptions属性" class="headerlink" title="6.3.7.2 Exceptions属性"></a>6.3.7.2 Exceptions属性</h5><p>方法描述时throws关键字后面列举的异常，和Code属性里的异常表不同。其属性表结构如下：</p>
<p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/class_file_attrs_exception.png" alt="Exceptions属性表"></p>
<h5 id="6-3-7-3-LineNumberTable属性"><a href="#6-3-7-3-LineNumberTable属性" class="headerlink" title="6.3.7.3 LineNumberTable属性"></a>6.3.7.3 LineNumberTable属性</h5><p>用于描述Java源码行号与字节码行号之间的对应关系，它不是必须的，可以通过javac -g:none取消该信息。没有该信息的影响是运行时抛异常不会显示出错的行号，在代码调试时无法按照源码行来设置断点。</p>
<p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/class_file_attrs_linenumber.png" alt="LineNumberTable属性"></p>
<h5 id="6-3-7-4-LocalVariableTable属性"><a href="#6-3-7-4-LocalVariableTable属性" class="headerlink" title="6.3.7.4 LocalVariableTable属性"></a>6.3.7.4 LocalVariableTable属性</h5><p>用于描述栈帧中局部变量与Java源码中定义的变量之间的关系，它不是运行时必须的，可以通过javac -g:none取消该信息。如果没有这个属性，所有的参数名称都会丢失，取之以arg0、arg1这样的占位符来替代。</p>
<p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/class_file_attrs_localvariable.png" alt="LocalVariableTable属性"></p>
<p>其中local_variable_info项代表了一个栈帧与源码中局部变量的关联，如下所示：</p>
<p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/class_file_attrs_localvariable2.png" alt="local_variable_info结构"></p>
<h5 id="6-3-7-5-SourceFile属性"><a href="#6-3-7-5-SourceFile属性" class="headerlink" title="6.3.7.5 SourceFile属性"></a>6.3.7.5 SourceFile属性</h5><p>用于记录生成这个Class的源码文件名称，这个属性也是可选的。</p>
<p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/class_file_attrs_sourcefile.png" alt="SourceFile属性"></p>
<h5 id="6-3-7-6-ConstantValue属性"><a href="#6-3-7-6-ConstantValue属性" class="headerlink" title="6.3.7.6 ConstantValue属性"></a>6.3.7.6 ConstantValue属性</h5><p>作用是通知虚拟机自动为静态变量赋值，只有被static关键字修饰的变量才可以用这个属性。对于非static类型的变量的赋值是在实例构造器<init>方法中进行的；而对于类变量有两种方式：在类构造器<clinit>方法中或者使用ConstantValue属性。目前Sun javac编译器的选择是：同时使用final和static修饰的变量且为基本数据类型或String类型使用ConstantValue属性初始化，否则使用<clinit>初始化。</clinit></clinit></init></p>
<p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/class_file_attrs_constantvalue.png" alt="ConstantValue属性"></p>
<h5 id="6-3-7-7-InnerClass属性"><a href="#6-3-7-7-InnerClass属性" class="headerlink" title="6.3.7.7 InnerClass属性"></a>6.3.7.7 InnerClass属性</h5><p>用于记录内部类与宿主类之间的关联。</p>
<p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/class_file_attrs_innerclass.png" alt="InnerClass属性"></p>
<p>其中number_of_class代表需要记录多少个内部类信息，每个内部类的信息都由一个inner_class_info表进行描述。</p>
<p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/class_file_attrs_innerclass2.png" alt="inner_class_info表的结构"></p>
<h5 id="6-3-7-8-Deprecated及Synthetic属性"><a href="#6-3-7-8-Deprecated及Synthetic属性" class="headerlink" title="6.3.7.8 Deprecated及Synthetic属性"></a>6.3.7.8 Deprecated及Synthetic属性</h5><p>Deprecated（不推荐使用）和Synthetic（不是由Java源码直接产生编译器自行添加的，有两个例外是实例构造器<init>和类构造器<clinit>）这两个属性都属于布尔属性，只存在有和没有的区别，没有属性值的概念。在属性结构中attribute_length的数据值必须为0x00000000。</clinit></init></p>
<p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/class_file_attrs_deprecated.png" alt="Deprecated及Synthetic属性"></p>
<h5 id="6-3-7-9-StackMapTable属性"><a href="#6-3-7-9-StackMapTable属性" class="headerlink" title="6.3.7.9 StackMapTable属性"></a>6.3.7.9 StackMapTable属性</h5><p>这是一个复杂的变长属性，位于Code属性的属性表中。这个属性会在虚拟机类加载的字节码验证阶段被新类型检查验证器使用，目的在于代替以前比较消耗性能的基于数据流分析的类型推导验证器。</p>
<p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/class_file_attrs_stackmaptable.png" alt="StackMapTable属性"></p>
<h5 id="6-3-7-10-Signature属性"><a href="#6-3-7-10-Signature属性" class="headerlink" title="6.3.7.10 Signature属性"></a>6.3.7.10 Signature属性</h5><p>一个可选的定长属性，在JDK 1.5发布后增加的，任何类、接口、初始化方法或成员的泛型签名如果包含了类型变量或参数化类型，则Signature属性会为它记录泛型签名信息。这主要是因为Java的泛型采用的是擦除法实现的伪泛型，在字节码中泛型信息编译之后统统被擦除，在运行期无法将泛型类型与用户定义的普通类型同等对待。通过Signature属性，Java的反射API能够获取泛型类型。</p>
<p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/class_file_attrs_signature.png" alt="Signature属性"></p>
<h5 id="6-3-7-11-BootstrapMethods属性"><a href="#6-3-7-11-BootstrapMethods属性" class="headerlink" title="6.3.7.11 BootstrapMethods属性"></a>6.3.7.11 BootstrapMethods属性</h5><p>一个复杂的变长属性，位于类文件的属性表中，用于保存invokedynamic指令引用的引导方法限定符。</p>
<h3 id="6-4-字节码指令简介"><a href="#6-4-字节码指令简介" class="headerlink" title="6.4 字节码指令简介"></a>6.4 字节码指令简介</h3><p>Java虚拟机的指令由一个字节长度的、代表着特定操作含义的数字（操作码）以及跟随其后的零至多个代表此操作所需参数（称为操作数）而构成。由于Java虚拟机采用面向操作数栈而不是寄存器的架构，所以大多数的指令都不包含操作数，只有一个操作码。</p>
<p>在指令集中大多数的指令都包含了其操作所对应的数据类型信息，如iload指令用于从局部变量表中加载int类型的数据到操作数栈中。</p>
<ul>
<li>加载和存储指令：iload/iload<em><n>等（加载局部变量到操作栈）、istore/istore</n></em><n>等（从操作数栈存储到局部变量表）、bipush/sipush/ldc/iconst_<n>（加载常量到操作数栈）、wide（扩充局部变量表访问索引）；</n></n></li>
<li>运算指令：没有直接支持byte、short、char和boolean类型的算术指令而采用int代替；iadd/isub/imul/idiv加减乘除、irem求余、ineg取反、ishl/ishr位移、ior按位或、iand按位与、ixor按位异或、iinc局部变量自增、dcmpg/dcmpl比较；</li>
<li>类型转换指令：i2b/i2c/i2s/l2i/f2i/f2l/d2i/d2l/d2f；</li>
<li>对象创建与访问指令：new创建类实例、newarray/anewarray/multianewarray创建数组、getfield/putfield/getstatic/putstatic访问类字段或实例字段、baload/iaload/aaload把一个数组元素加载到操作数栈、bastore/iastore/aastore将一个操作数栈的值存储到数组元素中、arraylength取数组长度、instanceof/checkcast检查类实例类型；</li>
<li>操作数栈管理指令：pop/pop2一个或两个元素出栈、dup/dup2复制栈顶一个或两个数组并将复制值或双份复制值重新压力栈顶、swap交互栈顶两个数值；</li>
<li>控制转移指令：ifeq/iflt/ifnull条件分支、tableswitch/lookupswitch复合条件分支、goto/jsr/ret无条件分支；</li>
<li>方法调用和返回指令：invokevirtual/invokeinterface/invokespecial/invokestatic/invokedynamic方法调用、ireturn/lreturn/areturn/return方法返回；</li>
<li>异常处理指令：athrow</li>
<li>同步指令：monitorenter/monitorexit</li>
</ul>
<h3 id="6-5-公有设计和私有实现"><a href="#6-5-公有设计和私有实现" class="headerlink" title="6.5 公有设计和私有实现"></a>6.5 公有设计和私有实现</h3><ul>
<li>Java虚拟机的实现必须能够读取Class文件并精确实现包含在其中的Java虚拟机代码的含义；</li>
<li>但一个优秀的虚拟机实现，通常会在满足虚拟机规范的约束下具体实现做出修改和优化；</li>
<li>虚拟机实现的方式主要有两种：将输入的Java虚拟机代码在加载或执行时翻译成另外一种虚拟机的指令集或宿主主机CPU的本地指令集。</li>
</ul>
<h3 id="6-6-Class文件结构的发展"><a href="#6-6-Class文件结构的发展" class="headerlink" title="6.6 Class文件结构的发展"></a>6.6 Class文件结构的发展</h3><ul>
<li>Class文件结构一直比较稳定，主要的改进集中向访问标志、属性表这些可扩展的数据结构中添加内容；</li>
<li>Class文件格式所具备的平台中立、紧凑、稳定和可扩展的特点，是Java技术体系实现平台无关、语言无关两项特性的重要支柱；</li>
</ul>
<h3 id="6-7-本章小结"><a href="#6-7-本章小结" class="headerlink" title="6.7 本章小结"></a>6.7 本章小结</h3><p>本章详细讲解了Class文件结构的各个部分，通过一个实例演示了Class的数据是如何存储和访问的，后面的章节将以动态的、运行时的角度去看看字节码在虚拟机执行引擎是怎样被解析执行的。</p>
<p><strong>系列读书笔记</strong></p>
<ul>
<li><a href="http://ginobefunny.com/post/deep_in_jvm_notes_part1">《深入理解Java虚拟机》读书笔记1：Java技术体系、Java内存区域和内存溢出异常</a></li>
<li><a href="http://ginobefunny.com/post/deep_in_jvm_notes_part2">《深入理解Java虚拟机》读书笔记2：垃圾收集器与内存分配策略</a></li>
<li><a href="http://ginobefunny.com/post/deep_in_jvm_notes_part3">《深入理解Java虚拟机》读书笔记3：虚拟机性能监控与调优实战</a></li>
<li><a href="http://ginobefunny.com/post/deep_in_jvm_notes_part4">《深入理解Java虚拟机》读书笔记4：类文件结构</a></li>
<li><a href="http://ginobefunny.com/post/deep_in_jvm_notes_part5">《深入理解Java虚拟机》读书笔记5：类加载机制与字节码执行引擎</a></li>
<li><a href="http://ginobefunny.com/post/deep_in_jvm_notes_part6">《深入理解Java虚拟机》读书笔记6：程序编译与代码优化</a></li>
<li><a href="http://ginobefunny.com/post/deep_in_jvm_notes_part7">《深入理解Java虚拟机》读书笔记7：高效并发</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;国内JVM相关书籍NO.1，Java程序员必读。读书笔记第四部分对应原书的第六章，主要介绍类文件结构的组成，并通过一个实例一步步了解各个部分的结构。&lt;br&gt;
    
    </summary>
    
      <category term="JVM" scheme="http://ginobefunny.com/categories/JVM/"/>
    
    
      <category term="Java" scheme="http://ginobefunny.com/tags/Java/"/>
    
      <category term="读书笔记" scheme="http://ginobefunny.com/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
      <category term="JVM" scheme="http://ginobefunny.com/tags/JVM/"/>
    
      <category term="虚拟机" scheme="http://ginobefunny.com/tags/%E8%99%9A%E6%8B%9F%E6%9C%BA/"/>
    
      <category term="字节码" scheme="http://ginobefunny.com/tags/%E5%AD%97%E8%8A%82%E7%A0%81/"/>
    
      <category term="类文件" scheme="http://ginobefunny.com/tags/%E7%B1%BB%E6%96%87%E4%BB%B6/"/>
    
      <category term="常量池" scheme="http://ginobefunny.com/tags/%E5%B8%B8%E9%87%8F%E6%B1%A0/"/>
    
  </entry>
  
  <entry>
    <title>Guice简明教程</title>
    <link href="http://ginobefunny.com/post/learning_guice/"/>
    <id>http://ginobefunny.com/post/learning_guice/</id>
    <published>2017-01-17T03:26:53.000Z</published>
    <updated>2017-02-10T03:38:03.360Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://github.com/google/guice/" target="_blank" rel="external">Guice</a>是Google开源的一个依赖注入类库，相比于Spring IoC来说更小更快。Elasticsearch大量使用了Guice，本文简单的介绍下Guice的基本概念和使用方式。<br><a id="more"></a></p>
<h2 id="学习目标"><a href="#学习目标" class="headerlink" title="学习目标"></a>学习目标</h2><ul>
<li>概述：了解Guice是什么，有什么特点；</li>
<li>快速开始：通过实例了解Guice；</li>
<li>核心概念：了解Guice涉及的核心概念，如绑定（Binding）、范围（Scope）和注入（Injection）；</li>
<li>最佳实践：官方推荐的最佳实践；</li>
</ul>
<h2 id="Guice概述"><a href="#Guice概述" class="headerlink" title="Guice概述"></a>Guice概述</h2><ul>
<li>Guice是Google开源的依赖注入类库，通过Guice减少了对工厂方法和new的使用，使得代码更易交付、测试和重用；</li>
<li>Guice可以帮助我们更好地设计API，它是个轻量级非侵入式的类库；</li>
<li>Guice对开发友好，当有异常发生时能提供更多有用的信息用于分析；</li>
</ul>
<h2 id="快速开始"><a href="#快速开始" class="headerlink" title="快速开始"></a>快速开始</h2><p>假设一个在线预订Pizza的网站，其有一个计费服务接口：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">BillingService</span> </span>&#123;</div><div class="line"> <span class="comment">/**</span></div><div class="line">  * 通过信用卡支付。无论支付成功与否都需要记录交易信息。</div><div class="line">  *</div><div class="line">  * <span class="doctag">@return</span> 交易回执。支付成功时返回成功信息，否则记录失败原因。</div><div class="line">  */</div><div class="line">  <span class="function">Receipt <span class="title">chargeOrder</span><span class="params">(PizzaOrder order, CreditCard creditCard)</span></span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>使用new的方式获取信用卡支付处理器和数据库交易日志记录器：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RealBillingService</span> <span class="keyword">implements</span> <span class="title">BillingService</span> </span>&#123;</div><div class="line">  <span class="function"><span class="keyword">public</span> Receipt <span class="title">chargeOrder</span><span class="params">(PizzaOrder order, CreditCard creditCard)</span> </span>&#123;</div><div class="line">    CreditCardProcessor processor = <span class="keyword">new</span> PaypalCreditCardProcessor();</div><div class="line">    TransactionLog transactionLog = <span class="keyword">new</span> DatabaseTransactionLog();</div><div class="line"></div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">      ChargeResult result = processor.charge(creditCard, order.getAmount());</div><div class="line">      transactionLog.logChargeResult(result);</div><div class="line"></div><div class="line">      <span class="keyword">return</span> result.wasSuccessful()</div><div class="line">          ? Receipt.forSuccessfulCharge(order.getAmount())</div><div class="line">          : Receipt.forDeclinedCharge(result.getDeclineMessage());</div><div class="line">     &#125; <span class="keyword">catch</span> (UnreachableException e) &#123;</div><div class="line">      transactionLog.logConnectException(e);</div><div class="line">      <span class="keyword">return</span> Receipt.forSystemFailure(e.getMessage());</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>使用new的问题是使得代码耦合，不易维护和测试。比如在UT里不可能直接用真实的信用卡支付，需要Mock一个CreditCardProcessor。相比于new，更容易想到的改进是使用工厂方法，但是工厂方法在测试中仍存在问题（因为通常使用全局变量来保存实例，如果在用例中未重置可能会影响其他用例）。更好的方式是通过构造方法注入依赖：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RealBillingService</span> <span class="keyword">implements</span> <span class="title">BillingService</span> </span>&#123;</div><div class="line">  <span class="keyword">private</span> <span class="keyword">final</span> CreditCardProcessor processor;</div><div class="line">  <span class="keyword">private</span> <span class="keyword">final</span> TransactionLog transactionLog;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="title">RealBillingService</span><span class="params">(CreditCardProcessor processor, </span></span></div><div class="line">      TransactionLog transactionLog) &#123;</div><div class="line">    <span class="keyword">this</span>.processor = processor;</div><div class="line">    <span class="keyword">this</span>.transactionLog = transactionLog;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">public</span> Receipt <span class="title">chargeOrder</span><span class="params">(PizzaOrder order, CreditCard creditCard)</span> </span>&#123;</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">      ChargeResult result = processor.charge(creditCard, order.getAmount());</div><div class="line">      transactionLog.logChargeResult(result);</div><div class="line"></div><div class="line">      <span class="keyword">return</span> result.wasSuccessful()</div><div class="line">          ? Receipt.forSuccessfulCharge(order.getAmount())</div><div class="line">          : Receipt.forDeclinedCharge(result.getDeclineMessage());</div><div class="line">     &#125; <span class="keyword">catch</span> (UnreachableException e) &#123;</div><div class="line">      transactionLog.logConnectException(e);</div><div class="line">      <span class="keyword">return</span> Receipt.forSystemFailure(e.getMessage());</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>对于真实的网站应用可以注入真正的业务处理服务类：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</div><div class="line">    CreditCardProcessor processor = <span class="keyword">new</span> PaypalCreditCardProcessor();</div><div class="line">    TransactionLog transactionLog = <span class="keyword">new</span> DatabaseTransactionLog();</div><div class="line">    BillingService billingService</div><div class="line">        = <span class="keyword">new</span> RealBillingService(processor, transactionLog);</div><div class="line">    ...</div><div class="line">  &#125;</div></pre></td></tr></table></figure>
<p>而在测试用例中可以注入Mock类：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RealBillingServiceTest</span> <span class="keyword">extends</span> <span class="title">TestCase</span> </span>&#123;</div><div class="line"></div><div class="line">  <span class="keyword">private</span> <span class="keyword">final</span> PizzaOrder order = <span class="keyword">new</span> PizzaOrder(<span class="number">100</span>);</div><div class="line">  <span class="keyword">private</span> <span class="keyword">final</span> CreditCard creditCard = <span class="keyword">new</span> CreditCard(<span class="string">"1234"</span>, <span class="number">11</span>, <span class="number">2010</span>);</div><div class="line"></div><div class="line">  <span class="keyword">private</span> <span class="keyword">final</span> InMemoryTransactionLog transactionLog = <span class="keyword">new</span> InMemoryTransactionLog();</div><div class="line">  <span class="keyword">private</span> <span class="keyword">final</span> FakeCreditCardProcessor processor = <span class="keyword">new</span> FakeCreditCardProcessor();</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testSuccessfulCharge</span><span class="params">()</span> </span>&#123;</div><div class="line">    RealBillingService billingService</div><div class="line">        = <span class="keyword">new</span> RealBillingService(processor, transactionLog);</div><div class="line">    Receipt receipt = billingService.chargeOrder(order, creditCard);</div><div class="line"></div><div class="line">    assertTrue(receipt.hasSuccessfulCharge());</div><div class="line">    assertEquals(<span class="number">100</span>, receipt.getAmountOfCharge());</div><div class="line">    assertEquals(creditCard, processor.getCardOfOnlyCharge());</div><div class="line">    assertEquals(<span class="number">100</span>, processor.getAmountOfOnlyCharge());</div><div class="line">    assertTrue(transactionLog.wasSuccessLogged());</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>那通过Guice怎么实现依赖注入呢？首先我们需要告诉Guice如果找到接口对应的实现类，这个可以通过<strong>模块</strong>来实现：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BillingModule</span> <span class="keyword">extends</span> <span class="title">AbstractModule</span> </span>&#123;</div><div class="line">  <span class="meta">@Override</span> </div><div class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">()</span> </span>&#123;</div><div class="line">    bind(TransactionLog.class).to(DatabaseTransactionLog.class);</div><div class="line">    bind(CreditCardProcessor.class).to(PaypalCreditCardProcessor.class);</div><div class="line">    bind(BillingService.class).to(RealBillingService.class);</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这里的模块只需要实现Module接口或继承自AbstractModule，然后在configure方法中设置<strong>绑定</strong>（后面会继续介绍）即可。然后只需在原有的构造方法中增加@Inject注解即可<strong>注入</strong>：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RealBillingService</span> <span class="keyword">implements</span> <span class="title">BillingService</span> </span>&#123;</div><div class="line">  <span class="keyword">private</span> <span class="keyword">final</span> CreditCardProcessor processor;</div><div class="line">  <span class="keyword">private</span> <span class="keyword">final</span> TransactionLog transactionLog;</div><div class="line"></div><div class="line">  <span class="meta">@Inject</span></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="title">RealBillingService</span><span class="params">(CreditCardProcessor processor,</span></span></div><div class="line">      TransactionLog transactionLog) &#123;</div><div class="line">    <span class="keyword">this</span>.processor = processor;</div><div class="line">    <span class="keyword">this</span>.transactionLog = transactionLog;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">public</span> Receipt <span class="title">chargeOrder</span><span class="params">(PizzaOrder order, CreditCard creditCard)</span> </span>&#123;</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">      ChargeResult result = processor.charge(creditCard, order.getAmount());</div><div class="line">      transactionLog.logChargeResult(result);</div><div class="line"></div><div class="line">      <span class="keyword">return</span> result.wasSuccessful()</div><div class="line">          ? Receipt.forSuccessfulCharge(order.getAmount())</div><div class="line">          : Receipt.forDeclinedCharge(result.getDeclineMessage());</div><div class="line">     &#125; <span class="keyword">catch</span> (UnreachableException e) &#123;</div><div class="line">      transactionLog.logConnectException(e);</div><div class="line">      <span class="keyword">return</span> Receipt.forSystemFailure(e.getMessage());</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>最后，再看看main方法中是如何调用的：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</div><div class="line">    Injector injector = Guice.createInjector(<span class="keyword">new</span> BillingModule());</div><div class="line">    BillingService billingService = injector.getInstance(BillingService.class);</div><div class="line">    ...</div><div class="line">  &#125;</div></pre></td></tr></table></figure>
<h2 id="绑定"><a href="#绑定" class="headerlink" title="绑定"></a>绑定</h2><h3 id="连接绑定"><a href="#连接绑定" class="headerlink" title="连接绑定"></a>连接绑定</h3><p>连接绑定是最常用的绑定方式，它将一个类型和它的实现进行映射。下面的例子中将TransactionLog接口映射到它的实现类DatabaseTransactionLog。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BillingModule</span> <span class="keyword">extends</span> <span class="title">AbstractModule</span> </span>&#123;</div><div class="line">  <span class="meta">@Override</span> </div><div class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">()</span> </span>&#123;</div><div class="line">    bind(TransactionLog.class).to(DatabaseTransactionLog.class);</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>连接绑定还支持链式，比如下面的例子最终将TransactionLog接口映射到实现类MySqlDatabaseTransactionLog。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BillingModule</span> <span class="keyword">extends</span> <span class="title">AbstractModule</span> </span>&#123;</div><div class="line">  <span class="meta">@Override</span> </div><div class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">()</span> </span>&#123;</div><div class="line">    bind(TransactionLog.class).to(DatabaseTransactionLog.class);</div><div class="line">    bind(DatabaseTransactionLog.class).to(MySqlDatabaseTransactionLog.class);</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h3 id="注解绑定"><a href="#注解绑定" class="headerlink" title="注解绑定"></a>注解绑定</h3><p>通过一个类型可能存在多个实现，比如在信用卡支付处理器中存在PayPal的支付和Google支付，这样通过连接绑定就搞不定。这时我们可以通过注解绑定来实现：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="meta">@BindingAnnotation</span> </div><div class="line"><span class="meta">@Target</span>(&#123; FIELD, PARAMETER, METHOD &#125;) </div><div class="line"><span class="meta">@Retention</span>(RUNTIME)</div><div class="line"><span class="keyword">public</span> <span class="meta">@interface</span> PayPal &#123;&#125;</div><div class="line"></div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RealBillingService</span> <span class="keyword">implements</span> <span class="title">BillingService</span> </span>&#123;</div><div class="line"></div><div class="line">  <span class="meta">@Inject</span></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="title">RealBillingService</span><span class="params">(@PayPal CreditCardProcessor processor,</span></span></div><div class="line">      TransactionLog transactionLog) &#123;</div><div class="line">    ...</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// 当注入的方法参数存在@PayPal注解时注入PayPalCreditCardProcessor实现</span></div><div class="line">bind(CreditCardProcessor.class).annotatedWith(PayPal.class).to(PayPalCreditCardProcessor.class);</div></pre></td></tr></table></figure>
<p>可以看到在模块的绑定时用annotatedWith方法指定具体的注解来进行绑定，这种方式有一个问题就是我们必须增加自定义的注解来绑定，基于此Guice内置了一个@Named注解满足该场景：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RealBillingService</span> <span class="keyword">implements</span> <span class="title">BillingService</span> </span>&#123;</div><div class="line"></div><div class="line">  <span class="meta">@Inject</span></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="title">RealBillingService</span><span class="params">(@Named(<span class="string">"Checkout"</span>)</span> CreditCardProcessor processor,</span></div><div class="line">      TransactionLog transactionLog) &#123;</div><div class="line">    ...</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// 当注入的方法参数存在@Named注解且值为Checkout时注入CheckoutCreditCardProcessor实现</span></div><div class="line">bind(CreditCardProcessor.class).annotatedWith(Names.named(<span class="string">"Checkout"</span>)).to(CheckoutCreditCardProcessor.class);</div></pre></td></tr></table></figure>
<h3 id="实例绑定"><a href="#实例绑定" class="headerlink" title="实例绑定"></a>实例绑定</h3><p>将一个类型绑定到一个具体的实例而非实现类，这个通过是在无依赖的对象（比如值对象）中使用。如果toInstance包含复杂的逻辑会导致启动速度，此时应该通过@Provides方法绑定。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">bind(String.class).annotatedWith(Names.named(<span class="string">"JDBC URL"</span>)).toInstance(<span class="string">"jdbc:mysql://localhost/pizza"</span>);</div><div class="line">bind(Integer.class).annotatedWith(Names.named(<span class="string">"login timeout seconds"</span>)).toInstance(<span class="number">10</span>);</div></pre></td></tr></table></figure>
<h3 id="Provides方法绑定"><a href="#Provides方法绑定" class="headerlink" title="@Provides方法绑定"></a>@Provides方法绑定</h3><p>模块中定义的、带有@Provides注解的、方法返回值即为绑定映射的类型。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BillingModule</span> <span class="keyword">extends</span> <span class="title">AbstractModule</span> </span>&#123;</div><div class="line">  <span class="meta">@Override</span></div><div class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">()</span> </span>&#123;</div><div class="line">    ...</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="meta">@Provides</span></div><div class="line">  <span class="function">TransactionLog <span class="title">provideTransactionLog</span><span class="params">()</span> </span>&#123;</div><div class="line">    DatabaseTransactionLog transactionLog = <span class="keyword">new</span> DatabaseTransactionLog();</div><div class="line">    transactionLog.setJdbcUrl(<span class="string">"jdbc:mysql://localhost/pizza"</span>);</div><div class="line">    transactionLog.setThreadPoolSize(<span class="number">30</span>);</div><div class="line">    <span class="keyword">return</span> transactionLog;</div><div class="line">  &#125;</div><div class="line">  </div><div class="line">  <span class="meta">@Provides</span> <span class="meta">@PayPal</span></div><div class="line">  <span class="function">CreditCardProcessor <span class="title">providePayPalCreditCardProcessor</span><span class="params">(@Named(<span class="string">"PayPal API key"</span>)</span> String apiKey) </span>&#123;</div><div class="line">    PayPalCreditCardProcessor processor = <span class="keyword">new</span> PayPalCreditCardProcessor();</div><div class="line">    processor.setApiKey(apiKey);</div><div class="line">    <span class="keyword">return</span> processor;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="Provider绑定"><a href="#Provider绑定" class="headerlink" title="Provider绑定"></a>Provider绑定</h3><p>如果使用@Provides方法绑定逻辑越来越复杂时就可以通过Provider绑定（一个实现了Provider接口的实现类）来实现。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Provider</span>&lt;<span class="title">T</span>&gt; </span>&#123;</div><div class="line">  <span class="function">T <span class="title">get</span><span class="params">()</span></span>;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DatabaseTransactionLogProvider</span> <span class="keyword">implements</span> <span class="title">Provider</span>&lt;<span class="title">TransactionLog</span>&gt; </span>&#123;</div><div class="line">  <span class="keyword">private</span> <span class="keyword">final</span> Connection connection;</div><div class="line"></div><div class="line">  <span class="meta">@Inject</span></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="title">DatabaseTransactionLogProvider</span><span class="params">(Connection connection)</span> </span>&#123;</div><div class="line">    <span class="keyword">this</span>.connection = connection;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">public</span> TransactionLog <span class="title">get</span><span class="params">()</span> </span>&#123;</div><div class="line">    DatabaseTransactionLog transactionLog = <span class="keyword">new</span> DatabaseTransactionLog();</div><div class="line">    transactionLog.setConnection(connection);</div><div class="line">    <span class="keyword">return</span> transactionLog;</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BillingModule</span> <span class="keyword">extends</span> <span class="title">AbstractModule</span> </span>&#123;</div><div class="line">  <span class="meta">@Override</span></div><div class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">()</span> </span>&#123;</div><div class="line">    bind(TransactionLog.class).toProvider(DatabaseTransactionLogProvider.class);</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="无目标绑定"><a href="#无目标绑定" class="headerlink" title="无目标绑定"></a>无目标绑定</h3><p>当我们想提供对一个具体的类给注入器时就可以采用无目标绑定。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">bind(MyConcreteClass.class);</div><div class="line">bind(AnotherConcreteClass.class).in(Singleton.class);</div></pre></td></tr></table></figure>
<h3 id="构造器绑定"><a href="#构造器绑定" class="headerlink" title="构造器绑定"></a>构造器绑定</h3><p>3.0新增的绑定，适用于第三方提供的类或者是有多个构造器参与依赖注入。通过@Provides方法可以显式调用构造器，但是这种方式有一个限制：无法给这些实例应用AOP。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BillingModule</span> <span class="keyword">extends</span> <span class="title">AbstractModule</span> </span>&#123;</div><div class="line">  <span class="meta">@Override</span> </div><div class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">      bind(TransactionLog.class).toConstructor(DatabaseTransactionLog.class.getConstructor(DatabaseConnection.class));</div><div class="line">    &#125; <span class="keyword">catch</span> (NoSuchMethodException e) &#123;</div><div class="line">      addError(e);</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="范围"><a href="#范围" class="headerlink" title="范围"></a>范围</h2><p>默认情况下，Guice每次都会返回一个新的实例，这个可以通过范围（Scope）来配置。常见的范围有单例（@Singleton）、会话（@SessionScoped）和请求（@RequestScoped），另外还可以通过自定义的范围来扩展。</p>
<p>范围的注解可以应该在实现类、@Provides方法中，或在绑定的时候指定（优先级最高）：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="meta">@Singleton</span></div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">InMemoryTransactionLog</span> <span class="keyword">implements</span> <span class="title">TransactionLog</span> </span>&#123;</div><div class="line">  <span class="comment">/* everything here should be threadsafe! */</span></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// scopes apply to the binding source, not the binding target</span></div><div class="line">bind(TransactionLog.class).to(InMemoryTransactionLog.class).in(Singleton.class);</div><div class="line"></div><div class="line"><span class="meta">@Provides</span> <span class="meta">@Singleton</span></div><div class="line"><span class="function">TransactionLog <span class="title">provideTransactionLog</span><span class="params">()</span> </span>&#123;</div><div class="line">    ...</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>另外，Guice还有一种特殊的单例模式叫饥饿单例（相对于懒加载单例来说）：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Eager singletons reveal initialization problems sooner, </span></div><div class="line"><span class="comment">// and ensure end-users get a consistent, snappy experience. </span></div><div class="line">bind(TransactionLog.class).to(InMemoryTransactionLog.class).asEagerSingleton();</div></pre></td></tr></table></figure></p>
<h2 id="注入"><a href="#注入" class="headerlink" title="注入"></a>注入</h2><p>依赖注入的要求就是将行为和依赖分离，它建议将依赖注入而非通过工厂类的方法去查找。注入的方式通常有构造器注入、方法注入、属性注入等。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// 构造器注入</span></div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RealBillingService</span> <span class="keyword">implements</span> <span class="title">BillingService</span> </span>&#123;</div><div class="line">  <span class="keyword">private</span> <span class="keyword">final</span> CreditCardProcessor processorProvider;</div><div class="line">  <span class="keyword">private</span> <span class="keyword">final</span> TransactionLog transactionLogProvider;</div><div class="line"></div><div class="line">  <span class="meta">@Inject</span></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="title">RealBillingService</span><span class="params">(CreditCardProcessor processorProvider,</span></span></div><div class="line">      TransactionLog transactionLogProvider) &#123;</div><div class="line">    <span class="keyword">this</span>.processorProvider = processorProvider;</div><div class="line">    <span class="keyword">this</span>.transactionLogProvider = transactionLogProvider;</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// 方法注入</span></div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PayPalCreditCardProcessor</span> <span class="keyword">implements</span> <span class="title">CreditCardProcessor</span> </span>&#123;</div><div class="line">  <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String DEFAULT_API_KEY = <span class="string">"development-use-only"</span>;</div><div class="line">  <span class="keyword">private</span> String apiKey = DEFAULT_API_KEY;</div><div class="line"></div><div class="line">  <span class="meta">@Inject</span></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setApiKey</span><span class="params">(@Named(<span class="string">"PayPal API key"</span>)</span> String apiKey) </span>&#123;</div><div class="line">    <span class="keyword">this</span>.apiKey = apiKey;</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// 属性注入</span></div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DatabaseTransactionLogProvider</span> <span class="keyword">implements</span> <span class="title">Provider</span>&lt;<span class="title">TransactionLog</span>&gt; </span>&#123;</div><div class="line">  <span class="meta">@Inject</span> Connection connection;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">public</span> TransactionLog <span class="title">get</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> <span class="keyword">new</span> DatabaseTransactionLog(connection);</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// 可选注入：当找不到映射时不报错</span></div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PayPalCreditCardProcessor</span> <span class="keyword">implements</span> <span class="title">CreditCardProcessor</span> </span>&#123;</div><div class="line">  <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String SANDBOX_API_KEY = <span class="string">"development-use-only"</span>;</div><div class="line">  <span class="keyword">private</span> String apiKey = SANDBOX_API_KEY;</div><div class="line"></div><div class="line">  <span class="meta">@Inject</span>(optional=<span class="keyword">true</span>)</div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setApiKey</span><span class="params">(@Named(<span class="string">"PayPal API key"</span>)</span> String apiKey) </span>&#123;</div><div class="line">    <span class="keyword">this</span>.apiKey = apiKey;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="辅助注入"><a href="#辅助注入" class="headerlink" title="辅助注入"></a>辅助注入</h2><p>辅助注入（Assisted Inject）属于Guice扩展的一部分，它通过@Assisted注解自动生成工厂来加强非注入参数的使用。</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div></pre></td><td class="code"><pre><div class="line">// RealPayment中有两个参数startDate和amount无法直接注入</div><div class="line">public class RealPayment implements Payment &#123;</div><div class="line">  public RealPayment(</div><div class="line">        CreditService creditService,  // from the Injector</div><div class="line">        AuthService authService,  // from the Injector</div><div class="line">        Date startDate, // from the instance's creator</div><div class="line">        Money amount); // from the instance's creator</div><div class="line">  &#125;</div><div class="line">  ...</div><div class="line">&#125;</div><div class="line"></div><div class="line">// 一种方式是增加一个工厂来构造</div><div class="line">public interface PaymentFactory &#123;</div><div class="line">  public Payment create(Date startDate, Money amount);</div><div class="line">&#125;</div><div class="line"></div><div class="line">public class RealPaymentFactory implements PaymentFactory &#123;</div><div class="line">  private final Provider&lt;CreditService&gt; creditServiceProvider;</div><div class="line">  private final Provider&lt;AuthService&gt; authServiceProvider;</div><div class="line"></div><div class="line">  @Inject</div><div class="line">  public RealPaymentFactory(Provider&lt;CreditService&gt; creditServiceProvider,</div><div class="line">      Provider&lt;AuthService&gt; authServiceProvider) &#123;</div><div class="line">    this.creditServiceProvider = creditServiceProvider;</div><div class="line">    this.authServiceProvider = authServiceProvider;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  public Payment create(Date startDate, Money amount) &#123;</div><div class="line">    return new RealPayment(creditServiceProvider.get(),</div><div class="line">      authServiceProvider.get(), startDate, amount);</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">bind(PaymentFactory.class).to(RealPaymentFactory.class);</div><div class="line"></div><div class="line">// 通过@Assisted注解可以减少RealPaymentFactory</div><div class="line">public class RealPayment implements Payment &#123;</div><div class="line">  @Inject</div><div class="line">  public RealPayment(</div><div class="line">        CreditService creditService,</div><div class="line">        AuthService authService,</div><div class="line">        @Assisted Date startDate,</div><div class="line">        @Assisted Money amount);</div><div class="line">  &#125;</div><div class="line">  ...</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Guice 2.0</div><div class="line">// bind(PaymentFactory.class).toProvider(FactoryProvider.newFactory(PaymentFactory.class, RealPayment.class));</div><div class="line">// Guice 3.0</div><div class="line">install(new FactoryModuleBuilder().implement(Payment.class, RealPayment.class).build(PaymentFactory.class));</div></pre></td></tr></table></figure>
<h2 id="最佳实践"><a href="#最佳实践" class="headerlink" title="最佳实践"></a>最佳实践</h2><ul>
<li>最小化可变性：尽可能注入的是不可变对象；</li>
<li>只注入直接依赖：不用注入一个实例来获取真正需要的实例，增加复杂性且不易测试；</li>
<li>避免循环依赖</li>
<li>避免静态状态：静态状态和可测试性就是天敌；</li>
<li>采用@Nullable：Guice默认情况下禁止注入null对象；</li>
<li>模块的处理必须要快并且无副作用</li>
<li>在Providers绑定中当心IO问题：因为Provider不检查异常、不支持超时、不支持重试；</li>
<li>不用在模块中处理分支逻辑</li>
<li>尽可能不要暴露构造器</li>
</ul>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a href="https://github.com/google/guice/wiki/Motivation" target="_blank" rel="external">Guice用户指南</a></li>
<li><a href="https://www.youtube.com/watch?v=hBVJbzAagfs" target="_blank" rel="external">Google I/O 2009 - Big Modular Java with Guice</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://github.com/google/guice/&quot;&gt;Guice&lt;/a&gt;是Google开源的一个依赖注入类库，相比于Spring IoC来说更小更快。Elasticsearch大量使用了Guice，本文简单的介绍下Guice的基本概念和使用方式。&lt;br&gt;
    
    </summary>
    
      <category term="OpenSource" scheme="http://ginobefunny.com/categories/OpenSource/"/>
    
    
      <category term="Guice" scheme="http://ginobefunny.com/tags/Guice/"/>
    
      <category term="Google" scheme="http://ginobefunny.com/tags/Google/"/>
    
      <category term="DI" scheme="http://ginobefunny.com/tags/DI/"/>
    
      <category term="IoC" scheme="http://ginobefunny.com/tags/IoC/"/>
    
      <category term="依赖注入" scheme="http://ginobefunny.com/tags/%E4%BE%9D%E8%B5%96%E6%B3%A8%E5%85%A5/"/>
    
      <category term="控制反转" scheme="http://ginobefunny.com/tags/%E6%8E%A7%E5%88%B6%E5%8F%8D%E8%BD%AC/"/>
    
      <category term="入门" scheme="http://ginobefunny.com/tags/%E5%85%A5%E9%97%A8/"/>
    
      <category term="教程" scheme="http://ginobefunny.com/tags/%E6%95%99%E7%A8%8B/"/>
    
      <category term="实例" scheme="http://ginobefunny.com/tags/%E5%AE%9E%E4%BE%8B/"/>
    
  </entry>
  
  <entry>
    <title>Elasticsearch插件与集成介绍</title>
    <link href="http://ginobefunny.com/post/elasticsearch_plugins_and_integrations/"/>
    <id>http://ginobefunny.com/post/elasticsearch_plugins_and_integrations/</id>
    <published>2017-01-16T07:35:53.000Z</published>
    <updated>2017-01-16T07:46:49.177Z</updated>
    
    <content type="html"><![CDATA[<p>Elasticsearch的能力虽然已经非常强大，但是它也提供了基于插件的扩展功能，基于此我们可以扩展查询、分词、监控、脚本等能力。<br>这是学习Elasticsearch插件的第一篇，主要是阅读<a href="https://www.elastic.co/guide/en/elasticsearch/plugins/2.3/index.html" target="_blank" rel="external">官方文档</a>的笔记，介绍官方的一些插件和优秀的社区插件；后面一篇主要是通过源码来深入学习Elasticsearch插件的开发，并通过实战开发一个自定义的插件。</p>
<a id="more"></a>
<h2 id="Plugin-Management"><a href="#Plugin-Management" class="headerlink" title="Plugin Management"></a>Plugin Management</h2><ul>
<li>Plugins are a way to enhance the core Elasticsearch functionality in a custom manner. They range from adding custom mapping types, custom analyzers, native scripts, custom discovery and more.</li>
<li>Site plugins and mixed plugins are deprecated and will be removed in 5.0.0. Instead, site plugins should either be migrated to Kibana or should use a standalone web server.</li>
<li>The <em>plugin</em> script is used to install, list, and remove plugins. </li>
</ul>
<pre><code>sudo bin/plugin install [plugin_name]   #Core Elasticsearch plugins
sudo bin/plugin install [org]/[user|component]/[version] #Community and non-core plugins
sudo bin/plugin install [url]           #Custom URL or file system
sudo bin/plugin list                    #Listing plugins
sudo bin/plugin remove [plugin_name]    #Removing plugins
</code></pre><h2 id="API-Extension-Plugins"><a href="#API-Extension-Plugins" class="headerlink" title="API Extension Plugins"></a>API Extension Plugins</h2><p>API extension plugins add new functionality to Elasticsearch by adding new APIs or features, usually to do with search or mapping.</p>
<ul>
<li>[Core]<a href="https://www.elastic.co/guide/en/elasticsearch/plugins/2.3/plugins-delete-by-query.html" target="_blank" rel="external">Delete By Query Plugin</a>：Elasticsearch 1.0版本有一个delete-by-query的API，由于存在兼容性、一致性和可靠性的问题而被<a href="https://www.elastic.co/guide/en/elasticsearch/plugins/2.3/delete-by-query-plugin-reason.html" target="_blank" rel="external">移除</a>，该插件使用scroll获取文档ID和版本然后使用bulk进行批量删除；</li>
<li><a href="https://github.com/carrot2/elasticsearch-carrot2" target="_blank" rel="external">carrot2 Plugin</a>：可以自动地将相似的文档组织起来，并且给每个文档的群组分类贴上相应的较为用户可以理解的标签。这样的聚类也可以看做是一种动态的针对每个搜索和命中结果集合的动态 facet。可以在<a href="http://search.carrot2.org/stable/search?query=elasticsearch&amp;results=200&amp;view=foamtree" target="_blank" rel="external">Carrot2 demo page</a>体验一下这个工具。</li>
<li><a href="https://github.com/kzwang/elasticsearch-image" target="_blank" rel="external">Elasticsearch Image Plugin</a>：基于<a href="https://github.com/dermotte/lire" target="_blank" rel="external">LIRE</a>的相似图片搜索插件；</li>
<li><a href="https://github.com/YannBrrd/elasticsearch-entity-resolution" target="_blank" rel="external">Entity Resolution Plugin</a>：基于贝叶斯概率模型去除重复数据的插件；</li>
<li><a href="https://github.com/NLPchina/elasticsearch-sql/" target="_blank" rel="external">SQL language Plugin</a>：支持采用SQL查询的ES插件；</li>
<li><a href="https://github.com/codelibs/elasticsearch-taste" target="_blank" rel="external">Elasticsearch Taste Plugin</a>：基于用户和内容推荐的ES插件；</li>
</ul>
<h2 id="Analysis-Plugins"><a href="#Analysis-Plugins" class="headerlink" title="Analysis Plugins"></a>Analysis Plugins</h2><p>Analysis plugins extend Elasticsearch by adding new analyzers, tokenizers, token filters, or character filters to Elasticsearch.</p>
<ul>
<li>[Core]<a href="https://www.elastic.co/guide/en/elasticsearch/plugins/2.3/analysis-icu.html" target="_blank" rel="external">ICU</a>：使用ICU实现的一个针对亚洲语言的分词器插件；</li>
<li>[Core]<a href="https://www.elastic.co/guide/en/elasticsearch/plugins/2.3/analysis-smartcn.html" target="_blank" rel="external">SmartCN</a>：官方提供的一个基于概率的针对中文或中英混合的分词器；</li>
<li><a href="https://github.com/yakaz/elasticsearch-analysis-combo/" target="_blank" rel="external">Combo Analysis Plugin</a>：通常一个分析器里只能配置一个分词器，该插件支持能配置多个分词器组合；</li>
<li><a href="https://github.com/medcl/elasticsearch-analysis-ik" target="_blank" rel="external">IK Analysis Plugin</a>：一个非常流行的中文分析器插件，迁移自Lucene的IK分析器；</li>
<li><a href="https://github.com/medcl/elasticsearch-analysis-mmseg" target="_blank" rel="external">Mmseg Analysis Plugin</a>：基于MMSEG算法的中文分析器，在中英混合时分词效果较差；</li>
<li><a href="https://github.com/medcl/elasticsearch-analysis-pinyin" target="_blank" rel="external">Pinyin Analysis Plugin</a>：将中文转换为拼音的分析器，支持首字母和连接符配置；</li>
</ul>
<h2 id="Discovery-Pluginsedit"><a href="#Discovery-Pluginsedit" class="headerlink" title="Discovery Pluginsedit"></a>Discovery Pluginsedit</h2><p>Discovery plugins extend Elasticsearch by adding new discovery mechanisms that can be used instead of <a href="https://www.elastic.co/guide/en/elasticsearch/reference/2.3/modules-discovery-zen.html" target="_blank" rel="external">Zen Discovery</a>.</p>
<ul>
<li>[Core]AWS Cloud/Azure Cloud/GCE Cloud：官方提供的基于各种云服务的插件；</li>
<li><a href="https://github.com/grmblfrz/elasticsearch-zookeeper" target="_blank" rel="external">ZooKeeper Discovery Plugin</a>：基于ZooKeeper的Elasticsearch集群发现插件；</li>
<li><a href="https://github.com/fabric8io/elasticsearch-cloud-kubernetes" target="_blank" rel="external">Kubernetes Discovery Plugin</a>：使用K8 API单播发现插件；</li>
</ul>
<h2 id="Security-Plugins"><a href="#Security-Plugins" class="headerlink" title="Security Plugins"></a>Security Plugins</h2><ul>
<li><a href="https://github.com/codecentric/elasticsearch-shield-kerberos-realm" target="_blank" rel="external">Kerberos/SPNEGO Realm</a>：基于Kerberos/SPNEGO验证HTTP和传输请求。</li>
<li><a href="https://github.com/sscarduzio/elasticsearch-readonlyrest-plugin" target="_blank" rel="external">Readonly REST</a>：只对外暴露查询相关的操作，拒绝删除和更新操作。</li>
</ul>
<h2 id="Integrations"><a href="#Integrations" class="headerlink" title="Integrations"></a>Integrations</h2><p>Integrations are not plugins, but are external tools or modules that make it easier to work with Elasticsearch.</p>
<ul>
<li><a href="https://github.com/jprante/elasticsearch-jdbc" target="_blank" rel="external">JDBC importer</a>：通过JDBC将数据库的数据导入到Elasticsearch中；</li>
<li><a href="https://github.com/BigDataDevs/kafka-elasticsearch-consumer" target="_blank" rel="external">Kafka Standalone Consumer(Indexer)</a>：读取kafka消息并处理，最终批量写入到Elasticsearch中；</li>
<li><a href="https://github.com/ozlerhakan/mongolastic" target="_blank" rel="external">Mongolastic</a>：将MongoDB的数据迁移到Elasticsearch中；</li>
<li><a href="https://github.com/Aconex/scrutineer" target="_blank" rel="external">Scrutineer</a>：比较Elasticsearch和数据库中数据的一致性；</li>
</ul>
<h2 id="Help-for-plugin-authors"><a href="#Help-for-plugin-authors" class="headerlink" title="Help for plugin authors"></a>Help for plugin authors</h2><p>插件描述文件plugin-descriptor.properties可以参考：<a href="https://github.com/elastic/elasticsearch/blob/2.4/dev-tools/src/main/resources/plugin-metadata/plugin-descriptor.properties" target="_blank" rel="external">https://github.com/elastic/elasticsearch/blob/2.4/dev-tools/src/main/resources/plugin-metadata/plugin-descriptor.properties</a></p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://www.elastic.co/guide/en/elasticsearch/plugins/2.3/index.html" target="_blank" rel="external">Elasticsearch Plugins and Integrations</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Elasticsearch的能力虽然已经非常强大，但是它也提供了基于插件的扩展功能，基于此我们可以扩展查询、分词、监控、脚本等能力。&lt;br&gt;这是学习Elasticsearch插件的第一篇，主要是阅读&lt;a href=&quot;https://www.elastic.co/guide/en/elasticsearch/plugins/2.3/index.html&quot;&gt;官方文档&lt;/a&gt;的笔记，介绍官方的一些插件和优秀的社区插件；后面一篇主要是通过源码来深入学习Elasticsearch插件的开发，并通过实战开发一个自定义的插件。&lt;/p&gt;
    
    </summary>
    
      <category term="Elasticsearch" scheme="http://ginobefunny.com/categories/Elasticsearch/"/>
    
    
      <category term="Elasticsearch" scheme="http://ginobefunny.com/tags/Elasticsearch/"/>
    
      <category term="Elasticsearch插件" scheme="http://ginobefunny.com/tags/Elasticsearch%E6%8F%92%E4%BB%B6/"/>
    
  </entry>
  
  <entry>
    <title>《深入理解Java虚拟机》读书笔记3：虚拟机性能监控与调优实战</title>
    <link href="http://ginobefunny.com/post/deep_in_jvm_notes_part3/"/>
    <id>http://ginobefunny.com/post/deep_in_jvm_notes_part3/</id>
    <published>2017-01-11T11:30:12.000Z</published>
    <updated>2017-01-31T09:49:32.478Z</updated>
    
    <content type="html"><![CDATA[<p>国内JVM相关书籍NO.1，Java程序员必读。读书笔记第三部分对应原书的第四章和第五章，主要介绍虚拟机的性能监控、故障处理及调优实战。<br><a id="more"></a></p>
<h2 id="第四章-虚拟机性能监控与故障处理工具"><a href="#第四章-虚拟机性能监控与故障处理工具" class="headerlink" title="第四章 虚拟机性能监控与故障处理工具"></a>第四章 虚拟机性能监控与故障处理工具</h2><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>定位问题时，知识和经验是关键基础、数据（运行日志、异常堆栈、GC日志、线程快照、堆转储快照）是依据、工具是运用知识处理数据的手段。</p>
<h3 id="JDK的命令行工具"><a href="#JDK的命令行工具" class="headerlink" title="JDK的命令行工具"></a>JDK的命令行工具</h3><p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/tools_in_sun_jdk.png" alt="Sun JDK工具"></p>
<h4 id="jps-虚拟机进程状况工具"><a href="#jps-虚拟机进程状况工具" class="headerlink" title="jps: 虚拟机进程状况工具"></a>jps: 虚拟机进程状况工具</h4><p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/tools_jps_opt.png" alt="jsp options"><br><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/tools_jps.png" alt="jsp"></p>
<ul>
<li>功能：可以列出正在运行的虚拟机进程，并线上虚拟机执行的主类名称及其本地虚拟机唯一ID（LVMID）；</li>
<li>对于本地虚拟机来说，LVMID和操作系统的进程ID是一致的；</li>
<li>其他的工具通常都需要依赖jps获取LVMID；</li>
<li>主要选项：-q（只输出LVMID）、-m（输出传给main函数的参数）、-l（输出主类的全名）、-v（输出虚拟机启动JVM参数）；</li>
</ul>
<h4 id="jstat：虚拟机统计信息监视工具"><a href="#jstat：虚拟机统计信息监视工具" class="headerlink" title="jstat：虚拟机统计信息监视工具"></a>jstat：虚拟机统计信息监视工具</h4><p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/tools_jstat_opt.png" alt="jstat options"><br><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/tools_jstat.png" alt="jstat"></p>
<ul>
<li>功能：监视虚拟机各种运行状态信息，包括类装载、内存、垃圾收集、JIT等；</li>
<li>纯文本监控首选；</li>
</ul>
<h4 id="jinfo：Java配置信息工具"><a href="#jinfo：Java配置信息工具" class="headerlink" title="jinfo：Java配置信息工具"></a>jinfo：Java配置信息工具</h4><p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/tools_jinfo.png" alt="jinfo"></p>
<ul>
<li>功能：实时地查看虚拟机各项参数。虽然jps -v可以查看虚拟机启动参数，但是无法查看一些系统默认的参数。</li>
<li>支持运行期修改参数的能力，格式为“jinfo -flag name=value pid”；</li>
</ul>
<h4 id="jmap：Java内存映像工具"><a href="#jmap：Java内存映像工具" class="headerlink" title="jmap：Java内存映像工具"></a>jmap：Java内存映像工具</h4><p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/tools_jmap_opt.png" alt="jmap options"><br><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/tools_jmap.png" alt="jmap"></p>
<ul>
<li>功能：用于生成堆转储快照（一般称为heapdump或dump文件）；</li>
<li>其他可生成heapdump的方式：使用参数-XX:+HeapDumpOnOutOfMemoryError；使用参数-XX:+HeapDumpOnCtrlBreak然后使用Ctrl+Break生成；Linux系统使用kill -3生成；</li>
<li>另外它还可以查询finalize执行队列、Java堆和永久代的详细信息；</li>
</ul>
<h4 id="jhat：虚拟机堆转储快照分析工具"><a href="#jhat：虚拟机堆转储快照分析工具" class="headerlink" title="jhat：虚拟机堆转储快照分析工具"></a>jhat：虚拟机堆转储快照分析工具</h4><p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/tools_jhat.png" alt="jhat"><br><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/tools_jhat_view.png" alt="jhat效果"></p>
<ul>
<li>功能：用于分析jmap生成的heapdump。其内置了一个微型的HTTP服务器，可以在浏览器查看分析结果；</li>
<li>实际很少用jhat，主要有两个原因：一是分析工程会耗用服务器资源；功能相对BisualVM、IBM HeapAnalyzer较为简陋；</li>
</ul>
<h4 id="jstack：Java堆栈跟踪工具"><a href="#jstack：Java堆栈跟踪工具" class="headerlink" title="jstack：Java堆栈跟踪工具"></a>jstack：Java堆栈跟踪工具</h4><p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/tools_jstack_opt.png" alt="jstack options"><br><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/tools_jstack.png" alt="jstack"></p>
<ul>
<li>功能：用于生成虚拟机当前时刻的线程快照（一般称为threaddump或javacore文件）。javacore主要目的是定位线程出现长时间停顿的原因，比如死锁、死循环、请求外部资源响应长等；</li>
<li>另外JDK 1.5后Thread类新增了getAllStackTraces()方法，可以基于此自己增加管理页面来分析；</li>
</ul>
<h4 id="HSDIS：JIT生成代码反编译"><a href="#HSDIS：JIT生成代码反编译" class="headerlink" title="HSDIS：JIT生成代码反编译"></a>HSDIS：JIT生成代码反编译</h4><p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/tools_hsdis.png" alt="HSDIS"><br><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/tools_hsdis_result.png" alt="HSDIS Output"></p>
<ul>
<li>现代虚拟机的实现慢慢地和虚拟机规范产生差距，如果要分析程序如果执行，最常见的就是通过软件调试工具（GDB、Windbg等）断点调试，但是对于Java来说，很多执行代码是通过JIT动态生成到CodeBuffer中的；</li>
<li>功能：HSDIS是官方推荐的HotSpot虚拟机JIT编译代码的反汇编工具，它包含在HotSpot虚拟机的源码中但没有提供编译后的程序，可以自己下载放到JDK的相关目录里；</li>
</ul>
<h3 id="JDK的可视化工具"><a href="#JDK的可视化工具" class="headerlink" title="JDK的可视化工具"></a>JDK的可视化工具</h3><h4 id="JConsole：Java监视与管理控制台"><a href="#JConsole：Java监视与管理控制台" class="headerlink" title="JConsole：Java监视与管理控制台"></a>JConsole：Java监视与管理控制台</h4><ul>
<li>是一种基于JMX的可视化监控和管理工具，它管理部分的功能是针对MBean进行管理，由于MBean可以使用代码、中间件服务器或者所有符合JMX规范的软件进行访问，因此这里着重介绍JConsole的监控功能；</li>
<li>通过jconsole命令启动JConsole后，会自动搜索本机所有虚拟机进程。另外还支持远程进程的监控；</li>
<li>进入主界面，支持查看以下标签页：概述、内存、线程、类、VM摘要和MBean；</li>
</ul>
<h4 id="VisualVM：多合一故障处理工具"><a href="#VisualVM：多合一故障处理工具" class="headerlink" title="VisualVM：多合一故障处理工具"></a>VisualVM：多合一故障处理工具</h4><p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/tools_visualvm_2.png" alt="VisualVM"></p>
<ul>
<li>目前为止JDK发布的功能最强调的运行监控和故障处理程序，另外还支持性能分析；</li>
<li>VisualVM还有一个很大的优点：不需要被监视的程序基于特殊Agent运行，对应用程序的实际性能影响很小，可直接应用在生成环境中；</li>
<li>VisualVM基于NetBeans平台开发，具备插件扩展功能的特性，基于插件可以做到：显示虚拟机进程以及进程配置、环境信息（jps、jinfo）、监视应用程序的CPU、GC、堆、方法区以及线程的信息（jstat、jstack）、dump以及分析堆转储快照（jmap、jhat）、方法级的程序运行性能分析，找出被调用最多运行时间最长的方法、离线程序快照（收集运行时配置、线程dump、内存dump等信息建立快照）、其他plugins的无限可能。</li>
<li>使用jvisualvm首次启动时需要在线自动安装插件（也可手工安装）；</li>
<li>特色功能：生成浏览堆转储快照（摘要、类、实例标签页、OQL控制台）、分析程序性能（Profiler页签可以录制一段时间程序每个方法执行次数和耗时）、BTrace动态日志跟踪（不停止目标程序运行的前提下通过HotSwap技术动态加入调试代码）；</li>
</ul>
<h3 id="本章小结"><a href="#本章小结" class="headerlink" title="本章小结"></a>本章小结</h3><p>本章介绍了随JDK发布的6个命令行工具以及两个可视化的故障处理工具，灵活运行这些工具可以给问题处理带来很多便利。我的总体感觉是可视化工具虽然强大，但是加载速度相比命令行工具慢很多，这个时候专注于某个功能的命令行工具是更优的选择。</p>
<h2 id="第五章-调优案例分析与实战"><a href="#第五章-调优案例分析与实战" class="headerlink" title="第五章 调优案例分析与实战"></a>第五章 调优案例分析与实战</h2><h3 id="概述-1"><a href="#概述-1" class="headerlink" title="概述"></a>概述</h3><p>除了第四章介绍的知识和工具外，在处理实际问题时，经验同样很重要。</p>
<h3 id="案例分析"><a href="#案例分析" class="headerlink" title="案例分析"></a>案例分析</h3><p>以下的案例大部分来源作者处理过的一些问题，还有小部分是网络上笔记有代表的案例总结。</p>
<h4 id="高性能硬件上的程序部署策略"><a href="#高性能硬件上的程序部署策略" class="headerlink" title="高性能硬件上的程序部署策略"></a>高性能硬件上的程序部署策略</h4><h5 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h5><ul>
<li>一个每天15万PV左右的在线文档网站升级了硬件，4个CPU，16GB物理内存，操作系统为64位CentOS 5.4，使用Resin作为Web服务器，没有部署其他的应用。</li>
<li>管理员选用了64位的JDK 1.5，并通过-Xmx和-Xms参数将Java堆固定在12GB。</li>
<li>使用一段时间不定期出现长时间失去响应的情况；</li>
</ul>
<h5 id="问题分析"><a href="#问题分析" class="headerlink" title="问题分析"></a>问题分析</h5><ul>
<li>升级前使用32位系统，Java堆设置为1.5GB，只是感觉运行缓慢没有明显的卡顿；</li>
<li>通过监控发现是由于GC停顿导致的，虚拟机运行在Server模式，默认使用吞吐量优先收集器，回收12GB的堆，一次Full GC的停顿时间高达14秒；</li>
<li>并且由于程序设计的原因，很多文档从磁盘加载到内存中，导致内存中出现很多由文档序列化生成的大对象，这些大对象进入了老年代，没有在Minor GC中清理掉；</li>
</ul>
<h5 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h5><ul>
<li>在虚拟机上建立5个32位的JDK逻辑集群，每个进程按2GB内存计算（其中堆固定为1.5GB），另外建议一个Apache服务作为前端均衡代理访问门户；</li>
<li>另外考虑服务压力主要在磁盘和内存访问，CPU资源敏感度较低，因此改为CMS收集器；</li>
<li>最终服务没有再出现长时间停顿，速度比硬件升级前有较大提升；</li>
</ul>
<h4 id="集群间同步导致的内存溢出"><a href="#集群间同步导致的内存溢出" class="headerlink" title="集群间同步导致的内存溢出"></a>集群间同步导致的内存溢出</h4><h5 id="问题描述-1"><a href="#问题描述-1" class="headerlink" title="问题描述"></a>问题描述</h5><ul>
<li>一个基于B/S的MIS系统，硬件为两台2个CPU、8GB内存的HP小型机，服务器为WebLogic 9.2，每台机器启动了3个WebLogic实例，构建一个6台节点的亲和式集群（一个固定的用户请求永远分配到固定的节点处理）。</li>
<li>由于有部分数据需要共享，原先采用数据库，后因为读写性能问题使用了JBossCache构建了一个全局缓存；</li>
<li>正常使用一段较长的时间，最近不定期出现了多次的内存溢出问题；</li>
</ul>
<h5 id="问题分析-1"><a href="#问题分析-1" class="headerlink" title="问题分析"></a>问题分析</h5><ul>
<li>监控发现，服务内存回收状况一直正常，每次内存回收后都能恢复到一个稳定的可用空间</li>
<li>此次未升级业务代码，排除新修改代码引入的内存泄漏问题；</li>
<li>服务增加-XX:+HeapDumpOnOutOfMemoryError参数，在最近一次内存溢出时，分析heapdump文件发现存在大量的org.jgroups.protocols.pbcast,NAKACK对象；</li>
<li>最终分析发现是由于JBossCache的NAKACK栈在页面产生大量请求时，有个负责安全校验的全局Filter导致集群各个节点之间网络交互非常频繁，当网络情况不能满足传输要求时，大量的需要失败重发的数据在内存中不断堆积导致内存溢出。</li>
</ul>
<h5 id="解决办法-1"><a href="#解决办法-1" class="headerlink" title="解决办法"></a>解决办法</h5><ul>
<li>JBossCache版本改进；</li>
<li>程序设计优化，JBossCahce集群缓存同步，不大适合有频繁写操作的情况；</li>
</ul>
<h4 id="堆外内存导致的溢出错误"><a href="#堆外内存导致的溢出错误" class="headerlink" title="堆外内存导致的溢出错误"></a>堆外内存导致的溢出错误</h4><h5 id="问题描述-2"><a href="#问题描述-2" class="headerlink" title="问题描述"></a>问题描述</h5><ul>
<li>一个学校的小型项目，基于B/S的电子考试系统，服务器是Jetty 7.1.4，硬件是一台普通PC机，Core i5 CPU，4GB内存，运行32位Windows操作系统；</li>
<li>为了实现客户端能实时地从服务器端接收考试数据，使用了逆向AJAX技术（也称为Comet或Server Side Push），选用CometD 1.1.1作为服务端推送框架；</li>
<li>测试期间发现服务端不定期抛出内存溢出；加入-XX:+HeapDumpOnOutOfMemoryError后抛出内存溢出时什么问题都没有，采用jstat观察GC并不频繁且GC回收正常；最后在内存溢出后从系统日志发现如下异常堆栈：</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/example_direct_memory_error.png" alt="堆外内存溢出日志"></p>
<h5 id="问题分析-2"><a href="#问题分析-2" class="headerlink" title="问题分析"></a>问题分析</h5><ul>
<li>在第二章里曾经说过直接内存溢出的场景，垃圾收集时，虚拟机虽然会对直接内存进行回收，但它只能等老年代满了触发Full GC时顺便清理，否则只能等内存溢出时catch住然后调用System.gc()，如果虚拟机还是不听（比如打开了-XX:+DisableExplictGC）则只能看着堆中还有许多空闲内存而溢出；</li>
<li>本案例中的CometD框架正好有大量的NIO操作需要使用直接内存；</li>
</ul>
<h4 id="外部命令导致系统缓慢"><a href="#外部命令导致系统缓慢" class="headerlink" title="外部命令导致系统缓慢"></a>外部命令导致系统缓慢</h4><h5 id="问题描述-3"><a href="#问题描述-3" class="headerlink" title="问题描述"></a>问题描述</h5><ul>
<li>一个数字校园应用系统，运行在一个4个CPU的Solaris 10操作系统上，中间件为GlassFish服务器；</li>
<li>系统在做大并发压力测试时，发现请求响应时间比较慢，通过监控工具发现CPU使用率很高，并且系统占用绝大多数的CPU资源的程序并不是应用系统本身；</li>
<li>通过Dtrace脚本发现最消耗CPU的竟然是fork系统调用（Linux用来产生新进程的）；</li>
</ul>
<h5 id="问题分析-3"><a href="#问题分析-3" class="headerlink" title="问题分析"></a>问题分析</h5><ul>
<li>最终发现是每个用户请求需要执行一个外部的shell脚本来获取一些系统信息，是通过Runtime.getRuntime().exec()方法调用的；</li>
<li>Java虚拟机在执行这个命令时先克隆一个和当前虚拟机拥有一样环境变量的进程，再用这个新进程去执行外部命令，如果频繁地执行这个操作，系统消耗会很大；</li>
<li>最终修改时改用Java的API去获取这些信息，系统恢复了正常；</li>
</ul>
<h4 id="服务器JVM进程奔溃"><a href="#服务器JVM进程奔溃" class="headerlink" title="服务器JVM进程奔溃"></a>服务器JVM进程奔溃</h4><h5 id="问题描述-4"><a href="#问题描述-4" class="headerlink" title="问题描述"></a>问题描述</h5><ul>
<li>一个基于B/S的MIS系统，硬件为两台2个CPU、8GB内存的HP系统，服务器是WebLogic 9.2（和案例”集群间同步导致的内存溢出”相同的系统）；</li>
<li>正常运行一段时间后发现运行期间频繁出现集群节点的虚拟机进程自动关闭的现象，留下一个hs_err_pid###.log，奔溃前不久都发生大量相同的异常，日志如下所示：</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/example_jvm_coredown.png" alt="JVM进程奔溃日志"></p>
<h5 id="问题分析-4"><a href="#问题分析-4" class="headerlink" title="问题分析"></a>问题分析</h5><ul>
<li>这是一个远端断开连接的异常，得知在MIS系统工作流的待办事项变化时需要通过Web服务通知OA门户系统；</li>
<li>通过SoapUI测试发现调用后竟然需要长达3分钟才能返回，并且返回结果都是连接中断；</li>
<li>由于MIS使用异步方式调用，两边处理速度不对等，导致在等待的线程和Socket连接越来越多，最终在超过虚拟机承受能力后进场奔溃；</li>
<li>解决方法：将异步调用修改为生产者/消费者模型的消息队列处理，系统恢复正常；</li>
</ul>
<h4 id="不恰当数据结构导致内存占用过大"><a href="#不恰当数据结构导致内存占用过大" class="headerlink" title="不恰当数据结构导致内存占用过大"></a>不恰当数据结构导致内存占用过大</h4><h5 id="问题描述-5"><a href="#问题描述-5" class="headerlink" title="问题描述"></a>问题描述</h5><ul>
<li>有一个后台RPC服务器，使用64位虚拟机，内存配置为-Xms4g -Xmx8g -Xmn1g，使用ParNew + CMS的收集器组合；</li>
<li>平时Minor GC时间约在20毫秒内，但业务需要每10分钟加载一个约80MB的数据文件到内存进行数据分析，这些数据会在内存中形成超过100万个HashMap<long, long=""> Entry，在这段时间里Minor GC会超过500毫秒，这个时间过长，GC日志如下：</long,></li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/example_incorr_collection1.png" alt="不恰当数据结构GC日志1"><br><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/example_incorr_collection2.png" alt="不恰当数据结构GC日志2"></p>
<h5 id="问题分析-5"><a href="#问题分析-5" class="headerlink" title="问题分析"></a>问题分析</h5><ul>
<li>在分析数据文件期间，800M的Eden空间在Minor GC后对象还是存活的，而ParNew垃圾收集器使用的是复制算法，把这些对象复制到Survivor并维持这些对象引用成为沉重的负担，导致GC时间变长；</li>
<li>从GC可以将Survivor空间去掉（加入参数-XX:SurvivorRatio=65536、-XX:MaxTenuringThreshold=0或者-XX:AlwaysTenure），让新生代存活的对象第一次Minor GC后立即进入老年代，等到Major GC再清理。这种方式可以治标，但也有很大的副作用。</li>
<li>另外一种是从程序设计的角度看，HashMap<long, long="">结构中，只有key和value所存放的两个长整形数据是有效数据，共16B（2 * 8B），而实际耗费的内存位88B（长整形包装为Long对象需要多8B的MarkWord、8B的Klass指针，Map.Entry多了16B的对象头、8B的next字段和4B的int型hash字段、为对齐添加的4B空白填充，另外还有8B的Entry引用），内存空间效率（18%）太低。</long,></li>
</ul>
<h4 id="由Windows虚拟内存导致的长时间停顿"><a href="#由Windows虚拟内存导致的长时间停顿" class="headerlink" title="由Windows虚拟内存导致的长时间停顿"></a>由Windows虚拟内存导致的长时间停顿</h4><h5 id="问题描述-6"><a href="#问题描述-6" class="headerlink" title="问题描述"></a>问题描述</h5><ul>
<li>有一个带心跳检测功能的GUI桌面程序，每15秒发送一次心跳检查信号，如果对方30秒内都没有信信号返回，则认为和对方已断开连接；</li>
<li>程序上线后发现有误报，查询日志发现误报是因为程序会偶尔出现间隔约1分钟左右的时间完全无日志输出，处于停顿状态；</li>
<li>另外观察到GUI程序最小化时，资源管理中显示的占用内存大幅减小，但虚拟内存没变化；</li>
<li>因为是桌面程序，所需内存不大（-Xmx256m），加入参数-XX:+PrintGCApplicationStoppedTime -XX：PrintGCDateStamps -Xloggc:gclog.log后，从日志文件确认是GC导致的，大部分的GC时间在100ms以内，但偶尔会出现一次接近1min的GC；</li>
<li>加入参数-XX：PrintReferenceGC参数查看GC的具体日志信息，发现执行GC动作的时间并不长，但从准备开始GC到真正GC直接却消耗了大部分时间，如下所示：</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/example_virtual_memory.png" alt="虚拟内存案例日志"></p>
<h5 id="问题分析-6"><a href="#问题分析-6" class="headerlink" title="问题分析"></a>问题分析</h5><ul>
<li>初步怀疑是最小化时工作内存被自动交换到磁盘的页面文件中，这样发生GC时就有可能因为恢复页面文件的操作而导致不正常的GC停顿；</li>
<li>在MSDN查证确认了这种猜想，加入参数-Dsun.awt.keepWorkingSetOnMinimize=true来解决；这个参数在很多AWT程序如VisualVM都有应用。</li>
</ul>
<h3 id="实战：Eclipse运行速度调优"><a href="#实战：Eclipse运行速度调优" class="headerlink" title="实战：Eclipse运行速度调优"></a>实战：Eclipse运行速度调优</h3><ul>
<li>升级JDK；</li>
<li>设置-XX:MaxPermSize=256M解决Eclipse判断虚拟机版本的bug；</li>
<li>加入参数-Xverfify:none禁止字节码验证；</li>
<li>虚拟机运行在client模式，采用C1轻量级编译器；</li>
<li>把-Xms和-XX：PermSize参数设置为-Xmx和-XX:MaxPermSize一样，这样强制虚拟机启动时把老年代和永久代的容量固定下来，避免运行时自动扩展；</li>
<li>增加参数-XX：DisableExplicitGC屏蔽掉显式GC触发；</li>
<li>采用ParNew+CMS的垃圾收集器组合；</li>
<li>最终从Eclipse启动耗时15秒到7秒左右， eclipse.ini配置如下：</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/example_eclipse.png" alt="Eclipse调优"></p>
<h3 id="本章小结-1"><a href="#本章小结-1" class="headerlink" title="本章小结"></a>本章小结</h3><p>Java虚拟机的内存管理和垃圾收集是虚拟机结构体系最重要的组成部分，对程序的性能和稳定性有非常大的影响。通过案例和实战部分，加深了对前面理论知识和工具的理解。</p>
<p><strong>系列读书笔记</strong></p>
<ul>
<li><a href="http://ginobefunny.com/post/deep_in_jvm_notes_part1">《深入理解Java虚拟机》读书笔记1：Java技术体系、Java内存区域和内存溢出异常</a></li>
<li><a href="http://ginobefunny.com/post/deep_in_jvm_notes_part2">《深入理解Java虚拟机》读书笔记2：垃圾收集器与内存分配策略</a></li>
<li><a href="http://ginobefunny.com/post/deep_in_jvm_notes_part3">《深入理解Java虚拟机》读书笔记3：虚拟机性能监控与调优实战</a></li>
<li><a href="http://ginobefunny.com/post/deep_in_jvm_notes_part4">《深入理解Java虚拟机》读书笔记4：类文件结构</a></li>
<li><a href="http://ginobefunny.com/post/deep_in_jvm_notes_part5">《深入理解Java虚拟机》读书笔记5：类加载机制与字节码执行引擎</a></li>
<li><a href="http://ginobefunny.com/post/deep_in_jvm_notes_part6">《深入理解Java虚拟机》读书笔记6：程序编译与代码优化</a></li>
<li><a href="http://ginobefunny.com/post/deep_in_jvm_notes_part7">《深入理解Java虚拟机》读书笔记7：高效并发</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;国内JVM相关书籍NO.1，Java程序员必读。读书笔记第三部分对应原书的第四章和第五章，主要介绍虚拟机的性能监控、故障处理及调优实战。&lt;br&gt;
    
    </summary>
    
      <category term="JVM" scheme="http://ginobefunny.com/categories/JVM/"/>
    
    
      <category term="Java" scheme="http://ginobefunny.com/tags/Java/"/>
    
      <category term="读书笔记" scheme="http://ginobefunny.com/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
      <category term="性能调优" scheme="http://ginobefunny.com/tags/%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/"/>
    
      <category term="JVM" scheme="http://ginobefunny.com/tags/JVM/"/>
    
      <category term="虚拟机" scheme="http://ginobefunny.com/tags/%E8%99%9A%E6%8B%9F%E6%9C%BA/"/>
    
      <category term="性能监控" scheme="http://ginobefunny.com/tags/%E6%80%A7%E8%83%BD%E7%9B%91%E6%8E%A7/"/>
    
  </entry>
  
  <entry>
    <title>《深入理解Java虚拟机》读书笔记2：垃圾收集器与内存分配策略</title>
    <link href="http://ginobefunny.com/post/deep_in_jvm_notes_part2/"/>
    <id>http://ginobefunny.com/post/deep_in_jvm_notes_part2/</id>
    <published>2017-01-08T01:29:44.000Z</published>
    <updated>2017-01-31T09:49:23.821Z</updated>
    
    <content type="html"><![CDATA[<p>国内JVM相关书籍NO.1，Java程序员必读。读书笔记第二部分对应原书的第三章，主要介绍JVM的垃圾回收算法、实现。<br><a id="more"></a></p>
<h2 id="第三章-垃圾收集器与内存分配策略"><a href="#第三章-垃圾收集器与内存分配策略" class="headerlink" title="第三章 垃圾收集器与内存分配策略"></a>第三章 垃圾收集器与内存分配策略</h2><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>思考GC需要完成的3件事情：</p>
<ul>
<li>哪些内存需要回收？</li>
<li>什么时候回收？</li>
<li>如何回收？</li>
</ul>
<p>再回头看看第二章介绍的Java内存运行时区域的各个部分：</p>
<ul>
<li>程序计时器、虚拟机栈、本地方法栈：随线程而灭，栈帧随方法而进行出栈和入栈，每一个栈帧分配的内存在类结构确定就已知，因此这几个区域不需要考虑回收；</li>
<li>对于Java堆和方法区，只有程序运行期间才知道会创建哪些对象，内存的分配和回收都是动态的，垃圾收集器所关注的是这部分内存；</li>
</ul>
<h4 id="对象已死吗？"><a href="#对象已死吗？" class="headerlink" title="对象已死吗？"></a>对象已死吗？</h4><p>在垃圾收集器进行回收前，第一件事就是确定这些对象哪些还存活，哪些已经死去。</p>
<h4 id="引用计数算法"><a href="#引用计数算法" class="headerlink" title="引用计数算法"></a>引用计数算法</h4><p>给对象添加引用计数器，当有地方引用它时就加1，引用失效就减1，为0时就认为对象不再被使用可回收。该算法失效简单，判断高效，但并不被主流虚拟机采用，主要原因是它很难解决对象之间相互循环引用的问题。</p>
<h4 id="可达性分析算法"><a href="#可达性分析算法" class="headerlink" title="可达性分析算法"></a>可达性分析算法</h4><p>通过一系列的称为“GC Roots”的对象作为起点，从这些节点开始向下搜索，搜索所走过的路径称为引用链（Reference Chain），如果一个对象到GC Roots没有引用链相连，则该对象是不可用的。</p>
<p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/gc_roots_reference_chain.png" alt="可达性分析"></p>
<p>在Java语言中，可作为GC Roots的对象包括：</p>
<ul>
<li>虚拟机栈（栈帧中的本地变量表）中引用的对象；</li>
<li>方法区中类静态属性引用的对象；</li>
<li>方法区中常量引用的对象；</li>
<li>本地方法栈中JNI（即一般说的Native方法）引用的对象；</li>
</ul>
<h4 id="再谈引用"><a href="#再谈引用" class="headerlink" title="再谈引用"></a>再谈引用</h4><p>在JDK 1.2之后，Java对引用的概念进行了扩充，将引用分为强引用、软引用、弱引用和虚引用，这4种引用强度依次减弱。</p>
<h4 id="生存还是死亡"><a href="#生存还是死亡" class="headerlink" title="生存还是死亡"></a>生存还是死亡</h4><p>要真正宣告一个对象死亡，至少要经历两次标记过程：如果对象在进行可达性分析后发现没有与GC Roots相连接的引用链，那它将会被第一次标记并且进行一次筛选，筛选的条件是此对象是否有必要执行finalize方法（如没有重写finalize方法或者已经被调用过则认为没有必要执行）；如果有必要执行则将该对象放置在F-Queue队列中，并在稍后由一个由虚拟机自己建立的、低优先级的Finalizer线程去执行它；稍后GC将对F-Queue中的对象进行第二次标记，如果对象还是没有被引用，则会被回收。</p>
<p>但是作者不建议通过finalize方法“拯救”对象，因为它运行代价高、不确定性大、无法保证各个对象的调用顺序。</p>
<h4 id="回收方法区"><a href="#回收方法区" class="headerlink" title="回收方法区"></a>回收方法区</h4><p>永久代的垃圾收集主要回收两部分内容：废弃常量和无用的类。</p>
<p>一个无用的类需要满足以下三个条件：</p>
<ul>
<li>该类的所有实例都已经被回收；</li>
<li>加载该类的ClassLoader已经被回收；</li>
<li>该类对象的java.lang.Class对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法；</li>
</ul>
<p>在大量使用反射、动态代理、CGLib等ByteCode框架、动态生成JSP以及OSGI这类频繁自定义ClassLoader的场景都需要虚拟机具备类卸载的功能（HotSpot提供-Xnoclassgc参数控制），以保证永久代不会溢出。</p>
<h3 id="垃圾收集算法"><a href="#垃圾收集算法" class="headerlink" title="垃圾收集算法"></a>垃圾收集算法</h3><ul>
<li>标记-清除算法：首先标记出所有需要回收的对象，然后统一回收所有被标记的对象；缺点是效率不高且容易产生大量不连续的内存碎片；</li>
<li>复制算法：将可用内存分为大小相等的两块，每次只使用其中一块；当这一块用完了，就将还活着的对象复制到另一块上，然后把已使用过的内存清理掉。在HotSpot里，考虑到大部分对象存活时间很短将内存分为Eden和两块Survivor，默认比例为8:1:1。代价是存在部分内存空间浪费，适合在新生代使用；</li>
<li>标记-整理算法：首先标记出所有需要回收的对象，然后让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存。适用于老年代。</li>
<li>分代收集算法：一般把Java堆分新生代和老年代，在新生代用复制算法，在老年代用标记-清理或标记-整理算法，是现代虚拟机通常采用的算法。</li>
</ul>
<h3 id="HotSpot的算法实现"><a href="#HotSpot的算法实现" class="headerlink" title="HotSpot的算法实现"></a>HotSpot的算法实现</h3><h4 id="枚举根节点"><a href="#枚举根节点" class="headerlink" title="枚举根节点"></a>枚举根节点</h4><ul>
<li>由于要确保在一致性的快照中进行可达性分析，从而导致GC进行时必须要停顿所有Java执行线程；</li>
<li>在HotSpot里通过一组OopMap数据结构来知道哪些地方存放着对象引用；</li>
</ul>
<h4 id="安全点"><a href="#安全点" class="headerlink" title="安全点"></a>安全点</h4><ul>
<li>HotSpot只在特定的位置记录了OopMap，这些位置称为安全点（SafePoint）；</li>
<li>即程序执行时并非在所有地方都能停顿下来开始GC，只有到达安全点时才能暂停；</li>
<li>对于安全点基本上是以程序“是否具有让程序长时间执行的特征”（比如方法调用、循环跳转、异常跳转等）为标准进行选定的；</li>
<li>另外还需要考虑如果在GC时让所有线程都跑到最近的安全点上，有两种方案：抢先式中断和主动式中断（主流选择）；</li>
</ul>
<h4 id="安全区域"><a href="#安全区域" class="headerlink" title="安全区域"></a>安全区域</h4><ul>
<li>如果程序没有分配CPU时间（如线程处于Sleep或Blocked），此时就需要安全区域（Safe Region），其是指在一段代码片段之中，引用关系不会发生变化；</li>
<li>线程执行到安全区域时，首先标识自己已经进入了安全区域，这样JVM在GC时就不管这些线程了；</li>
</ul>
<h3 id="垃圾收集器"><a href="#垃圾收集器" class="headerlink" title="垃圾收集器"></a>垃圾收集器</h3><ul>
<li>垃圾收集算法是内存回收的方法论，垃圾收集器就是内存回收的具体实现。</li>
<li>这里讨论JDK 1.7 Update 14之后的HotSpot虚拟机（此时G1仍处于实验状态），包含的虚拟机如下图所示（存在连线的表示可以搭配使用）：</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/gc_for_hotspot.png" alt="HotSpot垃圾收集器"></p>
<h4 id="Serial收集器"><a href="#Serial收集器" class="headerlink" title="Serial收集器"></a>Serial收集器</h4><p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/gc_serial.png" alt="Serial收集器"></p>
<ul>
<li>最基本、发展历史最悠久，在JDK 1.3之前是新生代收集的唯一选择；</li>
<li>是一个单线程（并非指一个收集线程，而是会暂停索引工作线程）的收集器；</li>
<li>现在依然是虚拟机运行在Client模式下的默认新生代收集器，主要就是因为它简单而高效（没有线程交互的开销）；</li>
</ul>
<h4 id="ParNew收集器"><a href="#ParNew收集器" class="headerlink" title="ParNew收集器"></a>ParNew收集器</h4><p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/gc_parnew.png" alt="ParNew收集器"></p>
<ul>
<li>其实就是Serial收集器的多线程版本；</li>
<li>ParNew收集器在单CPU环境中绝对不会有比Serial收集器更好的效果；</li>
<li>是许多运行在Server模式下虚拟机首选的新生代收集器，重要原因就是除了Serial收集器外，只有它能与CMS收集器配合工作；</li>
<li>并行（Parallel）：指多条垃圾收集线程并行工作，但此时用户线程仍处于等待状态；</li>
<li>并发（Concurrent）：指用户线程与垃圾收集线程同时执行，用户线程在继续执行而垃圾收集程序运行在另外一个CPU上；</li>
</ul>
<h4 id="Parallel-Scavenge收集器"><a href="#Parallel-Scavenge收集器" class="headerlink" title="Parallel Scavenge收集器"></a>Parallel Scavenge收集器</h4><ul>
<li>新生代收集器，使用复制算法，并行的多线程收集器；</li>
<li>与其他收集器关注于尽可能缩短垃圾收集时用户线程停顿时间不同，它的目标是达到一个可控制的吞吐量；</li>
<li>高吞吐量可以高效率利用CPU时间，适合在后台运算而不需要太多交互的任务；</li>
<li>-XX:MaxGCPauseMillis参数可以设置最大停顿时间，而停顿时间缩短是以牺牲吞吐量和新生代空间来换取的；</li>
<li>另外它还支持GC自适应的调节策略；</li>
</ul>
<h4 id="Serial-Old收集器"><a href="#Serial-Old收集器" class="headerlink" title="Serial Old收集器"></a>Serial Old收集器</h4><p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/gc_serial_old.png" alt="Serial Old收集器"></p>
<ul>
<li>是Serial收集器的老年代版本，同样是单线程，使用标记-整理算法；</li>
<li>主要是给Client模式下的虚拟机使用的；</li>
<li>在Server模式下主要是给JDK 1.5及之前配合Parallel Scavenge使用或作为CMS收集器的后备预案；</li>
</ul>
<h4 id="Parallel-Old收集器"><a href="#Parallel-Old收集器" class="headerlink" title="Parallel Old收集器"></a>Parallel Old收集器</h4><p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/gc_parallel_old.png" alt="Parallel Old收集器"></p>
<ul>
<li>是Parallel Scavenge的老年代版本，使用多线程和标记-整理算法；</li>
<li>是JDK 1.6中才开始提供的；</li>
</ul>
<h4 id="CMS收集器"><a href="#CMS收集器" class="headerlink" title="CMS收集器"></a>CMS收集器</h4><p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/gc_cms.png" alt="CMS收集器"></p>
<ul>
<li>是一种以获取最短回收停顿时间为目标的收集器，特别适合互联网站或者B/S的服务端；</li>
<li>主要包括4个步骤：初始标记、并发标记、重新标记和并发清除；</li>
<li>还有3个明显的缺点：CMS收集器对CPU非常敏感、无法处理浮动垃圾、大量内存碎片产生；</li>
</ul>
<h4 id="G1收集器"><a href="#G1收集器" class="headerlink" title="G1收集器"></a>G1收集器</h4><p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/gc_g1.png" alt="G1收集器"></p>
<ul>
<li>一款面向服务端应用的垃圾收集器，后续会替换掉CMS垃圾收集器；</li>
<li>特点：并行与并发（充分利用多核多CPU缩短Stop-The-World时间）、分代收集（独立管理整个Java堆，但针对不同年龄的对象采取不同的策略）、空间整合（基于标记-整理）、可预测的停顿（将堆分为大小相等的独立区域，避免全区域的垃圾收集）；</li>
<li>关于Region：新生代和老年代不再物理隔离，只是部分Region的集合；G1跟踪各个Region垃圾堆积的价值大小，在后台维护一个优先列表，根据允许的收集时间优先回收价值最大的Region；Region之间的对象引用以及其他收集器中的新生代与老年代之间的对象引用，采用Remembered Set来避免全堆扫描；</li>
<li>分为几个步骤：初始标记（标记一下GC Roots能直接关联的对象并修改TAMS值，需要STW但耗时很短）、并发标记（从GC Root从堆中对象进行可达性分析找存活的对象，耗时较长但可以与用户线程并发执行）、最终标记（为了修正并发标记期间产生变动的那一部分标记记录，这一期间的变化记录在Remembered Set Log里，然后合并到Remembered Set里，该阶段需要STW但是可并行执行）、筛选回收（对各个Region回收价值排序，根据用户期望的GC停顿时间制定回收计划来回收）；</li>
</ul>
<h4 id="理解GC日志"><a href="#理解GC日志" class="headerlink" title="理解GC日志"></a>理解GC日志</h4><p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/gc_log.png" alt="GC日志"></p>
<ul>
<li>最前面的数字代表GC发生的时间（虚拟机启动以后的秒杀）；</li>
<li>“[GC”和“[Full GC”说明停顿类型，有Full代表的是Stop-The-World的；</li>
<li>“[DefNew”、“[Tenured”和“[Perm”表示GC发生的区域；</li>
<li>方括号内部的“3324K -&gt; 152K(3712K)” 含义是 “GC前该内存已使用容量 -&gt; GC后该内存区域已使用容量(该区域总容量)”;</li>
<li>方括号之外的“3324K -&gt; 152K(11904)” 含义是 “GC前Java堆已使用容量 -&gt; GC后Java堆已使用容量(Java堆总容量)”;</li>
<li>再往后“0.0025925 secs”表示该内存区域GC所占用的时间；</li>
</ul>
<h4 id="垃圾收集器参数总结"><a href="#垃圾收集器参数总结" class="headerlink" title="垃圾收集器参数总结"></a>垃圾收集器参数总结</h4><p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/gc_param1.png" alt="垃圾收集器参数1"><br><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/gc_param2.png" alt="垃圾收集器参数2"></p>
<h3 id="内存分配与回收策略"><a href="#内存分配与回收策略" class="headerlink" title="内存分配与回收策略"></a>内存分配与回收策略</h3><ul>
<li>对象优先在新生代分配</li>
<li>大对象直接进入老年代</li>
<li>长期存活的对象将进入老年代</li>
<li>动态对象年龄判断：如果在Survivor空间中相同年龄所有对象大小总和大于Survivor空间的一半，大于或等于该年龄的对象直接进入老年代；</li>
<li>空间分配担保：发生Minor GC前，虚拟机会先检查老年代最大可用连续空间是否大于新生代所有对象总空间，如果不成立，虚拟机会查看HandlePromotionFailure设置值是否允许担保失败，如果允许继续检查老年代最大可用的连续空间是否大于历次晋升到老年代的平均大小，如果大于会尝试进行一次Minor GC；如果小于或者不允许冒险，会进行一次Full GC；</li>
</ul>
<h3 id="本章小结"><a href="#本章小结" class="headerlink" title="本章小结"></a>本章小结</h3><p>本章介绍了垃圾回收算法、几款JDK 1.7中提供的垃圾收集器特点以及运作原理。内存回收与垃圾收集器在很多时候都是影响系统性能、并发能力的主要因素之一，然而没有固定收集器和参数组合，也没有最优的调优方法，需要根据实践了解各自的行为、优势和劣势。</p>
<p><strong>系列读书笔记</strong></p>
<ul>
<li><a href="http://ginobefunny.com/post/deep_in_jvm_notes_part1">《深入理解Java虚拟机》读书笔记1：Java技术体系、Java内存区域和内存溢出异常</a></li>
<li><a href="http://ginobefunny.com/post/deep_in_jvm_notes_part2">《深入理解Java虚拟机》读书笔记2：垃圾收集器与内存分配策略</a></li>
<li><a href="http://ginobefunny.com/post/deep_in_jvm_notes_part3">《深入理解Java虚拟机》读书笔记3：虚拟机性能监控与调优实战</a></li>
<li><a href="http://ginobefunny.com/post/deep_in_jvm_notes_part4">《深入理解Java虚拟机》读书笔记4：类文件结构</a></li>
<li><a href="http://ginobefunny.com/post/deep_in_jvm_notes_part5">《深入理解Java虚拟机》读书笔记5：类加载机制与字节码执行引擎</a></li>
<li><a href="http://ginobefunny.com/post/deep_in_jvm_notes_part6">《深入理解Java虚拟机》读书笔记6：程序编译与代码优化</a></li>
<li><a href="http://ginobefunny.com/post/deep_in_jvm_notes_part7">《深入理解Java虚拟机》读书笔记7：高效并发</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;国内JVM相关书籍NO.1，Java程序员必读。读书笔记第二部分对应原书的第三章，主要介绍JVM的垃圾回收算法、实现。&lt;br&gt;
    
    </summary>
    
      <category term="JVM" scheme="http://ginobefunny.com/categories/JVM/"/>
    
    
      <category term="Java" scheme="http://ginobefunny.com/tags/Java/"/>
    
      <category term="读书笔记" scheme="http://ginobefunny.com/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
      <category term="JVM" scheme="http://ginobefunny.com/tags/JVM/"/>
    
      <category term="垃圾收集" scheme="http://ginobefunny.com/tags/%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86/"/>
    
      <category term="内存分配" scheme="http://ginobefunny.com/tags/%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D/"/>
    
  </entry>
  
  <entry>
    <title>《深入理解Java虚拟机》读书笔记1：Java技术体系、Java内存区域和内存溢出异常</title>
    <link href="http://ginobefunny.com/post/deep_in_jvm_notes_part1/"/>
    <id>http://ginobefunny.com/post/deep_in_jvm_notes_part1/</id>
    <published>2017-01-04T01:11:00.000Z</published>
    <updated>2017-01-31T09:49:12.825Z</updated>
    
    <content type="html"><![CDATA[<p>国内JVM相关书籍NO.1，Java程序员必读。读书笔记第一部分对应原书的前两章，主要介绍了Java的技术体系、Java虚拟机的发展历史、Java运行时区域的划分、对象的创建和访问以及内存溢出的实战。<br><a id="more"></a></p>
<h1 id="Part-1-走进Java"><a href="#Part-1-走进Java" class="headerlink" title="Part 1: 走进Java"></a>Part 1: 走进Java</h1><h2 id="第一章-走进Java"><a href="#第一章-走进Java" class="headerlink" title="第一章 走进Java"></a>第一章 走进Java</h2><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>Java的优点</p>
<ul>
<li>结构严谨、面向对象</li>
<li>摆脱平台的束缚，一次编写到处运行</li>
<li>提供了相对安全的内存管理和访问机制</li>
<li>实现了热点代码检测和运行时编译及优化</li>
<li>一套完善的应用程序接口以及无数的第三方类库</li>
</ul>
<h3 id="Java技术体系"><a href="#Java技术体系" class="headerlink" title="Java技术体系"></a>Java技术体系</h3><p>Sun官方所定义的Java技术体系包括：</p>
<ul>
<li>Java程序设计语言</li>
<li>各种硬件平台上的Java虚拟机</li>
<li>Class文件格式</li>
<li>Java API类库</li>
<li>来自商业机构和开源社区的第三方Java类库</li>
</ul>
<p>JDK是用于支持Java开发的最小环境，JRE是支持Java程序运行的标准环境，整个Java体系如下所示：</p>
<p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/java_tech_structure.png" alt="Java技术体系"></p>
<h3 id="Java发展史"><a href="#Java发展史" class="headerlink" title="Java发展史"></a>Java发展史</h3><p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/java_timeline.png" alt="Java技术发展"></p>
<ul>
<li>JDK 1.0: Java虚拟机、Applet、AWT等；</li>
<li>JDK 1.1：JAR文件格式、JDBC、JavaBeans、RMI、内部类、反射；</li>
<li>JDK 1.2：拆分为J2SE/J2EE/J2ME、内置JIT编译器、一系列Collections集合类；</li>
<li>JDK 1.3：JNDI服务、使用CORBA IIOP实现RMI通信协议、Java 2D改进；</li>
<li>JDK 1.4：正则表达式、异常链、NIO、日志类、XML解析器和XSLT转换器；</li>
<li>JDK 1.5：自动装箱、泛型、动态注解、枚举、可变参数、遍历循环、改进了Java内存模型、提供了java.util.concurrent并发包；</li>
<li>JDK 1.6：提供动态语言支持、提供编译API和微型HTTP服务器API、虚拟机优化（锁与同步、垃圾收集、类加载等）；</li>
<li>JDK 1.7：G1收集器、加强对Java语言的调用支持、升级类加载架构；</li>
<li>JDK 1.8：Lambda表达式等；</li>
</ul>
<h3 id="Java虚拟机发展史"><a href="#Java虚拟机发展史" class="headerlink" title="Java虚拟机发展史"></a>Java虚拟机发展史</h3><ul>
<li>Sun Classic/Exact VM：Classic VM是第一款商用虚拟机，纯解析器方式来执行Java代码，如果要使用JIT编译器就必须进行外挂，解析器和编译器不能配合工作，编译器执行效率非常差；Exact VM是Sun虚拟机团队曾在Solaris平台发布的虚拟机，支持两级即时编译器、编译器和解释器混合工作、使用准确内存管理（虚拟机可以知道内存中某个位置的数据具体是什么类型），但很快就被HotSpot VM所取代；</li>
<li>Sun HotSpot VM：Sun JDK和OpenJDK所带的虚拟机，目前使用范围最广；继承了前两款虚拟机的优点，还支持热点代码探测技术（通过计数器找出最具编译价值的代码）；2006年Sun公司宣布JDK包括HotSpot VM开源，在此基础上建立OpenJDK；</li>
<li>Sun Mobile-Embedded VM/Meta-Circular VM：还有一些Sun开发的面对移动和嵌入式发布的和实验性质的虚拟机；</li>
<li>BEA JRockit/IBM J9 VM：JRockit VM号称是世界上最快的Java虚拟机，专注于服务器端应用，不包含解析器实现，全部靠即时编译器编译执行；J9 VM定位于HotSpot比较接近，主要目的是作为IBM公司各种Java产品的执行平台；</li>
<li>Azul VM/BEA Liquid VM：特定硬件平台专有的高性能虚拟机；</li>
<li>Apache Harmony/Google Android Dalvik VM：Apache Harmony包含自己的虚拟机和Java库，但没有通过TCK认证；Dalvik VM是Android平台的核心组成部分，其并没有遵循Java虚拟机规范，不能直接执行Class文件，使用的是寄存器架构而不是JVM常见的栈架构；</li>
<li>Microsoft JVM及其他：微软曾经是Java技术的铁杆支持者，开发过Windows下性能最好的Java虚拟机，但后来被Sun起诉终止其发展；</li>
</ul>
<h3 id="展望Java技术的未来"><a href="#展望Java技术的未来" class="headerlink" title="展望Java技术的未来"></a>展望Java技术的未来</h3><ul>
<li>模块化</li>
<li>混合语言</li>
<li>多核并行</li>
<li>进一步丰富语法</li>
<li>64位虚拟机</li>
</ul>
<h3 id="实战：自己编译JDK"><a href="#实战：自己编译JDK" class="headerlink" title="实战：自己编译JDK"></a>实战：自己编译JDK</h3><ul>
<li>下载OpenJDK：<a href="https://jdk7.java.net/source.html" target="_blank" rel="external">https://jdk7.java.net/source.html</a></li>
<li>系统需求：Ubuntu 64位、5GB的磁盘、1G内存；</li>
<li>构建编译环境：需要Bootstrap JDK(JDK6以上)/Ant(1.7.1以上)/GCC。</li>
</ul>
<pre><code>sudo apt-get install build-essential gawk m4 openjdk-6-jdk libasound2-dev libcups2-dev libxrender-dev xorg-dev xutils-dev x11proto-print-dev binutils libmotif3 libmotif-dev ant
</code></pre><ul>
<li>进行编译：设置环境变量、make sanity检查、make编译、复制到JAVA_HOME、编辑env.sh</li>
</ul>
<h3 id="在IDE工具中进行源码调试"><a href="#在IDE工具中进行源码调试" class="headerlink" title="在IDE工具中进行源码调试"></a>在IDE工具中进行源码调试</h3><p>NetBeans（支持C/C++开发的版本）</p>
<h3 id="本章小结"><a href="#本章小结" class="headerlink" title="本章小结"></a>本章小结</h3><p>本章介绍了Java技术体系的过去、现在以及未来的一些发展趋势，并独立编译一个OpenJDK 7的版本。</p>
<h1 id="Part-2-自动内存管理机制"><a href="#Part-2-自动内存管理机制" class="headerlink" title="Part 2 自动内存管理机制"></a>Part 2 自动内存管理机制</h1><h2 id="第二章-Java内存区域与内存溢出异常"><a href="#第二章-Java内存区域与内存溢出异常" class="headerlink" title="第二章 Java内存区域与内存溢出异常"></a>第二章 Java内存区域与内存溢出异常</h2><h3 id="概述-1"><a href="#概述-1" class="headerlink" title="概述"></a>概述</h3><p>对于Java程序员来说，在虚拟机自动内存管理机制下，不需要为new操作去写配对的delete/free代码，不容易出现内存泄漏。但是如果出现内存泄漏问题，如果不了解虚拟机的机制，便难以定位。</p>
<h3 id="运行时数据区域"><a href="#运行时数据区域" class="headerlink" title="运行时数据区域"></a>运行时数据区域</h3><p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/jvm_runtime_data_area.png" alt="运行时数据区域"></p>
<h4 id="程序计数器"><a href="#程序计数器" class="headerlink" title="程序计数器"></a>程序计数器</h4><ul>
<li>一块较小的内存，可以看作是当前线程所执行的字节码的行号指示器；</li>
<li>在虚拟机概念模型（各种虚拟机实现可能不一样）中，字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令；</li>
<li>程序计数器是属于线程私有的内存；</li>
<li>如果执行的是Java方法，该计数器记录的是正在执行的虚拟机字节码指令的地址；如果是Native方法则为空；</li>
</ul>
<h4 id="Java虚拟机栈"><a href="#Java虚拟机栈" class="headerlink" title="Java虚拟机栈"></a>Java虚拟机栈</h4><ul>
<li>Java虚拟机栈也是线程私有的；</li>
<li>描述的是Java方法执行的内存模型：每个方法在执行的同时都会创建一个栈帧（Stack Frame）用于存储局部变量表、操作数栈、动态链接、方法出口等信息。每一个方法从调用直至执行完成的过程，就对应着一个栈帧在虚拟机中入栈到出栈的过程；</li>
<li>局部变量表存放了编译器可知的各种基本数据类型、对象引用和returnAddress类型；其所需的内存空间在编辑期完成分配，不会再运行期改变；</li>
<li>可能存在两种异常：StackOverflowError和OutOfMemoryError；</li>
</ul>
<h4 id="本地方法栈"><a href="#本地方法栈" class="headerlink" title="本地方法栈"></a>本地方法栈</h4><ul>
<li>与虚拟机栈非常相似，只不过是为虚拟机使用到的Native方法服务；</li>
<li>可能存在两种异常：StackOverflowError和OutOfMemoryError；</li>
</ul>
<h4 id="Java堆"><a href="#Java堆" class="headerlink" title="Java堆"></a>Java堆</h4><ul>
<li>Java堆是被所有线程共享的，在虚拟机启动时创建；</li>
<li>此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例都在这分配；</li>
<li>是垃圾收集器管理的主要区域，可以分为新生代和老年代；</li>
<li>可以物理不连续，只要逻辑上是连续的即可；</li>
<li>如果堆中没有内存完成实例分配也无法再扩展时，会抛出OutOfMemoryError异常；</li>
</ul>
<h4 id="方法区"><a href="#方法区" class="headerlink" title="方法区"></a>方法区</h4><ul>
<li>是线程共享的区域；</li>
<li>用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据；</li>
<li>该区域对于垃圾收集来说条件比较苛刻，但是还是非常有必要要进行回收处理；</li>
<li>当无法满足内存分配需求时，将抛出OutOfMemoryError异常；</li>
</ul>
<h4 id="运行时常量池"><a href="#运行时常量池" class="headerlink" title="运行时常量池"></a>运行时常量池</h4><ul>
<li>是方法区的一部分；</li>
<li>Class文件中除了有类的版本、字段、方法、接口等描述信息外，还有一项信息是常量池，用于存放编译器生成的各种字面量和符号引用，这部分内容将在类加载后进入方法区的运行时常量池中存放；</li>
<li>Java虚拟机规范要求较少，通常还会把翻译出来的直接引用也存储在此；</li>
<li>另外一个重要特征是具备动态性，可以在运行期间将新的常量放入池中，如String的intern方法；</li>
<li>可能存在的异常：OutOfMemoryError；</li>
</ul>
<h4 id="直接内存"><a href="#直接内存" class="headerlink" title="直接内存"></a>直接内存</h4><ul>
<li>并不是虚拟机运行时数据区的一部分，也不是Java虚拟机规范中定义的内存区域；</li>
<li>JDK 1.4的NIO引入了基于通道（Channel）和缓冲区（Buffer）的IO方法，可以使用Native函数库直接分配对外内存，然后通过一个存储在Java堆中的DirectByteBuffer对象作为这块内存的引用进行操作以提升性能；</li>
</ul>
<h3 id="HotSpot虚拟机对象探秘"><a href="#HotSpot虚拟机对象探秘" class="headerlink" title="HotSpot虚拟机对象探秘"></a>HotSpot虚拟机对象探秘</h3><p>进一步了解虚拟机内存中数据的其他细节，比如它们是如何创建、如何布局以及如何访问的。下面以虚拟机HotSpot和常用的内存区域Java堆为例，深入探讨HotSpot虚拟机在Java堆中对象分配、布局和访问的全过程。</p>
<h4 id="对象的创建"><a href="#对象的创建" class="headerlink" title="对象的创建"></a>对象的创建</h4><ul>
<li>虚拟机遇到一条new指令时，先检查指令的参数是否能在常量池中定位到一个类的符号，并且检查这个符号引用代码的类是否已被加载、解析和初始化过。如果没有，那必须先执行相应的类加载过程；</li>
<li>接下来虚拟机将为新生对象分配内存。对象所需的内存大小在类加载完成后便完全确定，为对象分配空间等同于把一块确定大小的内存从Java堆中划分出来。在使用Serial、ParNew等带Compact过程的收集器时，系统采用的分配算法是指针碰撞（内存绝对规整，只要通过指针作为分界点标识）；而使用CMS这种基于Mark-Sweep算法收集器时，通常使用空闲列表（内存不规整，通过维护一个列表记录那块内存是可用的）；</li>
<li>另外一个需要考虑的并发下的线程安全问题，有两种方案：一是分配内存空间的动作进行同步处理（实际上虚拟机采用CAS配上失败重试的方式保证更新操作的原子性）；二是为每个线程分配一小块内存（称为本地线程分配缓冲，TLAB），各个线程独立分配，只有TLAB用完需要分配新的才需要同步锁定，虚拟机通过-XX:+/-UseTLAB参数来设定；</li>
<li>内存分配完后，虚拟机将分配到的内存空间都初始化为零值（不包括对象头），这保证了对象的实例字段在Java代码中可以不赋值就直接使用，程序能访问到这些字段数据类型对应的零值；</li>
<li>接下来设置对象的对象头（Object Header）信息，包括对象是哪个类的实例、如何才能找到类的元数据信息、对象的哈希码、对象GC分代年龄等；</li>
<li>接着执行<init>方法，把对象按照程序员的意愿进行初始化，这样一个真正可用的对象才算完全产生出来；</init></li>
<li>HotSpot解释器的代码片段：略</li>
</ul>
<h4 id="对象的内存布局"><a href="#对象的内存布局" class="headerlink" title="对象的内存布局"></a>对象的内存布局</h4><ul>
<li>对象在内存中存储的布局可以分为3块区域：对象头（Object Header）、实例数据（Instance Data）和对齐填充（Padding）；</li>
<li>对象头包括两部分信息：第一部分用于存储对象自身的运行时数据，如哈希码、GC分代年龄、锁状态标志、线程持有的锁、偏向线程ID、偏向时间戳等；另一部分是类型指针，即对象指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是哪个类的实例（并不是所有虚拟机都必须在对象数据上保留类型指针）。另外如果对象是一个Java数组，对象头中还必须有一块用于记录数组长度的数据。</li>
<li>实例数据部分是真正存储的有效信息，也是在代码中所定义的各种类型字段内容。无论是父类继承的还是子类中定义的都需要记录下来。这部分存储的顺序会受到虚拟机分配策略参数和字段在Java源码中定义顺序的影响。</li>
<li>对齐填充不是必然存在的，主要是由于HotSpot VM的自动内存管理系统要求对象起始地址必须是8字节的整数倍。</li>
</ul>
<h4 id="对象的访问定位"><a href="#对象的访问定位" class="headerlink" title="对象的访问定位"></a>对象的访问定位</h4><ul>
<li>栈上的reference类型在虚拟机规范中只规定了一个指向对象的引用，并没有定义这个引用应该通过何种方式去定位、访问堆栈对象的具体位置，目前主流的方式方式有句柄和直接直接两种。</li>
<li>通过句柄：Java堆中划出一块内存作为句柄池，reference中存储的就是对象的句柄地址，而句柄中包含了对象实例数据与类型数据各自的具体地址信息。其最大好处就是reference存储的是稳定的句柄地址，在对象被移到（垃圾收集时移到）只改变实例数据指针，而reference不需要修改；</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/object_access_reference_1.png" alt="通过句柄访问对象"></p>
<ul>
<li>通过直接指针：Java堆对象的布局中必须考虑如果放置访问类型数据的相关信息，而reference中存在的直接就是对象地址。其最大好处在于速度更快，节省了一次指针定位的时机开销。HotSpot采用该方式进行对象访问，但其他语言和框架采用句柄的也非常常见。</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/object_access_reference_2.png" alt="通过直接指针访问对象"></p>
<h3 id="实战：OutOfMemoryError异常"><a href="#实战：OutOfMemoryError异常" class="headerlink" title="实战：OutOfMemoryError异常"></a>实战：OutOfMemoryError异常</h3><ul>
<li>通过代码验证Java虚拟机规范中描述各个运行时区域存储的内容；</li>
<li>在实际遇到内存溢出异常时，能根据异常的信息快速判断是哪个区域内存溢出；</li>
</ul>
<h4 id="Java堆溢出"><a href="#Java堆溢出" class="headerlink" title="Java堆溢出"></a>Java堆溢出</h4><p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/outofmemory_jvm_heap.png" alt="Java堆溢出"></p>
<p>解决思路：先通过内存映像分析工具对dump出来的堆转储快照进行分析，先分清楚是内存泄漏还是内存溢出；如果是内存泄漏，进一步查看泄漏对象到GC Roots的引用链，从而确认为什么无法回收；如果是内存溢出，则应当检查虚拟机堆参数（-Xmx与-Xmx）或检查是否存在对象生命周期过长、持有状态时间过长的情况；</p>
<h4 id="虚拟机栈和本地方法栈溢出"><a href="#虚拟机栈和本地方法栈溢出" class="headerlink" title="虚拟机栈和本地方法栈溢出"></a>虚拟机栈和本地方法栈溢出</h4><ul>
<li>HotSpot不区分虚拟机栈和本地方法栈；</li>
<li>StackOverflowError和OutOfMemoryError存在互相重叠的地方；</li>
<li>栈容量由-Xss参数设定；</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/stackoverflow_jvm_stack.png" alt="虚拟机栈溢出"></p>
<p>虚拟机的默认参数对于通常的方法调用（1000~2000层）完全够用，通常根据异常的堆栈日志就可以很容易定位问题。</p>
<h4 id="方法区和运行时常量池溢出"><a href="#方法区和运行时常量池溢出" class="headerlink" title="方法区和运行时常量池溢出"></a>方法区和运行时常量池溢出</h4><p>对于这个区域的测试，基本思路是运行时产生大量的类去填满方法区（比如使用反射和动态代理），这里我们借助CGLib直接操作字节码运行时产生大量的动态类（很对主流框架如Spring、Hibernate都会采用类似的字节码技术）。在这里需要特别注意垃圾回收的状况。</p>
<p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/outofmemory_java_methodarea_part1.png" alt="借助CGLib使方法区出现内存溢出异常1"><br><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/outofmemory_java_methodarea_part2.png" alt="借助CGLib使方法区出现内存溢出异常2"></p>
<h4 id="本机直接内存溢出"><a href="#本机直接内存溢出" class="headerlink" title="本机直接内存溢出"></a>本机直接内存溢出</h4><p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/outofmemory_jvm_direct.png" alt="本机直接内存溢出1"><br><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/outofmemory_jvm_direct_part2.png" alt="本机直接内存溢出2"></p>
<p>DirectMemory导致的内存溢出，在Heap Dump里不会看见明显的异常。如果发现OouOfMemory之后Dump文件很小，程序又使用了NIO，那就可以检查下是否这方面的原因。</p>
<h3 id="本章小结-1"><a href="#本章小结-1" class="headerlink" title="本章小结"></a>本章小结</h3><p>学习了虚拟机的内存是如何划分的，对象是如何创建、布局和访问的，哪部分区域、什么样的代码和操作可能导致内存的溢出异常。</p>
<p><strong>系列读书笔记</strong></p>
<ul>
<li><a href="http://ginobefunny.com/post/deep_in_jvm_notes_part1">《深入理解Java虚拟机》读书笔记1：Java技术体系、Java内存区域和内存溢出异常</a></li>
<li><a href="http://ginobefunny.com/post/deep_in_jvm_notes_part2">《深入理解Java虚拟机》读书笔记2：垃圾收集器与内存分配策略</a></li>
<li><a href="http://ginobefunny.com/post/deep_in_jvm_notes_part3">《深入理解Java虚拟机》读书笔记3：虚拟机性能监控与调优实战</a></li>
<li><a href="http://ginobefunny.com/post/deep_in_jvm_notes_part4">《深入理解Java虚拟机》读书笔记4：类文件结构</a></li>
<li><a href="http://ginobefunny.com/post/deep_in_jvm_notes_part5">《深入理解Java虚拟机》读书笔记5：类加载机制与字节码执行引擎</a></li>
<li><a href="http://ginobefunny.com/post/deep_in_jvm_notes_part6">《深入理解Java虚拟机》读书笔记6：程序编译与代码优化</a></li>
<li><a href="http://ginobefunny.com/post/deep_in_jvm_notes_part7">《深入理解Java虚拟机》读书笔记7：高效并发</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;国内JVM相关书籍NO.1，Java程序员必读。读书笔记第一部分对应原书的前两章，主要介绍了Java的技术体系、Java虚拟机的发展历史、Java运行时区域的划分、对象的创建和访问以及内存溢出的实战。&lt;br&gt;
    
    </summary>
    
      <category term="JVM" scheme="http://ginobefunny.com/categories/JVM/"/>
    
    
      <category term="Java" scheme="http://ginobefunny.com/tags/Java/"/>
    
      <category term="读书笔记" scheme="http://ginobefunny.com/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
      <category term="JVM" scheme="http://ginobefunny.com/tags/JVM/"/>
    
      <category term="垃圾收集" scheme="http://ginobefunny.com/tags/%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86/"/>
    
      <category term="内存溢出" scheme="http://ginobefunny.com/tags/%E5%86%85%E5%AD%98%E6%BA%A2%E5%87%BA/"/>
    
  </entry>
  
  <entry>
    <title>阅读随手记 201701</title>
    <link href="http://ginobefunny.com/post/reading_record_201701/"/>
    <id>http://ginobefunny.com/post/reading_record_201701/</id>
    <published>2017-01-03T08:23:21.000Z</published>
    <updated>2017-01-30T14:01:35.231Z</updated>
    
    <content type="html"><![CDATA[<p>关键字：架构, 微服务，消息中间件，性能调优，Elasticsearch，缓存，RPC，日志分析，平台化，DistributedLog，监控，负载均衡，高性能，高并发，高可用。<br><a id="more"></a></p>
<h2 id="千万级规模高性能、高并发的网络架构经验分享"><a href="#千万级规模高性能、高并发的网络架构经验分享" class="headerlink" title="千万级规模高性能、高并发的网络架构经验分享"></a>千万级规模高性能、高并发的网络架构经验分享</h2><p><a href="https://mp.weixin.qq.com/s?__biz=MzA3MzYwNjQ3NA==&amp;mid=401628413&amp;idx=1&amp;sn=91abfbad4c7dc882e94939042a8785a4" target="_blank" rel="external">https://mp.weixin.qq.com/s?__biz=MzA3MzYwNjQ3NA==&amp;mid=401628413&amp;idx=1&amp;sn=91abfbad4c7dc882e94939042a8785a4</a></p>
<h3 id="架构的本质"><a href="#架构的本质" class="headerlink" title="架构的本质"></a>架构的本质</h3><p><img src="http://oi46mo3on.bkt.clouddn.com/9_reading_201701/essence_of_architecture.png" alt="架构的本质"></p>
<h3 id="新浪微博整体架构"><a href="#新浪微博整体架构" class="headerlink" title="新浪微博整体架构"></a>新浪微博整体架构</h3><p><img src="http://oi46mo3on.bkt.clouddn.com/9_reading_201701/weibo_architecture.png" alt="新浪微博整体架构"></p>
<h3 id="微博架构的演变"><a href="#微博架构的演变" class="headerlink" title="微博架构的演变"></a>微博架构的演变</h3><p>不可能在第一代基础上通过简单的修修补补满足用户量快速增长的，同时线上业务又不能停， 这是我们常说的在飞机上换引擎的问题。建议在做服务化的时候，首先更多是偏向业务的梳理，同时要找准一个很好的切入点，既有架构和服务化上的提升，业务方也要有收益，比如提升性能或者降低维护成本同时升级过程要平滑，建议开始从原子化服务切入，比如基础的用户服务、基础的短消息服务、基础的推送服务。<br><img src="http://oi46mo3on.bkt.clouddn.com/9_reading_201701/weibo_architecture_improve.png" alt="新浪微博架构演变"></p>
<h3 id="微博的技术挑战"><a href="#微博的技术挑战" class="headerlink" title="微博的技术挑战"></a>微博的技术挑战</h3><p><img src="http://oi46mo3on.bkt.clouddn.com/9_reading_201701/weibo_architecture_challenge.png" alt="微博的技术挑战"></p>
<h3 id="正交分解法解析架构"><a href="#正交分解法解析架构" class="headerlink" title="正交分解法解析架构"></a>正交分解法解析架构</h3><p>一个维度是水平的分层拆分，第二从垂直的维度会做拆分。水平的维度从接口层、服务层到数据存储层。垂直怎么拆分，会用业务架构、技术架构、监控平台、服务治理等等来处理。</p>
<p>从业务架构来看，接口层有feed、用户关系、通讯接口；服务层，SOA里有基层服务、原子服务和组合服务，在微博我们只有原子服务和组合服务。原子服务不依赖于任何其他服务，组合服务由几个原子服务和自己的业务逻辑构建而成，资源层负责海量数据 的存储。</p>
<p>技术框架解决独立于业务的海量高并发场景下的技术难题，由众多的技术组件共同构建而成。在接口层，微博使用JERSY框架，帮助你做参数的解析、参数的验证、序列化和反序列化；资源层，主要是缓存、DB相关的各类组件，比如Cache组件和对象库组件。</p>
<p>监控平台和服务治理，完成系统服务的像素级监控，对分布式系统做提前诊断、预警以及治理。包含了SLA 规则的制定、服务监控、服务调用链监控、流量监控、错误异常监控、线上灰度发布上线系统、线上扩容缩容调度系统等。</p>
<p><img src="http://oi46mo3on.bkt.clouddn.com/9_reading_201701/weibo_architecture_explain.png" alt="正交分解法解析架构"></p>
<h3 id="常见的设计原则"><a href="#常见的设计原则" class="headerlink" title="常见的设计原则"></a>常见的设计原则</h3><ul>
<li>系统架构三个利器：RPC服务组件、消息中间件（交互异步化、流量削峰）、配置管理（灰度发布、降级）；</li>
<li>无状态：接口层最重要的就是无状态，将有状态的数据剥离到数据库或缓存中；</li>
<li>数据层比服务层更需要设计：存储、压缩、索引等；</li>
<li>物理结构与逻辑结构的映射：几个垂直的业务组加上一个基础技术架构组，精细化团队分工，有利于提高沟通协作的效率；</li>
<li>分布式系统，它最终的瓶颈一定落在CPU、内存、存储和网络上；</li>
</ul>
<h3 id="微博多级双机房缓存架构"><a href="#微博多级双机房缓存架构" class="headerlink" title="微博多级双机房缓存架构"></a>微博多级双机房缓存架构</h3><ul>
<li>微博使用了双层缓存，上面是L1，每个L1上都是一组(包含4-6台机器)，左边的框相当于一个机房，右边又是一个机房。两个机房是互为主备，或者互为热备 </li>
<li>L1缓存的作用：增加整个系统的QPS、以低成本灵活扩容的方式增加系统的带宽；</li>
<li>第二级缓存更多的是从容量上来规划，保证请求以较小的比例穿透到后端的数据库中；<br><img src="http://oi46mo3on.bkt.clouddn.com/9_reading_201701/weibo_multilevel_cache.png" alt="多级双机房缓存"></li>
</ul>
<h3 id="Feed的存储架构"><a href="#Feed的存储架构" class="headerlink" title="Feed的存储架构"></a>Feed的存储架构</h3><ul>
<li>内容表：每条内容一个索引，每天建一张表；</li>
<li>一级索引的时候会先根据关注的用户，取他们的前条微博ID，然后聚合排序。在做哈希(分库分表)的时候，同时考虑了按照UID哈希（分库）和按照时间维度（分表）。</li>
<li>二级索引，是我们里面一个比较特殊的场景，就是我要快速找到这个人所要发布的某一时段的微博时，通过二级索引快速定位。</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/9_reading_201701/weibo_feed_storage.png" alt="Feed的存储架构"></p>
<h3 id="分布式服务追踪系统"><a href="#分布式服务追踪系统" class="headerlink" title="分布式服务追踪系统"></a>分布式服务追踪系统</h3><p>一个请求从用户过来之后，在后台不同的机器之间不停的调用并返回。当你发现一个问题的时候，这些日志落在不同的机器上，你也不知道问题到底出在哪儿，各个服务之间互相隔离，互相之间没有建立关联。所以导致排查问题基本没有任何手段，就是出了问题没法儿解决。<br><img src="http://oi46mo3on.bkt.clouddn.com/9_reading_201701/ack_of_distributed_service.png" alt="分布式服务痛点"></p>
<p>解决办法是用一个请求ID，然后结合RPC框架，服务治理功能使日志联系起来。用JAVA的话就可以用AOP，要做到零侵入的原则，就是对所有相关的中间件打点，从接口层组件 (HTTP Client、HTTP Server)至到服务层组件(RPC Client、RPC Server)，还有数据访问中间件的，这样业务系统只需要少量的配置信息就可以实现全链路监控。</p>
<h2 id="RPC的概念模型与实现解析"><a href="#RPC的概念模型与实现解析" class="headerlink" title="RPC的概念模型与实现解析"></a>RPC的概念模型与实现解析</h2><p><a href="http://mp.weixin.qq.com/s?__biz=MzAxMTEyOTQ5OQ==&amp;mid=2650610547&amp;idx=1&amp;sn=2cae08dbf62d9a6c2f964ffd440c0077" target="_blank" rel="external">http://mp.weixin.qq.com/s?__biz=MzAxMTEyOTQ5OQ==&amp;mid=2650610547&amp;idx=1&amp;sn=2cae08dbf62d9a6c2f964ffd440c0077</a></p>
<ul>
<li>定义：RPC的全称是 Remote Procedure Call，是一种进程间通信方式。 它允许程序调用另一个地址空间（通常是共享网络的另一台机器上）的过程或函数，而不用程序员显式编码这个远程调用的细节。即程序员无论是调用本地的还是远程的函数，本质上编写的调用代码基本相同。</li>
<li>起源：上世纪80年代由 Bruce Jay Nelson提出；</li>
<li>目标：让构建分布式计算（应用）更容易，在提供强大的远程调用能力时不损失本地调用的语义简洁性。</li>
<li>分类：同步调用和异步调用，区分在于是否等待服务端执行完成并返回结果。</li>
<li>RPC理论模型：</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/9_reading_201701/rpc_model.png" alt="RPC理论模型"></p>
<p>这里User就是Client端。当User发起一个远程调用时，它实际是通过本地调用User-stub。 User-stub负责将调用的接口、方法和参数通过约定的协议规范进行编码并通过本地的RPCRuntime实例传输到远端的实例。远端RPCRuntime实例收到请求后交给 Server-stub进行解码后发起向本地端Server的调用，调用结果再返回给User端。</p>
<ul>
<li>RPC模型拆解</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/9_reading_201701/rpc_model.png" alt="RPC模型拆解"></p>
<p>RPC服务端通过RpcServer去导出远程接口方法，而客户端通过RpcClient去导入远程接口方法。客户端像调用本地方法一样去调用远程接口方法，RPC框架提供接口的代理实现，实际的调用将委托给代理RpcProxy。代理封装调用信息并将调用转交给RpcInvoker去实际执行。在客户端的RpcInvoker通过连接器RpcConnector去维持与服务端的通道RpcChannel，并使用RpcProtocol执行协议编码并将编码后的请求消息通过通道发送给服务端。</p>
<p>RPC服务端接收器RpcAcceptor接收客户端的调用请求，同样使用RpcProtocol执行协议解码。解码后的调用信息传递给RpcProcessor去控制处理调用过程，最后再委托调用给RpcInvoker去实际执行并返回调用结果。</p>
<ul>
<li>实现：微型RPC框架库<a href="https://github.com/mindwind/craft-atom" target="_blank" rel="external">craft-atom-rpc</a></li>
</ul>
<h2 id="Elastic-ON-Dev-China-Beijing-2016-Keynote"><a href="#Elastic-ON-Dev-China-Beijing-2016-Keynote" class="headerlink" title="Elastic{ON} Dev China Beijing 2016 Keynote"></a>Elastic{ON} Dev China Beijing 2016 Keynote</h2><p><a href="http://elasticsearch.cn/article/122" target="_blank" rel="external">http://elasticsearch.cn/article/122</a><br>阅读时结合：<a href="http://www.infoq.com/cn/news/2016/08/Elasticsearch-5-0-Elastic" target="_blank" rel="external">http://www.infoq.com/cn/news/2016/08/Elasticsearch-5-0-Elastic</a></p>
<p>目前ELK下载次数已经达到75M次。<br><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/elastic_development.png" alt="ELK发展情况"></p>
<p>有75%的用户将ELK使用于多个场景，60%的用户使用其数据搜索和分析功能，40%的用户使用日志分析功能。<br><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/elastic_cases.png" alt="ELK使用场景"></p>
<p>统一发布5.0版本后，拥有了ELKB和Elastic Cloud的全栈产品线。<br><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/ELKB_Stack.png" alt="ELKB产品线"></p>
<p>Elasticsearch5.0率先集成了Lucene 6版本，其中最重要的特性就是 Dimensional Point Fields，多维浮点字段，ES里面相关的字段如date, numeric，ip 和 Geospatial 都将大大提升性能。<br><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/es5_upgrade_1.png" alt="ElasticSearch优化1"><br><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/es5_upgrade_2.png" alt="ElasticSearch优化2"></p>
<p>ES5.0在Internal engine级别移除了用于避免同一文档并发更新的竞争锁，带来15%-20%的性能提升。<br><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/es5_upgrade_3.png" alt="ElasticSearch优化3"></p>
<p>ElasticSearch采用了更先进的Painless脚本。Painless使用白名单来限制函数与字段的访问，针对ES的场景来进行优化，只做ES数据的操作，更加轻量级，速度要快好几倍，并且支持Java静态类型，语法保持Groove类似，还支持Java的lambda表达式。<br><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/es5_upgrade_4.png" alt="ElasticSearch优化4"></p>
<p>毫秒级的Shrink API。它可将分片数进行收缩成它的因数，如之前你是15个分片，你可以收缩成5个或者3个又或者1个，那么我们就可以想象成这样一种场景，在写入压力非常大的收集阶段，设置足够多的索引，充分利用shard的并行写能力，索引写完之后收缩成更少的shard，提高查询性能。<br><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/es5_upgrade_5.png" alt="ElasticSearch优化5"></p>
<p>对日志类索引更友好的Rollover API。首先创建一个logs-0001的索引，它有一个别名是logs_write,然后我们给这个logs_write创建了一个rollover规则，即这个索引文档不超过1000个或者最多保存7天的数据，超过会自动切换别名到logs-0002,你也可以设置索引的setting、mapping等参数,剩下的es会自动帮你处理。这个特性对于存放日志数据的场景是极为友好的。<br><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/es5_upgrade_6.png" alt="ElasticSearch优化6"><br><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/es5_upgrade_7.png" alt="ElasticSearch优化7"></p>
<p>新增了一个Wait for refresh功能。大家知道elasticsearch可以设置refresh时间来保证数据的实时性，refresh时间过于频繁会造成很大的开销，太小会造成数据的延时，之前提供了索引层面的_refresh接口，但是这个接口工作在索引层面，我们不建议频繁去调用，如果你有需要修改了某个文档，需要客户端实时可见怎么办？在 5.0中，Index、Bulk、Delete、Update这些数据新增和修改的接口能够在单个文档层面进行refresh控制了，有两种方案可选，一种是创建一个很小的段，然后进行刷新保证可见和消耗一定的开销，另外一种是请求等待es的定期refresh之后再返回。<br><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/es5_upgrade_8.png" alt="ElasticSearch优化8"></p>
<p>新增Ingest Node。之前如果需要对数据进行加工，都是在索引之前进行处理，比如logstash可以对日志进行结构化和转换，现在直接在es就可以处理了，目前es提供了一些常用的诸如convert、grok之类的处理器，在使用的时候，先定义一个pipeline管道，里面设置文档的加工逻辑，在建索引的时候指定pipeline名称，那么这个索引就会按照预先定义好的pipeline来处理了。</p>
<p>这是一个原始的日志：</p>
<pre><code>{
  &quot;message&quot;: &quot;55.3.244.1 GET /index.html 15824 0.043”
}
</code></pre><p>使用Ingest就可以这么定义一个pipeline：</p>
<p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/es5_upgrade_9.png" alt="ElasticSearch优化9"></p>
<p>通过我们的pipeline处理之后的文档长什么样呢，我们获取这个文档的内容看看：<br><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/es5_upgrade_10.png" alt="ElasticSearch优化10"></p>
<p>另一个和aggregation的改进也是非常大，Instant Aggregations。Elasticsearch已经在Shard层面提供了Aggregation缓存，如果你的数据没有变化，ES能够直接返回上次的缓存结果。</p>
<p>新增了一个Sliced Scroll类型，现在Scroll接口可以并发来进行数据遍历了。每个Scroll请求，可以分成多个Slice请求，可以理解为切片，各Slice独立并行，利用Scroll重建或者遍历要快很多倍。</p>
<p>ES现在提供了Profile API来进行查询的优化，只需要在查询的时候开启profile：true就可以了，一个查询执行过程中的每个组件的性能消耗都能收集到。</p>
<p>还有一个和翻页相关的问题，就是深度分页，现在有一个新的 Search After 机制，其实和scroll类似，也是游标的机制，它的原理是对文档按照多个字段进行排序，然后利用上一个结果的最后一个文档作为起始值，拿size个文档，一般我们建议使用_uid这个字段，它的值是唯一的id。</p>
<p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/es5_upgrade_11.png" alt="ElasticSearch优化11"></p>
<p>新增Reindex。关于索引数据，大家之前经常重建，数据源在各种场景，重建起来很是头痛，那就不得不说说现在新加的Reindex接口了，Reindex可以直接在ElasticSearch集群里面对数据进行重建，如果你的mapping因为修改而需要重建，又或者索引设置修改需要重建的时候，借助Reindex可以很方便的异步进行重建，并且支持跨集群间的数据迁移。</p>
<p>ES 5.0里面提供了第一个Java原生的REST客户端SDK，相比之前的TransportClient，版本依赖绑定，集群升级麻烦，不支持跨Java版本的调用等问题，新的基于HTTP协议的客户端对Elasticsearch的依赖解耦，没有jar包冲突，提供了集群节点自动发现、日志处理、节点请求失败自动进行请求轮询，充分发挥Elasticsearch的高可用能力，并且性能不相上下。</p>
<p>另外还介绍了ELKB 5.0包括X-Pack组件的新特性，这里不做具体介绍，大家可以按照自己的兴趣阅读。</p>
<h2 id="基于Kibana和ES的苏宁实时日志分析平台"><a href="#基于Kibana和ES的苏宁实时日志分析平台" class="headerlink" title="基于Kibana和ES的苏宁实时日志分析平台"></a>基于Kibana和ES的苏宁实时日志分析平台</h2><p><a href="http://elasticsearch.cn/article/122" target="_blank" rel="external">http://elasticsearch.cn/article/122</a></p>
<h3 id="集群现状"><a href="#集群现状" class="headerlink" title="集群现状"></a>集群现状</h3><p><img src="http://oi46mo3on.bkt.clouddn.com/9_reading_201701/suning_elk_1.png" alt="集群现状"> </p>
<h3 id="日志平台架构演进"><a href="#日志平台架构演进" class="headerlink" title="日志平台架构演进"></a>日志平台架构演进</h3><p><img src="http://oi46mo3on.bkt.clouddn.com/9_reading_201701/suning_elk_2.png" alt="日志平台架构演进"></p>
<h3 id="优化总结-–-硬件"><a href="#优化总结-–-硬件" class="headerlink" title="优化总结 – 硬件"></a>优化总结 – 硬件</h3><ul>
<li>优先独立物理理机；</li>
<li>对于实时性要求非常高的需求，优先SSD；</li>
<li>适当调整OS的max_file_descriptors，解决Too many open files 异常；</li>
<li>单服务器运行多个node时，调整max user processes，否则容易易native thread<br>OOM；</li>
<li>关闭swap交换或锁内存 ulimit -l unlimited/bootstrap.mlockall: true</li>
</ul>
<h3 id="优化总结-–-ES"><a href="#优化总结-–-ES" class="headerlink" title="优化总结 – ES"></a>优化总结 – ES</h3><ul>
<li>根据数据量合理的规划索引pattern和shard数；</li>
<li>disabled _all 节省存储空间、提升索引速度；</li>
<li>不需要分词的字段设成 not_analyzed；</li>
<li>对于不要求100%高可用的内部系统，可不设置副本，提升index速度和减少<br>存储；</li>
<li>设置合理的refresh时间   index.refresh_interval: 300S</li>
<li>设置合理的flush间隔     index.translog.flush_threshold_size: 4g; index.translog.flush_threshold_ops: 50000</li>
<li>合理配置throttling      indices.store.throttle.max_bytes_per_sec: 200mb</li>
<li>适当调整bulk队列        threadpool.bulk.queue_size: 1000</li>
<li>有时可能因为gc时间过长，导致该数据节点被主节点踢出集群的情况，导致集群出现不健康的状态，为了解决这样的问题，我们适当的调整ping参数。(master)</li>
</ul>
<pre><code>discovery.zen.fd.ping_timeout: 40s
discovery.zen.fd.ping_interval: 5s
discovery.zen.fd.ping_retries: 5
</code></pre><ul>
<li>数据节点young gc频繁，适当调转新生代（-Xmn3g），降低young gc的频率。</li>
<li>在进行检索和聚合操作时，ES会读取反向索引，并进行反向解析，然后进行排序，将结果保存在内存中。这个处理会消耗很多Heap，有必要进行限制，不然会很容易出现OOM。</li>
</ul>
<pre><code>Disabled analyzed field fielddata
限制Field Data的Heap Size的使用
indices.fielddata.cache.size: 40%
indices.breaker.fielddata.limit: 50%
</code></pre><h2 id="美团点评搜索平台化实践之路"><a href="#美团点评搜索平台化实践之路" class="headerlink" title="美团点评搜索平台化实践之路"></a>美团点评搜索平台化实践之路</h2><p><a href="http://elasticsearch.cn/article/122" target="_blank" rel="external">http://elasticsearch.cn/article/122</a></p>
<h3 id="为什么需要平台化"><a href="#为什么需要平台化" class="headerlink" title="为什么需要平台化"></a>为什么需要平台化</h3><ul>
<li>重复建设严重</li>
<li>使用门槛高</li>
<li>缺少整体解决方案</li>
<li>长期演进不足</li>
<li>平台化：提供一整套的技术、运维方案和开发组件，最大化简化应用开发，提高开发效率、降低成本、提高可靠性。</li>
</ul>
<h3 id="平台化解决的主要问题"><a href="#平台化解决的主要问题" class="headerlink" title="平台化解决的主要问题"></a>平台化解决的主要问题</h3><ul>
<li>快速部署 -&gt; 代码库管理软件包；管理界面部署、重启、停止等操作；</li>
<li>集群高可用 -&gt; 引用集群组概念；双机房+双集群；双集群写；双集群读；</li>
<li>客户端使用门槛高 -&gt; 支持POJO功能；支持读写监控；支持多集群访问；</li>
<li>开源插件安全性弱、扩展性难 -&gt; 独立研发管理平台；</li>
<li>慢查询日志可视化和告警 -&gt; 通过Logstash抓取日志上报管理平台；管理平台提供查询、分析和统计；</li>
</ul>
<h3 id="平台技术架构"><a href="#平台技术架构" class="headerlink" title="平台技术架构"></a>平台技术架构</h3><p><img src="http://oi46mo3on.bkt.clouddn.com/10_deep_in_jvm/meituan_es_2.png" alt="ES平台技术架构"></p>
<h2 id="百度对Elasticsearch的优化改进"><a href="#百度对Elasticsearch的优化改进" class="headerlink" title="百度对Elasticsearch的优化改进"></a>百度对Elasticsearch的优化改进</h2><p><a href="http://elasticsearch.cn/article/122" target="_blank" rel="external">http://elasticsearch.cn/article/122</a></p>
<h3 id="分布式SQL查询层"><a href="#分布式SQL查询层" class="headerlink" title="分布式SQL查询层"></a>分布式SQL查询层</h3><p><img src="http://oi46mo3on.bkt.clouddn.com/9_reading_201701/baidu_es_1.png" alt="分布式SQL查询层"></p>
<ul>
<li>提供标准SQL接口，方便使用，降低学习成本</li>
<li>兼容Mysql协议，原Mysql/DDBS业务无缝迁移</li>
<li>兼容原始的HTTP协议</li>
</ul>
<h3 id="权限管理"><a href="#权限管理" class="headerlink" title="权限管理"></a>权限管理</h3><p><img src="http://oi46mo3on.bkt.clouddn.com/9_reading_201701/baidu_es_2.png" alt="权限管理"></p>
<ul>
<li>不同用户访问自己的表，增加database逻辑层，兼容ES和MySQL；</li>
<li>权限级别： db， table</li>
<li>用户级别：root， superuser，user</li>
<li>权限类型：read_only，read_write</li>
<li>白名单：IP（通配符）、hostname（BNS）</li>
</ul>
<h3 id="Online-schema-change"><a href="#Online-schema-change" class="headerlink" title="Online schema change"></a>Online schema change</h3><p><img src="http://oi46mo3on.bkt.clouddn.com/9_reading_201701/baidu_es_3.png" alt="Baidu-ES reindex"></p>
<h3 id="DistributedLog-数据一致性"><a href="#DistributedLog-数据一致性" class="headerlink" title="DistributedLog 数据一致性"></a>DistributedLog 数据一致性</h3><ul>
<li>需求背景：部分业务对ES可靠性要求很高；不能容忍脑裂、数据不一致、丢数据等情况；</li>
<li>需解决的问题：元数据一致性（脑裂）、强一致写、强一致读</li>
<li>解决方法：DistributedLog</li>
<li>元数据一致性（脑裂）：Master Leader向DL中写入Cluster State变更；其余的Master节点和所有的DataNode节点从DL中获取变更并向本地Apply；</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/9_reading_201701/baidu_es_4.png" alt="元数据一致性"></p>
<ul>
<li>强一致写：Master指定Primary；Primary将日志写入DL；Replica从DL中读取日志并回放；Translog必须在Lucene引擎内部实现为原子操作（如果先写log，Lucene可能写入不成功；如果先写Lucene，log有可能写入不成功）</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/9_reading_201701/baidu_es_5.png" alt="强一致写"></p>
<ul>
<li>强一致读 Lease机制：Primary需要定时从Master获取Lease；读取时首先检查Lease，当Lease Expire时不再提供读取服务；查询只查primary；</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/9_reading_201701/baidu_es_6.png" alt="强一致读"></p>
<h3 id="多集群数据同步"><a href="#多集群数据同步" class="headerlink" title="多集群数据同步"></a>多集群数据同步</h3><ul>
<li>需求背景：业务要求高可用，两地三中心部署；多个主备集群需实时同步增量数据；主备切换后的冲突处理；</li>
<li>设计实现：每个Doc都有修改的timestmap和version信息；Mirror Maker根据timestamp获取index修改的增量信息；Mirror Maker将增量的更新发往目标集群的Index中；冲突时根据version来判断是否覆盖目标集群里的Doc；默认version使用timestmap；</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/9_reading_201701/baidu_es_7.png" alt="多集群数据同步"></p>
<h3 id="多租户资源隔离"><a href="#多租户资源隔离" class="headerlink" title="多租户资源隔离"></a>多租户资源隔离</h3><ul>
<li>需求背景：多个业务使用公共集群，CPU、内存、IO、JVM等相互影响；云化部署；</li>
<li>设计实现：每台物理机启动多个ES进程组成大集群；cgroup对每个ES进程进行CPU、内存、IO等隔离；引入tenement 概念，分配不同的ES节点为每个租户创建自己的虚拟集群；Allocation filter限制租户的index只能创建在自己的节点上；每个租户分配不同DB，隔离访问权限；username@tenement，租户命名空间隔离租户信息；根据租户ID隔离settings，templates、nodes等；</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/9_reading_201701/baidu_es_8.png" alt="多租户资源隔离"></p>
<h3 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h3><p><img src="http://oi46mo3on.bkt.clouddn.com/9_reading_201701/baidu_es_9.png" alt="整体架构"></p>
<h3 id="整体规模"><a href="#整体规模" class="headerlink" title="整体规模"></a>整体规模</h3><p><img src="http://oi46mo3on.bkt.clouddn.com/9_reading_201701/baidu_es_10.png" alt="整体规模"></p>
<h2 id="万亿交易量级下的秒级监控"><a href="#万亿交易量级下的秒级监控" class="headerlink" title="万亿交易量级下的秒级监控"></a>万亿交易量级下的秒级监控</h2><p><a href="https://102.alibaba.com/newsInfo.htm?newsId=26" target="_blank" rel="external">https://102.alibaba.com/newsInfo.htm?newsId=26</a></p>
<p><img src="http://oi46mo3on.bkt.clouddn.com/9_reading_201701/ali_monitor_1.png" alt="阿里监控体系"></p>
<ul>
<li>阿里监控体系：集团层面的占整体80%；各个事业群根据自身特性自主研发了多套监控系统；规模已达到千万量级的监控项、PB级的监控数据、亿级的报警通知；</li>
<li>SunFire是一整套海量日志实时分析解决方案，以日志、REST接口、Shell脚本等作为数据采集来源，提供设备、应用、业务等各种视角的监控能力，从而快速发现、定位、分析和解决问题，为线上系统可用率提供有效保障；其利用文件传输、流式计算、分布式文件存储、数据可视化、数据建模等技术，提供实时、智能、可定制、多视角、全方位的监控体系；技术架构如下所示：</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/9_reading_201701/ali_monitor_2.png" alt="SunFire技术架构"></p>
<ul>
<li>Agent组件负责日志原始数据的采集，按周期查询日志，要求低耗、智能。</li>
<li>低耗的第一个要素就是避免跨机房传输，SunFire运行时组件自包含在机房内部，需要全量数据才从各机房查询合并；</li>
<li>SunFire还利用了zero-copy，即文件传输可以不经过用户态，实现极低CPU占用的文件传输；</li>
<li>要求按周期查询日志这个是Agent工程里最大的难题。RAF里通过指定offset和读取size可以很低耗地读取这部分内容，但是在计算平台周期任务驱动架构里，pull的方式无法提供offset，这个是通过二分法查找来猜；</li>
<li>当日志滚动的时候也是靠穷举的方式来猜offset；</li>
<li>支持两种查询服务：first query和ordinary query。一个周期的查询请求只有第一次需要猜offset。</li>
<li>另外一个是Map Reduce的计算组件，负责对所有采集内容进行加工计算，具备故障自动恢复及弹性伸缩能力；</li>
<li>计算组件的特性：纯异步（使用akka作为协程框架）、周期驱动、任务重试、输入共享；</li>
<li>其他组件：存储（HBase、MongoDB）、展示、自我管控。</li>
</ul>
<h2 id="万亿级数据洪峰下的分布式消息引擎"><a href="#万亿级数据洪峰下的分布式消息引擎" class="headerlink" title="万亿级数据洪峰下的分布式消息引擎"></a>万亿级数据洪峰下的分布式消息引擎</h2><p><a href="https://102.alibaba.com/newsInfo.htm?newsId=21" target="_blank" rel="external">https://102.alibaba.com/newsInfo.htm?newsId=21</a></p>
<ul>
<li>低延迟探索之路：JVM停顿（尽量避免Full GC、关闭偏向锁、输出GC日志到内存文件系统、关闭JVM输出的jstat日志）、利用CAS将RocketMQ核心链路无锁化、通过内核参数（vm.extra_free_kbytes和vm.swappiness）调优避免内存延迟、消除Page Cache延迟（内存预分配、文件预热、读写分离）；</li>
<li>容量保障三大法宝：降级、限流和熔断；</li>
<li>RocketMQ高可用：基于多机房部署，利用分布式锁和通知机制，借助Controller组件，设计并实现了Master/Slave结构的高可用架构；消息的读请求会命中Master，然后通过异步方式复制到Slave；消息的读请求优先命中Master，有压力时转移到Slave；</li>
</ul>
<p>分布式系统通用高可用解决方案</p>
<p><img src="http://oi46mo3on.bkt.clouddn.com/9_reading_201701/ali_rocketmq_ha.png" alt="分布式系统通用高可用解决方案"></p>
<p>RocketMQ高可用架构</p>
<p><img src="http://oi46mo3on.bkt.clouddn.com/9_reading_201701/ali_rocketmq.png" alt="分布式系统通用高可用解决方案"></p>
<h2 id="万亿级调用系统：微信序列号生成器架构设计及演变"><a href="#万亿级调用系统：微信序列号生成器架构设计及演变" class="headerlink" title="万亿级调用系统：微信序列号生成器架构设计及演变"></a>万亿级调用系统：微信序列号生成器架构设计及演变</h2><p><a href="http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2650992918&amp;idx=1&amp;sn=be5121c3c57257291a30715ef7130a90&amp;scene=21" target="_blank" rel="external">http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2650992918&amp;idx=1&amp;sn=be5121c3c57257291a30715ef7130a90&amp;scene=21</a></p>
<ul>
<li>微信立项之初就确立了利用数据版本号实现终端与后台数据增量同步的机制，确保消息可靠送达对方手机；这就需要一个高可用、高可靠的序列号生成器来产生同步数据用的版本号，这个生成器就是seqsvr；</li>
<li>数据版本号有两个性质：递增的64位整型变量；每个用户都有自己独立的64位sequence空间（避免申请互斥）；</li>
<li>架构原型（64位数组，每个用户保存最后一个seq） –&gt; 预分配中间层（放置一定步长的数据在内存，避免频繁更新）–&gt; 分号段共享存储（uid相连的用户属于同一号段，共享max_seq，减少重启时加载过长问题）；</li>
<li>工程实现：存储层（StoreSrv）利用多机NRW策略保证数据持久化不丢失；每个缓存中间层（AllocSrv）负责若干个号段的seq分配；整个系统按uid范围分Set，每个Set都是一个完整的、独立的子系统。</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/9_reading_201701/wx_seq_1.png" alt="微信seqserv"></p>
<h2 id="微信开源PhxSQL背后：强一致高可用分布式数据库的设计和实现哲学"><a href="#微信开源PhxSQL背后：强一致高可用分布式数据库的设计和实现哲学" class="headerlink" title="微信开源PhxSQL背后：强一致高可用分布式数据库的设计和实现哲学"></a>微信开源PhxSQL背后：强一致高可用分布式数据库的设计和实现哲学</h2><p><a href="http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2650994184&amp;idx=1&amp;sn=9be9eb8ab569ad281330b6ceeb490757&amp;scene=21" target="_blank" rel="external">http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2650994184&amp;idx=1&amp;sn=9be9eb8ab569ad281330b6ceeb490757&amp;scene=21</a></p>
<ul>
<li>为何研发：解决强一致性、高可用和serializable事务隔离；完全兼容Mysql；</li>
<li><a href="https://github.com/tencent-wechat/phxsql" target="_blank" rel="external">PhxSQL</a>是什么：建立在Paxos的一致性和Mysql的binlog流水的基础上的；提供两个服务端口：强一致读写端口和只读端口；</li>
<li>PhxSQL的强一致性指线性一致性，高可用是指只要多余一半机器工作和互联即可在保证线性一致性的质量下正常工作，提供和ZooKeeper相同的强一致性和高可用性；</li>
<li>设计原则：简单可逻辑证明的一致性模型（基于Paxos和binlog流水一致）；最小侵入Mysql原则；简单的架构、部署和运维；</li>
<li>局限性：DDL命令可能存在一致性风险、写入请求量很大主机死机时会有一段时间不可写；另外不支持多写和分表分库。</li>
</ul>
<h2 id="章文嵩博士和他背后的负载均衡帝国"><a href="#章文嵩博士和他背后的负载均衡帝国" class="headerlink" title="章文嵩博士和他背后的负载均衡帝国"></a>章文嵩博士和他背后的负载均衡帝国</h2><p><a href="http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2650992803&amp;idx=1&amp;sn=e2a46917301941faacc324af29013877&amp;scene=21" target="_blank" rel="external">http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2650992803&amp;idx=1&amp;sn=e2a46917301941faacc324af29013877&amp;scene=21</a></p>
<ul>
<li>常见的负载均衡技术：DNS轮询（易于实现，但存在会话粘连、DNS缓存滞后、容错、数据热点问题）；引入负载均衡器（集中分发、支持多种分发方式，但存在单点的问题）；健康监测（负载均衡的伴侣）；</li>
<li>VIPServer：阿里中间层负载均衡产品；是基于P2P模式的七层负载均衡产品；提供动态域名解析和负载均衡服务；支持一系列流量智能调度和容灾策略、支持多种健康监测协议、支持精细的权重控制；提供多级容灾、具有对称调用、健康阈值保护等功能。</li>
</ul>
<h2 id="阿里双十一大促，技术准备只做了这两件事情？"><a href="#阿里双十一大促，技术准备只做了这两件事情？" class="headerlink" title="阿里双十一大促，技术准备只做了这两件事情？"></a>阿里双十一大促，技术准备只做了这两件事情？</h2><p><a href="http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2650995093&amp;idx=1&amp;sn=574f6d83a48c2c596943b1fbeb25e4a7&amp;scene=21" target="_blank" rel="external">http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2650995093&amp;idx=1&amp;sn=574f6d83a48c2c596943b1fbeb25e4a7&amp;scene=21</a></p>
<ul>
<li>容量规划：在什么时候什么样的系统需要多少服务器？需要给出确定性、量化的数字；</li>
<li>容量规划三阶段：经验判断、线上压测（测性能、估机器、模拟回放、线上分流）和场景化压测，目前还做了全链路压测；</li>
<li>场景化容量评估：造流量，尽量模拟真实场景；流量隔离，通过负载均衡出一个在线集群；</li>
<li>流量评估的流程：数据构造（构造基础数据、业务模型预测、构造压测请求） -&gt; 环节准备（配置压测方案、上传压测数据、业务预热、生效压测passtoken、小流量预跑验证） -&gt; 压测执行&amp;总结（压测用户登录、压测执行&amp;实时调速、动态弹性伸缩、压测报告&amp;问题总结）；</li>
<li>容量评估的总结：</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/9_reading_201701/ali_yace.png" alt="容量评估的总结"></p>
<h2 id="Twitter再开源！这回是分布式高性能日志复制服务"><a href="#Twitter再开源！这回是分布式高性能日志复制服务" class="headerlink" title="Twitter再开源！这回是分布式高性能日志复制服务"></a>Twitter再开源！这回是分布式高性能日志复制服务</h2><p><a href="http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2650992712&amp;idx=1&amp;sn=727ce15ad3651ce43a710a165ed2495a&amp;scene=21" target="_blank" rel="external">http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2650992712&amp;idx=1&amp;sn=727ce15ad3651ce43a710a165ed2495a&amp;scene=21</a></p>
<ul>
<li><a href="https://github.com/twitter/distributedlog" target="_blank" rel="external">DistributedLog</a>是一个高性能的日志复制服务，提供了持久化、复制以及强一致性的功能，这对于构建可靠的分布式系统都是至关重要的，如复制状态机、通用的发布订阅系统、分布式数据库以及分布式队列；</li>
<li>DL会分类维护记录的序列并称其为Log，将记录写入DL Log的进程称之为Writer，从Log中读取并处理记录的进程称之为Reader，其整体架构如下所示：</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/9_reading_201701/twitter_dl.png" alt="DistributedLog架构"></p>
<ul>
<li>Log：是有序的、不可变的日志记录，它的数据结构如下所示：</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/9_reading_201701/twitter_dl_log.png" alt="Log的数据结构"></p>
<ul>
<li>日志记录：一个字节序列，会按照序列写入到日志流中，并且会分配一个DLSN的唯一序列号。应用程序还可以设置自己的序列号（称之为TransactionID），便于Reader从特定的日志记录读取；</li>
<li>Log分段：Log会被分解为Log分段，每个分段包含了记录的子集，分布式地存储（如BookKeeper）。DL会基于配置好的策略轮询每个Log分段。</li>
<li>命名空间：属于同一组织的Log流会归类在同一命名空间下，便于管理；</li>
<li>Writer：序列号由Writer负责的，这意味着对于某个Log，在给定的时间点上，只能有一个激活的Writer；Writer由名为Write Proxy的服务层来提供和管理，Write Proxy用来接受大量客户端的fan-in写入；</li>
<li>Reader：Reader会在一个给定的位置（DLSN或TransactionID）开始从Log中严格按顺序读取记录。在同一个Log中，不同的Reader可以在不同的起始位置读取记录。与其他的订阅发布系统不同，DL并不会记录和管理Reader的位置，它将跟踪的任务留给了应用程序本身；</li>
<li>优势总结：高性能（毫秒级延迟）、持久化和一致性、各种工作负载、多租户、分层架构；</li>
</ul>
<h2 id="如何用十条命令在一分钟内检查Linux服务器性能"><a href="#如何用十条命令在一分钟内检查Linux服务器性能" class="headerlink" title="如何用十条命令在一分钟内检查Linux服务器性能"></a>如何用十条命令在一分钟内检查Linux服务器性能</h2><p><a href="http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2650994146&amp;idx=1&amp;sn=f6b0987a06831805b4c343c417121827&amp;scene=21" target="_blank" rel="external">http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2650994146&amp;idx=1&amp;sn=f6b0987a06831805b4c343c417121827&amp;scene=21</a></p>
<pre><code>## 以下命令uptime用于快速查看机器的负载情况，输出1min、5min、15min的平均负载请假
[test@localhost ~]$ uptime   
 13:56:36 up 32 days, 13:12,  1 user,  load average: 0.00, 0.02, 0.00

## 以下命令用于输出系统日志的最后10行
[test@localhost ~]$ dmesg | tail
  alloc kstat_irqs on node -1
bnx2 0000:01:00.0: irq 65 for MSI/MSI-X
  alloc irq_desc for 66 on node -1
  alloc kstat_irqs on node -1
bnx2 0000:01:00.0: irq 66 for MSI/MSI-X
  alloc irq_desc for 67 on node -1
  alloc kstat_irqs on node -1
bnx2 0000:01:00.0: irq 67 for MSI/MSI-X
bnx2 0000:01:00.0: em1: using MSIX
bnx2 0000:01:00.0: em1: NIC Copper Link is Up, 100 Mbps full duplex, receive &amp; transmit flow control ON

## 以下命令用于输出一些系统核心指标，后面的参数1表示每秒输出一次统计信息。
[test@localhost ~]$ vmstat 1
procs -----------memory---------- ---swap-- -----io---- --system-- -----cpu-----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
 0  0      0 9118956 221536 10690528    0    0     0    19    0    2  2  0 98  0  0    
 0  0      0 9118816 221536 10690560    0    0     0     0 1180 2939  0  0 100  0  0    
 0  0      0 9118816 221536 10690584    0    0     0   464 1199 2798  0  0 99  0  0    
 0  0      0 9118824 221536 10690616    0    0     0     0 1089 2839  0  0 100  0  0    

## 以下命令用于显示每个CPU的占用情况
[test@localhost ~]$ mpstat -P ALL 1
Linux 2.6.32-431.el6.x86_64 (localhost.localdomain)     01/19/2017     _x86_64_    (8 CPU)

02:04:08 PM  CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest   %idle
02:04:09 PM  all    0.25    0.00    0.25    0.00    0.00    0.00    0.00    0.00   99.50
02:04:09 PM    0    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00  100.00
02:04:09 PM    1    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00  100.00
02:04:09 PM    2    0.00    0.00    1.00    0.00    0.00    0.00    0.00    0.00   99.00
02:04:09 PM    3    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00  100.00
02:04:09 PM    4    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00  100.00
02:04:09 PM    5    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00  100.00
02:04:09 PM    6    0.00    0.00    0.99    0.00    0.00    0.00    0.00    0.00   99.01
02:04:09 PM    7    0.99    0.00    0.00    0.00    0.00    0.00    0.00    0.00   99.01

## 以下命令用于输出进程的CPU占用率
[test@localhost ~]$ pidstat 1
Linux 2.6.32-431.el6.x86_64 (localhost.localdomain)     01/19/2017     _x86_64_    (8 CPU)

02:05:38 PM       PID    %usr %system  %guest    %CPU   CPU  Command
02:05:39 PM      2004    0.99    0.00    0.00    0.99     4  mysqld
02:05:39 PM     14777    0.99    0.99    0.00    1.98     0  pidstat
02:05:39 PM     25506    0.99    0.99    0.00    1.98     6  java
02:05:39 PM     25922    0.99    0.99    0.00    1.98     6  java

## 以下命令用于查看机器磁盘IO情况
[test@localhost ~]$ iostat -xz 1
Linux 2.6.32-431.el6.x86_64 (localhost.localdomain)     01/19/2017     _x86_64_    (8 CPU)

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
           1.72    0.00    0.26    0.18    0.00   97.84

Device:         rrqm/s   wrqm/s     r/s     w/s   rsec/s   wsec/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
sda               0.01    31.80    0.04    6.35     2.17   305.21    48.09     0.09   14.38    2.52   14.46   3.55   2.27

## 以下命令用于查看系统内存的使用情况
[test@localhost ~]$ free -m
             total       used       free     shared    buffers     cached
Mem:         32092      23216       8876          0        216      10458
-/+ buffers/cache:      12540      19551
Swap:         4095          0       4095

## 以下命令用于查看网络设备的吞吐率
[test@localhost ~]$ sar -n DEV 1
Linux 2.6.32-431.el6.x86_64 (localhost.localdomain)     01/19/2017     _x86_64_    (8 CPU)

02:11:06 PM     IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s
02:11:07 PM        lo    117.17    117.17      8.28      8.28      0.00      0.00      0.00
02:11:07 PM       em1    108.08    120.20      8.11     24.58      0.00      0.00      5.05
02:11:07 PM       em2      0.00      0.00      0.00      0.00      0.00      0.00      0.00

## 以下命令用于查看TCP连接状态
[test@localhost ~]$ sar -n TCP,ETCP 1
Linux 2.6.32-431.el6.x86_64 (localhost.localdomain)     01/19/2017     _x86_64_    (8 CPU)

02:12:25 PM  active/s passive/s    iseg/s    oseg/s
02:12:26 PM     80.81      0.00    585.86    772.73

02:12:25 PM  atmptf/s  estres/s retrans/s isegerr/s   orsts/s
02:12:26 PM      0.00     80.81      0.00      0.00    105.05

## top命令是前面好几个命令检查内容的汇总
top
</code></pre><h2 id="不谈架构，看看如何从代码层面优化系统性能！"><a href="#不谈架构，看看如何从代码层面优化系统性能！" class="headerlink" title="不谈架构，看看如何从代码层面优化系统性能！"></a>不谈架构，看看如何从代码层面优化系统性能！</h2><p><a href="http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2650993458&amp;idx=1&amp;sn=e959385bc0bddb4b7cfab84a1310f9e8&amp;scene=21" target="_blank" rel="external">http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2650993458&amp;idx=1&amp;sn=e959385bc0bddb4b7cfab84a1310f9e8&amp;scene=21</a></p>
<ul>
<li>数据库死锁问题改进：使用Redis做分布式锁；使用主键防重方法；使用版本号机制防重；</li>
<li>数据库事务占用时间过长：事务代码要尽量小，将不需要事务控制的代码移出；</li>
<li>CPU时间被占满：C3P0在大并发下性能差，改成使用<a href="https://github.com/akka/akka" target="_blank" rel="external">AKKA</a>；</li>
<li>日志打印问题：统一日志输出规范、日志输出行号有锁去除行号；</li>
</ul>
<h2 id="微服务那么热，创业公司怎么选用实践？"><a href="#微服务那么热，创业公司怎么选用实践？" class="headerlink" title="微服务那么热，创业公司怎么选用实践？"></a>微服务那么热，创业公司怎么选用实践？</h2><p><a href="http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2650994360&amp;idx=1&amp;sn=dd2664e2db6bfc7427a4ea738899840e&amp;scene=21" target="_blank" rel="external">http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2650994360&amp;idx=1&amp;sn=dd2664e2db6bfc7427a4ea738899840e&amp;scene=21</a></p>
<ul>
<li>SOA没有大范围取代单体应用的原因：其好处主要来自项目模块化而非模块服务化；没有解决多服务运维这个核心问题；</li>
<li><a href="http://www.martinfowler.com/articles/microservices.html" target="_blank" rel="external">微服务的九大特征</a>：服务即组件、按照业务域来组织微服务、按产品而非项目划分微服务、关注业务逻辑而非服务间通讯、分散式管理、分散式数据、基础设施自动化、容错、进化；</li>
<li>何时不需要微服务：你的代码没有模块化、你的服务要求极高性能、你没有一个好的容器编排系统；</li>
<li>技术选型：容器编排系统Kubernetes、编程语言Go、在线监控Prometheus+Grafana、离线数据分析fluentd+ODPS、同步通讯gRPC+HTTP Restful、异步通讯RabbitMQ、持续集成Jenkins、Docker私有仓库Harbor；</li>
</ul>
<h2 id="深入理解G1垃圾收集器"><a href="#深入理解G1垃圾收集器" class="headerlink" title="深入理解G1垃圾收集器"></a>深入理解G1垃圾收集器</h2><p><a href="http://ifeve.com/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3g1%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8/" target="_blank" rel="external">http://ifeve.com/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3g1%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8/</a></p>
<ul>
<li>G1相比CMS的优势：压缩空间方面有优势、通过Region避免内存碎片、可设置预期停顿时间避免应用雪崩、可在Young GC中使用；</li>
<li>一次完整的过程：YGC、并发阶段、混合模式、Full GC；</li>
</ul>
<pre><code>## YGC：在Eden充满时触发，回收之后所有值钱Eden的清空，至少有一个Survivor区，同时有一些数据移到了Old区；
23.430: [GC pause (young), 0.23094400 secs]
...
[Eden: 1286M(1286M)-&gt;0B(1212M)
Survivors: 78M-&gt;152M Heap: 1454M(4096M)-&gt;242M(4096M)]
[Times: user=0.85 sys=0.05, real=0.23 secs]

## 并发阶段：这个阶段主要是发现哪些区域包含可回收的垃圾最多
## 首先是初始标记阶段，该阶段会Stop-The-World，执行一次YGC
50.541: [GC pause (young) (initial-mark), 0.27767100 secs]
[Eden: 1220M(1220M)-&gt;0B(1220M)
Survivors: 144M-&gt;144M Heap: 3242M(4096M)-&gt;2093M(4096M)]
[Times: user=1.02 sys=0.04, real=0.28 secs]

## 接下来，G1开始扫描根区域（GC Root），这个过程是后台线程并行处理，不暂停应用线程
50.819: [GC concurrent-root-region-scan-start]
51.408: [GC concurrent-root-region-scan-end, 0.5890230]

## 接下来，开始进入并发标记阶段，该阶段也是后台线程执行的
111.382: [GC concurrent-mark-start]
....
120.905: [GC concurrent-mark-end, 9.5225160 sec]

## 然后是二次标记阶段和清理阶段：这两个阶段会暂停应用线程，但实际很短
120.910: [GC remark 120.959:
[GC ref-PRC, 0.0000890 secs], 0.0718990 secs]
[Times: user=0.23 sys=0.01, real=0.08 secs]
120.985: [GC cleanup 3510M-&gt;3434M(4096M), 0.0111040 secs]
[Times: user=0.04 sys=0.00, real=0.01 secs]

## 再之后还有额外的一次并发清理阶段
120.996: [GC concurrent-cleanup-start]
120.996: [GC concurrent-cleanup-end, 0.0004520]

## 混合GC：会同时进行YGC和清理上阶段标记为清理的区域，一直持续到几乎所有的标记区域垃圾对象都被回收
79.826: [GC pause (mixed), 0.26161600 secs]
....
[Eden: 1222M(1222M)-&gt;0B(1220M)
Survivors: 142M-&gt;144M Heap: 3200M(4096M)-&gt;1964M(4096M)]
[Times: user=1.01 sys=0.00, real=0.26 secs]
</code></pre><h2 id="京东分布式服务跟踪系统-CallGraph"><a href="#京东分布式服务跟踪系统-CallGraph" class="headerlink" title="京东分布式服务跟踪系统-CallGraph"></a>京东分布式服务跟踪系统-CallGraph</h2><p><a href="http://mp.weixin.qq.com/s/gy2a_nbYfUJq7DhlDFio6A" target="_blank" rel="external">http://mp.weixin.qq.com/s/gy2a_nbYfUJq7DhlDFio6A</a></p>
<ul>
<li>产生背景：SOA化和微服务；基于Google发表的分布式日志跟踪论文；相似的有淘宝鹰眼和新浪WatchMan；</li>
<li>核心概念：调用链包含了从源头请求到最后底层系统的所有环节，中间通过全局唯一的TraceID透传；</li>
<li>特性及使用场景：方法调用关系（单次调用的问题排查）、应用依赖关系（容量规划、调用来源、依赖度量、调用耗时、调用并行度、调用路由）、与业务数据集成（将公司业务与第三方业务进行关联）；</li>
<li>设计目标：低侵入性、低性能影响、灵活的应用策略、时效性；</li>
<li>实现架构：核心包（被各中间件引用，完成具体的埋点逻辑，日志存放在内存磁盘上由Agent收集发送到JMQ）、JMQ（充当日志数据管道）、Storm（对数据日志并行整理和计算）、存储（实时数据存储有JimDB/HBase/ES，离线数据存储包括HDFS和Spark）、CallGraph-UI（用户交互界面）、UCC（存放配置信息并同步到各服务器）、管理元数据（存放链路签名与应用映射关系等）；</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/9_reading_201701/jd_callgraph.png" alt="CallGraph实现架构"></p>
<ul>
<li>埋点和调用上下文透传：前端利用Web容器的Filter机制调用startTrace开启跟踪，调用endTrace结束跟踪；各中间件调用clientSend、serverRecv、serverSend和clientRecv等API；对于进程间的上下文透传，调用上下文放在ThreadLocal；对于异步调用，通过Java字节码增强方式织入，以透明的方式完成线程间上下文的透传。</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/9_reading_201701/jd_callgraph2.png" alt="CallGraph透传"></p>
<ul>
<li>日志格式设计：固定部分（TraceID、RpcID、开始时间、调用类型、对端IP、调用耗时、调用结果等）、可变部分；</li>
<li>高性能的链路日志输出：开辟专门的内存区域并虚拟成磁盘设备，产生的日志存放在这样的内存设备，完全不占用磁盘IO；专门的日志模块，输出采用批量、异步方式写入，并在日志量过大时采取丢弃日志；</li>
<li>TP日志和链路日志分离：链路日志通常开启采样率机制，比如1000次调用只收集1次；但是对于TP指标来说，必须每次记录，因此这两种数据是独立处理互不影响；</li>
<li>实时配置：通过CallGraph-UI和UCC实时配置，支持基于应用、应用分组、服务器IP多维度配置；</li>
<li>秒级监控：针对业务对实时分析的需求，采用JimDB存放实时数据，针对来源分析、入口分析、链路分析等可以提供1小时内的实时分析结果；</li>
<li>未来之路：延迟更低、完善错误发现和报警、借助深度学习挖掘价值。</li>
</ul>
<h2 id="京东消息中间件的演进"><a href="#京东消息中间件的演进" class="headerlink" title="京东消息中间件的演进"></a>京东消息中间件的演进</h2><p><a href="http://mp.weixin.qq.com/s/4dsdpL-9SqQWybb02tOpXQ" target="_blank" rel="external">http://mp.weixin.qq.com/s/4dsdpL-9SqQWybb02tOpXQ</a></p>
<h3 id="第一代JMQ"><a href="#第一代JMQ" class="headerlink" title="第一代JMQ"></a>第一代JMQ</h3><p><img src="http://oi46mo3on.bkt.clouddn.com/9_reading_201701/jd_jmq1.png" alt="第一代MQ"></p>
<ul>
<li>选型：基于ActiveMQ做消息核心，基于MySQL和ZooKeeper做配置中心，管理监控平台自己研发；</li>
<li>如何存储：ActiveMQ使用KahaDB存储引擎进行存储，BTree索引。为了保证可靠性，索引文件和日志文件都需要同步刷盘；一个Topic有多个订阅者，就为每个订阅者创建一个队列，broker会将消息复制多份；</li>
<li>如何支持集群：当时原生的ActiveMQ客户端看是不支持服务集群化的，所以采用ZK进行扩展，使客户端支持了集群的同时还实现了对服务器动态扩展的支持；</li>
<li>推还是拉：ActiveMQ采用的是push模式，消息由producer发送到broker端之后由broker推送给consumer；</li>
<li>如何处理失败消息：原生ActiveMQ会在失败之后将消息放到死信队列，该队列的信息不能被消费者所获取；扩展的方式是拦截错误信息，写入重试服务库之后给队列返回消费成功的ACK，而后consumer通过一定策略消费重试库里面的消息；</li>
<li>其他优化和扩展：生成消息轨迹、优化broker写逻辑提升性能、新的主从复制、新的主从选举、增加监控模块；</li>
</ul>
<h3 id="第二代JMQ"><a href="#第二代JMQ" class="headerlink" title="第二代JMQ"></a>第二代JMQ</h3><p><img src="http://oi46mo3on.bkt.clouddn.com/9_reading_201701/jd_jmq2.png" alt="第二代MQ"></p>
<ul>
<li>第一代JMQ的问题：新的跨机房部署问题、Broker的性能随消息积压而急剧下降、Broker对Topic的订阅复制影响性能、Broker逻辑复杂，无法扩展消息回放、顺序消息和广播消息等、重客户端等；</li>
<li>开启了JMQ的自研：JMQ服务端（实现轻量级的存储模型、支持消息回放、支持重试、支持消息轨迹、支持顺序消息、支持广播信息等，并兼容AMQ客户端使用的OpenWire协议）、JMQ客户端（轻量级只和Broker通讯，支持动态接收参数、内置性能采集、支持跨机房）、管理控制平台、HTTP代理（基于Netty，支持跨语言）；</li>
<li>如何解决IO问题：使用Netty 4.0减少服务端开发，在应用层自定义JMQ协议；</li>
<li>如何存储消息：日志文件journal（主要存储消息内容，包括消息所在队列文件的位置）、消息队列文件queue（主要存储消息所在日志文件的全局偏移量）、消费位置文件offset（存储不同订阅者针对某个topic所消费到的队列的一个偏移量）都保存在Broker所在机器的本地磁盘上。</li>
<li>如何容灾：采用一主一从，至少一个备份，主从分布在同一个数据中心、备份分布在其他数据中心，主从复制同步、备份异步复制。</li>
<li>推还是拉：采用pull模式，由consumer主动发起请求去broker上取消息；</li>
<li>如何处理失败消息：JMQ的broker直接就支持重试，consumer处理消息失败时直接向服务端发送一个重试消息命令，服务端接到命令后将此消息入库；consumer在拉取消息时，服务端根据一定策略从库里取出消息给consumer处理；</li>
<li>如何管理元数据：客户端不再直接连接ZooKeeper，连接Broker获取元数据；</li>
</ul>
<h3 id="第三代JMQ"><a href="#第三代JMQ" class="headerlink" title="第三代JMQ"></a>第三代JMQ</h3><p><img src="http://oi46mo3on.bkt.clouddn.com/9_reading_201701/jd_jmq3.png" alt="第三代MQ"></p>
<p>几个重要的目标：优化JMQ协议、优化复制模型、实现Kafka协议兼容、实现全局负载均衡、实现全新的选举方案、实现资源的弹性调度。</p>
<h2 id="京东JIMDB建设之路"><a href="#京东JIMDB建设之路" class="headerlink" title="京东JIMDB建设之路"></a>京东JIMDB建设之路</h2><p><a href="http://mp.weixin.qq.com/s/IzYj3R1mfFpd1pq-xYhbQg" target="_blank" rel="external">http://mp.weixin.qq.com/s/IzYj3R1mfFpd1pq-xYhbQg</a></p>
<p><img src="http://oi46mo3on.bkt.clouddn.com/9_reading_201701/jd_jimdb.png" alt="京东JIMDB"></p>
<ul>
<li>JimDB的特性：一键创建集群实例、在线全自动弹性伸缩、部分复制扩容、在线平滑升级、全自动故障恢复、支持多语言接入、支持多种读取策略、溶强化部署、增量复制；</li>
<li>主要包括：Server（提供KV服务，支持一主多从和读写分离）、Config Server（复制集群拓扑的维护）、Sentinel（用于判断服务端实例存活状态）、Failover（负责角色切换和故障实例的替换）、Scaler（当内存容量或者流量等达到阈值时对分片进行分裂扩容）、Info Collector（负责监控数据的采集）、Resource Manager（负责物理机资源的管理和容器的创建）；</li>
<li>自研第一版主要解决以下问题：精确的故障检测和自动故障切换（机房不同机架部署多个探测实例，只要有一个探测到存活就是存活的，没有反馈存活且超过半数认为其死亡则认为死亡）、无损扩容（服务端数据按slot进行组织，迁移时以slot为单位进行迁移）、提供监控和报警等服务；</li>
<li>自研第二版：自动弹性调度（利用监控指标和阈值进行扩容和缩容）、服务端升级（引入docker）、资源隔离（物理机分区，集群分区）、大KEY扫描、读策略优化；</li>
<li>现有系统的完善和改进：完善弹性调度、新特性（丰富数据结构、版本号、支持HashTag）、丰富监控和性能统计数据、客户端增加本地缓存功能、新客户端支持异步发送、大KEY的应急处理、支持KEY按范围扫描等。</li>
</ul>
<h2 id="奇虎360开源其日志搜索引擎，可处理百万亿级的数据"><a href="#奇虎360开源其日志搜索引擎，可处理百万亿级的数据" class="headerlink" title="奇虎360开源其日志搜索引擎，可处理百万亿级的数据"></a>奇虎360开源其日志搜索引擎，可处理百万亿级的数据</h2><p><a href="http://mp.weixin.qq.com/s/JhJ709gBeVNjViIFbngoRQ" target="_blank" rel="external">http://mp.weixin.qq.com/s/JhJ709gBeVNjViIFbngoRQ</a></p>
<p><img src="http://oi46mo3on.bkt.clouddn.com/9_reading_201701/qihu_poseidon.png" alt="Poseidon"></p>
<ul>
<li><a href="https://github.com/Qihoo360/poseidon" target="_blank" rel="external">Poseidon</a>系统是一个日志搜索平台，可以在百万亿条、100PB 大小的日志数据中快速分析和检索；</li>
<li>设计目标：原始数据不要额外存储、当前的Map/Reduce作业不用变更、自定义分词策略、秒级查询相应；</li>
<li>所用技术：倒排索引（构建日志搜索引擎的核心技术）、Hadoop（用于存放原始数据和索引数据，并用来运行Map/Reduce程序来构建索引）、Java（构建索引时是用Java开发的Map/Reduce程序）、Golang（检索程序是用Golang开发的）、Redis/Memcached（用于存储 Meta 元数据信息）；</li>
</ul>
<h2 id="为什么说传统分布式事务不再适用于微服务架构？"><a href="#为什么说传统分布式事务不再适用于微服务架构？" class="headerlink" title="为什么说传统分布式事务不再适用于微服务架构？"></a>为什么说传统分布式事务不再适用于微服务架构？</h2><p><a href="http://mp.weixin.qq.com/s/wPeDzVk7UKMFXNWyzUyugg" target="_blank" rel="external">http://mp.weixin.qq.com/s/wPeDzVk7UKMFXNWyzUyugg</a></p>
<ul>
<li>传统分布式事务不是微服务中数据一致性的最佳选择：单机数据库的ACID、分布式数据库的两阶段提交协议（2PC）；对于微服务，数据是微服务私有的且SQL和NoSQL混合使用，2PC很难适用；</li>
<li>微服务架构中应满足数据最终一致性原则：所用副本经过一段时间后最终能够达成一致；</li>
<li>微服务架构实现最终一致性的三种模式：可靠事件模式（保证可靠事件投递和避免重复消费）、业务补偿模式（使用一个额外的协调服务来协调各个需要保证一致性的微服务，关键在于业务流水的记录）和TCC模式（一个完整的TCC业务由一个主业务服务和若干个从业务服务组成，主业务服务发起并完成整个业务活动，从服务提供三个接口Try、Confirm和Cancel）；</li>
<li>对账是最后的终极防线</li>
</ul>
<h2 id="兼顾高可靠和低延迟，Google打算用QUIC协议替代TCP-UDP"><a href="#兼顾高可靠和低延迟，Google打算用QUIC协议替代TCP-UDP" class="headerlink" title="兼顾高可靠和低延迟，Google打算用QUIC协议替代TCP/UDP"></a>兼顾高可靠和低延迟，Google打算用QUIC协议替代TCP/UDP</h2><p><a href="http://mp.weixin.qq.com/s/O01HkvvpluaqzTyoxd7d8g" target="_blank" rel="external">http://mp.weixin.qq.com/s/O01HkvvpluaqzTyoxd7d8g</a></p>
<ul>
<li>TCP协议连接建立的成本相对较高；UDP协议是无连接协议，这样的好处是在网络传输层无需对数据包进行确认，但存在的问题就是为了确保数据传输的可靠性，应用层协议需要自己完成包传输情况的确认；QUIC协议可以在1到2个数据包内，完成连接的创建（包括TLS）；</li>
<li>QUIC协议的主要目的，是为了整合TCP协议的可靠性和UDP协议的速度和效率。对于Google来说优化TCP协议是一个长期目标，QUIC旨在创建几乎等同于TCP的独立连接，但有着低延迟，并对类似SPDY的多路复用流协议有更好的支持。如果QUIC协议的特性被证明是有效的，这些特性以后可能会被迁移入后续版本的TCP和TLS协议（它们都有很长的开发周期）。</li>
<li>QUIC协议特性：避免前序包阻塞、减少数据包、向前纠错、会话重启和并行下载；</li>
</ul>
<h2 id="配置高性能ElasticSearch集群的9个小贴士"><a href="#配置高性能ElasticSearch集群的9个小贴士" class="headerlink" title="配置高性能ElasticSearch集群的9个小贴士"></a>配置高性能ElasticSearch集群的9个小贴士</h2><p><a href="http://mp.weixin.qq.com/s/jfXxpQXxvPzpFG_NOd6j0A" target="_blank" rel="external">http://mp.weixin.qq.com/s/jfXxpQXxvPzpFG_NOd6j0A</a></p>
<ul>
<li>规划索引、分片以及集群增长情况</li>
<li>在配置前了解集群的拓扑结构：设置Master Node和Data Node；</li>
<li>内存设置：”bootstrap.mloclall: true”允许ES节点不交换内存；</li>
<li>discovery.zen属性控制ElasticSearch的发现协议：discover.zen.fd.ping_timeout属性控制超时、discovery.zen.minimum_master_nodes属性决定了有资格作为master的节点的最小数量、discovery.zen.ping.unicast.hosts属性指定一组通信主机；</li>
<li>当心DELETE _all：通过设置action.destructive_requires_name:true来禁用；</li>
<li>使用Doc Values：本质上是将ES转换成一个列式存储，从而使ES的许多分析类特性在性能上远超预期；</li>
<li>ElasticSearch配额类属性设置指南：属性cluster.routing.allocation.cluster_concurrent_rebalance决定了允许并发再平衡的分片数量，属性cluster.routing.allocation.disk.threshold_enabled值为true（默认值），在分配分片到一个节点时将会把可用的磁盘空间算入配额内。</li>
<li>Recovery属性允许快速重启</li>
<li>线程池属性防止数据丢失</li>
</ul>
<h2 id="基于-Kafka-和-ElasticSearch，LinkedIn是如何构建实时日志分析系统的？"><a href="#基于-Kafka-和-ElasticSearch，LinkedIn是如何构建实时日志分析系统的？" class="headerlink" title="基于 Kafka 和 ElasticSearch，LinkedIn是如何构建实时日志分析系统的？"></a>基于 Kafka 和 ElasticSearch，LinkedIn是如何构建实时日志分析系统的？</h2><p><a href="http://mp.weixin.qq.com/s/4dkaOWtEw-weLBI73A0JzQ" target="_blank" rel="external">http://mp.weixin.qq.com/s/4dkaOWtEw-weLBI73A0JzQ</a></p>
<ul>
<li>V1方案是ELK（Log通过Logstash读出来放到Elasticsearch中，然后Kibana去读）；存在Logstash Agent维护不理想和log标准化问题；</li>
<li>V2引入Kafka后，不需要每个host上都有Agent；通过Java Container Logger处理不同类型的日志；</li>
<li>V3按照业务功能拆分ELK Cluster；将Logstash和Elasticsearch分开运行；</li>
<li>V4引入Tribe解决跨数据中心Elasticsearch集群性能问题；</li>
<li>V5采用冷热分区解决数据访问速度问题；</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/9_reading_201701/linkin_log.png" alt="LinkedIn日志系统演进现状"></p>
<h2 id="究竟啥才是互联网架构“高可用”"><a href="#究竟啥才是互联网架构“高可用”" class="headerlink" title="究竟啥才是互联网架构“高可用”"></a>究竟啥才是互联网架构“高可用”</h2><p><a href="http://mp.weixin.qq.com/s/7nfSvxZ4vJAxpIN5rCdaCw" target="_blank" rel="external">http://mp.weixin.qq.com/s/7nfSvxZ4vJAxpIN5rCdaCw</a></p>
<ul>
<li>单点是系统高可用的大敌，高可用保证的原则是集群化或者叫冗余。通过自动故障转移来实现系统的高可用；</li>
<li>整个互联网分层系统架构的高可用，又是通过每一层的冗余+自动故障转移来综合实现的，具体的：<br>（1）【客户端层】到【反向代理层】的高可用，是通过反向代理层的冗余实现的，常见实践是keepalived + virtual IP自动故障转移<br>（2）【反向代理层】到【站点层】的高可用，是通过站点层的冗余实现的，常见实践是nginx与web-server之间的存活性探测与自动故障转移<br>（3）【站点层】到【服务层】的高可用，是通过服务层的冗余实现的，常见实践是通过service-connection-pool来保证自动故障转移<br>（4）【服务层】到【缓存层】的高可用，是通过缓存数据的冗余实现的，常见实践是缓存客户端双读双写，或者利用缓存集群的主从数据同步与sentinel保活与自动故障转移；更多的业务场景，对缓存没有高可用要求，可以使用缓存服务化来对调用方屏蔽底层复杂性<br>（5）【服务层】到【数据库“读”】的高可用，是通过读库的冗余实现的，常见实践是通过db-connection-pool来保证自动故障转移<br>（6）【服务层】到【数据库“写”】的高可用，是通过写库的冗余实现的，常见实践是keepalived + virtual IP自动故障转移</li>
</ul>
<h2 id="究竟啥才是互联网架构“高并发”"><a href="#究竟啥才是互联网架构“高并发”" class="headerlink" title="究竟啥才是互联网架构“高并发”"></a>究竟啥才是互联网架构“高并发”</h2><p><a href="http://mp.weixin.qq.com/s/AMPIwgParjbLUBuCxUCYmw" target="_blank" rel="external">http://mp.weixin.qq.com/s/AMPIwgParjbLUBuCxUCYmw</a></p>
<p>高并发（High Concurrency）是互联网分布式系统架构设计中必须考虑的因素之一，它通常是指，通过设计保证系统能够同时并行处理很多请求。提高系统并发能力的方式，方法论上主要有两种：垂直扩展（Scale Up）与水平扩展（Scale Out）。前者垂直扩展可以通过提升单机硬件性能，或者提升单机架构性能，来提高并发性，但单机性能总是有极限的，互联网分布式架构设计高并发终极解决方案还是后者：水平扩展。<br>互联网分层架构中，各层次水平扩展的实践又有所不同：<br>（1）反向代理层可以通过“DNS轮询”的方式来进行水平扩展；<br>（2）站点层可以通过nginx来进行水平扩展；<br>（3）服务层可以通过服务连接池来进行水平扩展；<br>（4）数据库可以按照数据范围，或者数据哈希的方式来进行水平扩展；<br>各层实施水平扩展后，能够通过增加服务器数量的方式来提升系统的性能，做到理论上的性能无限。</p>
<h2 id="自动化单元测试的落地方法，高效高质量部署并不难！"><a href="#自动化单元测试的落地方法，高效高质量部署并不难！" class="headerlink" title="自动化单元测试的落地方法，高效高质量部署并不难！"></a>自动化单元测试的落地方法，高效高质量部署并不难！</h2><p><a href="https://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2650995338&amp;idx=1&amp;sn=a9b14ea359a00b48c9bb5483e058b860" target="_blank" rel="external">https://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2650995338&amp;idx=1&amp;sn=a9b14ea359a00b48c9bb5483e058b860</a></p>
<ul>
<li>是否值得：Martin Fowler在博客中解释了TestPyramid，单元测试是整个金字塔的基石；实施单元测试，并不代表你的生产效率能提高迅猛，反而有时候阻碍了瞬间的生产效率，但是它最直接的是提升产品质量，从而提升市场的形象，间接才会提升生产效率；</li>
<li>关键部分：自动化单元测试有四个关键组成部分要做到统一，如下图所示：</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/9_reading_201701/unit_test_key.png" alt="自动化单元测试的关键部分"></p>
<ul>
<li>遵循流程：自动化单元测试的典型工作流程如下：</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/9_reading_201701/unit_test_flow.png" alt="典型工作流程"></p>
<ul>
<li>自动化单元测试原则：隔离UI操作；隔离数据库以及文件读写网络开销等操作；使用Mock替身与Spring容器隔离；设计简单的测试；定义测试套件的运行时间；</li>
<li>落地实践：Jenkins配置构建触发器推荐使用PollSCM；在Maven的pom.xml中配置sonar服务器信息；所有单元测试继承MockitoTestContext父类使Mockito相关注解生效；</li>
<li>最后，来一张图总结使用自动化单元测试前后的对比：</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/9_reading_201701/unit_test_compare.png" alt="使用前后对比"></p>
<h2 id="十分钟入门RocketMQ"><a href="#十分钟入门RocketMQ" class="headerlink" title="十分钟入门RocketMQ"></a>十分钟入门RocketMQ</h2><p><a href="http://jm.taobao.org/2017/01/12/rocketmq-quick-start-in-10-minutes/" target="_blank" rel="external">http://jm.taobao.org/2017/01/12/rocketmq-quick-start-in-10-minutes/</a></p>
<ul>
<li>消息中间件需要解决哪些问题：发布订阅、消息优先级、消息有序性、消息过滤、消息持久化、消息可靠性、低延时、消息必须投递一次、消息只能被发送和消费一次、Broker的buffer满了怎么办、回溯消费、消息堆积、分布式事务、定时消息、消息重试；</li>
<li>RocketMQ的特点：是一个队列模型的消息中间件，具有高性能、高可靠、高实时、分布式特点；Producer、Consumer、队列都可以分布式；Producer向一些队列轮流发送消息，队列集合称为Topic，Consumer如果做广播消费，则一个consumer实例消费这个Topic对应的所有队列，如果做集群消费，则多个Consumer实例平均消费这个topic对应的队列集合；能够保证严格的消息顺序；提供丰富的消息拉取模式；高效的订阅者水平扩展能力；实时的消息订阅机制；亿级消息堆积能力；较少的依赖；</li>
<li>RocketMQ物理部署结构：Name Server是一个几乎无状态节点，可集群部署，节点之间无任何信息同步；Broker部署相对复杂，Broker分为Master与Slave，一个Master可以对应多个Slave，但是一个Slave只能对应一个Master，Master与Slave的对应关系通过指定相同的BrokerName，不同的BrokerId来定义，BrokerId为0表示Master，非0表示Slave，Master也可以部署多个，每个Broker与Name Server集群中的所有节点建立长连接，定时注册Topic信息到所有Name Server；Producer与Name Server集群中的其中一个节点（随机选择）建立长连接，定期从Name Server取Topic路由信息，并向提供Topic服务的Master建立长连接，且定时向Master发送心跳，Producer完全无状态，可集群部署；Consumer与Name Server集群中的其中一个节点（随机选择）建立长连接，定期从Name Server取Topic路由信息，并向提供Topic服务的Master、Slave建立长连接，且定时向Master、Slave发送心跳，Consumer既可以从Master订阅消息，也可以从Slave订阅消息，订阅规则由Broker配置决定；</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/9_reading_201701/rocketmq_1.png" alt="RocketMQ物理部署结构"></p>
<ul>
<li>RocketMQ逻辑部署结构：如下图所示，RocketMQ的逻辑部署结构有Producer Group和Consumer Group；</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/9_reading_201701/rocketmq_2.png" alt="RocketMQ逻辑部署结构"></p>
<ul>
<li>Rocket数据存储结构：如下图所示，采取了一种数据与索引分离的存储方法，有效降低文件资源、IO资源、内存资源的消耗。</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/9_reading_201701/rocketmq_3.png" alt="RocketMQ数据存储结构"></p>
<h2 id="NetflixOSS：Hollow正式发布"><a href="#NetflixOSS：Hollow正式发布" class="headerlink" title="NetflixOSS：Hollow正式发布"></a>NetflixOSS：Hollow正式发布</h2><p><a href="http://www.infoq.com/cn/articles/netflixoss-hollow-officially-released" target="_blank" rel="external">http://www.infoq.com/cn/articles/netflixoss-hollow-officially-released</a></p>
<ul>
<li><a href="http://hollow.how/" target="_blank" rel="external">Hollow</a>是一种Java库，为中小规模的内存中数据集提供了一套全面的工具，适合从单一生成方到多个消耗方等不同场景下的数据只读访问；它会根据数据集调整自己的规模；</li>
<li>Hollow在内存中保留一份完整的、可供使用的只读数据集，借此可规避从不完整的缓存中更新和逐出数据所产生的后果；</li>
<li>Hollow不仅有助于改善性能，还可以大幅促进团队处理与数据有关的任务时的敏捷性；Hollow可根据指定的数据规模自动生成自定义API，可以极为迅速地将包含当前数据或过去时点的整个生成数据集分流到本地开发工作站，还包含大量已经开发完成可以使用的工具；</li>
<li>数据集具体变化的时间线可拆分为多个离散的数据状态，每个状态都是数据在特定时间的一个完整快照；Hollow可自动生成不同状态之间的增量，因此消耗方只需做最少量的工作即可保持自己所用数据为最新版本；Hollow会自动进行数据去重，借此将消耗方所有数据集的堆占用空间将至最低；</li>
<li>Hollow并未使用POJO作为内存中的具体呈现，而是使用了一种更紧凑的定长强类型数据编码方式；该编码方式可将数据集的堆占用空间和随时访问数据的CPU消耗降至最低；所有编码后的记录会打包为可重用的内存块（Slab），并在JVM堆的基础之上进行池化，借此避免服务器高负载时对GC行为产生影响；以下是Object类型记录在内存中布局方式的一种范例：</li>
</ul>
<p><img src="http://oi46mo3on.bkt.clouddn.com/9_reading_201701/hollow_1.png" alt="Hollow对象内存布局"></p>
<ul>
<li>Hollow技术的核心在于通过不同方式对数据创建的索引，以便可灵活访问数据中的相关记录，并构成强大的访问模式，而无须考虑数据模型最初的设计是否考虑过这种访问模式；</li>
<li>Hollow的配套工具非常易于设置和使用，历史工具可用于检测记录在一段时间以来的变化情况；</li>
</ul>
<p><strong>免责声明：相关链接版本为原作者所有，如有侵权，请告知删除。</strong></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;关键字：架构, 微服务，消息中间件，性能调优，Elasticsearch，缓存，RPC，日志分析，平台化，DistributedLog，监控，负载均衡，高性能，高并发，高可用。&lt;br&gt;
    
    </summary>
    
      <category term="Reading Record" scheme="http://ginobefunny.com/categories/Reading-Record/"/>
    
    
      <category term="读书笔记" scheme="http://ginobefunny.com/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
      <category term="Elasticsearch" scheme="http://ginobefunny.com/tags/Elasticsearch/"/>
    
      <category term="微服务" scheme="http://ginobefunny.com/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"/>
    
      <category term="DistributedLog" scheme="http://ginobefunny.com/tags/DistributedLog/"/>
    
      <category term="RPC" scheme="http://ginobefunny.com/tags/RPC/"/>
    
      <category term="性能调优" scheme="http://ginobefunny.com/tags/%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/"/>
    
      <category term="高性能" scheme="http://ginobefunny.com/tags/%E9%AB%98%E6%80%A7%E8%83%BD/"/>
    
      <category term="高并发" scheme="http://ginobefunny.com/tags/%E9%AB%98%E5%B9%B6%E5%8F%91/"/>
    
      <category term="高可用" scheme="http://ginobefunny.com/tags/%E9%AB%98%E5%8F%AF%E7%94%A8/"/>
    
      <category term="架构" scheme="http://ginobefunny.com/tags/%E6%9E%B6%E6%9E%84/"/>
    
      <category term="消息中间件" scheme="http://ginobefunny.com/tags/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
      <category term="缓存" scheme="http://ginobefunny.com/tags/%E7%BC%93%E5%AD%98/"/>
    
      <category term="日志分析" scheme="http://ginobefunny.com/tags/%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90/"/>
    
      <category term="平台化" scheme="http://ginobefunny.com/tags/%E5%B9%B3%E5%8F%B0%E5%8C%96/"/>
    
      <category term="监控" scheme="http://ginobefunny.com/tags/%E7%9B%91%E6%8E%A7/"/>
    
      <category term="负载均衡" scheme="http://ginobefunny.com/tags/%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/"/>
    
  </entry>
  
</feed>
